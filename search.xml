<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[对话系统领域研究综述（2018）]]></title>
    <url>%2F2018%2F08%2F20%2F%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E9%A2%86%E5%9F%9F%E7%A0%94%E7%A9%B6%E7%BB%BC%E8%BF%B0%EF%BC%882018%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Abstract：对话系统领域研究综述，2018. 1.相关概念术语及解释 深度学习（Deep Learning ）：机器学习的分支，是一种试图使用包含复杂结构或由多重非线性变换构成的多个处理层对数据进行高层抽象的算法。含多隐层的多层感知器就是一种深度学习结构。 人工神经网络（Neural Network）：是一种模仿生物神经网络的结构和概念的数学模型，用于对函数进行估计和近似。 对话系统：即使得人与机器可以通过自然语言进行对话交互的系统。 问答系统（Question Answering System）：信息检索系统的一种高级形式，能用准确、简洁的自然语言回答用户用自然语言提出的问题。 RNN（Recurrent Neural Network）：递归神经网络是两种人工神经网络的总称，分别是时间递归神经网络和结构递归神经网络，RNN一般指代时间递归神经网络。时间递归神经网络可以描述动态时间行为，因为和前馈神经网络（Feedforward Neural Network）接受较特定结构的输入不同，RNN将状态在自身网络中循环传递，故可接受更广泛的时间序列结构输入。RNN模型如下图： ​ Seq2Seq（Sequence to Sequence）模型：近几年比较热门的一个基于RNN的模型，主要应用在机器翻译、自动问答系统等领域，并且效果不错。seq2seq 模型利用 RNN 对时序序列天然的处理能力，试图建立一个能直接处理变长输入与变长输出的模型。 LSTM（Long Short-Term Memory）模型：一种时间递归神经网络，适合处理和预测时间序列中间隔和延迟非常长的重要事件。LSTM模型如下图： Beam Search（集束搜索）：一种启发式图搜索算法，通常用在图的解空间比较大的情况下，为了减少搜索所占用的空间和时间，在每一步深度扩展的时候，剪掉一些质量比较差的结点，保留下一些质量较高的结点。这样减少了空间消耗，并提高了时间效率，但缺点就是有可能存在潜在的最佳方案被丢弃，因此Beam Search算法是不完全的，一般用于解空间较大的系统中。 2.国际对话系统领域分析2.1 对话系统类型及方法对话系统按应用场景类型可分为两类： 1.任务导向型（task-oriented）对话系统： A. 作用：帮助用户完成实际具体的任务，如电商智能客服、酒店智能客服等。 B. 方法： （1）管道(Pipeline)方法 将对话响应视为一条管道（pipeline)，系统首先理解人类所传达的信息，将其作为一种内部状态，然后根据对话状态的策略采取一系列相应的行为，最后将动作转化为自然语言的表现形式。如下图所示： （2）端到端（end-to-end）方法 端到端神经生成模型为面向任务的对话系统构建了端到端的可训练框架。与传统的管道模型不同，端到端模型使用一个模块，并与结构化的外部数据库交互。 如下图的模型是一种基于网络的端到端可训练任务导向型对话系统，将对话系统的学习作为学习从对话历史到系统回复的映射问题，并应用encoder-decoder模型来训练。 2.非任务导向型（non-task-oriented）对话系统（又可称为聊天机器人) A. 作用：专注于在开放的领域与人交谈，提供合理的回复和娱乐消遣功能。 B. 主要有3种实现方法： （1）基于检索的方法：从事先定义好的索引中进行搜索，学习从当前对话中选择回复。关键是消息—回复匹配，匹配算法必须克服消息和回复之间的语义鸿沟。缺点在于过于依赖数据质量，如果选用的数据质量欠佳，那就很有可能前功尽弃。 （2）生成方法：比如使用端到端的Seq2Seq模型，可在对话过程中产生合适的全新的回复，因此相对更为灵活，但有一些弊端，比如有时候会出现语法错误，或者生成一些没有意义的回复。 （3）集成方法：基于检索的系统通常给出精确但是较为生硬的答案，基于生成的系统则倾向于给出流畅但却是毫无意义的回答。而将生成和检索方法结合起来能对系统性能起到显著的提升作用。在集成模型中，被抽取的候选对象和原始消息一起被输入到基于RNN的回复生成器中。这种方法结合了检索和生成模型的优点，这在性能上具备很大的优势。 2.2 非任务导向型对话系统——基于检索的方法1.单轮回复匹配 检索聊天机器人的早期研究主要集中在反应选择单轮的谈话，只有消息用于选择一个合适的回复。 目前比较新的方法如下图，利用深度卷积神经网络体系结构改进模型，学习消息和响应的表示，或直接学习两个句子的相互作用表示，然后用多层感知器来计算匹配的分数。 2.多轮回复匹配 基于检索的多轮对话，即在多轮回答选择中，将当前的消息和先前的话语作为输入。 模型选择一个自然的、与整个上下文相关的响应。重要的是要在之前的话语中找出重要的信息，并恰当地模仿话语的关系，以确保谈话的连贯性。 多轮对话的难点在于不仅要考虑当前的问题，也要考虑前几轮的对话情景。多轮对话的难点主要有两点： 1.如何明确上下文的关键信息（关键词，关键短语或关键句）。 2.在上下文中如何模拟多轮对话间的关系。 现有检索模型的缺陷：在上下文中容易丢失重要信息，因为它们首先将整个上下文表示为向量，然后将该上下文向量与响应 sentence 向量进行匹配。 下图是一个基于检索的多轮对话模型架构。此模型改进了话语关系和上下文信息的利用，通过将上下文中的语句与卷积神经网络的不同层级进行匹配，然后通过一个递归的神经网络在时间序列中堆积这些向量，以建立对话之间的关系。 2.3 非任务导向型对话系统——神经生成模型Neural Generative Models）1.Sequence-to-Sequence Model 给定包含个词语的输入序列（message）和长度为T的目标序列（response），模型最大化Y在X下的条件概率： p(y_1,...,y_{T'}|x_1,...,x_T)Seq2Seq模型的encoder-decoder结构： 编码器将X逐字读入，并通过递归神经网络（RNN）将其表示为上下文向量c, 然后解码器将 c 作为输入估计Y 的生成概率。 （1）Encoder Encoder过程很简单，一般直接使用RNN（一般用LSTM）进行语义向量生成： h_t=f(x_t,h_{t-1})，c=\phi(h_1,...,h_T)其中f 是非线性函数，例如LSTM,GRU是上一隐节点输出，是当前时刻的输入。向量c 通常为RNN中的最后一个隐节点(h, Hidden state)，或者是多个隐节点的加权和。 （2）Decoder 模型的decoder过程是使用另一个RNN通过当前隐状态来预测当前的输出符号，这里的和都与其前一个隐状态和输出有关，Seq2Seq的目标函数定义为： p((y_1,...,y_{T'}|x_1,...,x_T)=p(y_1|c)\prod^{T'}_{t=2}p(y_t|c,y_1,...,y_{t-1}))2.对话上下文（Dialogue Context） 考虑对话的上下文信息的是构建对话系统的关键所在，它可以使对话保持连贯和增进用户体验。使用层次化的RNN模型，捕捉个体语句的意义，然后将其整合为完整的对话。同时，分别用字级别和句子级别的注意力方法扩展层次化的结构。 通常，层次化 RNNs的表现通常优于非层次化的RNNs；在考虑上下文相关的信息后，神经网络趋向于产生更长的、更有意义和多样性的回复；引入Attention机制，能够让模型能够自动的学到词语与句子级别的重要度信息，从而更好的生成新一轮的对话。 3.回复多样性（Response Diverisity） 在当前的Seq2Seq对话系统中，一个具有挑战性的问题是，它们倾向于产生无关紧要的或不明确的、普通的、几乎没有意义的回复，比如“I don’t know”，“I am OK”。解决这类问题的一个很有效的方法是找到并设置一个更好的目标函数，或者增加模型的复杂度。 4.主题和个性化（Topic and Personality） 明确对话的内在属性是提高对话多样性和保证一致性的另一种方法。 5.外部知识库（Outside Knowledge Base） 人类对话与对话系统之间的一个重要区别是它是否与现实相结合。结合外部知识库(KB)是一种很有前途的方法，可以弥补背景知识之间的差距，即对话系统和人之间的差距。记忆网络（Memory Network）是一种以知识库处理问题的经典方法。因此，它非常直接的别用于在对话生成中。 6.对话系统性能评价 评价生成回复的质量是对话系统的一个重要方面。任务导向型的对话系统可以基于人工生成的监督信号进行评估，例如任务完成测试或用户满意度评分等。然而，由于高回复的多样性，自动评估非任务导向的对话系统所产生的响应的质量仍然是一个悬而未决的问题。目前的方法有以下几种： （1）计算 BLEU 值，即直接计算 word overlap、ground truth和你生成的回复。由于一句话可能存在多种回复，因此从某些方面来看，BLEU 可能不太适用于对话评测。 （2）计算 embedding的距离，这类方法分三种情况：直接相加求平均、先取绝对值再求平均和贪婪匹配。 （3）衡量多样性，主要取决于 distinct-ngram 的数量和 entropy 值的大小。 （4）进行图灵测试，用 retrieval 的 discriminator 来评价回复生成。 3. 国际 Seq2Seq 框架学术进展分析1.Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation （1）论文信息： 发表时间：2014年6月 作者：Cho、Bahdanau、Bengio 引用量：2250（目前为止） （2）论文特色：该论文提出一种RNN Encoder-Decoder的网络结构；可以看作是Seq-to-Seq的前身，因为它并未实现end-to-end训练，而是作为SMT翻译框架中的一部分进行训练。 图：RNN Encoder-Decoder网络结构 由上图可看出，这个 RNN Encoder-Decoder 网络结构的逻辑流程是： 1.先使用一个RNN模型将输入序列进行编码得到最终的隐藏层状态向量$C$作为输入序列的向量表示； 2.再使用另一个RNN模型对向量$C$进行解码，解码过程中每一步的输入是上一步的输出$y{t-1}$、上一时刻隐层状态$h{t-1}$和向量$C$，公式如下： h_{(t)}=f(h_{(t-1)},y_{t-1},c) 3.将所有输出$y_t$的概率相乘即是该序列的概率； 4.计算loss、反向传播即可完成对模型的训练。 （3）论文应用：该论文的应用场景是翻译系统，即将源语言翻译成另一目标语言，如“我爱你”—&gt;“I Love You”，这恰好是Seq-to-Seq的使用场景。在一开始的时候Seq-to-Seq也是用于翻译模型才慢慢被大家所重视，后来被应用到对话等领域中。 2.Sequence to Sequence Learning with Neural Networks （1）论文信息： 发表时间：2014年9月 作者：Ilya Sutskever 引用量：3293（目前为止） （2）论文特色：第一篇提出 Seq2Seq模型 的论文，实现了端到端的训练，使得模型更加简单且不需要特定领域知识和人工提取特征；同样适用于翻译模型，摈弃SMT框架，极大促进了翻译模型的发展。 图：Seq2Seq模型 此模型里$Deocoder$部分的输入是前一时刻的目标值，而不是上一时刻的输出和向量$C$，优点是正确的输入能够指导模型快速收敛并产生正确的输出。当模型输出\时，表示翻译结束。模型的输入包括3个：encoder_input:”ABC\“，”encoder_input: “WXYZ”，decoder_label: “WXYZ\“。并使用目标函数进行训练，目标函数公式如下： p(y_1,...,y_{T'}|x_1,...,x_T)=\prod^{T'}_{t=1}p(y_t|v,y_1,...,y_{t-1}) 深层LSTM模型：该论文使用4层LSTM神经网络作为encoder和decoder模型，且实验结果表明深层模型比浅层模型的效果更好； 反序输入source：输入时将“ABC”变成“CBA”，如此可解决长序列的long-term依赖，使模型可学习到更多对应关系，从而得到较好的效果； Beam Search：这是一种测试技巧，在training过程中不会使用。通常会采用greedy贪婪式的序列生成方法，即每一步都取最大概率的元素作为当前output，但此法弊端在于“一步错步步错”（一旦某一步输出选错，可能会导致最终结果出错），故使用beam search的方法可改善这种弊端，即每一步都取概率最大的k个序列（beam size）并作为下一次的input。 图：深层模型示例 （3）论文应用：主要应用于翻译领域，目前Google翻译系统就是采用这种结构。 3.Neural Machine Translation by Jointly Learning to Align and Translate （1）论文信息： 发表时间：2014年9月 作者：Dzmitry Bahdanau 引用量：2450（目前为止） （2）论文特色：提出Seq2Seq模型，最大亮点是融入了Attention机制，可很好地解决long-term长序列依赖问题，同样应用于翻译系统 ，可较大提高模型的准确度。 对于长序列而言，传统Seq2Seq模型是将source序列通过一个RNN模型encode为一个固定维度的向量，但这是远远不够解决long-term依赖问题的，因为一个向量无法编码该序列包含的所有信息。故该论文提出将encoder RNN的每个隐藏状态都保存在一个list中，在每次decode时都取计算st-1与所有隐藏向量之间的相关程度，并对其进行家全球和得到ci向量，即ci每次解码使都是不一样的，它会自动寻找最相关的hi向量，计算公式如下图： 图：Seq2Seq模型架构 4.本项目与上述3个论文的对比分析： 上述三篇论文是学界Seq2Seq模型的开山之作，而本项目的最大特色是“举一反三”，即将学界前沿的 RNN Seq2Seq 模型从机器翻译领域推广应用到问答系统领域，用Seq2Seq进行对话生成任务。 4. 国内外对话系统行业产品分析对话相关技术的逐步成熟引发了工业界研发对话产品的热潮。苹果公司率先发布了智能助理软件 Siri，用户可以通过自然语言完成搜索、信息查询、日历和闹铃设置等任务。百度于2015年推出的对话式智能秘书度秘，以索引真实世界的服务和信息为基础，为用户提供信息的同事也提供诸如叫车、订外卖、订票等服务，并满足用户的聊天需求。其他类似的系统还有 Google Assistant、微软 Cortana等。 同时聊天类对话系统也得到快速发展。微软于2014年发布的智能伴侣机器人小冰，可以借助情感计算基数和用户进行情感交流。类似的对话系统还有 SimSimi（小黄鸡）和小影机器人等。 随着智能硬件技术的兴趣，以 Amazon Alexa为代表的对话系统与硬件相结合的智能设备也日益增多。此外，问答类对话机器人也在近年得到飞速发展，IBM 的深度问答系统——Watson在电视节目中打败了人类选手，其技术潜力正在逐步转化为应用，推动行业转型。 以下将分别分析微软小冰、百度度秘、阿里小蜜三款不同的对话系统。 4.1 微软小冰对话系统：数据驱动的对话系统传统对话系统基于规则，而随着处理大数据的深度学习平台和方法的成熟，对话系统构建的焦点转移到了数据驱动的方法，其中检索模型和生成模型是两种主流方法。 1.单轮回复生成模型 此模型借鉴了机器翻译中的编码-解码模型，在此基础上将对话的话题信息引入到回复生成中。其工作原理是：首先利用一个 RNN（循环神经网络）将输入信息编码成向量，同时利用一个话题模型估计出回复可能用到的话题关键词。解码器是一个以编码向量和话题关键词为条件的通过 RNN 实现的语言模型。在生成回复中每一个词时，解码器都通过注意力机制对编码器中的每个向量和话题关键词进行加权处理。 2.生成式多轮对话模型 此模型通过两层 RNN 编码器来刻画词与词、句与句的关系，并在两层网络之间 搭载了两层的注意力模型，一个突出重要词对生成的影响，一个关注句子的重要性。通过量程编码器和两层注意力模型，一段对话上下文最终被表示为一个向量，用来作为生成回复中当前词的条件。 3.模型应用模型应用在微软的对话系统架构中，比如微软小冰（微软开发的聊天机器人），它的任务是给用户陪聊，具体衡量指标是多轮对话的平均轮数，小冰现在可完成平均23轮对话，最长对话超过7000轮。 4.2 百度开放域对话系统：基于多视角的回复选取模型1.模型介绍百度提出一种基于多视角的回复选取模型，多视角包括词序列视角（Word-level）与话语序列视角（utterance-level），如下图所示： 该方案基于以下假设：每个视角从特定角度表达了\与候选回复之间的语义关系，并且基于不同视角构建的模型相互之间具有互补性，且基于两个具有互补性的模型的打分可得到更好的回复排序结果。 使用 GRU（Gated Recurrent Unit Neural Network）为 Query 与候选回复构建次预计分布式向量表示，然后才有CNN（卷积模型）和 GRU 结合的方式构建话语级向量表示，以捕捉话语级别的语义和篇章信息。通过同时最小化以下两个因子来优化模型和参数：1.每个视角下打分模型的预测错误；2.两个视角下打分模型预测结果的不一致性。在训练过程中，两个视角下模型或向量表示网络的互补信息通过共享的词向量表进行互换。 2.模型应用该模型应用在百度 NLP 对话系统的聊天服务中，完成与用户的开放域聊天任务。 4.3 阿里小蜜：基于检索模型和深度学习模型相结合的智能对话系统通常chatbot包括2个部分：IR 模块和生成模块。针对用户的问题，IR 模块从 QA 知识库中检索到对应的答案，生成模块再用预训练好的 Seq2Seq 模型生成最终的答案。 但是已有的系统面临的问题是，对于一些长问句或复杂问句往往无法在 QA 知识库中检索到匹配的条目，并且生成模块也经常生成不匹配或无意义的答案。 故阿里小蜜的方法是将 IR 和生成模块聚合在一起，用一个 Seq2Seq 模型来对搜索结果做评估，从而达到优化的效果。 1.模型介绍整个模型方案如下图： （1）技术原理：阿里小蜜首先采用检索模型从 QA 知识库中找出候选答案集合，然后利用带注意力的 Seq2Seq 模型对候选答案进行排序，如果第一候选的得分超过某个阈值，则作为最终答案输出，否则利用生成模型生成答案。 （2）QA知识库 （3）IR 模块：利用倒排索引的方法将每个单词隐射到包含这个单词的一组问句中，并且对这些单词的同义词也做了索引，然后利用 BM25 算法来计算搜索到的问句和输入问句的相似度，取最相似问句的答案。 （4）生成模型：生成模型是一个 attentive seq2seq 的结构，如下图： 采用了一个 GRU（Gated Recurrent Unit Neural Network），由 question 生成 answer，计算生成单词的概率： p(y_i=w_i|\theta_i)=p(y_i=w_i|y_1,y_2,...,y_{i-1},c_i)=f(y_{i-1},s_{i-1},c_i)其中加了 context 向量，它是由图中的 α 求得的，α 表示的是当前步的输入单词，和上一步的生成单词之间的匹配度，用了一个 alignment 模型计算。 对于各个 QA 长度不等的情况，采用了 bucketing 和 padding 机制。另外用了 softmax 来随机采样词汇表中的单词，而不使用整个词汇表，从而加速了训练过程。还是用了 beam search decoder，每次维护 top-k 个输出，来取代一次一个输出的贪心搜索。 （5）Rerank模块 使用的模型和上面的一样，根据输入问题来为候选答案打分，使用平均概率作为评分函数： Mean-prob=\frac{1}{n}\sum^n_{i-1}p(y_i=w_i|\theta_i)]]></content>
      <categories>
        <category>Artificial Intelligence</category>
        <category>Natural Language Processing</category>
      </categories>
      <tags>
        <tag>Artificial Intelligence</tag>
        <tag>Natural Language Processing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《顾客为什么购买》阅读笔记]]></title>
    <url>%2F2018%2F08%2F20%2F%E3%80%8A%E9%A1%BE%E5%AE%A2%E4%B8%BA%E4%BB%80%E4%B9%88%E8%B4%AD%E4%B9%B0%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Abstract：尽最大可能在零售环境中做到对顾客友好。零售环境必须遵循人们的生理特点；必须适应不同性别和年龄的购物者的行为差异。微小的改变可能带来很大的影响。购物随社会变化而变化，商人若不能紧随其变则很可能遭受失败。 Main Points: 到底是什么引发人们的购买欲？ 顾客又是如何改变商店的？ 为什么网上购物不会取代大型购物中心？ Part1 谈谈购物学1. 一门科学的诞生零售环境：包括零售场景里的所有元素 购物学： 研究人们在商店里的行为；每个人都在做什么，去哪儿，不去哪儿，去时要经过哪些地方，看见什么，没看见什么，阅读了什么，没阅读什么，怎样处理遇到的东西，怎样购物 … 核心：通过研究、比较、分析以使商店和商品更加适应购物者的需求 推搡效应：人们会被过于拥挤的情况吓跑 把商品放在目标买主够得着的地方 比如狗粮是低频次消费产品很容易被放在货架的顶层 ，使得实际作为它真正买主的小孩或老人很难拿到 方便：物品的摆放应该让购买者感到方便，而不是让设计者感到方便 2. 零售商所不知道的转化率 吸引顾客到店：营销，广告，促销，地理位置 促使顾客转化为购买者：商品，雇员，商店本身的工作 购买者与非购买者 验证假设：是否实施购买行为与购物者在商店话费的时间相关 购买者花的时间是非购买者的平均3~4倍 在考虑怎样增加购物时间之前，首先应该知道顾客在你的商店购物花费的时间是多少 拦截率 即：与商店里任何雇员有过接触的顾客的比率 购物者与雇员联系的越多，平均销量越大 评估“等待时间” 等待时间：结账等待时间 Part2 像埃及人一样行走：购物机理引言购物学背后第一个也是最简单的原理是：适应性 即人类在生理和解剖学上具有一些共同的能力、倾向、局限和需要，零售环境必须适应人们的这些特点。 关注适应性，就能满足顾客的需要，同时保障利润 3.缓冲地带过渡区 不要在过渡区安排重要商品或活动；尽量缩小过渡区 可以提供购物篮、地图、优惠券；可以向顾客打招呼 4.你需要空出手来上帝给你两只手，你却只能用一只手来购物，岂不太遗憾了。 只要有顾客拿着3件及以上东西，就递给他一个购物篮。 空手购物：由店员或智能化机器帮助用户进行货物拣取、包装、外送 让用户100%无负担购物，零负担来，零负担购，零负担离开 5.怎样看广告牌广告：吸引用户注意力——&gt;条理清晰且有逻辑性地展示信息（开头，中间，结尾） 若一开始没抓住用户注意力，则之后什么都不会发生 若表达的内容太多太快，超过观众的最大承载能力，或把观众弄糊涂了，则他们不会理睬这些东西 考虑哪些问题 什么样的广告放在什么样的地方 广告上写几个字 摆放广告牌的逻辑 6.像常人一样行走好商店是能够将最多的商品在最多的购物者面前展示最长时间的商店。 人们看见反光表面会减速，看见银行会加速。 所以绝对不要在银行旁边开店。 人们总是向前看，向前走 把广告向侧边倾斜，以使顾客更容易看到 把商品展示给向前走的人 利用好最佳视线：我们能看见什么东西很大程度上取决于我们的行走方式和视线方向 吸引率 购物者最容易看见比眼睛高一点一直往下大约到膝盖范围内的东西 对于非关键视线范围以上或以下的位置：以下位置可摆放大件物品（大而清楚、色彩鲜明的东西更容易被看到） 把购物者留在在通道里 回转率：顾客从一个通道选了一些商品后不再往前走，而是调转折回的次数 想办法把顾客留在通道里，比如在地板画上跳格子的游戏 7.动态变化只有购物者才最终决定零售环境和所售商品的用法。 Part3 购物的人口统计学8.像男人那样购物男人购物特点 更容易被孩子的恳求说动 更容易受展品的影响 喜欢挥霍，享受付款的乐趣——卖给女人，靠近男人 不喜欢麻烦别人，不喜欢询问 寄存丈夫：想些办法吸引男士，被动制止 发动男人：设法使男人也参与购物 9.女人想要什么挑剔 在意环境 男人喜欢技术本身，女人接受新技术并加以应用（喜欢技术的实用性） 10.老人小号字体将老年顾客拒之门外 慎用黄色，增加反差 为老人而设计 视觉：大小，颜色，位置 11.孩子们假如一家商店不太欢迎孩子，父母会感受到这种信息而离开 比如过道容不下一个婴儿车 只要考虑到了孩子们的需要，孩子就可能成为热情的消费者 假如父母需要保持注意力谈话，那么必须有一种方法来转移好动的孩子的注意力 为孩子设计活动区域 必须是开放透明的区域 必须足够安全，足够大 最好隔离不同年龄的孩子 Part4 购物动力学：看，闻，触，买顾客喜欢什么： 触摸：感知物质世界 镜子 发现 交谈 熟识 讨价还价 顾客不喜欢： 镜子太多 排队 问愚蠢的 问题 商品缺货 价签不明 胁迫性服务 12.感官与购物感官是决定购买的最有诱惑力的因素。试用与触摸 试衣间是非常重要的地方，应该静心布置。 13.三大要素商店=设计（店面）+销售规划（在商店里卖什么）+运营（店员的行为） 一旦改变了一个因素，其他因素也都会随之改变 14.实际时间和感觉时间好时段和差时段 好时段：顾客在购买东西的时间 差时段：顾客被迫等候的时间 改善等待时间 一个队伍经理：与顾客沟通 告诉顾客等待时间是有限可控的 分散注意力：播放所有顾客喜欢看的片子 同伴 15.收款收银台的位置值得考究 在门口：方便顾客，节省门卫人力；造成店内拥挤的认知，拥堵入口 16.推销商品推销：商品连环套，即利用商品毗邻增加附加销售，比如把鼠标垫放在电脑旁边，把裤腰带放在裤子旁边 产品以合理的逻辑次序出现 17.结尾购物随社会变化而变化，商人若不能紧随其变则很可能遭受失败。 必须关注女人对生活的期望和她们的需要。 零售环境必须适应顾客。 微小的改变可能带来很大的影响。]]></content>
      <categories>
        <category>Product Manager</category>
        <category>Internet &amp; Product</category>
      </categories>
      <tags>
        <tag>Internet &amp; Product</tag>
        <tag>Reading Notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于深度学习的计算机视觉研究进展综述（2018）]]></title>
    <url>%2F2018%2F08%2F20%2F%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%A0%94%E7%A9%B6%E8%BF%9B%E5%B1%95%E7%BB%BC%E8%BF%B0%EF%BC%882018%EF%BC%89%2F</url>
    <content type="text"><![CDATA[摘要：近年来，基于深度学习的计算机视觉领域研究取得了巨大的进步。本文结合已现有文献资料，对深度学习在计算机视觉中的应用进行综述。首先，简要概述计算机视觉及其发展历史。第二，简要概述深度学习及其发展历史，再着重梳理基于深度学习的计算机视觉研究进展，本文将这一过程划分为三个阶段，分别为“RBM/AE阶段”、“CNN 阶段”和“RNN 阶段”，以及介绍了视觉领域的十大重要深度学习架构。第三，介绍基于深度学习的计算机视觉的细分领域研究进展，以目标检测和人脸识别为例。最后，讨论计算机视觉领域利用深度学习可能带来的未来研究方向。 关键词：深度学习；计算机视觉；卷积神经网络；循环神经网络； 1 计算机视觉1.1 概述计算机视觉（Computer Vision，CV）是一门研究如何利用计算机模拟人类视觉的科学，其主要任务是通过对采集到的图像或视频进行分析和理解，从而做出判断或决策。计算机视觉可以看作是研究如何使人工系统从图像或多维数据中“感知”的科学，其本质是在人工系统中实现人类的感知与观察。 人类了解世界的信息中 70%以上 来自视觉，同理计算机视觉成为机器认知世界的基础，终极目标是使计算机能够像人一样“看懂世界”。目前计算机视觉主要应用在人脸识别、图像识别方面（包括静态、动态两类信息）。 1.2 发展历程在过去几十年间，计算机视觉取得了巨大的进步，尤其是21世纪后深度学习对计算机视觉的发展起到重要作用。本文将计算机视觉的发展历程分为两个时期。 萌生期： 20世纪50年代，计算机视觉被归入模式识别——主要集中在二维图像分析和识别上。 1966年，人工智能学者 Marvin 令学生写出程序，让计算机自动“了解”所连接摄像头的内容，计算机视觉序幕被拉开。 20世纪60年代 MIT 的 Roberts 通过计算机程序从数字图像中提取诸如立方体、棱柱体等多面三位机构，并对物体形状及空间关系进行描述。 高速发展期： 20世纪80年代中期，计算机视觉蓬勃发展，新概念、新方法、新理论不断涌现，如专家推理系统。 1999年，Nivida 公司在推销自己的 Geforce256芯片时，提出了GPU 这个概念。GPU 是专为执行复杂的数学和集合计算而设计的数据处理芯片，它的出现让并行计算成为可能，对数据处理规模、数据运算速度带来了指数级的增长与改善，极大地促进计算机视觉的发展。 21世纪后，数据量的上涨（如图1）、运算力的提升和深度学习算法的出现促进了计算机视觉的发展，计算机视觉理论逐步成熟。 图1：2009-2000年全球总体数据量 1.3 主要任务计算机视觉任务的主要任务可分为四类： 1.物体识别/分类（Classification）：给出一张原始图像，识别出该图像中的物体属于哪个类别； 2.定位（Localization）：确定该物体在图像中的位置； 3.物体检测（Object Detection）：检测和定位图像中包含的物体或目标； 4.图像分割（Instance Segmentation）：目标是将每个像素映射到正确的分类。 图2：计算机视觉的主要任务 1.4 通用视觉识别技术流程通用视觉识别技术流程可分为三类： 1.目标检测：解决“去背景”的问题，即祛除背景中的不相关信息从而找出感兴趣的目标。 1）图像预处理：图像去噪、平滑、标准化配准、缺失值/异常值处理 … 2）图像分割：灰度分割、专家经验分割、统计分布分割。 2.目标识别：解决“是什么”的问题。 1）特征提取：特征选择（纹理，灰度，形状，结构等特征）； 2）判断匹配：分类、聚类。 3.行为识别：解决“干什么”的问题。 1）模型建立； 2）行为识别。 2 深度学习2.1 概述通常认为，深度学习是指具有多层非线性转换函数的网络结构。而深度神经网络（Deep Neural Network，DNN）是一种特殊的深度学习模型，也是目前大多数深度学习方法的主要实现手段，它通过学习深层的非线性神经网络结构，实现任意复杂函数的逼近。为了从大规模数据中建立有效模型，DNN自动学习多层信息表示，高层的特征利用低层的特征进行构建，并且以一种级联的方式逐层构成深度结构。因此，DNN能够自动学习从底层特征到高层特征的组合，从而有效减少对人工提取特征的依赖。 2.2 发展历史深度学习的历史最早可以追溯到20世纪40年代。概括来讲，深度学习经历了3次发展浪潮：20世纪40年代到60年代，称为控制论主义；20世纪80年代到90年代，称为连接主义；2006年之后，称之为深度学习第1次发展：代表性的算法为人工神经网络，由于其借鉴了生物大脑的方式，因此称之为神经网络。感知机模型是第一个被提出的神经网络模型，用于区分两种类别。原始的感知机的连接权重需要手工设定，后来发展为可以自动学习。同时期的模型，还包括自适应线性单元（adaptive linear neuron， ADALINE）用于预测实数值。该时期的学习算法深刻地影响了现在的机器学习领域。比如 ADALINE 算法采用的权重修正训练方法实际上就是现在普遍采用的随机梯度下降法的一个特例。然而，由于感知机模型是一种线性模型，当它被发现不能解决著名的异或（XOR）函数问题时，神经网络陷入了第1次衰落期。 第2次发展：神经网络的第2次发展，是随着连接主义的浪潮和分布式处理技术的流行而发展起来的。连接主义是基于简单的计算单元，通过网络连接的形式实现复杂的功能，其等价于在神经元中，引入隐藏层单元。该时期提出的很多概念至今依然有着重要的作用。比如，分布式表示的思想，其通过组合不同的特征表示进行学习并利用反向传播算法（backpropagation，BP）训练神经网络。第2次深度学习发展浪潮持续到了20世纪90年代中期。但是由于当时对神经网络抱有过大的预期，所以当实际效果没有达到预期的时候，引起了人们的质疑。与此同时，其他浅层的机器学习算法，包括核机器学习和图模型等快速发展，在许多计算机视觉任务中都表现出良好的性能。上述两个原因导致了这一阶段神经网络的衰落。但是在这个时期，神经网络依然在一些领域取得了不错的成绩，比如卷积神经网络（Convolutional Neural Network，CNN）被提出并用于手写体识别等问题中 第3次发展：2006年， Hinton等提出了逐层贪婪预训练的方法来高效训练神经网络，有效地解决了直以来多层深度网络难以训练的问题。在这一阶段，人们开始使用深度学习来表示多层神经网络.这一阶段发展初期，人们集中研究无监督深度模型。而今天深度学习的研究者更多研究基于海量的数据并利用有监督的深度学习方法进一步提高机器学习的性能目前，常见的深度学习模型包括:基于限制玻尔兹曼机的深度模型，基于自编码器（autoencoder，AE）的深度模型，基于卷积的深度模型以及基于递归的深度模型等。 3 基于深度学习的计算机视觉研究进展深度学习近几年成为国际上非常流行的重要的数据分析工具，在计算机视觉领域得到了广泛应用。 对于传统的视觉信息处理而言，一般首先要做特征提取然后利用特征进行模型学习，比如分类等。在这个过程中，涉及到模式识别研究中两个经典的问题，即特征的提取与表示和模型的学习。 传统算法通常利用经验知识来手工设置视觉特征，缺少与环境的信息交互以及知识库的决策支持。举个例子，给定一幅图像，我们希望知道这幅图像的目标类别（比如斑马）。按照传统的视觉模式分析流程，首先要提取特征，然后再和用SVM等进行模式分类。 而对于深度学习而言，它可以解决端到端的模式识别问题，即给定一幅图像，经过黑匣子式的学习，直接给出最终识别结果。在端到端模式识别过程中，不再区分特征提取和模式分类，而是把特征提取和分类模型学习体化；即通过深度神经网络来非线性模拟从直接图像像素级别到语义标签，实现了从数据直接到概念要素的变革性思路。 深度神经网络是采用脑启发机制设计的网络模型，它模拟了人脑层级化的信息处理机制。深度模型学习是大数据时代下计算机视觉的一个重要突破，尤其2012年后，深度学习推动了计算机视觉众多应用的飞速发展。 按照模型划分，深度学习在视觉领域经历了三个阶段。从2006年开始的RBM/AE第一阶段。第二个阶段是CNN，从2012年 Imagenet竞赛以后CNN引起了研究者的广泛关注，并且在视觉领域得到了广泛应用，在这个阶段更多是处理静态图像的建模问题。第三个阶段是从2014年开始RNN进入了大家的视野，它的出现能够更好地解决现实生活中很多时序数据的处理数问题。 3.1 第一阶段：RBM / AE 阶段（2006 —）3.1.1 AE 概述自编码器（AutoEncoder，AE）是神经网络的一种应用，一种数据压缩算法，其中数据的压缩和解压缩函数是数据相关的、有损的、从样本中自动学习的，目的是将大量的数据压缩、分配至较小维度的向量之中。 AE 通常包括输入层、输出层和隐藏层。在前向传递方面，有两个步骤：编码和解码。用于编码隐藏层中的特征的相同权重会被用于重建输出层中的图像。训练使用损失度量网络尝试重建输入时丢失的信息量。 3.1.2 RBM 概述受限玻尔兹曼机（Restricted Boltzmann Machine，RBM）是一种可通过输入数据集学习概率分布的随机生成神经网络，本质是一种无监督机器学习（Unsumervised Machine Learning）模型，用于对 input 数据进行重构，即有效地提取数据特征，构建新的数据结构进行预测分析。 受限玻兹曼机是玻兹曼机的一种变体，限定模型必须为二分图。模型中包含对应输入参数的输入可见单元和对应训练结果的隐单元，图中的每条边必须连接一个可见单元和一个隐单元。这一限定使得相比一般玻兹曼机更高效的训练算法成为可能，特别是基于梯度的对比分歧（contrastive divergence）算法。 受限玻兹曼机在降维、分类、协同过滤、特征学习和主题建模中得到了应用。根据任务的不同，受限玻兹曼机可以使用监督学习或无监督学习的方法进行训练。 RBM 和 AE 一样，可以不断堆叠实现深层神经网络挖掘数据的特征。深层信念网络（DBN）由多个 RBM 堆叠而成，并可使用梯度下降法和反向传播算法进行调优。 图3：包含3个可见单元和4个隐单元的 RBM 示意图 3.1.3 基于RBM / AE 的计算机视觉RBM 和 AE 都是深度神经网络里的经典模型。2006年 Science的文章重新让深度学习引起了大家的注意。这里的“深度”主要是指模型层数的增加。传统的感知机一般就是3层的神经网络，现在很多网络可以增加5层、8层、19层，甚至达到了152层的深度。2006年后，各种RBM或者AE的变体出现。 对 “RBM / AE” 阶段而言，模型特点更多是生成式的模型，使用的数据基本上是中等规模的数据库，采用相对较深层次的网络。而热点问题就是替代传统的手工设计特征，来自动进行数据的表示学习（representation learning）。深度学习或者说深度神经网络最大的功能就是具有分层地学习目标特征表示的能力，层级越高，学习到的特征越接近于目标的语义信息，这与大脑处理信息的流程是一致的，大脑的视觉信息处理是从视网膜到V1区、V2区，再到V4区。比如输入衣服人脸图像，首先是像素级别特征，经过一定层数以后可以学习到边界级别的特征，再学习就可以看到眼睛、鼻子等Part特征，更高层就更接近语义信息（比如类似人脸的特征表示）。也就说，深度神经网终的最大优点是能够层级化的分层学习特征，渐渐逼近于语义信息表示。深度学习在 representation learning方面具有较强的能力。 3.2 第二阶段：CNN 阶段（2012 —）3.2.1 CNN 概述卷积神经网络（Convolutional Neural Network，CNN）是一种前馈神经网络，它的人工神经元可以响应一部分覆盖范围内的周围单元，对于大型图像处理有出色表现。 卷积神经网络由一个或多个卷积层和顶端的全连通层组成，同时也包括关联权重和池化层（pooling layer）。这一结构使得卷积神经网络能够利用输入数据的二维结构。与其他深度学习结构相比，卷积神经网络在图像和语音识别方面能够给出更好的结果。这一模型也可以使用反向传播算法进行训练。相比较其他深度、前馈神经网络，卷积神经网络需要考量的参数更少，使之成为一种颇具吸引力的深度学习结构。下图是一种经典的 CNN 结构——LeNet-5网络。 图4：经典 CNN 结构——LeNet-5网络 3.2.2 基于 CNN 的计算机视觉“CNN 阶段”源于2012年的 ImageNet竞赛。当时 Hinton团队组织参加了 ImageNet竞赛，使用的模型就是基于GPU的CNN网络。在此之前，传统方法在该图像分类竞赛中最高的识别率是2011年的74%；CNN的使用将分类准确度提升了11个点。在此后几年里，所有参加 Imagenet竞赛的团队，基本上都是使用CNN模型，准确度在逐年提升，比如2013年的89%、20144年的92%、2015年的95%等。所以说2012年是CNN进入视觉计算领域的重要转折点。 自2012年 Imagenet竞赛之后，CNN受到了大量关注，其强大的学习能力也在不同视觉应用中得到证明。比如 Deepface人脸识别、 Deeppose姿势估计、 Deep Ranking 图像检索、 Deep video 视频分类、 Deepedge 边界检测、 Deep Segmentation 图像分割等。对“CNN阶段”而言，模型特点可以简单地归纳为判别式的模型、深层网络、并行分布式计算。这个阶段的热点问题更多侧重于处理静态图像相关的各种任务，在很多领域刷新了当前最好的性能。 CNN 本质上是层次抽象的滤波型局部特征，主要作用在特征学习环节，是一种数据驱动的权值学习，最有利于目标函数达成。理想的深度卷积神经网络是从数据中学习多层特征，如对图像中人脸的学习，可从底层的像素特征学习到第一层的边的特征，然后再到第2层基本脸部部位器官（如鼻子，嘴）等特征，再到高维的特征脸特征。图5展示 CNN 从图像中学习高维特征的例子。 图5：CNN 从图像中学习到有用的高维特征 3.3 第三阶段：RNN 阶段（2014 —）3.3.1 RNN 概述递归神经网络（Recurrent Neural Networks，RNN）是两种人工神经网络的总称：时间递归神经网络（recurrent neural network）和结构递归神经网络（recursive neural network）。时间递归神经网络的神经元间连接构成有向图，而结构递归神经网络利用相似的神经网络结构递归构造更为复杂的深度网络。 RNN 一般指代时间递归神经网络。单纯递归神经网络因为无法处理随着递归，权重指数级爆炸或消失的问题（Vanishing gradient problem），难以捕捉长期时间关联；而结合不同的LSTM可以很好解决这个问题。时间递归神经网络可以描述动态时间行为，因为和前馈神经网络（feedforward neural network）接受较特定结构的输入不同，RNN将状态在自身网络中循环传递，因此可以接受更广泛的时间序列结构输入。图6是一个典型 RNN 网络。 图6：典型 RNN 网络 3.3.2 双向 RNN 概述单向RNN的问题在于时刻进行分类的时候只能利用时刻之前的信息， 但是在时刻进行分类的时候可能也需要利用未来时刻的信息。双向RNN（bi-directional RNN）模型正是为了解决这个问题， 双向RNN在任意时刻都保持两个隐藏层，一个隐藏层用于从左往右的信息传播， 另一个隐藏层用于从右往左的信息传播。 图7：双向 RNN 3.3.3 LSTM 概述长短时记忆性神经网络（Long Short-Term Memory networks，LSTM）是时间递归神经网络中的一种，每个神经元是一个“记忆细胞”，细胞里面有一个“输入门”（input gate）， 一个“遗忘门”（forget gate）， 一个“输出门”（output gate）。在“输入门”中，根据当前的数据流来控制接受细胞记忆的影响；接着，在 “遗忘门”里，更新这个细胞的记忆和数据流；然后在“输出门”里产生输出更新后的记忆和数据流。由于具有时间记忆性，故 LSTM 适合于处理和预测时间序列中间隔和延迟非常长的重要事件。 图8：LSTM 网络结构 3.3.4 基于 RNN 的计算机视觉前面两个阶段没有考虑如何对序列数据进行建模，特别是对序列数据中的时间相依关系进行建模。而我们周围很多数据都是时间变化数据，比如视频数据、天气数据等。因此，迫切需要能够处理时序数据建模的一些模型。RNN可以对时序数据进行建模，时刻的隐含层不仅接受时刻的输入，同时接受此前时刻的隐含层输入，因此RNN能够更好地解决序列数据中时间依赖关系的建模问题。“RNN 阶段”的模型特点是时序模型，热点问题是对序列数据中的时间相依关系进行建模。 自2014年后，RNN在计算视觉领域得到了广泛的应用，比如行为识别、场景理解、图像生成，以及最近流行的图像视频描述等方向。 3.4 10大重要深度学习架构应用在计算机视觉领域的10大重要深度学习架构： 1.AlexNet： 首个深度学习架构，简单却功能强大，由深度学习先驱 Geoffrey Hinton 及其同僚共同引入。AlexNet 早在 80 年代已被概念化，突出特征是执行任务的规模大和使用 GPU 训练，AlexNet 借助 GPU 将训练提速了 10x。图9是架构示图。 论文：ImageNet Classification with Deep Convolutional Neural Networks 图9：AlexNet 网络架构 2.VGG Net： 由牛津可视化图形组（Visual Graphics Group）开发。特点是金字塔形 ，与图像最近的底层较宽，顶层较深；适合在特定任务上进行基准测试。图10是VGG Net 网络架构示图。 论文：Very Deep Convolutional Networks for Large-Scale Image Recognition 图10：VGG Net 网络架构 3.GoogleNet（Inception Net）： 谷歌研究者设计的网络架构，2014年 ImageNet 冠军，当时最强大的模型。训练速度快于 VGG，预训练规模小于 VGG。图11是序列架构示图。 论文：Rethinking the Inception Architecture for Computer Vision 图11：GoogleNet 序列架构 4.ResNet： 残差网络，包含多个后续残差模块，残差模块一个个堆叠组成完整的端到端网络。图12是架构示图。 论文：Deep Residual Learning for Image Recognition 图12：ResNet 网络架构 5.ResNeXt： 建立在 Inception 和 ResNet 的概念上进行改进的新架构，据说是解决目标识别问题的最先进技术。图13是对 ResNeXt 模块中的残差模块的总结。 论文：Aggregated Residual Transformations for Deep Neural Networks 图13：ResNeXt 模块中的残差模块 6.RCNN（基于区域的 CNN）： 据说是所有深度学习架构中对目标检测问题最有影响力的架构。为了解决检测问题，RCNN 尝试在图像中所有物体上画出边界框，然后识别图像中的物体。图14是工作原理示意，图15是架构示图。 论文：Faster R-CNN： Towards Real-Time Object Detection with Region Proposal Networks 图14：RCNN 工作原理示意 图15：RCNN 网络架构 7.YOLO（You Only Look Once）： 当前深度学习领域解决图像检测问题最先进的实时系统。YOLO 首先将图像划分为规定的边界框，然后对所有边界框并行运行识别算法，来确定物体所属的类别。确定类别之后，yolo 继续智能地合并这些边界框，在物体周围形成最优边界框。这些步骤全部并行进行，因此 YOLO 能够实现实时运行，并且每秒处理多达 40 张图像。图16是工作原理示意。 论文：You Only Look Once： Unified， Real-Time Object Detection 图16：YOLO 工作原理示意 8.SqueezeNet： 在移动平台这样的低宽带场景中极其强大的一种架构，它只占用 4.9 MB 的空间，而 Inception 架构大小为 100MB。图17是完整架构示图。 论文：SQUEEZENET： ALEXNET-LEVEL ACCURACY WITH 50X FEWER PARAMETERS AND &lt;0.5MB MODEL SIZE 图17：SqueezeNet 网络架构 9.SegNet： 一个用于解决图像分割问题的深度学习架构。它包含处理层（编码器）序列，之后是对应的解码器序列，用于分类像素。一个主要特征是在编码器网络的池化指标与解码器网络的池化指标连接时，分割图像保留高频细节。简言之，直接进行信息迁移，而非卷积它们。在处理图像分割问题时，SgeNet 是最好的模型之一。图18是 SegNet 解析图。 论文：SegNet： A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation 图18：SegNet 解析 10.GAN（Generative Adversarial Network）： 无监督学习模型，通过让两个神经网络相互博弈的 方式进行学习，常用于生成以假乱真的图片、视频、三维物体模型等。图19是工作原理解析图。 论文：Generative Adversarial Networks 图19：GAN 工作原理示意 4 基于深度学习的计算机视觉细分领域研究进展计算机视觉本身包括许多不同的研究方向，比如物体识别和检测（Object Detection），语义分割（Semantic Segmentation），运动和跟踪（Motion &amp; Tracking），三维重建（3D Reconstruction），视觉问答（Visual Question &amp; Answering），动作识别（Action Recognition）等。本文选取目标检测和人脸识别（目标检测的一种狭义类型）为例介绍基于深度学习的计算机视觉细分领域研究。 4.1 目标检测（OBJECT DETECTION）目标检测是将定位和识别合而为一，既要检测出物体在图像中的位置，还需要识别出物体的类别。最终需要同时检测和分类多个目标。目标检测是在图像上定位和分类数量可变的对象的问题。准确性和实时性是衡量其算法效果的标准。 目标检测领域使用的深度学习算法课划分为两类： 1）two-stage detector：以R-CNN为代表；首先找到可能存在物体的候选框，然后对候选框进行类别预测和边界调整，检测准确度高但检测速度慢。主要算法：R-CNN、Fast R-CNN、Faster R-CNN、R-FCN等。 2）one-stage detector：以YOLO为代表；是以回归的方式直接预测固定数量的候选框，检测准确度较低但检测速度快，能实时检测。主要算法：YOLO、YOLOv2、SSD等。 图20：基于深度学习的目标检测算法 目标检测使用的基准数据集主要有 MS COCO，Caltech，ImageNet，通用类别物体检测场景（PASCAL、VOC等），智能驾驶场景（KITTI）。 目标检测领域的进展主要存在三个问题： 1）目标的可变数量问题： 训练机器学习模型时，通常需要将数据表示成固定大小的向量。由于事先不知道图像中目标的数量，所以我们不知道正确的输出数量。正因为如此，我们需要一些后续处理，这也增加了模型的复杂度。 这种输出数量不定的问题已经使用基于滑动窗口的方法得到了解决，在图片的不同位置得到滑窗的固定大小特征。在得到所有的预测值之后，一些滑窗被丢弃，一些被合并，从而得到最终输出。 2）目标的大小不一致问题： 当进行简单分类时，我们希望能对占图像比例最大的目标进行分类。另一方面，想要找到的目标可能只有几个像素大小(或只占原始图像的很小一部分)。传统方法使用不同大小的滑动窗口解决了这一问题，这种方法虽然简单但是效率很低。 3）同时解决分类和定位问题： 如何将定位和分类这两种不同类型的问题最理想地结合进一个单一模型。 4.2 人脸识别（FACE RECOGNITION）人脸识别的研究历史较长，一直以来都是计算机视觉领域的一个热点研究方向。人脸识别是目标检测的一种狭义类型，问题可描述为：输入场景中的图像或视频，使用人脸数据库辨识或验证场景中的一个或多个人。人脸识别的关键在于是否拥有尖端的核心算法，并使识别结果具有实用化的识别率和识别速度。 人脸图像中包含的模式特征十分丰富，如直方图特征、颜色特征、模板特征、结构特征及Haar特征等。人脸检测就是把这其中有用的信息挑出来，并利用这些特征实现人脸检测。 人脸识别的进展有两大主要难题，一是环境光照发生变化时，识别效果会急剧下降。解决光照问题的方案有三维图像人脸识别，和热成像人脸识别。但这两种技术还远不成熟，识别效果不尽人意。二是相似性、易变性，即人脸作为生物特征的特点所带来的的困难。 人脸识别常用算法有：基于人脸特征点的识别算法（Feature-based recognition algorithms），基于整幅人脸图像的识别算法（Appearance-based recognition algorithms），基于模板的识别算法（Template-based recognition algorithms），利用神经网络进行识别的算法（Recognition algorithms using neural network）。 1）深度学习方法流行前：人脸识别的研究主要集中在特征提取（如 Gabor小波特征、局部二值模式特征等）、降维（如主成分分析、鉴别成分分析等子空间学习方法）和分类器设计（如最近邻、k最近邻等）。 2）基于SOM和CNN的人脸识别方法：1997年由 Lawrence等提出；通过SOM保持输入输出空间的邻域结构，利用CNN逐层自动学习特征；但是由于训练数据有限，故限制了该方法的推广。 3） 基于深度CNN的人脸识别方法：如 Deep face 算法（Facebook 提出）、Deep ID 算法（港中文多媒体实验室.Sun 等提出）；目前在测试集上准确率非常高，甚至超过人类水平，最高准确率超过99%。但是最新研究结果表明，当数据库中存在大量的干扰人脸(即非查询人脸)时，人脸辨识率和验证率仍然比较低，尤其是在姿态和年龄变化情况下识别性能下降严重。 4.3 视频识别（VIDEO RECOGNITION）视频识别即定位视频中的目标，并确定目标是什么。视频识别技术在互联网视频中应用需求很大。短视频、直播视频中大部分承载的是人物+场景+动作+语音的内容信息，如何用有效的特征对其内容进行表达是进行该类视频理解的关键。 目前主要的视频识别技术有： 1）基于单帧的识别方法：即先将视频进行截帧，然后基于图像粒度（单帧）进行深度学习表达，视频的每一帧会通过网络获得一个识别结果。学习视频时间域上的表达是提高视频识别的主要因素。 2）基于 CNN 扩展网络的识别方法：在 CNN 框架中寻找时间域上的某个模式来表达局部运动信息，从而获得总体识别性能的提升。 3）双路 CNN 的识别方法：使用两个独立的神经网络，再把两个模型的结果平均。 4）基于 LSTM 的识别方法：用LSTM对帧的CNN最后一层的激活在时间轴上进行整合。通过LSTM引入的记忆单元，可以有效地表达帧的先后顺序。 5）3维卷积核（3D CNN）法：3D CNN 应用于一个视频帧序列图像集合，不是简单地把图像集合作为多通道来看待输出多个图像（这种方式在卷积和池化后就丢失了时间域的信息）， 而是让卷积核扩展到时域，卷积在空域和时域同时进行，输出仍然是有机的图像集合。与单帧图特征在视频测试集上进行对比，3D CNN有更强的区分度。 5 未来研究方向预想计算机视觉领域利用深度学习带来的未来研究方向主要可能有六个： 1.深度图像分析。 目前基于深度学习的图像算法在实验数据库上效果还是不错的，但是远远不能够满足实际大规模应用需求，需要进一步的提升算法性能从而能够转化相应的实际应用。比如这个基于图片的应用，可以估计性别和年龄，但是其实经常会犯错，因此需要进一步提升深度图像分析的性能。 2.深度视频分析。 视频分析牵扯到大量的数据和计算量，所以做起来更加麻烦。当前深度视频分析还处于起步的阶段，然而视频应用非常广泛，比如人机交互智能监控（行为识别）等，所以加强深度视频分析是个重要的方向。 图21：深度视频分析 3.大规模深度学习。 随着时间的推移，数据量将呈指数级增长。为了处理更大规模的数据，需要进行多GPU并行及分布式计算。开发大规模深度学习算法是相当必要的。 4.无监督(半监督)学习。 这个方向是很明显的，因为实际应用中监督信息可能常常是缺失的，在大数据时代背景下要想标注所有的数据代价也是昂贵的。为了充分应用无标记的数据，进行无监督(或半监督)学习是非常重要的。近来的预测学习本质上与无监督学习是对应的。 5.大规模多模态学习。 多模态数据无处不在，尤其在互联网时代，网络上的图像、文本、语音等同时存在。多模态数据具有语义一致性、信息互补性的特点，互补性可做多模态数据的融合，一致性可做跨模态关联(如跨模态检索)。多模态学习研究的重点是对模态间的关联关系进行建模。视觉信息的有效理解离不开周边文本等其他模态数据，因此多模态学习是非常有意义的研究方向。 6.类脑智能研究。 神经网络本身是模拟大脑认知机理提出的网络结构。当前部分生物机制已经被应用到深度学习中，比如注意机制、神经元跨层连接机制等。在全球推动脑计划的大背景下，研究类脑智能显得尤为迫切和必要。 6 结论计算机视觉领域在深度学习的加持下取得了很大的发展，且仍有非常巨大的发展空间。本文主要对深度学习在计算机视觉中的应用进行了深入的介绍，全面综述了基于深度学习的计算机视觉研究进展，包括视觉领域的重要深度学习架构，以及选取目标检测和人脸识别这两个细分方向为例进行具体介绍。最后对计算机视觉领域利用深度学习可能带来的未来研究方向提出了构想。 参考文献[1] IAN G， YOSHUA B， ARON C.Deep learning[M].Massachusetts:2013, 5(8):1798-1828. [2] ROSENBLATT F.The perception:a probabilistic model for information storage and organization in the brain[J] Psychological Review, 1958,65(6): 386-408. [3] MINSKY M L, PAPERT S A Perceptrons: expanded edition[M].Massachusetts: MIT Press, 1988:1-308. [4] LAWRENCE S. GILES C L, TSOI A C, et al.Face recognition: a convolutional neural-network approach IEEE Transactions on Neural Networks, 1997, 8(1): 98-113. [5] 尹宝才, 王文通, 王立春.深度学习研究综述[J].北京工 业大学学报, 2015, 41(1):48-59. [6] 严严, 陈日伟, 王菡子.基于深度学习的人脸分析研究进展[D].厦门大学: 2015. [7] 王亮.深度学习与视觉计算[J].中国科学院自动化研究所中国人工智能学会通讯, 2017, 7(4), 41-56.]]></content>
      <categories>
        <category>Artificial Intelligence</category>
        <category>Machine Vision &amp; Computer Vision</category>
      </categories>
      <tags>
        <tag>Machine Vision &amp; Computer Vision</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于场景的新零售商品推荐研究]]></title>
    <url>%2F2018%2F08%2F20%2F%E5%9F%BA%E4%BA%8E%E5%9C%BA%E6%99%AF%E7%9A%84%E6%96%B0%E9%9B%B6%E5%94%AE%E5%95%86%E5%93%81%E6%8E%A8%E8%8D%90%E7%A0%94%E7%A9%B6%2F</url>
    <content type="text"><![CDATA[摘要：新零售是以消费者体验为中心的数据驱动的零售新模式，是中国实体经济增长的新动力。而智能商品推荐则是新零售消费者购物体验的重要突破点。在移动互联与传播时代，场景是新零售业务的一大关键要素，基于场景特征和数据的智能商品推荐将成为新零售商品推荐的重要研究和应用方向。消费者对新零售的了解程度、线下购物月支出、对店内智能导航服务的需求等因素影响消费者对智能商品推荐服务的需求。空间与环境、实时状态、生活习惯、社交元素是新零售场景的四个基本特征，基于对这些场景特征和数据的深度挖掘可以实现更加精准的新零售商品推荐。 关键词：新零售，智能商品推荐，场景 一、研究设计与数据来源本文采用定量研究与定性研究相结合的方法，研究的主题是基于场景的新零售商品推荐。本文具体阐述三个方面：场景是新零售商品推荐的关键要素，消费者对智能商品推荐服务的需求的影响因素，新零售场景的四个基本特征。 定量研究方面，本文于2018年6月对173位受访者进行了较为深入全面的问卷调研，回收有效问卷173份。受访者的男女比为4：6，年龄主要分布在10-25岁、31-50岁之间，学历主要为本科、专科、专科以下，是否婚配的比例为3：7，从事的行业主要有 IT 行业、媒体、金融、文化教育、卫生服务、工业制造建筑电力水利，工资/生活费主要在1000-5000之间。 二、场景：新零售商品推荐的关键要素1.新零售：零售业的未来2016年10月阿里云栖大会上，阿里巴巴董事长马云在演讲中第一次提出了新零售，“未来的十年、二十年，没有电子商务一说，只有新零售。” 2016年11月11日国务院印发了《关于推动实体零售创新转型的意见》，从总体要求、调整商业结构、优化发展环境、创新发展方式、促进跨界融合、强化政策支持六大部分、总计18个方面为新零售发展指明了方向。 新零售的诞生恰逢其时，有其特殊的时代背景，以下将依次从行业、技术、消费者三个层面分析新零售的诞生原因。 行业层面。根据 MobData 于2017年12月发布的《2017年中国新零售研究报告》^[1]^，如图1，2012年-2016年中国网络零售用户规模持续上涨，但用户增长速度下降明显，说明中国网络购物人口的红利基本结束，实体零售的发展遭遇天花板，亟待寻找新的增长动力。此外，中国30年间相继出现百货、购物中心和连锁超市业态，实体零售仍处于追赶式发展的初级阶段，发展不均衡，且盈利模式自身存在问题；流通效率整体偏低；传统零售地位严重下滑。 图1 2012-2016年网络零售用户规模（亿人） 技术层面。互联网的发展逐步释放经济与社会价值，推动全球化3.0进程。同时，中国新商业基础设施已经初具规模，大数据、云计算、移动互联网、物联网、人工智能等发展迅速，助推新的商业模式诞生。 消费者层面。根据普华永道2015年发布的研究报告《2015年全零售：零售商与变革的时代》，85%的中国消费者会首先利用数字化渠道研究新商品，51%的消费者会通过数字化渠道购买，超过全球34%的平均水平，说明中国消费者数字化程度非常高。^[2]^ 2016年，麦肯锡在其发布的研究报告《重塑全球消费格局的中国力量》中预测，未来15年中国将贡献全世界消费市场增量的30%，中国消费者消费模式正在发生转变，消费结构与发达国家日益相像。中国消费升级将引领全球消费增长，新一代消费价值主张出现。^[3]^ 从内容来看，新零售涉及零售服务、平台、科技与业态等全方位的创新发展，而不单单是一个流通消费问题。因此，对于新零售的研究需要全方位 、多视角地进行深入探讨。然而，目前国内外学界对新零售的研究仍处于起步 阶段，缺乏系统性研究和特定的分析框架，国内学界以基础性研究为主，主要包括新零售的内涵、发展动因、模式、发展路径等。国外对新零售的学术研究更是少之又少，且主要聚焦于已设立企业生存与保持竞争力的关键要素，并没有侧重新零售。基于此，我们可认为新零售不仅具有广阔的实践发展前景，还可为国内外学者提供巨大的研究空间。^[4]^ 对于新零售的概念，学界与业界从不同的角度作出了不同的界定。综合这些学者的定义，本文将新零售的概念界定为：新零售是依托人工智能、互联网、物联网、云计算等技术驱动，对商品生产、流通与销售环节进行全方位升级改造，进而重塑零售业态结构和生态圈，并深度融合线上服务、线下体验和现代物流，以消费者体验为中心的数据驱动的零售新模式。 新零售本质上是渠道、技术变革带来的经济效率提升与社会效益增加。区别于以往任何一次零售变革，新零售将通过数据与商业逻辑的深度结合，真正实现消费方式逆向牵引生产变革。它将为传统零售业态插上数据的翅膀，重塑价值链，引领消费升级，形成零售新业态，是中国零售大发展的新契机，亦是实体经济增长的新动力。 2.智能商品推荐：新零售消费者购物体验的重要突破点在消费者端，新零售的目标是依托技术和基于数据去感知消费者的消费习惯，甚至预测消费趋势、引导生产制造，为消费者提供多样化、个性化的产品和服务。其中的关键问题是新零售环境下的智能商品推荐。 购物模式从搜寻到推荐。传统零售时代，消费者需要在琳琅满目的商品中去寻找自己所需要的商品，是一种主动搜寻的行为，比较耗时耗力。根据本研究的问卷调查，在173名受访者中，14.5%的受访者在线下购物中最关注的因素包含“商品的易找程度”；如图2，67.6%的受访者表示在线下购物过程中希望被提供智能动态导航服务以帮助节省自己的时间。这些数据都表明，传统零售的消费者主动搜寻商品的购物模式给用户体验造成不便，是一种较为低效的购物模式。而这种不便和低效正可成为新零售消费者购物体验提升的一大突破点。解决这一问题，一方面固然可以通过提供智能导航服务来导引消费者快速精准地找到所需的商品，但这只能做到满足用户的期望，而无法做到超出用户期望。 新零售的本质应该是无时无刻地始终为消费者提供超出期望的“内容”。本文认为，智能商品推荐可以通过挖掘消费者潜在需求，从而针对客户进行个性化商品推荐，做到持续为消费者提供超出期望的“内容”，并将成为新零售消费者购物体验提升的重要突破点，而新零售场景下消费者的购物模式也将会从传统的搜寻模式升级为“主动搜寻与智能推荐并重”的模式。 图2 新零售场景中消费者对智能动态导航的需求情况 根据本文的调查，如图3，50.9%的受访者表示，在线下购物过程中希望被提供实时的智能商品推荐服务。这表明尽管还未在线下购物场景中体验过智能商品推荐服务，消费者已经对之存在较强的期待和需求。 图3 新零售场景中消费者对智能商品推荐服务的需求情况 本文认为，智能商品推荐本质上是基于对消费者、商品、场景三大特征的挖掘与分析，找到隐含的适配关系，然后在合适的地点、合适的时间将合适的商品推荐给合适的人。这里的商品既包括有形的产品，也包括无形的服务。智能商品推荐不仅能够通过快速匹配用户偏好为用户推荐个性化商品，从而提升消费者的购物体验，也能使商家实现更加精准的营销，从而提高经济效益。 机器学习、深度学习是推荐系统的技术发展方向，而特征和数据决定机器学习的上限。所以要构造良好的基于机器学习的推荐系统，最首要和最关键的工作是构建有效的特征并获取到特征对应的数据。而构建特征最关键的是挖掘到业务背后的规律。本文认为，在移动互联与传播时代，场景是新零售业务的一大关键要素，基于场景特征和数据的智能商品推荐将成为新零售商品推荐的重要研究和应用方向。 3.场景：移动传播时代的价值凸显移动互联网时代来临为新媒介提供了更加广阔的平台，越来越多地影响着人们的生活时间、生活场景的分配与重构，对于场景的研究日渐成为学界关注的热点。 “场景”的概念由罗伯特·思考博和谢尔·伊斯雷尔于2014年出版的《即将到来的场景时代：移动、传感、数据和未来隐私》中提出，书中指出大数据、移动设备、社交媒体、传感器、定位系统是与场景时代相关的五个要素，合称为“场景五力”，并认为“五种原力正在改变你作为消费者、患者、观众或者在线旅行者的体验，它们同样改变着大大小小的企业”。^[5]^该书的英文标题是“Age of Context”，“Context”一词一般译为情境，而国内则主要用“场景”一词。 从不同的角度出发，场景可以有不同的概念化定义。2015年，彭兰认为，广义的场景包含情境，场景同时涵盖基于空间和基于行为与心理的环境氛围，其基本构成要素包括：空间与环境、用户实时状态、用户生活惯性和社交氛围。^[6]^ 同年，Guanqing Liang 和 Jiannang Gao 在技术层面上对场景尤其是社会场景进行定义，认为社会纽带指的是不同用户相互关联的特征，如社会纽带和群体行为。2017年，喻国明认为，场景是社会与个人双重作用下的人为构设且“被建立”的环境，其生成要素包括社会条件、个人条件两个维度，受众行为意向的产生主要受到社会性要素、象征性要素的影响；其基本类型包括基于有形环境的现实场景、基于行为活动和心理氛围的虚拟场景、基于新媒介技术和虚拟环境创设的现实增强场景，不同维度下有不同的类属划分；并且，主要由认同感、归属感和社交支持干构成的场景印象会影响用户/消费者的行为意向，包括停留意向、消费意向、推荐意向和重构意向。^[7]^ 在新零售消费环境下，场景对智能商品推荐有关键作用。更广泛地来说，场景是智能信息推荐的关键要素。智能商品推荐是一种以用户为对象的个性化商品推荐。故本文研究的场景是一种以用户为中心的场景，是基于用户的个人特征、行为特征和心理特征而构建起的环境氛围。场景分析的直接目的是寻找并构建场景特征、获取到场景特征对应的数据，从而用于商品推荐系统的构建。最终目的是通过基于场景挖掘和适配用户个性化的消费需求，从而影响用户的信息选择意向和行为，以及基于场景适配设计出满足用户需求的产品和服务。 图4 基于场景的新零售商品推荐的流程框架 图4是本文构建的基于场景的新零售商品推荐的流程框架。场景提供数据，从下至上经过数据采集、数据分析、智能推荐三大环节，然后将商品个性化适配给特定的消费者。 三、消费者对智能商品推荐服务的需求的影响因素上文中提到超过一半的受访者表示在线下购物过程中希望被提供实时的智能商品推荐服务，表明消费者已经对之存在较强的期待和需求。研究消费者对智能商品推荐服务的需求的影响因素，对于零售商如何推广和优化智能商品推荐服务有实际价值。本文将影响因素分为三类，即消费者的个人基本特征、个人媒介和终端接触情况、线下购物态度和行为，逐一量化分析相关性。 1.消费者个人基本特征表1 消费者个人基本特征与其对智能商品推荐服务的需求的相关性 如表1， 本文将受访者的四项个人基本特征与其对智能商品推荐服务的需求做交叉分析，发现：受访者的性别、年龄、学历、月工资/生活费水平、婚配状况与其对智能商品推荐服务的需求之间的相关性系数分别为-0.038、-0.038、-0.005、-0.073、-0.038，显著性分别为0.603、0.951、0.341、0.202、0.603，均没有太过显著的影响，且均为负面的倾向，说明这四项消费者的个人基本特征与其对智能商品推荐服务的需求基本无关。 2.消费者个人媒介和终端接触情况表2 消费者个人媒介和终端接触情况与其对智能商品推荐服务的需求的相关性 如表2， 本文将受访者的获取资讯的媒介接触渠道、拥有的智能设备数与其对智能商品推荐服务的需求做交叉分析，发现：媒介接触渠道、拥有的智能设备数与其对智能商品推荐服务的需求之间的相关性系数分别为0.050、0.119，显著性分别为0.513、0.118，均没有太过显著的影响，说明消费者个人媒介和终端接触情况与其对智能商品推荐服务的需求也基本无关。 3.消费者线下购物态度和行为表3 消费者线下购物态度和行为与其对智能商品推荐服务的需求的相关性 如表3，本文将多项反映受访者线下购物的态度和行为的因素与其对智能商品推荐服务的需求做交叉分析。 （1）对新零售的了解程度：问卷中要求受访者对其对新零售的概念和模式的了解程度进行评分，从5到1，5为非常了解，1为完全不了解。数据显示，受访者对新零售的了解程度均值为2.43，其中82.8%的受访者表示对新零售的了解程度在3分及以上。对新零售的了解程度与其对智能商品推荐服务的需求之间的相关性系数为0.144，显著性分别为0.050，二者显著正相关。即对新零售越了解的消费者，越希望被提供智能商品推荐的服务。 （2）购物倾向性（线上还是线下）、线下购物频率、线下购物逗留时间与其对智能商品推荐服务的需求的相关性系数分别为-0.026、0.040、-0.014，显著性分别为0.737、0.598、0.857，均没有显著相关关系。 （3）线下购物月支出对其对智能商品推荐服务的需求的相关性系数为0.128，显著性为0.044，二者显著正相关。即线下购物月支出越高的消费者，越希望被提供智能商品推荐的服务。 （5）对店内智能导航服务的需求与其对智能商品推荐服务的需求之间的相关性系数为0.522，显著性为0.000，二者显著正相关。即越希望被提供店内智能导航服务的消费者，同时也越希望被提供智能商品推荐的服务。 综上，对于零售商来说，若想推广智能商品推荐服务，可以通过加强对新零售概念和模式的宣传，提高消费者对新零售的了解水平；可以先从对购物体验要求高的消费者入手，因为可能比起额外消费支出他们更加在意优化的购物体验；可以先从月购物支出水平高的消费者入手，因为他们可能不太介意购买被推荐的商品带来的额外消费支出。 四、新零售场景的四个基本特征：空间与环境、实时状态、生活习惯、社交元素智能推荐的本质可以说是对用户、信息、场景三者的匹配，但目前的推荐系统设计主要挖掘的是用户特征（如性别、年龄、地域、职业、行业、兴趣爱好、社交标签等）和信息特征（如内容标签等），而远远还未做到对场景特征的挖掘，使得目前的智能推荐仍停留在较为浅层次的匹配阶段。这由浅及深的关键就在于对场景特征的挖掘。并且，新零售是一种场景的重要性更为突出的领域，所以研究场景特征至关重要。新零售场景的基本特征应该包括：空间与环境、实时状态、生活习惯、社交元素。 1. “空间与环境”特征场景既是一种空间位置指向，也包含着与特定空间或行为相关的环境特征，以及在此环境中的人的行为模式和互动模式，空间与环境互相联系，不可分割，故将二者当作一个整体看待。^[8]^ 从空间与环境这个变量来看，新零售的场景可分两种：固定场景和移动场景。 固定场景指的是人们在相对静止的状态下所处的空间环境，是与人们日常活动规律相关联的环境，这些场景通常是相对稳定的，人在这些场景下的行为模式、心理模式、互动模式也通常是相对稳定的。比如家里、公司、餐厅、图书馆、健身房等。 图5 消费者最喜欢的新零售模式 新零售是一种线上与线下深度融合的泛零售形态，具有多种不同的购物模式，可提供给用户全渠道的购物体验。本文调查了消费者最喜欢的新零售模式，如图5。其中59%的受访者表示最喜欢“线上下单，线下物流快速送达”模式，22%表示最喜欢“线下购买，线下付款”模式，19.1%表示最喜欢“线下购买，线上付款”模式。 固定场景关注的是人们当前所处的场景。在特定的固定场景下，人们会自觉或不自觉地调整到处于该场景下自己对应的行为模式、心理模式，故会具有不同的购物需求。比如，你在图书馆里看中了几本经典著作，可能会想产生想购买它们的精装版并且期望马上到手的冲动；你在健身房锻炼时，可能会更倾向于购买健身相关的书籍、器材用具、服饰等 … 这些在固定场景下产生的购物需求都可以通过“线上下单—线下物流快速送达”、或者将用户从线上导流到线下零售店”的“线下购买—线下/线上付款”的购物模式得到满足。商家可通过分析消费者当前所处的固定场景，针对性向他推荐在此场景下他可能会需要的商品。 移动场景指的是人们在活动中不断遭遇的环境变量，即人们的空间移动轨迹。移动场景着眼的是人们所处的空间场景切换的前后逻辑性和关联性，实际上是分析与人们的移动轨迹和空间切换相交互的固定场景的使用。更具体来说就是分析用户“从哪来”、“正在哪”、“到哪去”。 移动场景的分析与应用涉及三个阶段，分别是“从哪来”、“正在哪”、“到哪去”。 “从哪来”阶段分析用户此前的空间与当前空间的关联性、此前行为与当前行为的关联性。“正在哪”阶段分析与满足用户此时此刻在此场景中的需求。分析用户从何处到达此处，能够更好地理解和推断用户在此时此地的目的、可能的心理特点和行为特点。例如，同样是逛书店，用户是在逛完商场后顺便去看一下旁边的书店，与他从离这里很远的家特地过来这个书店，二者有本质的差异。顺带逛书店是一种偶然性行为，意味着他的需求可能是漫无目的的，对自己的购买需求没有提前的思考和期望，所以在书店停留的时间可能是短暂的，对书的浏览可能是漫不经心的。要想留住他并让他发生购买行为，就需要主动挖掘他的潜在需求。一方面，如果书店有这个用户的历史数据，则可基于对其历史数据的分析发掘他的兴趣，针对性推荐他可能会感兴趣的书籍。另一方面，如果是一个没有历史数据的新用户，则可使用计算机视觉技术对用户的外观年龄、性别、着装等进行分析，预测他可能会感兴趣的书籍，或者通过推送排行榜、打折信息等来留住用户。而特意来书店的用户，往往是提前已经对书店有较为明确的需求，可能是想购买特定的书，也可能是想感受书店的氛围。这种情况下，需要注重的主要不是对其需求的诱导，而是对其现有需求的更好满足。 “到哪去”阶段为用户提供行动路线的导航和新的需求的诱导。在满足了用户此时此刻的需求后，商家可以通过预测用户下一步的行动方向，从而及时提供相应产品或服务；或者可以通过理解用户此时的行为，从而诱导他们的未来需求和行动方向。例如，对于在电影院的顾客，可以引导他们接下来去附近的餐馆就餐，或者去附近的电玩城玩玩游戏、去旁边的咖啡厅喝杯咖啡，等等。 图6 消费者去往零售店的此前场景 图7 消费者离开零售店的此后场景 根据本文对于“消费者去往零售店的此前场景”的调查，如图6、图7，受访者去往零售店的前一场景主要是家/住所（81.5%）、公司/单位（31.2%）、学习场所（44.5%）、饮食场所（15%）；完成购物、离开零售店去往的此后场景主要是家/住所（86.1%）、学习场所（36.4%）、公司/单位（26%）、文化场所（18.5%）。这说明，消费者去往零售店的此前场景通常是满足其居住、工作、学习、饮食需求的场景；此后场景通常是满足其居住、工作、学习、文化需求的场景。当然，这样的调查只是一种基于用户共性的研究，而在新零售场景中我们需要实时获知每个用户的动态数据和计算他们的个性。 2. “用户实时状态”特征用户实时状态，指的是用户在当前零售场景下的各种身体、行为、需求等动态变化的特征和数据。 新零售是线上与线下全渠道融合的零售新模式。按购物渠道的不同，可将用户实时状态分为线上实时状态和线下实时状态。 用户的线上实时状态，既包括用户在网络中留下的历史行为痕迹，即用户曾经使用网络做过哪些事情；又包括用户在网络中的实时行为状态，即用户此时此刻正在使用网络干什么。通过分析历史行为痕迹，可以挖掘用户的兴趣偏好。通过对用户线上实时行为状态的实时分析，可以从用户此时此刻正在关注的事物上挖掘用户潜在的购物需求，从而诱导用户进行“场景触发式购物”。 场景触发式购物是一种由场景实时生发出的购物欲望和继而马上实施的购物行为，一大关键特征是顾客能即时买到心仪的商品，具有“冲动性”、“随时随地随性购物”、“乘兴之所至”的特点。比如，当看到网络偶像剧中明星穿的时装、使用的美容产品，瞬间就会被点燃购物欲望，而这正是商家可诱导用户快速进行冲动性消费的契机。购物的冲动来去如风，零售商必须在消费者改变主意前快速打动他们，才能增加销量。足够快的速度可以实现这个目的。一方面，在用户还未脱离当前场景、犹沉浸在这种购物欲的时候，快速及时进行线上的实时推荐，诱导用户快速下单购买；另一方面，高效即时送达货品（如1小时内），在用户可能产生后悔情绪之前将商品马上送至眼前，如此还可大幅提高客户满意度。这种“即时达”是可以实现的，因为O2O送餐平均用时不到30分钟，其他商品当然也可借此即时送达。新零售商需要打破传统电商“次日达已经足够快”的理念，可借鉴甚至利用 O2O 送餐平台的半小时内高效送货服务，通过加快商品物流配送，从而提升冲动性消费的转化率。 用户的线下实时状态，指的是用户在零售店内的行动路径、行为动作、眼球等动态特征数据，包括路线移动、视线移动、身体和视线在特定品类或商品前的停留时长、身体动作等。通过分析这些数据，一方面可以获知特定顾客查看了哪些商品，身体和目光在哪些地方停留过和停留过多久，把哪些商品放进了购物车以及所花的时间等，以及顾客在零售店活动的一般规律；另一方面可以分析出哪些商品受到过顾客的关注，特定品牌受到关注的程度等。通过这些分析而得的信息，第一，可以继续分析出用户的商品偏好，从而进行实时精准的商品推荐。第二，可以分析出用户的购物行为特点，从而针对性采取个性化措施诱导和促进用户的消费，比如当用户的视线在一件昂贵的裙子上停留许久但迟迟没有购买，那么她可能是因裙子的价格过于昂贵而心生纠结，此时可通过推送打折信息、优惠券的方式诱导用户下决心购买这条裙子，还可以将同款式但价格相对低一些的裙子推荐给该用户，同样可促进用户的消费行为。第三，购物篮分析，可以根据用户已经放入购物车的商品，分析用户的个人特征，将可能会愿意购买的相关联商品推荐给用户，比如数据挖掘界的经典案例“啤酒与尿布”，即在美国有婴儿的家庭中，一般是母亲在家中照看婴儿，年轻父亲前去超市购买尿布，父亲在购买尿布的同时往往会顺便为自己购买啤酒，所以“啤酒”与“尿布”这两件看上去毫无关系的商品会经常出现在同一个购物篮中。 对于用户线下实时状态数据的实时采集，可以通过各种传感器设备实现，可分为移动设备和固定设备。移动设备包括智能手机、智能可穿戴设备、零售店的智能会员卡等；固定设备包括零售店的智能采集摄像头、智能面板等监测设备。而对这些采集到的数据进行实时处理和计算，则需要强大的云计算能力。 3.“用户生活习惯”特征用户生活习惯，指的是用户过往的生活经验和习惯。用户在各种场景下的需求和行为模式往往是相对稳定的，具有一定的惯性。用户生活习惯，既包括用户的购物习惯，比如历史购物偏好，往次购物时间、时长和商品类型及数量，往次购物的前一场景和后一场景等；又包括用户的日常行为习惯，比如出行路线信息、阅读偏好等。 通过分析用户的生活习惯，可以了解他们的行为特点和走向，以及个人特质，从而实现对用户更加精准个性化的商品推荐。例如，用户每天都在中午12点来超市都购买某几个品类的食品，且全部都是低热量食品，说明用户可能是正在进行减肥计划，那么明天用户再来购买时可以尝试给用户推荐其他的低热量食品。 4.“用户社交元素”特征社交元素，包括用户在各种场景中所表现出的社交关系、社交特征及社交标签。人们身处于各种场景之中，一般会与场景本身和场景中的人事物发生一定的交互，场景本身带有一定的互动性。通过对这些社交元素的分析，可以对用户的需求进行更深层次的洞察，从而更加精准的商品推荐。 社交关系，指的是用户在线上和线下场景中体现出的个人人际关系。线上社交关系，是一种完全虚拟或虚实结合的关系，既包括用户与真实的人在线上发生的互动关系，也包括用户与虚拟世界中的虚拟人或事物发生的互动关系。例如，用户在某游戏世界里饲养了一只很可爱的上古猫兽作为宠物，在这只虚拟猫兽身上花费了很多钱，并且每天都坚持“喂养”它，说明该用户可能是一个不折不扣的猫奴，那么零售店可以利用对用户的这一社交特性背后潜在的购物需求的洞察，在线上或线下向用户推荐各种符合该用户审美的宠物猫。而线下社交关系，则是一种与真实的人事物之间发生的社交关系。例如，用户牵着着一个5岁的小男孩来超市购物，超市的智能设备通过对这一画面的采集与分析，判断用户与该小孩可能是亲子关系，于是基于对这一亲子关系的洞察可以向用户推荐各种5岁小男孩可能会喜欢的玩具和零食。 社交特征，指的是用户在线上的社交媒体或线下行为里表现出的在社交方面的个人特征。例如，用户在社交媒体上表现出乐善好施的富有公益心和爱心的形象，经常在微博上晒自己的一些公益行动，说明该用户努力想塑造自己的公益爱心形象且具有一定的表现欲。那么零售店可基于对用户这些社交特征的洞察，向用户推荐带有公益色彩的商品，比如“买一捐一”，如此则既满足了用户行公益的需求，又提供给用户在社交媒体上“晒公益行为”的材料。 社交标签，指的是用户因其所处的社群、圈子而无形中给自己贴上的标签，比如果粉、二次元、文艺青年等。本文的问卷中要求受访者对他们所处的社群/圈子对他们的线下购物行为的影响程度进行评分，从5到1，5为影响很大，1为完全没有影响。如图8，数据显示，所处的社群/圈子对消费者线下购物行为的影响程度均值为2.77，接近于中值3，并且表示影响程度在一般及以上的比例为61.3%，远远超过50%，这说明目前社群/圈子对消费者线下购物行为有较大的影响。未来新零售场景下线上线下更加深度融合，相信社群、圈子等社交标签对消费者购物行为的影响会越来越大。 图8 社群/圈子对消费者线下购物行为的影响 五、总结与展望本文围绕基于场景的新零售商品推荐主题，主要阐述了“场景是新零售商品推荐的关键要素”，“消费者对智能商品推荐服务的需求的影响因素”，“新零售场景包含的四类基本特征”这三个问题。 本研究的局限性与未来研究方向主要表现在：第一，在研究方法上，问卷调查的样本数量较为有限，且样本的代表性可能不足，未来的研究可以扩大样本容量和提高样本的代表性。第二，本文详细阐述了新零售场景的四个基本特征，未来的研究可对每个场景特征分别进行更为深入的定性或定量研究，比如建立每个场景特征的特征体系框架，以及研究它们在新零售产品和服务设计方面的应用。 参考文献[1] MobData.线上线下拥抱融合 2017年中国新零售研究报告[R].上海.MobData大数据云平台，2017. [2] 普华永道.2015年全零售：零售商与变革的时代[R].伦敦.普华永道会计师事务所，2015. [3] 王玮,卜览,廖念玲,徐雷.重塑全球消费格局的中国力量[R].上海.麦肯锡公司，2017. [4] 王坤,相峰.“新零售”的理论架构与研究范式[J].中国流通经济,2018,32(01):3-11. [5] 即将到来的场景时代[M]. 北京联合出版公司 , 斯考伯, 2014 [6] 彭兰.场景:移动时代媒体的新要素[J].新闻记者,2015(03):20-27. [7] 喻国明,梁爽.移动互联时代:场景的凸显及其价值分析[J].当代传播,2017(01):10-13+56. [8] 彭兰.场景:移动时代媒体的新要素[J].新闻记者,2015(03):20-27.]]></content>
      <categories>
        <category>Product Manager</category>
        <category>Internet &amp; Product</category>
      </categories>
      <tags>
        <tag>Internet &amp; Product</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[粤传媒2017年度财报分析报告]]></title>
    <url>%2F2018%2F08%2F20%2F%E7%B2%A4%E4%BC%A0%E5%AA%922017%E5%B9%B4%E5%BA%A6%E8%B4%A2%E6%8A%A5%E5%88%86%E6%9E%90%E6%8A%A5%E5%91%8A%2F</url>
    <content type="text"><![CDATA[摘要：粤传媒是首家获得中宣部和国家新闻出版总署批准并在中小板上市的报业传媒公司，业务涉及广告运营、发行物流、报刊出版、印刷包装、新媒体等，同时涉足文化传媒产业投资相关领域。公司2017年净利润下滑64.9%。本文是对粤传媒2017年度财报的分析。 一、公司简介1.1 公司介绍广东广州日报传媒股份有限公司（简称：粤传媒）成立于1992年12月，2007年11月公司成功登陆深圳证券交易所，证券代码002181，证券简称“粤传媒”，是首家获得中宣部和国家新闻出版总署批准并在中小板上市的报业传媒公司。2012年6月，广州日报经营性资产整体注入上市公司，成为广东省唯一报业传媒集团整体上市公司。 粤传媒旗下拥有数十家子公司和分公司，多家报纸刊物及网站，羊城地铁报、足球报、舞台与银幕等十多家系列报在专业垂直领域保持领先地位。业务涉及广告运营、发行物流、报刊出版、印刷包装、新媒体等，同时涉足文化传媒产业投资相关领域。 粤传媒坚持“以媒为本，多元多赢”和“资源整合、媒体融合、转型升级”的发展战略，深化产业融合，加快媒体跨界创新发展的步伐，全力构筑拥有强大传播力、公信力、影响力和竞争力的新型文化传媒集团。 1.2 业务体系粤传媒利用自身的品牌价值、区域用户集聚、营销渠道、发行物流物流和服务布点的诸多优势，以创新促成效，以改革谋发展，把握行业发展规律和政策机遇，不断拓宽文化产业经营领域和发展空间，已经初步构建起了多元化的业务体系。 图1：粤传媒业务体系 2017年内，粤传媒在稳固传统业务优势、推进资源整合优化、创新经营模式和开拓业务前景等方面取得了进展。 二、能力分析2.1 营运能力分析收入，成本，盈利，营收账款，存货周转 1.营业收入情况表1：营业收入构成 （1）总体营运情况： 2017年度，公司实现营业总收入89,397.02万元，同比下降12.43%；营业利润12,904.21万元，同比增长228.2%；利润总额8,29747万元，同比下降55.14%；归属于上市公司股东的净利润7,25512万元，同比下降61.72%。 （2）分业务营运情况： 从业务来看：2017年内，一方面，粤传媒的三大主营业务总体营收下滑，广告业务营业收入31,183.90万，同比下降22.40%；发行业务营业收入21,471.41万，同比下降12.59%；印刷业务营业收入21,867.29万，同比增长4.34%。另一方面，新业务占营收比重非常小，除网络服务外的业务营收均同比下滑；网络服务占总营收比重2.11%，同比增长20.49%；物流运输占总营收比重5.66%，同比下降10.72%；旅店服务业营收同比下降24.26%；商品销售同比下降1.50%。 （3）分产品营运情况： 从粤传媒两大主营产品来看：2017年内，粤传媒报刊产品营业收入52,655.31万，同比下降15.18%；商业印刷产品营业收入21,867.29万，同比增长4.34%。 （4）分地区营运情况： 粤传媒的用户主要集聚在华南地区，2017年内，华南地区营业收入87,052.86万，占总营收98.92%，营收同比下降13.80%，华中、华东的营收也均同比下滑；而华北、西南、西北地区的营收同比增长迅速，但占总营收比重加起来不足2%。 总结：2017年内，粤传媒总营收同比下滑，且下滑幅度较大；依然是主营业务的广告、发行、印刷业务贡献了绝大部分营收；尽管着力发展新业务，但增长乏力，且对营收的贡献占比非常小；除印刷和网络服务外，其他所有业务的营收均同比下滑。此外，粤传媒在其用户集聚区的华南地区的总营收同比下滑。 2.营收成本表2：营业成本构成 2017年内，粤传媒总营业成本为134,542.27万，比上一年同比上升9.5%。从成本构成来看，销售费用23,621.04万，同比下降11.94%；管理费用16,098.44万，同比增长36.19%；财务费用-137.56万，同比增长51.35%。研发投入119.34万，同比下降58.15%。 从业务来看，广告业务和发行业务的营业成本比上一年同比下降较多，而印刷业务的营业成本（受纸张成本上升的影响）比上一年同比上升6.53%。 2.2 盈利能力分析（1）成本费用利润率： 注：企业一定期间的利润总额与成本、费用总额的比率。成本费用利润率指标表明每付出一元成本费用可获得多少利润，体现了经营耗费所带来的经营成果。该项指标越高，利润就越大，反映企业的经济效益越好。 成本费用利润率 = 利润总额/成本费用总额*100\%2016年成本费用利润率 = 15.84% 2017年成本费用利润率 = 7.91% 由成本费用利润率可见，粤传媒的经济效益比较差，且2017年成本费用利润率比上一年有所下降，反映粤传媒的经济效益比上一年恶化。 （2）营业利润率： 表：2012-2017年粤传媒营业利润率 粤传媒的盈利主要来自广告、发行、印刷三大传统业务。粤传媒的营业利润率呈现波动上升态势。2017年营业利润率扭转前两年的负值态势，重归正利率，说明粤传媒的营收能力较前两年有所提升。不过对新业务的投入仍未产生明显回报，要产生高额收入增长难以期。 （3）净资产收益率、总资产利润率：衡量公司对股东投入资本的利用效率、企业利用资金进行盈利活动的基本能力。 表：2012-2017年粤传媒净资产收益率 表3：2012-2017年粤传媒总资产利润率 2017年粤传媒的净资产收益率为1.85%、总资产利润率为1.59%，两项指标均比上一年下降不少，且整体均呈现波动下降趋势，表明2017年粤传媒对股东投入资本的利用效率下降，盈利能力降低，资产状况不健康。 2.3 偿债能力分析1.短期偿债能力表4：合并资产负债表（部分） （1）企业流动资产流动比率： 流动比率 = 流动资产/流动负债 注：衡量企业资产的变现能力 / 变为现金用于偿还负债的能力。比率越高，说明企业资产的变现能力越强，短期偿债能力亦越强；反之则弱。一般认为流动比率至少保持在2:1左右是比较适宜的。 2016年流动比率 = 258,583.85 / 84,073.31 = 3.075694891 2017年流动比率 = 246,768.03 / 35,699.67 = 6.912333643 （2）速动比率： 速动比率 = 速动资产/流动负债 注：衡量企业流动资产中可以立即变现用于偿还流动负债的能力，其值在1左右较为适宜。速动资产包括货币资金、短期投资、应收票据、应收账款及其他应收款，可以在较短时间内变现。 2016年速动比率 = （55,940.31+880.21+31,713.82+91.99+587.30）/ 84,073.31 89,213.63 / 84,073.31 = 1.061140926 2017年速动比率 = （27,626.95+2,899.76+23,588.79+1.74）/ 35,699.67 = 54,117.24 / 35,699.67 = 1.515903088 （3）现金比率： 现金比率=现金类资产/流动负债2016年现金比率 = 55,940.31 / 84,073.31 = 0.665375373 2017年现金比率 = 27,626.95 / 35,699.67 = 0.773871299 分析：流动比率用于衡量企业资产的变现能力。粤传媒2016年流动比率较低，说明2016年短期偿债能力弱，2017年达到6.912333643，说明其2017年短期偿债能力增强。速动比率衡量企业流动资产中可以立即变现用于偿还流动负债的能力。2017年粤传媒的速动比率比上一年有所上升，表明企业短期偿债能力增强。2017年粤传媒的现金比率比上一年有所上升，说明财务弹性变好，偿债能力增强。 2.长期偿债能力资产负债率： 资产负债率=全部负债/全部资产 注：资产负债率是期末负债总额除以资产总额的百分比，即负债总额与资产总额的比例关系。该指标是评价公司负债水平的综合指标。同时也是一项衡量公司利用债权人资金进行经营活动能力的指标。一般来说，资产负债率越低，企业偿债能力越强。 2016年资产负债率 = 89,242.89 / 469,000.74 = 0.190283047 2017年资产负债率 = 37,550.07 / 430,502.70 = 0.087223774 分析：粤传媒2017年的资产负债率比上一年有所下降，说明公司企业长期偿债能力增强。 2.4 发展能力分析表5：粤传媒主要财务指标 （1）销售增长率： 销售增长率=本年销售增长额÷上年销售总额 注：销售增长率是衡量企业经营状况和市场占有能力、预测企业经营业务拓展趋势的重要指标，也是企业扩张增量资本和存量资本的重要前提。该指标越大，表明其增长速度越快，企业市场前景越好。 2015年销售增长率 = （89,397 -102,083）/ 102,083 = -0.12427143 2016年销售增长率 = （102,083 -129,225）/129,225 = -0.21003676 2017年销售增长率 = （129,225 - 161,792）/161,792 = -0.20128931 2015年到2017年粤传媒的销售额持续保持负增长，且销售增长率呈现波动下降状态，2017年比上一年有所回升，但仍有20%的负增长，表明粤传媒2017年经营状况仍然不善，企业市场前景堪忧。 （2）总资产增长率： 总资产增长率=企业本年总资产增长额/年初资产总额*100\% 注：总资产增长率反映企业本期资产规模的增长情况。总资产增长率越高，表明企业一定时期内资产经营规模扩张的速度越快。 2015年总资产增长率 = （452,609-506,182）/ 506,182 *100% = -10.58% 2016年总资产增长率 = （469,001-452,609）/ 452,609 *100% = 3.62% 2017总资产增长率 =（430,503-469,001）/ 469,001 *100% = -8.21% 2015年到2017年粤传媒的总资产增长率呈现波动状态，2017年总资产增长率为负值，且比上一年有较大下降，表明2017年粤传媒总资产规模缩水，后续发展能力欠佳。 （3）股利增长率： 股利增长率=本年每股股利增长额/上年每股股利*100\%2015年股利增长率 =（-0.38-0.33）/ （-0.38）= 1.868 2016年股利增长率 = [（0.16 -（-0.38）] / 0.16 = 3.375 2017年股利增长率 =（0.06-0.16） / 0.06 = -1.667 粤传媒2017年股利增长率比前两年下降，呈现负增长状态，表明粤传媒的发展能力下降，同时股票价格下跌。 三、风险分析1.市场风险受国内宏观经济增速放缓和新兴媒体发展的影响，粤传媒平面媒体广告经营报刊发行、报纸印刷等传统业务均呈下滑态势。同时，新媒体业务尚处于投入、探索期，尚未形成成熟的盈利模式和盈利能力，亦面临激烈的市场竞争，并受宏观调控影响，业务增速具有不确定性。粤传媒需要将加快媒体融合转型，一方面对报纸细分市场进行挖潜，另一方面通过整合优化全媒体发布渠道，为用户提供更多的增值服务，提升公司对市场风险的抵抗力。 2.盈利能力风险表6：2012-2017年粤传媒净利润与主营收 由前文的分析和上表可知，粤传媒的各项利润率与主营收整体均处于波动下降状态，企业盈利能力越来越弱。寻找企业盈利的关键发力点，拉动企业经济效益增长，是粤传媒需要重点关注的问题。 3.股票市场风险表7：2012-2017年粤传媒每股收益 由前文的分析和上表可知，2017年粤传媒的股利增长率呈现负增长，且2012年以来每股收益整体处于波动下降趋势，说明粤传媒的股票收益下降，需要警惕股民抛售股票的风险。 4.原材料上涨风险表8：2016-2017粤传媒营业成本 由前文的分析和上表可知，2017年粤传媒的印刷业务成本有较大幅度上升，且印刷成本占营业成本的比重最大。2017年中国市场造纸原料高成本推涨纸价的趋势或仍将持续。纸价是公司平面媒体成本的重大构成之一，未来纸价上涨将进一步增加公司平面媒体经营压力。 5.资产周转风险表9：2012-2017粤传媒存货周转率和总资产周转率 2012-2017年粤传媒的存货周转率和总资产周转率不断降低，说明公司的资金周转速度变慢，销售能力和变现能力降低，需要警惕资产周转的风险。 6.项目投资风险粤传媒采取多种方式投资具有发展潜力的项目，挖掘新的利润增长点，以期进一步提高公司盈利能力。由于项目实施及经营过程中将会受到宏观政策、市场供求变化等因素影响，导致投资项目实施会存在一定风险。为应对此类风险，公司需要加强风险管理体制，做好投后管理和风险防控工作，增强公司对风险的可控性。 7.被中国证监会立案调查的风险粤传媒于2016年10月19日收到中国证监会《调查通知书》(粤证调查通字160076号)，因公司涉嫌违反证券法律法规，根据《中华人民共和国证券法》的有关规定，中国证监会决定对粤传媒进行立案调查。这一事项存在对粤传媒的经营产生一定程度的负面影响的风险。粤传媒需要全面配合中国证监会的调查工作，同时严格按照相关法律法规及监管要求履行信息披露义务，采取措施降低可能带来的负面影响的风险。 四、总体综述通过前面的众多分析，我们可以发现，粤传媒是一家发展状态不佳的公司。2017年，粤传媒的营收能力、盈利能力、发展能力整体均在变弱，偿债能力在增强，是一家经营状况不佳、增长乏力的企业。 粤传媒的主营业务（除印刷外）均呈下滑状态，而新业务尚处于投入、探索期，尚未形成成熟的盈利模式和盈利能力，对企业的经济效益贡献甚微。同时，粤传媒公司也存在着一些直接和潜在的风险，比如市场风险、盈利能力风险、原材料上涨风险、资产周转风险等。 粤传媒需要将加快媒体融合转型，一方面对主营业务的细分市场进行挖潜，另一方面通过着力发展新业务，找到新的利润增长点，拉动企业经济效益提升，整合优化全媒体发布渠道，为用户提供更多的增值服务。此外，还需要注意提升公司对各类风险的抵抗力。]]></content>
      <categories>
        <category>Communication Science</category>
        <category>Medium Study</category>
      </categories>
      <tags>
        <tag>Medium Study</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《传播统计学》课程笔记]]></title>
    <url>%2F2018%2F08%2F20%2F%E3%80%8A%E4%BC%A0%E6%92%AD%E7%BB%9F%E8%AE%A1%E5%AD%A6%E3%80%8B%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Abstract：传播统计学，主要教授统计学的方法以及如何将统计学的方法应用到解决社会科学问题中。参考教材是祝建华老师的同名专著。 Chapter1 初导1.1 统计数据与统计学统计学 探索数据内在规律性 搜集，整理，分析，显示，解读数据 统计研究对象的特点 数量性 总体性：研究总体中各单位普遍存在的事实，更关注总体的普遍特征 变异性：总体各单位的特征表现存在差异，且无法提前预知差异 统计学的应用领域 企业发展战略，产品质量管理，市场研究，财务分析，经济预测，人力资源管理 1.2 统计学的产生和发展萌芽时期 德国 算术 古典概率论 近代时期 数理统计 社会统计 现代时期 数学关系密切 与计算机结合 1.3 数据分析方法 描述统计：收集、可视化描述数据（统计学的基础） 推断统计：对样本进行估计、假设检验、预测或其他推断（统计学的核心），利用部分数据推断全局 课程内容 传播统计学概论 描述统计 推断统计 实践 1.4 统计学的研究分支 按研究阶段 按研究方向 按研究环节 收集数据：抽样调查，实验设计 分析数据 统计推断形式：参数估计，假设检验 统计观点：贝叶斯统计，统计决策理论 理论模型：非参数统计，多元统计分析，时间序列分析 etc 按应用领域:宏观，微观 推断统计学内容体系 1.5 传播统计学传播研究：人类传播行为 1.应用领域 受众研究 印刷媒体研究 视听率研究 传播效果研究 方法：因子分析法，多元方差分析，多元回归分析，路径分析，结构方程式模型 etc. 广告与公共关系研究 广告文案测试：测试消费者对广告的印象、认知、感情投入、购买意图、广告效果 媒体研究：传播到达率、传播频次、视听率、受众规模和构成媒体计划、竞争分析 公共关系研究：监测和评估 2.什么是统计数据？ 对现象进行计算的结果 数据集 统计数据的分类 按计量尺度：分类的，顺序的，数值的；定性数据（定序，定类）+ 定类数据（定比，定距） 按收集方法：观察数据（直接+间接）+ 实验数据（直接数据） 直接：直接获取的原始数据； 间接：通过统计计算得出的数据. 按时间状况：横剖面数据（静态数据）+ 纵剖面数据（动态数据） 静态数据：在相同或近似的时间收集到的数据； 动态数据（引入时间维度）：在不同时间上收集到的数据；描述现象随时间的变化 按数据的表现形式： 绝对数，相对数（通过对比），平均数 时期数据（现象在某一时期内的数量;eg.人口出生数），时点数据（反映现象在某一时刻的数量；eg.人口数，学生人数） 3.统计学基本概念1.变量 分类 确定性变量 + 随机变量（无法确定其精准的值） 定性变量 + 定量变量 2.总体、个体与样本 总体与总体分布 总体：研究对象的整体；同质性+大量性+差异性 个体：总体中的每个成员 总体量：个数 有限总体 &amp; 无限总体 总体分布中的参数：总体的某种特征值 样本与样本分布 简单随机样本：可用与总体独立同分布的n个相互独立的随机变量$X{1}X{2},…,X_{n}$ 表示 联合分布函数$F(x{1},x{2},…,x{n})=\prod$，联合概率密度函数 $f(x{1},x{2},…,x{n})=\prod$ 总体、样本、样本值的关系 总体分布决定样本取到样本值的规律，故可用样本值推断总体 3.统计量 统计量是统计理论中用来对数据进行分析、检验的变量，不含任何未知参数的样本的函数(可直接有样本数据计算出来) e.g.$\overline{X}=\frac{1}{n}\sum{i=1}{n}X{i}$ ​ 4.描述性统计 vs 推断性统计描述性统计变量观测值的分布 分布的中心趋势：平均数，众数，中位数 分布的伸展程度：极差，四分位数差，五数综合，方差和标准差 五数综合(MIn,Q1,M,Q3,Max), Q1、Q3分别为第一、三四分位数，M为平均数 分布的形状：斜度(skewed),峰度(kurtosis) 变量观测值间的相关：相关系数 推断性统计What：根据数据对总体进行估计和推断；依据概率和概概率分布理论来推断总体 1.总体与样本 2.随机变量与概率 随机现象 随机变量与概率 总体均值与方差 4.常用概率分布 二项分布 正态分布 5.参数估计和假设检验 推断方法 演绎论述 归纳论述 A.参数估计 点估计：直接用样本统计量的值去估计总体参数 区间估计：用样本统计量构造的1个区间去估计总体参数的范围(置信区间)；置信度 B.假设检验/显著性检验 即根据样本资料所提供的信息，推断对有关总体参数的某个断言或假设的不利证据 例如,在受众研究中,研究者猜测不同性别的受众接触媒介的行为是有显著差异的:男、女电视观众所喜欢的电视节目不同每日收看的时间也不同等等。这些研究者的猜测常常构造成所谓的理论假设H1。 在统计推断的假设检验中,一般是先假设这些差异或效应不存在,即先提出一个“没有差异”或“没有效应”的所谓零假设(也叫原假设 null hypothesis)Ho例如:“不同性别的居民收看电视的平均时间长度之间没有显著的差异”。然后根据样本数据,去寻找不利于这个零假设H、即支持研究者的理论假设H1的证据,并利用概率值( probability-value)P来判定这种否定零假设的证据有多强. 概率值P(或叫P值)是在原假设H0为真的前提下计算或查表得到的,它表示:“如果Ho为真,检验统计量大到(或小到)至少和实际观测值那么大(或小)的概率”。P值越小,说明样本资料所提供的对原假设H0的支持程度越小,即所提供的否定H的证据就越强。一般是将P值和一个事先确定的称之为检验水平level of test)或显著性水平( level of sigη lificance)的固定数值a相比较,通常取a=0.05或0.01: 如果P≤a,则拒绝H0,或称该样本资料在a水平下具有统计显著性 statistical significance; 如果P&gt;a,则不能拒绝H,即样本资料在a水平下不具有统计显著性 5.实验统计实验统计 包括：实验设计 + 统计方法 在一受控制的环境下，使其他因素保持不变，研究所控制的变量对某些变量的影响（因果关系） 实验设计 规定实验单位，及如何分组 要控制哪些自变量、因变量 如何控制外来变量 重要术语 实验误差：外来变量和测量误差所导致的影响 内部有效性：自变量对因变量的影响的准确性 外部有效性：实验结果推广到实验环境意外的可能性 实验法分类 按实验场所分 实验室实验 现场实验 按是否将实验单位随机分组 非随机化实验 随机化实验 非随机化实验：按照是否将实验单位分成处置组和控制组,以及按照是否只做事后测量还是同时也做事前测量,可以分成四种:事后设计、有控制组的事后设计、事前事后设计有控制组的事前事后设计. 随机化实验：根据自变量(处置变量或因子)的多少,外来因素的多少,以及是否考虑因子之间的交互作用,可以分为五种:完全随机化单因子设计随机区组单因子设计拉丁方单因子设计、多因子设计、正交实验设计,等等。 Chapter2 传播研究设计中的统计方法2.1 研究主题、模型与假设1.研究主题与统计方案的设计2.传播模型的表示法What：通过模型对各种传播现象、传播理论和各种理论假设进行简洁的描述，勾画出理论框架中研究对象各要素间的联系(联系的结构，方向，强度) 类型 图示模型 数学模型 包含待估计参数和误差 例如,祝建华在一项关于“项目无回答”( Item ron一 response,简称INR)的研究中(详见本书第六部分),采用了一种称之为“逻辑斯蒂回归”( logistic regression)的统计数学模型,来描述“项目无回答”NR与可能影响NR的诸因素之间的联系,并以此模型估计14种因素对INR可能影响的强度(大小)和方向(正负) $logY=b{0}+b{1}Age+b{2}Sex+b{3} Education+…+b_{4}Setting$ 式中: log表示对Y取对数(以e为底); Y表示发生“项目无回答”(INR=1)的概率与不发生“项目无回答”(INR=0)的概率之比,即: $Y=Prob(INR=1)/Pob (INR=0)$ 在实际分析时,分别用INR=1(被访者表示“我不知道”、“我没有什么看法”、“我不想回答”或“没有回答”等)和INR=0被访者给出了实在的回答)比例来估计这两个概率 Age为被访者年龄的标准化得分; Sex为被访者的性别,规定男性=1、女性=0 education为被访者文化程度的标准化得分 setting表示访问的地点,规定在工作地点或公共场所=1、在家中=0。 3.理论的假设与统计的假设2.2 分析单位的确定与设计1.调查法与实验法中的分析单位2.内容分析法中的分析单位2.3 抽样设计与确定样本量的原则1.抽样2.简单随机抽样3.实用抽样技术4.确定样本量的方法2.4 调查表设计与信度、效度分析Chapter3 统计数据的收集、整理和显示2.1 数据收集1.数据搜集方法直接观察法 报告法(通讯法) 采访法：口头问询法，自填法；电访，网络调查 登记法 实验设计调查法：如设计分组对照实验 调查数据 &amp; 实验数据 2.统计调查的组织形式分类（1） 普查：非经常性；全面性 非全面性调查：分为抽样调查、重点调查、典型调查 抽样调查：简单随机抽样 + 非简单随机抽样 最大抽样效果：成本min——误差min 与 精度max 精度与成本往往相矛盾. 分类（2） 连续性调查：随研究现象变化，连续不断地调查 非连续性调查：有时间间隔；eg.普查 统计报表 统计调查体系 2.2 数据整理核心工作：统计分组和统计指标 步骤：设计整理方案、确定统计指标——>计算统计指标——>统计描述和可视化结果 流程：审核——分组——制表作图 审核 完整性检查：资料是否齐全，答案是否完整 准确性检查：逻辑性检查，计算方法等 分组 原则：穷尽 &amp; 互斥 分组标准 平行分组体系：使用多种平行标准对同一总体进行分组 复合分组体系：使用多种重叠标准对同一总体进行分组 2.3 频数分布1.what：把数据按某一标志进行归类排列(v.)，以反映数据的分布状况 2.步骤 1.求出极差$R=Xmax-Xmin$ 2.确定组数$n$、组距$d$、组段 3.列表划记 $n=1+3.3lg(N)$ $d=R/n=(Xmax-Xmin)/n$ N:总体单位数 3.数量分组 等距分组 异距分组：如标志值分布很不均匀 频率密度=频率/组距；各组频率密度与组距的乘积之和=1 4.频数分布的类型 钟形 U形 J形 2.4 数据描述(数据可视化)2.4.1 资料的初步展现1.频数表2.均值表反映分布 3.饼形图和条形图描述定性类型的变量分布 4.折线图反映变量随时间变化 2.4.2 单一变量的分布1.直方图反映定量类型变量分布 2.茎叶图数据规模较小时，可用茎叶图得到比直方图更多的信息 3.盒形图(boxplot)表现数值型变量分布，利用五数综合进行作(min,Q1,M,Q3,max) 分位数是将总体的全部数据按大小顺序排列后，处于各等分位置的变量值。 四分位数间距：第三四分位数与第一四分位数的差距，反映数据分布的分散程度 纵轴表示变量的取值范围，横轴无用 盒子下半部比上半部短，说明分布右偏 播出效果：晚上、上午&gt;中午&gt;下午 2.4.3 两个关联变量的资料展现1.变量的关联性和交互表 2.条形图 3.散点图(scatterplot)定距或定比变量分析，反映两变量关联的方向和强度 4.相关系数和相关系数表相关系数$r$: 只能反映两变量直线相关的强度；会受到少数偏离值的严重影响 Chapter 传播研究资料分析中的统计方法(一元和二元统计篇)对总体的估算统计推断的理论依据统计推断——归纳（个别到一般） 根据概率论所揭示的随机变量的一般规律性,利用抽样调查所获得的样本信息,对总体的某些性质或数值特征进行推断 抽样分布是统计推断的基础 1.参数和统计量参数：描述总体的数字，如总体总量，总体均值，总体回归系数，总体相关系数等 统计量：描述样本的数字，是样本$X{1},X{2},…,X{n}$的1个函数 抽样分布：统计量的概率分布，如均值、方差 2.样本均值的抽样分布所有可能样本的均值构成的概率分布 \sigma^{2}=\frac{1}{N}\sum_{i=1}^{N}(X_{i}-\mu)^{2}参数估计统计方法 描述统计 推断统计：参数估计+假设检验（用样本推断总体） 估计量与估计值 估计量：估计总体参数的随机变量 样本均值（总体均值$\mu$的一个估计量），样本比例，样本方差etc 估计值：计算出的估计量的值 $\overline{x}=90$ 参数量：总体参数$\theta$的估计量$\hat{theta}$ 参数估计 对已知分布类型的总体，利用样本去估计其未知参数 估计方法：点估计（矩法，极大似然） ； 区间估计 估计量是随机变量，期望是数值. 矩法理论依据：大数定律 总体k阶原点矩：$E{X^k}$ 样本k阶原点矩：$\frac{1}{n}\sumX^{k}$ 总体k阶中心距：$$ 样本k阶中心距：$$ 矩法：用样本矩代替总体矩 前提：总体各节矩存在 尽量用低阶矩估计 极大似然法（Maximum Likelihood Estimate，MLE）极大似然法： 选择1个参数使实验结果具有最大概率 L(\theta)=f(x_{1},x_{2},...,x_{n};\theta)=\prod P(x_{n};\theta) 用使$L(\theta)$达到最大值的$\theta$去估计$\theta$，$L(\theta)$是似然函数 求极大似然估计量： 1.构造$L(\theta)$ 2.取对数 3.求偏导：令=0 4.求解 估计量评价方法 原则上，任何统计量都可以作为估计量；不同估计方法得出的估计量可能不同 参数估计的评价标准 1.无偏性（unnbiasedness） 用样本做的估计量=实际真值 一个参数的无偏估计可以有很多，但无偏估计只能保证无系统误差，但却可能有极大的偏差。一个优良的估计量还需要有较小的方差（低偏差）。 2.有效性（efficiency） 均方误差：用偏差的平方的期望来衡量估计量偏差的程度 3.一致性 相合估计量：当样本逐渐增大时，估计量逐渐趋近于真值 点估计的缺陷 区间估计抽样分布t分布假设检验Chapter5 分类数据统计分析数据 数值型：离散 &amp; 连续 分类型数据：连列分析（列联表：交叉分析） 边缘分布 行边缘分布：单行合计 列边缘分布：单列合计 百分比分布 行百分比分布：行的每一个观察频数除以行合计数 列百分比分布：列的每一个观察频数除以行合计数 期望频数 一个实际频数的期望频数$e_{ij}$，是总频数的个数n e_{ij}=n*\frac{r_i}{n}\frac{c_i}{n}=\frac{r_i*c_j}{n}$\chi^2$ 统计量 用于测量两个分类变量之间的相关程度；检验列联表中变量间的拟合优度和独立性 \chi^2=\sum(f_{ij}-e_{ij})^2/e_{ij}分类数据的假设检验（对列联表的2种检验） 1.拟合优度检验（$\chi^2$ 统计量） 检验一个分类变量中各类别的期望频数和观察频数是否有显著差异 实际为：假设检验 检验步骤：确定假设——检验 2.独立性检验 根据次数资料判断两类因子彼此相关或相互独立的假设检验 关联的测度 对于分类变量，常用基于卡方统计量的各种统计量来进行关联性度量 一组统计量，表示交叉表中两个变量之间的关联的紧密程度和方向 对于2x2的列联表：$\phi$ 相关系数；$\phi=1$ 表示完全相关 \phi=\sqrt{\chi^2/n} 对于测度大于2x2的列联表：C系数表示相关程度 C=\sqrt{\chi^2/(\chi^2+n)} V系数 V=\sqrt{\chi^2/(n*min[(r-1)(c-1)])} 一种定序的关联测度：$\gamma$ 同序对Ns 异序对Nd \gamma= (Ns-Nd)/(Ns+Nd)Chapter6 方差分析方差分析的基本原理方差分析（ANOVA） 作用：两个及两个以上样本均数有无差异的显著性检验 What：利用试验观测值总偏差的可分解性，将不同条件所引起的偏差与随机误差分解开，按照一定规则进行比较，以确定各自偏差的影响程度和大小（去噪：去除随机误差的影响） 又称：F检验，变异数分析 几个常用术语 试验指标：考察的对象的某种特征 试验因素：影响试验指标的因素A,B,C,D,… 单因素试验 多因素试验 因素水平：试验因素所处的不同水平 试验处理：在试验单位上实施的具体项目 试验单位：观测数据的单位 重复 3.误差 随机误差：在因素的同一水平（同一个总体）下，样本的各观察值之间的差异 如同种颜色的爽肤水在不同卖场的销售量差异可看成是随机误差 系统误差：在因素的不同水平下，各观察值之间的差异 同一家卖场 ，不同颜色爽肤水的销售量差异若是由颜色差异引起，则是系统误差 4.比较 比较的基础是方差比：组内方差 &amp; 组间方差 组内方差：= 随机误差 组间方差：在因素的不同水平下各样本之间的方差，包括随机误差 + 系统误差 5.离均差平方和（SS）：反映变异大小情况 总变异：组内变异 + 组间变异 总变异：所有测量值之间总的变异程度 SS_T=\sum^{a}_{i=1}\sum^{n_i}_{j=1}(Y_{ij}-\overline{Y})^2=\sum^{a}_{i=1}\sum^{n_i}_{j=1}Y^2_{ij}-C=\sum^N_{ij}Y_{ij}^2-C=N\sigma^2 组间变异：各组均数与总均数的离均差平方和 组内变异：在同一处理组内的差异（随机误差），即组内各值与该组均值的差的平方和 离均差平方和的分解：$SS总=SS{组内}+SS{组间}$；$V总=V{组内}+V{组间}$ 6.均方差，均方（MS，mean square） MS_{组间}=SS_{组间}/v_{组间} MS_{组内}=SS_{组内}/v_{组内} v是自由度 7.F值与F分布 F=MS_{组间}/MS_{组内} v_1=v_{组间} F值接近于1，就没有理由拒绝$H_0$ ；反之F值越大，拒绝$H_0$的理由越充分。当$H_0$成立时，服从F分布 是单尾检验/右尾检验 8.方差分析的步骤 1.建立假设： H0：均值全相等 H1：均值不全相等 2.计算检验统计量F 计算离均差平方和 总离差平方和：$SST=N\sigma{ij}^2$ 组间离差平方和：$SSA=N\sigma{\overline{y_i}^2}$ 组内差平方和：$SS_E=SS_T-SS_A$ 计算自由度 总自由度 组间自由度 组内自由度 计算均方（方差） 组间方差 组内方差 计算机统计量F值 3.查表求临界值：F 值表（方差分析用单侧检验）；$F{\alpha}=F{\alpha}(组间自由度，组内自由度)$ 4.比较F值与临界值：$F&gt;=F_{\alpha}$，则拒绝H0 5.列方差分析表 列：方差来源，平方和，自由度，均方，F比 行：组间，组内，总和 列方差分析表： 方差来源 平方和 自由度 均方 F比 组间 $SSA$ $K-1$ $MS_A=SS_A-df_A$ $F=MS_A/MS_E$ 组内 $SSE$ $N-K$ $MS_E=SS_E-df_E$ 总和 $SST$ $N-1$ 9.方差分析的前提条件 每个总体都服从正态分布 总体的方差必须相同 不同水平的样本相互独立 单因素方差分析单因素完全随机方差分析验后多重比较 F检验否定H0，不表明任意两个均数间都存在显著差异 功能：发现哪两个均数间存在显著差异 常用方法：最小显著差数法，最小显著极差法 单因素随机区组设计方差分析两因素方差分析多因素实验：实验因素不止一个 数据的离差平方和分解形式： SST=SSA+SSB+SSE SST=\sum\sum(X_{ij}-\overline{X})^2；df=nr-1 SSA=\sum\sum(\overline{X_i}-\overline{X})^2 SSB=\sum\sum(\overline{X_j}-\overline{X})^2Chapter7 简单线性回归回归 ：变量间在数量上有依存变化关系（针对连续变量） 回归方程 变量间的关系 确定性关系：可用一个具体函数式表示出来 非确定性关系：宏观有相关关系，但未精确到用具体函数式表示 线性回归 描述两变量在数量上有非确定性的线性变化关系 回归直线： \widehat{Y}=a+bx回归参数估计 $\widehat{Y}$是由X推算的Y的估计值（实测点到回归直线的纵向线段与直线的交点对应的纵坐标），a是截距，b是回归系数，X变动一个单位时，Y平均变动b个单位 计算原理：最小二乘法，即保证各实测点到回归直线的纵向距离的平方和最小 \sum{(Y-\widehat{Y})}^2=\sum(Y-(a+bx))^2 b=\sum(X-\overline{X})(Y-\overline{Y})/\sum(X-X)^2=l_{XY}/l_{XX} a=\overline{Y}-b\overline{X} $Y-\overline{Y}$ 为残差 \sum{(Y-\widehat{Y})}^2 残差平方和，最小二乘：残差平方和最小 总体回归系数的假设检验 对样本的回归系数b进行假设检验，以判断b是否是从回归系数为零的总体中抽得的 key：b与0相差多少可以认为具有统计学意义 步骤 建立假设：H0: $\beta=0$ 回归方程无意义；H1: … 选择假设检验方法：方差分析或t检验，计算统计量 计算概率值P 拟合优度（绝对系数） $R^2$取值0~1之间，反映回归贡献的相对程度 R^2=SS_{回}-SS_{总}残差图 标准残差：（残差-均值）/ 标准差 残差图：以自变量或因变量为横坐标，标准残差为纵坐标 Chapter8 多元线性回归函数关系（确定性关系） 相关关系（不确定关系） Y_i=F(X_i,\delta) 因变量与自变量可单相关也可多相关 分类：线性相关（直线形式） &amp; 非线性相关 相关程度：完全相关，不完全相关，完全不相关 多元线性回归的流程：采集样本信息——散点图——回归方程——回归方程的显著性检验——对现实进行预测和控制 多元线性回归 建立模型：回归方程 y=\beta_o+\beta_1x_1+\beta_2x_2+...+\beta_mx_m+\epsilon $\beta_i$ ：回归系数，m+1个待定参数 $x_{ij}$ ：观测值 $\epsilon$ ：随机误差，$\epsilon$ ~$N(0,\sigma^2)$，即$y$无法用$x_i$表示的其他各种随机因素造成的误差 y=X\beta+\epsilon y=[Y_1,Y_2,...,Y_n]^T，\beta=[\beta_1,\beta_2,...,\beta_n]^T，\epsilon=[\epsilon_1,\epsilon_2,...,\epsilon_n]^T X：自变量，观测值构成的$n * k$矩阵变量，注意：X矩阵前面有一列1 Y：$n * 1$矩阵 $\beta$：$k*1$ $\epsilon$ ：$n*1$ 基本假定 假定1：零均值假定 假定2和3：同方差和无自相关假定 假定4：随机扰动项与假设解释变量不相关 假定5：无多重共线性假定 假定6：正态性假定 $y_i - \hat{y}_i=…$：回归值与实际值间有误差 多元线性回归的最小二乘估计公式： B=A^{-1}C=(X^TX)^{-1}X^TY 省略了随机误差项。 实例： 12345678910111213import numpy as npX = np.mat([[1,1,1,1,1,1,1,1,1,1,1,1,1,1], [41, 45, 51, 52, 59, 62, 69,72, 78, 80, 90, 92, 98, 103], [49, 58, 62, 71, 62, 74, 71, 74, 79, 84, 85, 94, 91, 95]])X=X.TY = np.mat([28, 39, 41, 44, 43, 50, 51, 57, 63, 66, 70, 76, 80, 84]).TB = (X.T * X).I * X.T * Yprint B 123[[-15.93836228] # beta0 [ 0.52227044]# beta1 [ 0.47382726]] # beta2 注意：X矩阵前面有一列1，指的是与$\beta$ 相乘后需要得到的$\beta_0$ 多元线性回归的拟合优度检验 多重可决系数$R^2$ ：表示由多个解释变量联合解释了的Y的变差在Y的总变差中占的比重 修正的可决系数（调整后 R平方） \overline{R^2}=1-(1-R^2)\frac{n-1}{n-k} 回归方程的显著性检验（F检验） 原假设H0：$\beta_2=\beta_3=…=\beta_k=0$ 备择假设H1：不全为0 目的：推翻原假设 建立统计量 F = \frac{ESS/(k-1)}{ RSS/(n-k)} ~当$R^2=0$，$F=0$$ （表示以概率为0拒绝H0）；当$R^2=1$，$F-&gt;无穷$（表示以概率为1拒绝H0） 若F&lt;$F_{\alpha}$ ，接受原假设 各回归系数的显著性检验（t检验） 原假设H0：$\beta_j=0$ 备择假设H1：$\beta_j !=0$ t=\frac{}{}应变量的均值估计 Y均值的区间预测 应变量个别值的预测 Reference《概率论》的读书笔记]]></content>
      <categories>
        <category>Computer Science</category>
        <category>Data Mining and Analysis</category>
      </categories>
      <tags>
        <tag>Data Mining and Analysis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Web 信息架构》课程笔记]]></title>
    <url>%2F2018%2F08%2F20%2F%E3%80%8AWeb-%E4%BF%A1%E6%81%AF%E6%9E%B6%E6%9E%84%E3%80%8B%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Abstract：李卫东老师的 Web 信息架构课，李老师是一位教学严谨且非常负责的好老师。这门课非常系统地讲解了information Architechture以及以之为基点的互联网产品设计，让我受益良多 ~ Part1 原理篇Chapter1 Web信息架构（Information Architechture）洞察 + 灵性 APP用户体验存在的问题（易用性，舒适度） 查找信息困难 信息超载 信息异质 搜索问题 成本问题 信息架构（Information Architechture） 易理解性：使信息易于理解 ——&gt;使用通俗易懂的语言 化繁为简：将复杂的信息简单地传达给用户 ——&gt;寻找合适的方法来组织信息 类比 构造——建筑——城镇 页面——APP/Web——互联网 速度分层（依据变化速度来分层的思维方法） 产品架构的三个层次 业务架构：产品概念设计 应用模式 信息架构：产品详细设计 设计文档 技术架构：产品实现设计 程序代码 产品构成的三个要素 视觉传达 信息架构 技术平台 信息架构 —（可视化）—&gt; 界面设计 隐藏层——&gt;表现层 广义IA与狭义IA 互联网整体的信息架构 Web 信息架构设计的层级 互联网整体的 IA：互联网中各站点互联互通 企业级 IA 网站 IA：网站结构，导航，标签 页面中的 IA（页面结构设计） 智能终端的 IA Chapter2 信息架构有哪些内容组织系统，导航系统，标识系统，搜索系统 组织系统深度与广度的平衡 层级式 &amp; 辐射式 导航系统层级之间如何链接 标识系统Labeling Systems 为内容确定名称、描述等一系列标识方案 搜索系统WEB信息框架的完整结构 静态框架 数据对象模型：数据对象是什么 数据显示模型：如何显示数据；组织系统，导航系统，标识系统，搜索系统 动态框架 数据处理模型：如何动态处理数据 Chapter3 如何设计信息架构微信之前版本的搜索功能的导航系统设计得不科学（用关键词搜索公众号，关注后想再继续看那个公众号列表） IA设计需要具备的知识基础Who Design IA：系统分析师，系统设计师 IA 需要的知识能力 知识：社会学，商业战略，人类工程学，认知科学，HCI，页面设计 能力：抽象理解能力，组织协调能力 IA 设计的理念、系统和模式 理念 复杂系统： 用户：受众，任务，需求，信息搜索行为，体验 情境：用户使用场景 内容：内容对象、数量、类型，现存架构 无形的工作：信息架构，操作界面 知识网络 信息搜寻行为 信息架构的用户思维IA 设计原则 简洁性：信息内容化繁为简 习惯性：符合用户操作习惯（eg. 上下左右滑动） 可理解性：用户永远不会出错 高效性：减少用户操作量 引导性：让用户能确定自己当前所在的位置 UCD（User Centered Design） 以用户为中心 你永远无法满足所有人 用户永远是对的，但也不能完全听信用户的言语，用户说的需求不一定是真需求 妥善管理用户的需求变化 把握用户需求的优先级 确定需求优先级的方法：MoSCow 必须有（Must Have） 应该有（Should Have） 可以有（Could Have） 这次不会有（Wont have this time）：当现有产品已经可以打败绝大多数对手，就没必要把能想到的好 功能全部推出，否则容易“黔驴技穷”，所以需要有所保留，持续不断给用户新鲜感 以用户为中心的难点 用户目标与我的目标不一致：鱼和熊掌可以兼得，但不是马上 用户的关注点与我的关注点不一样 用户和用户不一样 Web信息架构的建构 情境分析+用户分析：应用模式 ——&gt;数据对象建模：数据内容结构，用例图 ——&gt;数据显示建模：四大系统 ——&gt;数据处理建模：功能逻辑模型，活动图，序列图 区分： 页面层次结构图 功能结构图 Chapter4 面向对象的方法——用户建模分析当你面对的应用很复杂，深入了解面向对象的思想，可以提升你的抽象能力和分类能力，用这种方法描绘的网站结构图也更容易被开发团队所理解。——张小龙 面向对象的方法 面向对象的用例视图 详细的用例归约相当于详细的产品设计文档 获取需求——&gt;建立用户用例图 活字印刷，面向对象 可维护性：要改只需改特定的字 可复用性：每个字都可以重复使用 可扩展性 灵活性：可灵活排列 设计产品要考虑面对需求的易变性的程序设计的可拓展性、可复用性 面向对象分析（OOA）理论概述面向对象分析（OOA，ObjectedOriented Analysis） 作用：需求分析 关键概念： 问题域：应用领域 系统边界：能做什么不能做什么的分界线 参与者：与系统进行交互的任何事物 系统分析面临的主要问题 问题域与系统责任 交流 需求的不断变化 复用的要求 OOA方法的主要原则 抽象，封装，继承，分类，聚合，关联 消息通讯，粒度控制，行为分析 面向对象分析建模原理需求分析阶段 + 系统分析阶段 1.需求分析阶段 用例模型：描述功能需求；包括：业务用例，业务场景，系统用例，用例归约 2.系统分析阶段 根据用例创建分析模型： 使用UML图描述；采用分析类，是高层次的系统视图 包括：静态视图（描述事物的静态结构，包括类图、包图），动态视图（描述事物的动态行为，包括序列图、状态图、协作图、活动图等） 建模之用例图（UserCaseDiagram） 包括的要素：参与者，用例，关联，系统边界 参与者：以某种方式参与用例的执行过程；触发 分类：发起参与者；参加参与者（分为主要参与者+次要参与者） 用例：外部可见的系统功能单元；定义连贯的行为；包含它做必需的所有行为（执行用例的主线次序，标准行为的不同变形，一般行为下的所有异常情况和预期反应） 识别用例：从参与者开始分析 Q： 参与者希望系统提供什么功能 系统是否存储和检索信息，如果是，由哪个参与者触发 当系统改变状态时，是否通知参与者 是否存在影响系统的外部事件 哪个参与者通知系统这些事件 用例粒度：功能单元的大小和数量 用例间关系：包含关系，扩展关系，泛化关系 包含关系：将包含用例的事件流插入到基本用例中（必然发生） 扩展关系：（可能发生） 1- **泛化关系（继承）**：将多个用例的共性抽象为**父用例**，而其他用例作为泛化关系中的子用例；eg1.文字阅后即焚、音频阅后即焚、视频阅后即焚三个子用例的共同父用例是阅后即焚；eg2.验证登录是父用例，其子用例有账户密码验证、人脸识别验证、指纹识别、声纹识别 ... 系统边界：系统之间的界限 系统的组合递归进化性，系统模块化（划分） 用例模型设计实例1.准备工作： 了解问题领域（需求） 确定参与者：业主，业务提出者，业务管理者，业务执行者，第三方，用户 发现业务参与者 2.需求分析： 获取业务用例 建立业务模型 3.系统分析： 建立系统用例：从业务用例扩展而来 用例归约： 简要说明：简要介绍用例的作用和目的 事件流：基本流（用例正常运行时的场景） + 备选流（描述异常或偶然情况） 基本路径 可选路径 用例场景： 特殊需求：描述与该用例相关的非功能性需求 前置条件：执行用例前系统必须所处的状态 后置条件：执行用例前系统可能所处的状态 业务用例实例： 系统用例实例： 用例说明实例： 面向对象分析——静态视图类图静态视图是在用例图的基础上设计！ 类图： 描述类、接口、协作和它们之间关系的集合体 静态结构，展示类与类之间的关系 类图七元素：类、接口、协作、依赖关系、泛化关系、关联关系、实现关系 在类图基础上，可使用状态图、协作图、组件图、配置图等进一步描述系统特性 对象图： 类图的实例 类图与对象图的区别： 类图的组成： 1.类： 包括：物理实体，逻辑事物，商业事物，应用事物，行为事物，纯粹概念性的事物 etc. 描述类：名称，属性，操作（设计依据：用例图），职责，约束，注释 2.接口： 含义：在没有给出对象的实现和状态的情况下对对象行为的描述 3.类之间的关系： 依赖关系： 类之间的使用关系；指两个或多个模型元素之间语义上的关系，被依赖元素的变化会要求或指示依赖元素的改变 用虚线箭头表示，箭头上可加关键词指明依赖的种类 依赖种类：use，Access … 泛化关系：类间“一般——特殊关系”，父类与子类 关联关系：二元关联，三元关联 重数：表示有多少个对象与对方对象相连接，如“0..1”表示0或1 有序关联与导航 使用箭头 聚合关系：关联关系的一种强化，需满足： 表示整体和部分的关系（所属） 部分可被多个整体同时共享 整体消失，部分实例依然存在 如班级——学生 组合关系：是进一步强化的聚合关系 必须同时存在，且整体不能共享部分的实例，比如活人&lt;——跳动的心脏 关联的约束：各种模型元素的一种语义条件或限制，一条约束只能应用于同一类元素 用一对花括号括起来，eg.{ordered}，括号里为约束内容 对泛化的约束：complete；disjoint；incomplete；overlapping 关联的约束：implicit（概念性关联，但在模型精化时不再关联）；ordered（信息显示有序）；changeable（关系是可变、可修改的）；addonly（可在任意时刻增加新的链接）；frozen（冻结已创建的对象，不能再修改）；xor（或约束，当前时刻只有一个当前关联实例） 建立静态模型： 分析类：分析类是业务需求向系统设计转化过程中最重要的元素；分析类代表“系统中具备职责和行为的事物”；包括边界类、控制类、实体类 边界类：用于对系统外部环境与其内部运作之间的交互进行建模的类；包括：用户窗口、通信协议、打印机接口、传感器、终端等 控制类：对一个或几个用例所特有的控制行为进行建模；控制对象通常控制其他对象，因此他们的行为具有协调性质 实体类：对必须存储的信息和相关行为建模的类；源于业务模型中的业务实体 包图 面向对象分析：动态视图 描述事物的动态行为，在静态视图的基础上 序列图、协作图、状态图、活动图 序列图：描述系统中各个对象按照时间顺序的交互过程 构成：对象，生命线，激活，消息 注意：边界类发送消息到控制类，控制类发送信息到实体类，然后实体类返回消息到控制类，控制类返回消息到边界类 图中的序号是对消息的编号 协作图：描述控制类内部的实现逻辑 构成：对象，消息，链 强调对象之间的关系 图中的序号是对消息的编号 状态图：通过建立类对象的生存周期模型来描述对象随时间变化的动态行为，描述事件如何引起对象的状态变迁 构成：状态，触发事件 状态转换：两状态之间的关系 状态转换的组成：源状态；事件触发；监护条件（guard Condition）；动作；目标状态 状态内部构成：进入/退出动作；活动（在一个状态内执行的处理过程）；内部转换（响应）；子状态（状态间的包含关系） 活动图：即流程图，描述工作流 要素：活动，活动之间的转移，判断，保证条件，同步条 活动图——泳道： 活动图——对象流：可显示对象的角色、状态、属性值的改变 分叉与分支的区别：分叉——子路径是都需要做的，分支——可选路径、选了一个就不选另一个 静态模型设计实例1.提取业务对象 2.对象的属性设计 3.基本操作设计 4.关系设计 关联关系设计 业务对象模型：一定要模块化设计 Chapter5 信息架构之内容分析概述内容分析的任务——建立组织体系 五帽架法则：按不同标准对同一集合进行分类（类别、时间、位置、层次、名称字母顺序） 内容模型化： 模型化：分析整理内容间的语义关系，本质是建立叙词表 语义关系：等价，等级 ，相关 ​ 叙词表 refer：http://cdc.tencent.com/2011/06/13/%E5%8F%99%E8%AF%8D%E8%A1%A8/ Chapter6 情景分析目标设定目标设定：明确产品能为用户提供什么价值 项目蓝图：项目的长远设计 现状分析修改方案： 启发式评估 可用性测试 日志分析 标杆分析：竞品分析 Chapter7 网站结构设计网站结构设计的任务： 用户动线设计 网站内信息的分类整理：层级型，分面分类型 目录主页，列表页面，详细页面等页面形式的定义 内容量（页面数量）的确定 网站结构模式： 层级型 分面分类型：根据信息的各个侧面的性质来识别和分类信息；适合用户根据自身的观点来检索信息 web 型：各种信息通过超链接平等连接 辐射型 直线型 综合型：融合多种模式 视觉辞典： 连接节点 Chapter8 界面设计页面类型： 门户页面 列表页面：文本，缩略图，缩略图+文本 内容页面 功能页面 设计流程： 交互流程设计 风格定位：色彩 功能 icon 设计 界面视觉的整体优化 Chapter8 导航系统设计文字导航条 vs 图片导航条 浏览器导航压力测试方法：随机进入任一页面，测试能否进入其他任何页面 设计原则： 强化信息层次：使用户对内容组织方式越来越熟悉 便捷性，直接可达性 避免信息冗余 分类 &amp; 索引：满足不同需求 Chapter9 标签系统设计标签：网站内的索引文本 标签设计注意事项： 避免专用、网络流行语、情景式表达 注重词语粒度：粒度太大容易模糊用户的行为导向（比如美女定义“漂亮”没有任何意义，关键是如何漂亮，可以使用“细眉”、“大眼”…） 避免使用动词：名词和形容词更合适，因为动作描述过于精准反而不利于内容组织]]></content>
      <categories>
        <category>Product Manager</category>
        <category>Internet &amp; Product</category>
      </categories>
      <tags>
        <tag>Internet &amp; Product</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《新媒体与社会》课程笔记]]></title>
    <url>%2F2018%2F08%2F20%2F%E3%80%8A%E6%96%B0%E5%AA%92%E4%BD%93%E4%B8%8E%E7%A4%BE%E4%BC%9A%E3%80%8B%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Abstract：新媒体与社会的课堂笔记，余红老师的这门课主要讲互联网对政治、经济、文化、社会等各方面产生的影响。 互联网与技术OTT（over-the-top） 越过中间运营商去提供服务，通常是指内容或服务建构在基础电信服务之上从而不需要网络运营商额外的支持。 早期特指音频和视频内容的分发，后来逐渐包含了各种基于互联网的内容和服务。 eg. 微信电话（绕过通信运营商），网络视频（绕过广电），网络金融（绕过银行） 网络金融：仅限于小微贷款 移动通讯 业务模块不再清晰：融合交叉 要求运营商同时具有应用开发和商业化的能力（不应只满足于做靠通道费生存的“收租婆”，必须主动参与竞争） 移动互联网 去中心化 用户体验 直播平台的主要开支：请网红主播 + 流量费 用户年轻化 + 终端智能化 + 超级应用 + 营销精准化 + O2O融合 技术 PK 社会：是技术改变社会？还是社会制约技术？ 技术到应用的转化 社会会制约技术的采纳 Social Media 社会化媒体 社交媒体与用户 ​ 连结性文化里的工程化社交性（van Dijck） 社交媒体的人际互动，不止依赖人与人的互动，更包括平台演算法的推荐好友、主动tag等机制 演算法如何决定讯息的可见性，将朋友转化为可计量的节点，这些设计影响了社交媒体中的人际互动讯息流动 四种类型： 社交网络平台（SNSs）：Facebook 用户生产内容平台（UGCs）：Youtube 交易营销平台（TMSs）：eBay 游戏平台（PGS） 社交媒体的内在结构： 技术如何编码社交行为： 资料 算法 协定 介面（interface） 预设值（default） 社会、文化、伦理对技术的制约，需要知道底线在哪里 社交媒体的使用的东西方差异存在： 西方人个人生活与工作界限分明，Facebook &amp; linkeln；中国微信——混淆 中国社交媒体的使用：人情的功能 开放 纯粹社交即时通讯；带有社交属性的各种其他类型的平台（游戏、电商、内容（文字、直播）） 大数据精准营销 传统统计分析面对大数据的挑战：样本的代表性 大数据的5V 特点：Volume（大量），Velocity（高速），Variety（多样），Value（低价值密度），Veracity（真实性） 精准营销：在合适的时间、合适的地点、将合适的产品以合适的方式推荐给合适的人 大数据营销数据类型： 人口统计学数据：用户的年龄、性别、国籍、注册时提供的信息 用户行为数据：访问、页面停留时长、触点 用户内容偏好数据：感兴趣的话题、评论内容、品牌偏好、位置偏好、时间偏好 交易数据：实际订单、客单价、订单转化率、促销响应率 媒体之恶： 监控：全景监狱（福科），任何个体都无法逃脱 个性化——工业流水线化的社会歧视：个性化定价策略 互联网的基因：金钱取向（比如在利益与用户权益之间，互联网会偏向于前者） 在技术有作恶能力的情况下如何防止技术作恶：技术可以通过数据操纵人类群体的行为、态度、观点 产品——垄断：比如微信 问题： 互联网平台权力的边界？平台就可以任性？ 我们未来会成为巨头算法的奴隶吗？ 技术 vs 社会： 技术决定论 社会决定论 结构化理论 符合中国国情的网络产品设计路径： 技术的“变异” ​ 技术理性：缺乏人性；冷血； 互联网与中国政治需要有政治头脑与智慧 新媒体舆论事件 政策议程设置：（区别：民众参与程度） 决策者：关门模式 vs 动员模式（民众被引导；典型报道） 智囊团：内参模式 vs 借力模式 民间：上书模式 vs 外压模式 压力锅模式（pressure cooker model）：微博民意主导，公权介入 解压阀模式（safety valve model）：专业媒体爆料 专业媒体会对事件做议程设置，引导社会舆论讨论 政府希望看到这种模式 解压：有序引导社会积压情绪的释放 媒介公共领域正在形成？中国公民社会正在形成？ 互联网成为多方博弈的场所 互联网监管体系：中央政府——主导者，地方政府和相关部门——执行者，运营机构（互联网平台）——协作者，网民——自律和相互监督 不同角色的行动逻辑不同 政策条文 vs 法律：暂时性（容易导致机会主义） vs 永久性 互联网与文化网络文化现象：网络流行语 ，网络恶搞，网络直播，民间影像 大众意识的觉醒 新媒体数字短片：生产机制，传播方式，消费过程 网络自拍： 技术基础 心理动因 咪蒙：清醒地迷失在名利的洪流中 UGC 生产的文化价值？ UGC 文章、UGC 短视频 —— 社会价值？ 互联网与经济电商：资金、政策、人力倾斜（实体经济不景气的情况下） 移动分享经济： 需求：社会资源的优化配置，闲置资源；资源拥有者/使用者，分离，让渡使用权；连接，撮合平台 ；某一特定产品或服务的共享模式的全社会化和商业化 用户动机：赚钱 + 社交 商业模式：金融 政策 技术：物联网；5G 时代万物互联 互联网医疗： 远程会诊，医生只负责开处方，病人可拿处方自行去药店买药，避免以药养医的怪象 大数据医疗公司：三端服务——医院、药企（预测未来药品研发方向）、患者（挖掘患者需求） 互联网平台： 互联网媒体平台：门户——搜索——SNS（社交媒体） 互联网交易平台：网络支付使网上交易的闭环形成 互联网分享平台： 互联网平台的权力边界？ 企业与政府之间的权力边界明晰 案例：滴滴通缉令 动机：故作姿态 打乱警方的计划、造成社会恐慌 侵犯人权：隐私权、名誉权，在没有彻底定罪前]]></content>
      <categories>
        <category>Communication Science</category>
        <category>Medium Study</category>
      </categories>
      <tags>
        <tag>Medium Study</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《数字媒体技术》课程笔记]]></title>
    <url>%2F2018%2F08%2F20%2F%E3%80%8A%E6%95%B0%E5%AD%97%E5%AA%92%E4%BD%93%E6%8A%80%E6%9C%AF%E3%80%8B%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Abstract：数字媒体技术课的课堂笔记，按课程流记录的，不全，没怎么整理。 Chapter1 多媒体技术基本概念2M+2O：Multi-media，多机并行处理，object（面向对象），open 计算机三大能力：计算能力，存储能力，传输能力 媒体的定义和分类 媒体的分类 感觉媒体 表示媒体：给计算机看的，编码方式 显示/表现媒体：显示器，扫描仪 存储媒体：外存，HDD，磁盘阵列 传输媒体 多媒体的特征：集成 &amp; 交互 &amp; 多维性 交互性：用户获取信息是主动而非纯被动 多媒体软硬件平台多媒体硬件 视频信号IO接口卡 音频信号IO接口卡 视频、音频压缩解压缩 视频、音频信号实时处理 驱动软件（Driver）：在初始化引导程序作用下把它安装到系统RAM中常驻内存；一个驱动器对应一种硬件设备的接口 驱动器接口模板：AB（地址总线）+ DB（数据总线）+ CB（控制总线） 视频/音频支持系统或视频/音频核心部件：多媒体计算机的核心软件 专用芯片的类型 固定功能的芯片 可编程的处理器 数字化后的音视频处理：数据量大，需要高速度和输入实时性 片上系统 （SOC，System on a chip） 单核 &amp; 多核处理器 处理：CPU，GPU，OPU（加速处理器）， ASIC（专门集成电路），FPGA（现场可编程逻辑门阵列），Gate，TPU（张量处理器，专为TensorFlow设计） 传输：总线（CPU内部，系统总线，I/O总线） 数据压缩和编码技术 计算机三大板块：处理，存储，传输 多媒体压缩：必要性（有需求），可能性（大量冗余） 统计冗余的分类：时间冗余，空间冗余 多媒体网络与分布式处理 分布式：分布而自治，逻辑上整体 超媒体=多媒体+超文本 SAN：存域网 虚拟现实技术 生成 交互：人用自然技能与虚拟实体进行交互 集成各领域：计算机图形学，AI，人机交互，传感与测量技术，仿真，微电子 网购：事务处理，I/O密集型任务 ISDN：综合业务数字网络 人擅长形象思维：通过视听获取信息 Chapter2 多媒体计算机系统的组成组成：声卡，视频编码卡，图像采集卡 常用I/O设备鼠标 点输入，伴随GUI（图形用户接口）的普及；机械鼠标 、光学鼠标 性能指标：分辨率，与计算机的接口（串口，USB口，PS/2口） 条码设备 图像扫描仪 图像输入设备 触摸屏 电阻式 电容式 红外线式 声表面波式 显卡 作用：对图形函数进行加速 显存：32MB或64MB 绘图仪 打印机 通信设备调制解调器（Modem） 利用模拟信号传输线路传输数字信号 网卡（NIC，网络接口卡） 工作原理：整理主机发往网线上的数据，并将数据分解为适当大小的数据包之后向网络发送出去 局域网最基本的部件之一 作用：流量控制，矫形 传真卡 存储设备云计算 ：并行计算+虚拟化 并行不容易实现：比如若程序是串行的，必须顺序实现，故很难实现 虚拟存储：软件实现，把物理上相互独立的存储模块用软硬件集中起来管理，形成逻辑上的虚拟存储单元从而使主机访问。 物理：面向计算机，逻辑：面向用户 虚拟存储分为对称式和非对称式 作用：提高存储利用率，降低成本，简化存储管理 USB设备USB（Universal Serial Bus，通用串行总线）： 主要特点：即插即用，可热插拔，自动配置 硬件结构 软件结构 数据流传输 数字摄像设备 CCD（Charge Coupled Device，电路耦合元件） CMOS（金属氧化物半导体元件，Complementary Metal-Oxide） Chapter3 数字图像处理技术信号处理的基本术语采样与量化 信号的数字化处理包括2个步骤： 采样：信号在时间上的离散化 量化：信号在幅度上的离散化，用二进制描述 DFT和IDFT DFT（Discrete…，离散傅里叶变换）：将时域信号转变成频域信号的一种数学方法 IDFT（Inverse）：反变换 小波变换 小波：在有限周期内的波形，它的平均值为0 正弦波是傅里叶变换的基础，无限定周期，平滑且可预测；而小波信号是不规则不对称的 小波变换：将信号转变成很多不同比例的小波的叠加 优点：解决锯齿效应和噪声 图像数据压缩基础色彩 从人的视觉系统来看，色彩：色调，饱和度，亮度【H(hues)表示色相，S(saturation)表示饱和度，B（brightness）】 图像深度：位图中记录每个像素点所占的位数，决定彩图中可出现的最多颜色数 真彩色（RGB）：图像深度为24位； 伪彩色： 调配色：通过每个像素点的RGB分量分别作为单独的索引值进行变换，经相应的色彩变换表找出 各自的基色强度，用变换后的RGB强度值产生的色彩；与伪彩色一样都是采用查找表 显示深度：屏幕上显示的点的位数 色调和色相 视觉系统对颜色的感知 彩色空间及其变换 RGB颜色模式：对RGB进行8位数编码可形成16万种颜色 Lab颜色模式：发光率（luminance）+两个颜色轴（a,b） HSB 彩色空间的线性变换标准： YUV颜色模式 YIQ：适合计算机编码使用 ，Y是亮度信号 CMYK（把四种混合色作为颜色空间的基础，cyan、magenta、yellow、black）：靠颜色相减 ；彩色打印系统 颜色模型的色域：一个色系能够打印的颜色范围 图像的种类 标准单色图，标准灰度图 256色标准图像，24位标准图像 图像数据压缩的可能性：冗余（rebundancy） 统计冗余：统计特征上的冗余； 动态图像序列——时间冗余（帧与帧的重复） 静态单帧图像——空间冗余 信息熵冗余： 熵（entropy） 信息熵：定义为一组数据所表示的信息量，$E=\sum_{i=0}^{N-1}p_ilog_2p_i$，N为数据的种类(码元)个数 结构冗余 知识冗余 视觉冗余：图像中存在很多人眼感知不到的细节，人眼的分辨力为64灰度级，而图像为256灰度级 图像压缩算法 评价压缩算法的指标： 压缩比：压缩前后数据大小的比值 算法复杂性和运算速度 失真度 压缩算法分类： 无损编码：无任何失真 有损编码：有一定偏差或失真，但不影响视觉或听觉效果 行程长度编码 香农-范诺算法 哈夫曼编码（需要统计概率；没有香农精确） 算术编码 词典编码 预测编码：根据像素间的相关性来预测像素的灰度；分为线性预测编码（DPCM——差分脉冲编码调制） + 非线性预测编码 变换编码：将时域信号变换到频域信号上进行处理 模型法编码：利用CV和计算机图形学知识对图像信号的分析与合成，对特定图像建模，并根据模型确定特征参数 图像文件的一般结构 图像图形格式都是代码组成的：文件头+文件体（压缩算法，图像数据）+文件尾 常用图像、图形文件的格式 矢量图和位图： 矢量图：指令绘图 位图：点阵绘图 GIF TIF格式：Tagged Image Format File文件，支持任意大小的图像，从单色二值图到24色真彩图 PNG：把图像文件压缩到极限以利于网络传输，但能保证图像品质 JPEG标准：变换编码 2种工作方式：顺序方式 &amp; 渐进方式 实现方式：谱选择法；逐次逼近；阶梯方式 3种级别编码算法：基本系统、扩展系统、无失真系统 基本系统算法：DCT（离散余弦变换） JPEG2000：高压缩率——离散小波变换（DWT）；无损压缩——预测法；渐进传输；感兴趣区域压缩 动态图像压缩 动态图像分类： 视频 动画 特点：连续性，时延性，相关性 MPEG1标准： 用于多媒体存储与再现，如VCD MPEG数据流采用分层结构： 采用2种帧间编码技术：预测 &amp; 插值 MPEG算法的2个基础：基于16x16的运动补偿的缩短时间冗余；基于DCT的缩短空间冗余 MPEG考虑3种画面：内帧（I） &amp; 预测帧（P） &amp; 内插帧（B） P通过I获得（前向预测），B通过I和P获得（双向预测） 原因：考虑随机访问视频的重要性；运动补偿插值可显著降低位速率 运动补偿预测 缩短空间冗余度 MPEG2标准： 支持DVD 基本算法与MPEG1一致 与MPEG1的区别：支持电视的隔行扫描格式；支持可分级的可调视频编码，可提供多种质量的视频业务 H.261 标准：H系列主要用于视频电话会议 视频压缩算法核心：运动估值预测 &amp; DCT编码 用于视听业务 CIF格式 H.263 标准： 在现有的电话网上传输动态图像 基于预测差分块编码系统 帧内编码——DCT变换、哈夫曼码 帧间编码——运动估计和补偿，只对预测误差编码 QCIF格式 Chapter4 音频音频编码基础音频处理 音频传播媒体特征 音频信号：数字 &amp; 分析-合成 音频编码基础 频带宽度：频带越宽音质越好 人耳听到的频率：20HZ ~22KHZ 信噪比（SNR）：有用信号与噪声之比 度量方法：主观度量法 音频信息分类：不规则声音（指不携带信息的噪声） &amp; 规则声音（语音、音乐、音效） 声波：模拟信号，可分解为一系列正弦波的线性叠加 基频与音调： 音频三要素：音调，音色 音调由频率 w 决定 谐波 泛音：$nw_0$ 称为$ w_0$的高次谐波分量 音色由混入基音的泛音所决定 频带：频带宽度 音频信号处理方法：采样（时间离散化）——量化（频率离散化） 音频存储格式：.wav（波形格式） 声音质量的度量：用带宽度量 DAT——CD——FM——AM——数字电话（由高到低） 度量方法： 信噪比（SNR）：客观度量 平均意见得分（MOS，mean opinion score）：主观意见得分，1~5（失真级别反映质量） 音频信号压缩技术脉冲编码调制（PCM） 声音数字化：采样——量化 量化：均匀量化 &amp; 非均匀量化 增量调制（DM，data modulation）：属于预测编码技术 原理：对实际采样信号与预测采样信号之差的极性值进行编码，编码值只有0或1，故又称1比特编码 存在的问题：斜率过载，粒状噪声 自适应脉冲编码调制（APCM）： 根据输入信号 差分脉冲编码调制（DPCM，Differential Pulse code modulation） 自适应差分脉冲编码调制（ADPCM）：预测编码 原理 子带编码（ SBC） 音频编码标准ITU-TG （国际电信同盟）系列声音压缩标准 G.711：1972，为电话质量的语音压缩而制定，使用 PCM G.722：1988，为调幅广播质量的音频信号压缩而制定，使用 SBC + ADPCM 相继编码 G.723：1996，使用多脉冲激励最大似然量化算法（MP-MLQ），用于可视电话和 IP 电话系统 G.728 G.729 比较 MP3（MPEG-Layer3）压缩技术标准 可伸缩性 MP4 压缩技术 以“知觉编码”为关键技术的 a2b 音乐压缩技术，压缩比达15：1 MIDI（乐器数字接口） MIDI 是一个协议，只包含用于产生特定声音的指令（调用乐器音色、声音强弱、持续时间等） 声卡 多媒体：集成 + 交互 声道：单声道，立体声，四声道环绕（左声道、右声道、左环绕、右环绕），5.1声道 语音识别（sound Recognition） 难点：非特定人群、特殊场景的识别（如感冒患者的声音、各种口音、噪音环境） 发展历程 技术：特征提取，模式匹配 语音识别单元的选取：单词，音节，音素 模式匹配技术： 动态时间规正技术 ANN 语音识别系统的类型： 根据对说话人的依赖程度可分为：特定人识别识别系统（加语音签名），非特定人识别系统 孤立词识别，连续词识别 应用：数据库输入和询问应用，语音命令和控制应用 Chapter5 光盘存储原理和相关标准CD-ROM VCD：更小的坑道，更紧密的空间轨道 DVD BD-ROM 操作系统：文件，进程 超文本：以非线性的网状结构组织信息 基于内容的信息检索（CBR）： 根据媒体对象的语义和上下文联系进行检索 信息：图像，视频，音频 基于内容的视频检索： 基于关键帧的检索 基于运动的检索 人的敏感声频：20 HZ ~ 20/22 KHZ PC 机处理人耳能听到的音频的频率范围是20HZ ~ 44.1KHZ（PC 机要乘以2） MIDI：乐器数字接口，一种通信协议 采用多媒体技术的主要目的：增强计算机的处理能力 Wavelet：小波变换 JPEG 编码： DCT——离散余弦变换；变换编码，时域变频域 在量化环节会损失数据：当频率系数经过量化后，将频率系数由浮点数转变为整数，这才便于执行最后的编码。不过，经过量化阶段后，所有数据只保留整数近似值，也就再度损失了一些数据内容 DCT 变换是最小均方误差准则下得出的次最佳正交变换 动态图像 —— 采用运动估计：运动估计的基本思想是将图像序列的每一帧分成许多互不重叠的宏块，并认为宏块内所有象素的位移量都相同，然后对每个宏块到参考帧某一给定特定搜索范围内根据一定的匹配准则找出与当前块最相似的块，即匹配块，匹配块与当前块的相对位移即为运动矢量。视频压缩的时候，只需保存运动矢量和残差数据就可以完全恢复出当前块。 MPEG：音视频的压缩标准（不是传输标准）；高达200：1的压缩比；帧间编码和运动补偿 MPEG4：最大特点是更强的交互能力，允许用户加入其中 视频点播： VOD：使用 RAID 技术（磁盘阵列） 提供实时数据流：流调度算法 Huffman：无损编码 图像三要素：色调（由波长决定），饱和度，亮度 声音三要素：音调（与频率有关），音强，音色（由混入基音的泛音决定） YUV：数字化位数为8：4：4 CMYK：相减色 MP3：MPEG-1 Audio Layer3 声卡 MIDI 音效合成的方式：FM 合成 ，波表合成 RGB 4：4：2可生成的颜色数为：1024（2的10次方，4+4+2=10）]]></content>
      <categories>
        <category>Computer Science</category>
        <category>Basis of Computer Engineering</category>
      </categories>
      <tags>
        <tag>Computer Science</tag>
        <tag>Basis of Computer Engineering</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《技术的本质》阅读笔记]]></title>
    <url>%2F2018%2F08%2F20%2F%E3%80%8A%E6%8A%80%E6%9C%AF%E7%9A%84%E6%9C%AC%E8%B4%A8%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Abstract：《The Nature of Technology》，核心想阐述的是技术是什么以及它是如何进化的。 Chapter1 问题Main Points：技术是什么，它从何而来，如何进化。 技术的循环：技术总是进行这样的循环，为 解决老问题去采用新技术，新技术又引起新问题，新问题的解决又要诉诸更新的技术。 技术理论的缺失：技术一直处于科学的阴影中，那些认真思考技术的人的大多数是社会学家和哲学家。但他们往往 从技术的外部将技术当做独立的客体来看待，只能告诉我们技术如何进入经济生活，而技术的“黑箱”被藏了起来，其内部无法展现。 组合进化：技术一定是来自此前已有技术的新的组合。技术还来自于对自然现象的捕捉和征服。 Chapter2 组合与结构技术模块化：将技术进行功能性分组，可大大简化设计过程。 技术的组合与递归特征。 技术是实现目的的一种手段，技术提供功能。 技术结构的形成 技术结构：由零部件组成的组件系统或模块，其中有核心集合与次集合。 将技术的构件模块化可以更好地预防不可预知的变动，同时简化设计过程。但只有当模块被反复使用且使用次数足够多时，才值得付出代价将技术分割成功能单元。 模块化至于技术经济 vs 劳动分工之于制造业经济 类比传统工厂生产线：亚当斯密劳动分工理论，只有在生产的数量足够大的情况下，才值得将工厂的工作划分成专业工作。 技术的分解随着市场细分而加剧 递归性及其作用 技术组合原理：技术 = 集成体 + 系统 + 单一零件 技术具有层级结构，可从概念上将技术从上到下分解为不同的功能组件：主集成 + 次级集成 + 次次级集成，直至分解为原子部分 在经济的世界里，技术以一定的尺度存在。所有的技术都可作为组件为其他新技术服务。 可重构性和可复用性 技术是流动的，永远不会静止，不会完结，不会完美 高度可重构 Chapter3 现象Main Point：技术如何利用现象？ 技术是对现象有目的的编程。 技术总是需要依赖于某种可被开发或利用的自然现象。 技术指向某种目的，总是利用了某些从现象中挖掘出的核心效应。 基因是生物进化的基本单元，类似地我们可以把现象称为技术的基因。生物对基因加以编程从而产生无数的结构，技术对现象加以编程从而产生无数的应用。 现象是以累积式建构起来的 技术的本质：技术是被捕获并使用的现象。 有目的的系统（Purposed System）：所有“实现目的的手段”的总体 技术与科学 科学是一套实践和思维方式，包括理论化、想象、猜测；科学建构于技术 技术从科学与自身经验两方面建立起来 Chapter4 域域：为了共享现象族群和共同目标或分享同一个理论，个体技术就会聚集成群。工程设计时从选择某个域开始的，这个自动和下意识的选择过程叫做“域定”。设计过程就如同语言表达。 共享同一个理论：比如统计软件包拥有的共同假设前提是抽样在总体上是正态分布的。 前提：存在使各种元素能共同工作的能力 设计即表达：语素以一定的规则（语法 ）组合在一起 好的设计就像一首好诗，从众多可能性中为每个部分作出完全正确的选择 Chapter5 工程和对应的解决方案内部视角看技术 技术如何在其生命周期内进行自我修正 设计是关于解决方案的选择 一个解决方案若被使用的次数足够多，它就成为了一个模块，并因作为适用于标准用途的模块而具有包容性，它自己也成为了一项技术 Chapter6 技术的起源发明：发现合适的可行性解决方案。 技术的两种发展机制： 内部替换（internal replacement）：使用更好的部件或子技术更换某一形成阻碍的部件 结构深化（Structural deepening）：寻找更好的部件、材料或加入新组件 当内部替换和结构深化都不能再提高性能做什么的时候，技术就成熟了。这时，若想取得进一步的发展，则需要一个全新的原理。 在新旧原理更替的过程中，旧原理往往已经被锁定了，有4个原因导致旧原理通常会存在较长的时间: 经过精致、繁复的过程之后，已经成熟的旧原理会表现得比它的新对手好。 采用新原理可能意味着改变周围的结构和组织，因为成本过高，所以可能不会被实现。 从业者不认可这个新原理带来的愿景或承诺 新原理将使旧知识过时，它在潜在的新原理与安全的旧原理之间制造了一种认知失调以及情感上的不匹配。 新的技术被促逼着陷入局限，并通过优化零部件和深化结构来获得提高。 自适应性延伸：新的和旧的解决方式之间的距离越大，对传统方式的锁定就越牢固。新技术被非常成功的旧技术所阻碍，技术上的转换不顺畅。 Chapter8 颠覆性改变与重新域定Chapter9 进化机制组合是新技术的潜在来源。组合的威力在于它的指数级增长。 Chapter10 技术进化所引发的经济进化众多技术集合在一起，创造力经济。经济从它的技术中浮现，不断从它的技术中创造自己，并决定哪种新技术将会进入其中。 Chapter11 我们的立场技术的生物属性 现代技术的本质的转变： 管理上：从优化生产过程到创造新产品 从以商品为基础的公司到以技术为基础的公司 从稳态操作到不断适应 文丘里谈建筑： 我喜欢的元素是杂交的而不是纯种的;是妥协折中的而不是一以贯之的;是曲折蜿蜒的而不是直截了当的;是模糊歧义的，而不是清晰缜密的。它们既客观又倔强，它们既无聊又有趣。它们是依惯例传统的而不是设计出来的;是随和迁就的而不是特立独行的;是冗余累赘的而不是简洁单纯的。它们既残缺不全又富于创新，是前后矛盾、模棱两可的而不是直接和清楚的。我赞同凌乱的活力优于明显的统我容纳不合理的结论。我赞成丰富和含意深长胜于含义清楚，我既赞同隐含的功能，又赞同外显的功能。 凌乱的生命力 连通性，适应性，进化趋势，有机性]]></content>
      <categories>
        <category>Product Manager</category>
        <category>Internet &amp; Product</category>
      </categories>
      <tags>
        <tag>Internet &amp; Product</tag>
        <tag>Reading Notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《谷歌和亚马逊如何做产品》阅读笔记]]></title>
    <url>%2F2018%2F08%2F20%2F%E3%80%8A%E8%B0%B7%E6%AD%8C%E5%92%8C%E4%BA%9A%E9%A9%AC%E9%80%8A%E5%A6%82%E4%BD%95%E5%81%9A%E4%BA%A7%E5%93%81%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Abstract：比较完整地介绍了 PM 工作的流程和经验，算是PM 入门级的书籍。 Main Points：软件交付 亚马逊和谷歌如何交付软件：按照项目流程顺序，包括用户需求研究、用户体验设计、项目管理、测试、发布 团队主管带领团队成功交付所需要的技术积累、最佳实践、技能 Part1 交付卓越产品，步步为“赢”框定了研究范围和对象， Chapter1 赢在使命和策略使命：能够唤起兴趣 + 指明方向 + 适合印在T恤上 策略：在竞争对手的压力下，利用公司独特优势来争取目标用户的粗略计划。 Google Talk：使人与人之间在任何地点均能通过任何终端沟通。 必须想清楚：如何长期为客户提供比竞争对手更优质的产品（长期的竞争优势）。否则，竞争对手会迅速模仿一个跟你一模一样而价格更低的新产品打败你。 Chapter2 产品定义产品定义 撰写公开稿：使各方了解产品 创建并不断更新FAQ文档：记录争议点和重要细节 绘制流程图和线框图 撰写产品单页或10min演示文稿：写给老板、投资人看的 写API文档 PRD 邀请设计和技术来参与产品评审：找出边界情况 找客户测试产品概念 命名、定价和收益 收益模型 1.估算买家总体市场规模 2.预估市场规模的增速：市场规模扩大时，销售规模也随之扩大 3.估算你的目标市场占总体市场的比率 4.估算通过市场推广你能触碰到的用户规模 5.预估触碰触碰的人中有多少会转化成产品用户 6.找到其他新用户增长渠道并加入到模型中 7.将产品定价乘以每个时期的用户数 = 收益；若产品是付费订阅类，还可预估一个续费率然后计算利润 用户体验（UX）：关注用户如何完成任务和如何优化向用户展现信息的方式 信息架构（IA）：研究信息如何在用户界面中呈现，而不关心底层数据结构 视觉设计（Visual Design）：根据既定的信息架构美化界面 用户体验研究（UXR）：研究用户如何看待产品 通过研究获得关于产品成败的统计显著性数据 但不提供解决方案 6个用户体验问题 该用户界面要求用户完成的最重要的任务是什么? 这是最筒单的解决方案吗? 信息是否组织得当? 设计是否易用且一目了然? 标准是否一致? 能否减少用户点击次数? 如何与设计师沟通 1.以用户的口吻说话 2.以提问的方式建立共识 3.反复讲述业务目标，若有些目标相互冲突，则反复江苏它们之间的相对优先级 Chapter4 项目管理1.创建一张简单的计划表并持续维护 包含：任务列表 + 每个任务的工程评估量（即完成所需时间） 将任务按优先级顺序分配 2.跟踪BUG，观察燃尽图，计算实现零bug率的日期 反映bug数量随时间变化情况图，可预测产品何时能够交付 需要为不同严重等级的bug各绘制一条曲线 bug修复率：当修复率 &lt; 1 时，即每天修复的bug数量超过发现的bug数量时，才能 确定bug的具体规模并精准预测发布日期 3.谨慎管理你的依赖：最小化可能由依赖引发的风险 如果去除它也可以运转，那就去除它 如果内部能构建没救内部 构建 如果必须添加一个依赖，那就趁早添加 如果必须添加一些 依赖，那就依靠它上一个已构建的版本 如果交付的早，被依赖伤害的可能性就小 Chapter5 测试检查测试用例是否包含以下描述性要素 领域：描述哪部分用户 体验将被测试 严重性：定义若测试失败，你将会将此归为哪个级别的bug，通常1~4级 前置条件：指定测试人员在测试前必须做的事，比如测试购物车中信用卡验证过程，前置条件应该是用户已登录、购物车已添加货品、送货地址已填写，然后测试才能开始 需执行的任务：任务由多个步骤组成，是测试的主要内容。任何步骤失败测试都失败。 后置条件：描述应用程序在任务执行完毕后所处的状态。以上一个例子为例，后置条件可能是用户看到一个包含确认编号的确认页面，同时信用卡会在10秒内扣除相应的金额。 测试用例表格： 若时间不够宽裕，可每轮测试只执行高严重性的测试用例。 关键评审内容： 1.关键用户体验 2.安全和隐私 3.依赖：一定要严格测试一些非内部构建的数据库、第三方服务和软件 自动化测试： 搭建一套独立于产品代码的测试系统，事半功倍，大大节省人力 推行内部试用： 观点： 欲卖狗食，必先食之 强制性试用 悬赏奖励发现bug的人（T恤之类的奖励很吸引人） 实践操作： 计划一次内部试用发布 使其他使用者能方便地提交bug报告 让内部试用成为团队文化 软件发布后应继续进行内部试用 准确且有条理地处理bug 基于频率、严重性、解决成本对bug分级 每天与开发和测试主管碰一次，评审新增bug 不断施压以减少bug出现 bug评审会议 确定通用的bug评判标准：比如我的大学同学遇到这个bug后我会因此羞愧吗？遇到这个bug后会对它们产生持久伤害吗 先处理最严重的bug 限定会议时长：若会议超出时长则将剩下的安排在明天继续讨论 只围绕频率、严重性、修复成本来讨论 讨论每个bug的世界不要超过一分钟：避免过于细节的讨论，没人喜欢处理bug这件事 发挥可信测试者的作用 可信测试者：产品发布前的试用用户 操作实践： 签署保密协议：主要保证你有权在产品中使用来自客户的改进建议 制作粗略的新手指南文档 创建一个包含整个工程团队的邮件组：让工程团队尽可能接近用户 让用户使用与工程师同版本的试用产品 调研可信测试者：做问卷调研 通知可信测试者产品新版本 以新用户的方式来使用整个产品 Chapter6 量化一个优秀的量化指标需具备的5个关键特点： 测量成本低廉 测量可靠且可重复检验 能频繁测量，最好能实时测量 团队能根据它做出明智的改变：能够知道在一个量化数据发生变化后知道该做些什么 专注于客户：尽可能选取贴近客户的数据和指标，尽可能测量客户体验的末端（如软件启动量 优于 下载量，因为下载量只反映产品营销做得如何，但启动量反映用户参与度和增长情况） 执行力指标：零bug到达如期（反映开发进度，可实时更新，为团队指明方向） 三类需要跟踪的关键量化指标： 1.目标进度：反映目标的完成进度 比如：7天活跃用户数（有时比日均用户数更合理，因为你很少会做一个预期用户每天使用的产品），30天活跃用户数，利润，注册量，下载量，安装量 2.经营绩效：反映产品问题在哪里以及如何提升用户体验，通常以比率表示 比如：转化率、参与度；7天平均发帖量，7天用户平均停留时间 避免虚荣指标：即总是在增长的指标，因为它们不会帮助你发现产品的问题在哪儿 3.系统性能：性能指标，说明产品的实时健康度 比如：99.9%平均延迟、每秒请求数、并发用户数、每秒下单数、其他基于时间的指标 从统计过程控制（SPC，Statistical Process Control）视角考察性能指标 专注于目标本身，忽略细枝末节 几乎所有的指标都可以通过一些巧妙的手法进行操纵。以之前举过的发布日期这个例子为例，为了将发布日期提前，我们可以将更多Bug归到“不影响发布”这一类去，也可以简单地终止测试(表面上看还是一个“双赢”的做法)。 Chapter7 发布几个主要的发布步骤确保发布质量： 1.对改动说不 “发布手中有的（即使不完美），而非脑中想的”，否则永远完成不了交付 通过核查团队是否对目前的产品抱有自豪感来确定产品是否可以发布 建立一个发布后第一时间（IPL，immediately post-launch）修改的需求清单：认知到有些改动不一定要在发布前完成；这样做可以让团队更有信心，因为他们知道他们顾虑的东西会很快被解决 对改动说不的主要原因：几乎所有对代码的改动 都会引发新的风险，如引入新bug或重新引入老bug（回归bug） 可以维护一套发布分支：则有两个需要并行维护的分支（开发分支+发布分支），打算发布的版本在发布分支上，新特性的开发则在开发分支上进行；当出现安全、隐私或重大性能问题时可以基于发布分支短时间开发一个“热修补补丁” 2.开启作战室 效率，可以每日例会，紧迫气氛 狗屎三明治：当给员工反馈时，可以先表扬他们（第一片面包），再给他们一些难以完成的挑战（狗屎 夹心），最后提醒他们你有多么看好他们的能力（第二片面包） 3.营造紧迫的气氛 4.核查发布清单 发布清单：确保软件发布中所有需要跟进的事项都被有序安排且被详细描述 清单项：领域，事项，负责人，状态，预期完成时间，参考信息 5.撰写博文 阐述你的使命、目标可和和你能解决的问题 6.发布软件 实验性框架：部署实验发布版测试新特性 不要选择在周五或临近假期发布：若要改bug，则会让员工和自己牺牲假期时间 7.亲自验证软件 一轮验证：版本验证测试（BVT，Build Verification Test） 一般在新版本发布后进行 目的：确保推送到生产服务器上的版本是正确的，且所有配置文件都已推送成功并安装正确 二轮验证：以新用户的身份亲自体验整个产品，确保所有功能都能正常使用，有些功能常会出现问题，如注册流程、上传数据、搜索、表单提交 … 8.应对发布后带来的各种影响 应对回滚： 回滚：指把软件撤回到预发布状态 只要成功回滚，没有对用户产生明显伤害，就还没失败 最好的防守是制定周详的撤退计划 若可回滚，则撤回对产品的改动，从容地修复问题，然后再试一次 处理产品危机 比如网站被瞬间大规模的流量冲垮，安全漏洞，隐私侵害，定价事故 建立有效的沟通渠道 不要惊慌 —— 评估影响范围 尝试回滚 推迟任何产品推广计划 知会相关方 知会用户社区 保持BUG的更新 危机收尾：撰写事故调查报告（发生了什么？谁受到了影响？问题是何时出现、何时解决？为什么会发生这个事情？如何防止问题的再次发生？） 演示产品： 必须简洁，不超过 10 min 强调使命 以讲故事的方式 应对媒体 庆祝发布： 感谢整个团队 公开嘉奖要谨慎，防止打击其他团队的信心 危机事故调查报告示例： Part2 掌握卓越技能，更胜一筹Chapter8 胜在团队雇佣主管Key：组建一个杰出的团队 优秀的人总是聚集在一起工作。 雇佣PM至关重要：雇佣错误的PM会让整个团队痛不欲生，就像有一把撒了盐的刀每天捅一次工程团队的集体肾脏。这个错误的人选会将产品带向糟糕的方向，会欺上瞒下，会拖延任务，或者虽然完成得很快但质量奇差，还会引发工程团队内部的极大焦虑。不消说，你一定要谨慎地选择，宁可不雇，也不能雇佣错误的PM。 软件业务的四种角色： 1.项目集经理：比PM更少关注业务，比项目经理更少关注项目的技术 角色；key function是黏合和润滑整个团队 2.PM：侧重软件业务；做除了写工程代码之外的任何事 MBA出身的PM：专注品牌管理、定价、市场 进入策略等 3.项目经理：排定项目计划和协调团队 4.工程经理 雇佣主管的5个原则： 1.雇佣比你聪明的人 核查简历 绩点 关键加分项：实际推出过很多产品 2.雇佣懂得自己不是来当老板的人 3.雇佣表达清晰、言之有物的人 4.雇佣习惯用数据说话的人 5.雇佣充满活力的人 用数据说话实例： 收购公司收购一家公司的目的： 知识产权：需要计算是自己研发的成本低还是收购别人的成本低，计算出可接受的收购额；必须检查让一个未来不会涉足这个业务 的高级工程师去检查这家公司的代码质量和架构 人才：关键人才，好的雇员，多余人才 客户：客户转移的过程可能会流失 防御（让别人没法买它或让它不再跟自己竞争） 收购建议： 将你的团队的部分人员调入他们团队：商业文化、商业实践的感染；确保新老团队紧密融合；调入最优秀的主管很有利 计划整合产品 了解之前所有的交易和负债 Chapter9 技术四个S ：服务器（Server），服务（Service），速度（Speed），扩容（scaling） 1.服务器（Server）服务器（Server） 优秀的工程师会学习如何避免去做运维的琐事，因为任何学到的关于服务器的知识都会在6个月内过时 尽量不要买服务器，采用托管主机更省心 系统的三层架构： 展现层（JavaScript，CSS，HTML） 业务逻辑（Java，C++） 数据（SQL，S3） 2.服务（Service）API PRC（Remote Procedure Call）：远程过程调用 SOA（Service-Oriented Achitechture）：面向服务的架构 3.速度Key：解决服务链的速度问题 封装（Encapsulation）：将不相关的功能分开封装，则可以并行加载，提高速度 缓存（Caching）： 缓存是数据源的一份副本，比如页面缓存、XML缓存、硬盘缓存，可以缓存任何东西 缓存可从后备存储器中复制数据 缓存缺失：即服务所需的数据不在缓存里 优秀的缓存机制：会通读后备存储器并传回数据，而劣质的不会传回任何数据 缓存更新：当数据存储中的某个值发生 变化，它必须先写入后备存储器，然后后备存储器必须更新所有缓存。在劣质缓存机制中，这会导致缓存的不一致性。 扩容如何询问正确的技术问题1.能请你给我画张系统图么？ 目的：理解系统图中的所有方框都是干什么的 存放了什么数据 ，干了什么，接收和发送了什么数据 2.结果从这个方框传到那个方框的延迟是多少？ 通过系统图识别服务链，询问这类设计的必要性，并 弄清楚整体相应时间是多少和最坏的情况下是多少 ​ 3.可以扩容到N吗？ 4.去掉B方框有什么影响？ 了解系统会如何出错并如何恢复 了解系统的哪些部分会产生致命错误，并让工程团队优先考虑对保证这部分的稳定性进行投入 5.能通过缓存什么数据来提升性能？ 6.能通过独立加载什么数据来提升性能？ 比如独立加载广告系统，则即便广告系统坏了也不影响用户体验 Chapter10 沟通1.尽可能少开会，但不要不开会 能通过邮件解决的事情就不要开会 如果不能将事物简单地表达出来，你就没有真正理解它 2.写清爽的邮件 用建议取代质疑 主次分明，分点阐述 3.如何组织会议 会后立即发出会议纪要 允许改变开会的目的 不要发泄负面情绪 4.如何做好演示 控制在15min内 永远只传达一个核心信息 讲故事： 创意与生活联系 让观众跟着你的节奏走 例子必须使观众能够理解 描述你要解决的问题和你的解决方案、价值 制作综述单页 重点演示用户体验 极度专注倾听 讲故事实例： 5.团队合作沟通 不说你或我，聚焦在产品上 使用客观指标：比如决策矩阵（多项&lt;带权重的&gt;标准打分，然后取总分值最高的选择） 6.合理管理时间精力 分优先级，重要性 十大交付原则1.你不是来当老板的团队主管是仆人，他们存在的目的就是为了伺候工程团队 2.从用户角度出发。 3.用独特的方法解决很多人都有的大问题。 4.坏的消息就是好的消息。—杰克·韦尔奇 5.先寻求理解，再寻求被理解。——史蒂芬·柯维 6.构建最简洁的可用的产品。 7.交付手中有的，而非脑中想的。 8.无法测量的东西也就无法提升。—开尔文勋爵 9.你不可能做完所有工作，所以你应首先做那些只有你能做的工作。 10.永远走在交付的康庄大道上。 必须的工件■轮值表—将寻呼器号码和手机号码的清单复制到一张钱包大小的纸上。 ■使用Wiki搭建的“联络簿”，用于遇到故障、突发事件或问题时寻找相关负责 人。这个列表应该包括法务、公关、市场、产品团队、工程团队和网络运维(或 者任何负责生产基础设施的部门)的负责人和联系信息。 ■描述使命的语句。 ■关于未来两年的清晰策略。 页简要说明产品的人物/事件/原因/时间/方法的文档 ■产品需求文档，或者叫功能规格说明。 口新闻稿。 ■线框原型图或者餐巾纸草图。 ■内部FAQ文档，其中部分间答打上外部FAQ标签以作为客户支持内容的原始 素材。 ■沟通文档，包括关键信息、有潜在危险的问题和对这些问题的回应 ■发布时穿的T恤衫。 ■包含测试时间的开发计划表。 ■未来两年的路线图。 ■内部用户列表和迁移时间表(适用于基础设施项目)。 ■可信测试者列表(适用于面向外部的产品) 特性需求列表，并将内部和外部客户中呼声最高的三个特性需求高亮。 ■开放问题列表，并清晰标记这些问题的状态。 进行中的会议纪要。最好建一个文档保存项目所有的历史会议纪要。 发布计划或发布规程。 ■记录什么特性在什么时间发布的生成变更列表。在排查客户问题时特别有用。 ■对增长预测和硬件配置需求提前进行计划的生产设计文档。 ■专利注册文件、商标注册文件和版权申明文件 ■隐私说明。 ■出色的数据指标—包含内部的状态面板和一些供外部消费的清洗过的数据指标。 ■为幻灯片、演示、评审、发布准备的产品截图。 口团队本季度目标以及上季度目标完成情况。 ■Bug状态面板和阻碍每个发布的Bug列表。 ■错误原因报告或事后调查报告。 ■会议纪要和团队周会、用户界面评审、产品评审、工程评审、Bug处理、法务 评审、业务拓展周会以及客户支持碰头会的时间计划表。]]></content>
      <categories>
        <category>Product Manager</category>
        <category>Internet &amp; Product</category>
      </categories>
      <tags>
        <tag>Internet &amp; Product</tag>
        <tag>Reading Notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Machine Learning Yearning》阅读笔记]]></title>
    <url>%2F2018%2F08%2F20%2F%E3%80%8AMachine-Learning-Yearning%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Abstract：吴恩达2018的新书，主要讲解了机器学习模型训练过程中的一些重要的技巧，可帮助提升机器学习工程的准确率和效率，挺实用的 ~ Part1 Getting Started1.1 机器学习应用的改进策略1.数据方面： 数据量 ++ 数据多样性 ++：收集更加多样化的训练数据集，比如处于不常见位置的猫的图片，颜色奇异的猫的图片，以及使用不同相机参数拍摄的猫的图片 2.算法方面： 迭代：增加梯度下降的迭代次数，使算法训练得久一些 NN：尝试一个拥有更多层（layer）/更多隐藏元（hidden units）/更多参数（parameters）的，规模更大的神经网络 正则化：尝试加入正则化（例如 L2 正则化） 改变神经网络的架构（激活函数，隐藏元数量等等） 1.2 策略选择的重要性时间、效率，领先与被领先 1.3 Key Value of this bookKey Value of this book：深层次了解“如何在机器学习项目中设定一个技术方向”。 1.4【补充】线性回归 vs 逻辑回归1.回归算法：通过最小化预测值与实际结果值之间的差距，而得到输入特征之间的最佳组合方式的一类算法 线性回归：对连续值的预测，一元或多元 逻辑回归：对离散值、类别值的预测，解决分类问题 2.线性回归（linear regression）： 损失函数：衡量参数选择的准确性，值越小，损失越小；凸函数，可使用凸优化方法最小化损失函数的值 梯度下降：逐步最小化损失函数的过程，如同下山的过程，找准下山方向（梯度），每次迈进一步，直至山底。 学习率α：决定下降节奏的快慢；过大则可能导致震荡，过小则导致收敛很慢 欠拟合：函数假设太简单导致无法覆盖足够的原始数据，可能造成数据预测的不准确 过拟合：函数假设太复杂导致泛化能力变差，无法有效预测新样本。解决方法： 减少特征个数：手工选择保留特征、模型选择的算法选择特征 正则化：L2正则化即在原来的损失函数中加入θθ的平方项，来防止波动太大，留下所有的特征，但是减少参数的大小 3.逻辑回归（logistic regression）： sigmoid 函数：可以把任何连续值映射到[0,1]之间，数越大越趋向于0，越小越趋向于1。$g(z)=\frac{1}{1+e^{-z}}$ 以概率的形式输出结果，不只是0和1的判定 1.4 规模驱动机器学习发展规模： 数据规模：数据可用性（data availability） 计算规模（computational scale） 学习算法的学习曲线： 海量数据对旧算法的性能提升作用不明显（趋于平稳，瓶颈，学习曲线变平缓），如 logistic regression；而NN 则可能获得较好的性能表现 对于小数据集，旧算法和 小型 NN 的性能表现可能差不多，此时特征工程对旧算法的表现的影响会更大 NN 规模越大，性能表现会越好 如何获得最佳性能表现： 训练大型 NN 拥有海量数据 其他细节：NN 的架构 Part2 Setting up development and test sets（建立开发集和测试集）2.1 开发集和测试集1.如果训练和测试数据集与实际数据的差异较大，则算法可能无法很好地泛化到实际数据的分布（actual distribution） 2.训练集（training set）：运行学习算法 开发集（development set）：又称留出交叉验证集（hold-out cross validation set），用来对训练集训练出来的模型进行测试，通过测试结果来不断地优化模型（用于调整参数、选择特征、以及对学习算法作出其他决定） 测试集：最终评估算法性能，但不会据此决定使用什么学习算法或参数 3.划分比例：小数据——7：3或8：2；大数据——98：1：1 4.处理方法：合理地设置开发集和测试集，使之近似模拟可能的实际数据情况，并处理得到一个好的结果；使用实际的用户数据对开发集和测试集进行更新。 2.2 开发集和测试集应该服从同一分布1.一旦定义了开发集和测试集，你的团队将专注于提高开发集的性能表现 2.开发集和测试集的分布不同将导致： 在开发集上过拟合 测试集比开发集更难进行预测，尽管算法做得足够好了，却很难有进一步的改进空间 测试集不一定更难预测，但与开发集性质不同（分布不同）。因此在开发集上表现良好的算法不一定在测试集上也能够表现良好。如果是这种情况，大量改进开发集性能的工作将会是徒劳的 策略：选择服从相同分布的开发集和测试集数据，这会让你的团队更有效率 2.3 开发集和测试集应该有多大1.开发集规模 考虑区分度：开发集的规模应该大到足以区分出你所尝试的不同算法间的性能差异。例如，如果分类器 A 的准确率为 90.0% ，而分类器 B 的准确率为 90.1% ，那么仅有 100 个样本的开发集将无法检测出这 0.1% 的差异。 统计显著性检验：从理论上说，还可以检测算法的变化是否会在开发集上造成统计学意义上的显著差异，但通常没有发现有多大差异 提高开发集规模：有利于检测到不易察觉的效果提升 2.测试集规模 规模应该大到使你能够对整体系统的性能进行一个高度可信的评估 一种常见的启发式策略是将 30% 的数据用作测试集，这适用于数据量规模一般的情况（比如 100 至 10,000 个样本） 我们并不需要远超过评估算法性能所需的开发集和测试集规模，即开发集和测试集的规模并不是越大越好 2.4 使用单值评估指标进行优化1.单值评估指标（single-number evaluation metric）： 如分类准确率：你在开发集（或测试集）上运行分类器后，它将返回单个数值，代表着被正确分类的样本比例。根据这个指标，如果分类器 A 的准确率为 97％，而分类器 B 的准确率为 90%，那么我们可以认为分类器 A 更优秀。 优点：使算法优劣的评价更直观简单 2.多值评估指标： 如查准率（precision）和查全率（recall） 缺点：使算法之间的优劣比较变得更加困难 Solution：使用合并的单值指标来评估，比如取简单平均或加权平均、调和评价。例： F1 Score是计算二者的调和平均数，为2/((1/Precision)+(1/Recall)) 2.5 优化指标和满意度指标1.当多值属于不同类型的值时，则不太合适将它们合并成单个指标，如准确率与运行时间。 2.Solution：先设置阈值，在阈值的范围内最大程度优化指标 首先定义一个“可接受的”运行时间，一般低于 100ms 。接着在限定的运行时间范围内最大化分类器的准确率。此处的运行时间是一个“满意度指标” —— 你的分类器必须在这个指标上表现得“足够好”，这儿指的是它应该至多需要 100ms，而准确度是一个“优化指标”。 如果考虑 N 项不同的标准，比如模型的二进制文件大小（这对移动端 app 尤为重要，因为用户不想下载体积很大的 app）、运行时间和准确率。你或许会考虑设置 N-1 个“满意度”指标，即要求它们满足一定的值，下一步才是定义一个“优化”指标。例如为二进制文件的大小和运行时间分别设定可接受的阈值，并尝试根据这些限制来优化准确率指标。 举例：假设你正在设计一个硬件设备，该设备可以根据用户设置的特定“唤醒词”来唤醒系统，类似于Amazon Echo 监听词为 “Alexa”，苹果（Apple） Siri 监听词为 “Hey Siri”，安卓（Android） 监听词为 “Okay Google”，以及百度（Baidu）应用监听 “Hello Baidu.” 我们关心的是假正例率（false positive rate）—— 用户没有说出唤醒词，系统却被唤醒了，以及假反例率（false negative rate）——用户说出了唤醒词，系统却没能正确被唤醒。这个系统的一个较为合理的优化对象是最小化假反例率（优化指标），同时受到每24小时不超过一次误报的约束（满意度指标）。 2.6 通过开发集和度量指标加速迭代开始建立一个 ML 系统：idea——&gt;code——&gt;experiment 尝试一些关于系统构建的想法（idea）。 使用代码（code）实现想法。 根据实验（experiment）结果判断想法是否行得通（在开发集上验证 idea 的可行性，判断依据就是度量指标）。（第一个想到的点子一般都行不通！）在此基础上学习总结，从而产生新的想法，并保持这一迭代过程。 2.7 何时修改开发集、测试集和度量指标1.度量开发集和测试集的选择的重要性，从而确定其优先级。准备初始开发集和测试集的时间长短可按优先级来确定，一周到数月不等。 2.快速选好开发集和测试集，可以帮助团队尽快制定明确 的研发目标 3.若发现初始开发集、测试集和度量指标与期望有一定差距，一定要快速想办法改进它们。 例如你的开发集与度量指标在排序时将分类器 A 排在 B 的前面，然而你的团队认为分类器 B 在实际产品上的表现更加优异，这个时候就需要考虑修改开发集和测试集，或者是你的评估指标了。 在上面的例子里，有三个主要原因可能导致开发集/评估指标错误地将分类器A排在B前面： 1）数据分布差异：你需要处理的实际数据的分布和开发集/测试集数据的分布情况不同。Solu：更新开发集和测试集，使之更具代表性。 假设你的初始开发集和测试集主要是成年猫的图片，然而你在 app 上发现用户上传的更多是小猫的图片，这就导致了开发集和测试集的分布与你需要处理的实际分布情况不同。 2）开发集上过拟合了：在开发集上反复评估想法会导致算法“过拟合”到开发集上。Solu：获取一个新的开发集。 当你完成开发后，应该在测试集上评估你的系统。如果你发现算法在开发集上的性能比测试集好得多，则表明你很有可能在开发集上过拟合了。 如果需要跟踪团队的进度，你可以每周或者每月在测试集上对系统进行一次定期评估。但不要根据测试集对算法做任何决定，包括是否将系统回滚到前一周的状态。坚持这样做会导致算法在测试集上开始过拟合，且不要再指望根据它对你的系统性能进行完全无偏估计（这对发表研究论文以及需要做出商业决策的人来说影响很大）。 3）该指标所度量的不是项目应当优化的目标。 假设你的猫咪应用当前的度量指标为分类准确率，而该指标认为分类器 A 优于分类器 B。然而在尝试了两种算法后，你发现分类器 A 竟然允许出现一些色情图片，这实在难以容忍。应该怎么办呢？ 此时的度量指标并不能辨别出算法 B 在实际产品的表现比 A 更好，因此根据该指标来选择算法就不那么可靠了，说明是时候改变现有的评估指标了。你可以修改指标，使之对出现色情图片的情况进行严重惩罚。强烈建议你选择一个新的指标并为你的团队制定一个新的目标，而不是在不可信的指标上耗费太多的时间后，最终回过头对分类器进行人工选择。 4.总结：及时修改开发集、测试集或者度量指标：在项目中改变开发集、测试集或者度量指标是很常见的。一个初始的开发集、测试集和度量指标能够帮助团队进行快速迭代，当你发现它们对团队的导向不正确时，不要担心，你只需要对其进行修改并确保团队了解新的方向是什么。 2.8 小结：建立开发集和测试集1.数据集与实际数据分布一致： 选择作为开发集和测试集的数据，应当与你预期在将来获取并良好处理的数据有着相同的分布，而不一定和训练集数据的分布一致。 2.开发集和测试集的分布应当尽可能一致 3.使用单值评估指标进行优化： 为你的团队选择一个单值评估指标进行优化。需要考虑多项目标时，不妨将它们整合到一个表达式里（比如对多个误差指标取平均），或者定义满意度指标和优化指标。 4.多次尝试不怕失败： 机器学习是一个高度迭代的过程：在最终令人满意的方案出现前，你可能要尝试很多想法。 5.通过开发集和度量指标加速迭代 拥有开发集、测试集和单值评估指标可以帮你快速评估一个算法，从而加速迭代过程。 6.根据重要性和项目排期确定“建立开发集、测试集和指标”的优先级 当你探索一个全新的应用时，尽可能在一周内建立你的开发集、测试集和指标，而在成熟的应用上则可以花费更长的时间。 7.训练集、开发集和测试集的比例划分 传统的 70% / 30% 训练集/测试集划分对大规模数据并不适用，实际上开发集和测试集的比例会远低于 30%。 8.开发集、测试集的规模需要足够做出高度可信且精确的评估 开发集的规模应当大到能够检测出算法精度的细微改变，但也不用太大；测试集的规模应该大到能够使你对系统的最终性能作出一个充分的估计。 9.有问题就及时修改开发集、测试集和评估指标： 当开发集和评估指标不再能给团队一个正确的导向时，就尽快修改它们： 如果你在开发集上过拟合，则获取更多的开发集数据。 如果开发集和测试集的数据分布和实际关注的数据分布不同，则获取新的开发集和测试集。 如果评估指标不能够对最重要的任务目标进行度量，则需要修改评估指标。 Part3 Basic Error Analysis（基本误差分析）3.1 快速构建并迭代你的第一个系统3.2 误差分析：根据开发集样本评估想法1.误差分析：检查算法误分类的开发集样本的过程，以便你找到造成这些误差的原因。这将帮助你确定项目优先级并且获得关于新方向的灵感。 2.提升效果预估：在进行一项任务的开发前，先预估该任务能提升多少系统精度，更加理性判断是否值得花 xx 久的时间去做这件事，还是将这段时间用于其他任务。 例如，你在使用猫咪 app 时注意到它将一些狗误分类为猫了，因为有些狗的确很像猫的样子。团队中有人建议加入第三方软件来帮助系统更好地处理狗的样本，这需要一个月的时间去完成。 【判断是否进行一项减少误差的任务的策略 —— 误差分析】： 1）人为收集误分类样本：收集 100 个开发集中被误分类的样本，即造成系统误差的样本。 2）人为查看这些样本，并计算其中狗的比例。 3）这个比例即反映消除误差能带来的系统精度的提升程度，根据这个提升程度大小来判断是否要做这项任务。 若只有5%误分类的图像是狗，则说明无论你在狗的问题上做多少的算法改进，最终都不会消除超过原有的 5% 误差 . 也即是说 5% 是该计划项目所能起到帮助的“上限”（最大可能值）。所以如果整个系统的当前精度为 90%（对应误差为 10%），那么这种改进最多能将精度提升到 90.5% （对应误差下降到 9.5% ， 改进了原有 10% 误差其中的 5%）。 相反，如果你发现 50% 的误分类图像是狗，那就可以自信地说这个项目将效果明显，它可以将精度从 90% 提升到 95% （相对误差减少 50%，整体误差由 10% 下降到 5%）。 人为检查 100 个样本并不会占用太多的时间。即使你每分钟只检查一张图片，也可以在两小时内完成，而这两个小时可能帮你节省一个月的工作。 3.3 在误差分析时并行评估多个想法你的团队有一些关于改进猫检测器的想法： 修正算法将狗误分类为猫的问题。 修正算法将大型猫科动物（比如狮子、黑豹等等,下面用大猫代指）误分类为家猫的问题。 改善系统在模糊图像上的表现。 … 这些想法都可在误差分析阶段并行评估。可创建一个 excel 表（如图），然后对误分类样本进行人为标注，最后计算出各项比例，从而判断处理这一类别的误差会带来多少精度的提升。 3.4 清洗误标注的开发集和测试集样本1.发现误标注的样本：在进行误差分析时，可能会发现一些开发集的样本被误标注（mislabeled ）了。此处的“误标注”指的是图像在使用算法处理前，已经被负责标注的人员进行了错误的标注，也即是说某个样本的分类标签（label） 的值并不正确。比如一些不是猫的图片被误标注为猫，反之亦然。 2.跟踪记录误标注样本的比例：如果你不确定误这些标注的图片是否很关键，可以添加一个类别来跟踪记录误标注样本的比例： 3.修正误标注的标签：回忆一下设立开发集的目标，是为了帮助你快速评估算法性能，从而判断算法 A 和 B 哪一个更好。如果开发集中误标注的部分影响了你的判断，那便值得去花时间改正这些标签。 例如，假设你的分类器表现如下： 开发集整体精度…………….. 90% （10% 整体误差） 误标注样本造成的误差…… 0.6% （6% 开发集误差） 其它原因造成的误差………. 9.4% （94% 开发集误差） 相对于你正在改进的 9.4% 误差，误标记的 0.6% 误差就不那么重要了。 假设你不断地改进猫分类器，并达到了以下性能： 开发集整体精度…………….. 98.0% （2.0% 整体误差） 误标注样本造成的误差…… 0.6% （30% 开发集误差） 其它原因造成的误差………. 1.4% （70% 开发集误差） 此时 30% 的开发集误差是由误标注样本造成的，这对精度估计造成了显著的影响。这时就应该考虑改进开发集样本中的标签质量。处理这些误标注的样本将帮助你找出分类器的误差是接近 1.4% 还是 2.0% ，之间的差异是显著的。 在初始阶段容许一些误标注的开发集/测试集样本并不罕见，你可以选择在系统改进到一定程度时再来考虑误标注的样本，因为这些误差在整体误差中的占比会逐渐增大。 4.注意： 同步修改测试集：不论你使用什么方式修正开发集标签，请记住要将同样的方式应用于你的测试集，这可以保持二者服从相同的分布。 同步修改算法预测的标签：请仔细检查系统中误分类和正确分类的样本的标签。对于某个样本，可能原始标注和算法预测的标签都是错误的。如果你仅修复系统误分类的样本标签，则有可能在评估中引入偏差。 3.5 将大型开发集拆分为两个子集，专注其一把大型开发集拆分成 Eyeball 开发集和 Blackbox 开发集。Eyeball —— 人为检查误分类，Blackbox —— 验证评估。 假设你有一个含有 5000 个样本的大型开发集，并有着 20% 的误差，这表示算法将误分类 1000 张开发集图片。手动检查这 1000 张图片会花费很长时间，所以我们在误差分析时没必要使用所有的图片。 在这种情况下，我会明确地将开发集分成两个子集，并只手动检查其中的一个。你将会更快地过拟合手动查看的那些图片，而另一部分没有被手动查看的图片可以拿来调参。 继续上面的例子，在该例子中算法将误分类 5000 个开发集样本中的 1000 个。假设我们想手动检查约 100 个错误样本（整体的10%）进行误差分析。你应该随机选择 10% 的开发集，并将其放入 Eyeball 开发集中（译者注：直译为眼球不妥，保留原文），以提醒我们自己，我们正在用眼睛看它。（对于语音识别项目，你可能需要听一些音频的剪辑，此时则将数据集称为 Ear 开发集）。因此 Eyeball 开发集将有 500 个样本，其中预计的算法误分类样本约 100 个。 第二个子集叫做 Blackbox 开发集（直译为黑箱，保留原文），它有着剩余的 4500 个样本。你可以使用 Blackbox 开发集，通过测量错误率来自动评估分类器，也可以使用它来选择算法或调整超参数。但是，你应该避免将目光聚焦于此。我们使用术语“ Blackbox ”是因为我们只使用该子集来获得分类器的“ Blackbox ”评价。 为什么我们将开发集明确分为 Eyeball 开发集和 Blackbox 开发集呢？因为你在 Eyeball 开发集中建立对样本的直观认识之后，则容易更快地过拟合到Eyeball开发集上。如果你发现 Eyeball 开发集的性能比 Blackbox 开发集提升得更快，说明已经过拟合到 Eyeball 开发集了。此时可能需要丢弃它并寻找一个新的 Eyeball 开发集，比如可以将更多 Blackbox 开发集中的样本移到 Eyeball 开发集中，也可以获取新的标注数据。 将开发集明确地分为 Eyeball 和 Blackbox 开发两个子集将很有帮助，它使你了解在人为的误差分析过程中 Eyeball 开发集何时开始发生过拟合。 3.6 Eyeball 和 Blackbox 开发集该设置多大？1.Eyeball 开发集应该大到能够让你对算法主要的错误类别有所察觉。 1）如果你正在处理一项人类表现良好的任务（比如识别图像中的猫），下面是一些粗略的指导方案： 如果分类器在 Eyeball 开发集上只犯错 10 次，这个开发集就有点小了。只有 10 个错误样本的话，很难准确估计不同错误类别的影响。但如果数据非常少且不能提供更多的 Eyeball 开发集样本时，聊胜于无，这将有助于确立项目的优先级。 如果分类器在 Eyeball 开发集样本上犯了约 20 次错误，你将会开始大致了解主要的误差来源。 如果有约 50 个错误样本，你将会比较好地了解主要的误差来源。 如果有约 100 个错误样本，你将会很清楚主要的误差来源。 假设你的分类器有 5% 的错误率。为了确保在 Eyeball 开发集中有约 100 个误分类的样本，样本开发集应该有约 2000 个样本（因为 0.05 * 2000 = 100）。分类器的错误率越低，为了获得足够多的错误样本进行误差分析，需要的 Eyeball 开发集就越大。 2）如果你正在处理一个人类也做不好的任务，那么检查 Eyeball 开发集将不会有大的帮助，因为很难找出算法不能正确分类一个样本的原因。此时你可能也不需要建立 Eyeball 开发集。 2.Blackbox 开发集的大小： 一个有 1000-10000 个样本的 Blackbox 开发集通常会为你提供足够的数据去调超参和选择模型，即使数据再多一些也无妨。而含有 100 个样本的 Blackbox 开发集虽然比较小，但仍然有用。 如果开发集较小，那么你可能没有足够的数据将其分成足够大的 Eyeball 开发集和 Blackbox 开发集来满足目的。相反，你的整个开发集可能需要用作 Eyeball 开发集——即你将手动检查所有的开发集数据。 3.Eyeball 开发集 比 Blackbox更加重要。 假设你正在研究一个人类能够很好解决的问题，检查这些样本能帮你更有洞悉力。如果你只有一个 Eyeball 开发集，你可以在这个开发集上进行误差分析、模型选择和超参数调整，缺点是过拟合开发集的风险更大。 如果你有充足的数据，那么 Eyeball 开发集的大小将主要取决于你能够手动分析样本的时间。实际上很少有人手动分析超过 1000 个错误样本。 3.7 小结：基础误差分析1.当你开始一个新项目，尤其是在一个你不擅长领域时，很难正确猜测出最有前景的方向。 所以，不要在一开始就试图设计和构建一个完美的系统。相反，应尽可能快（可能在短短几天内）地构建和训练一个基本系统。然后使用误差分析去帮助你识别出最有前景的方向，并据此不断迭代改进你的算法。 2.误差分析：通过手动检查约 100 个算法错误分类的开发集样本来执行误差分析，并计算主要的错误类别。用这些信息来确定优先修正哪种类型的错误。 3.考虑将开发集分为人为检查的 Eyeball 开发集和非人为检查的 Blackbox 开发集。如果在 Eyeball 开发集上的性能比在 Blackbox 开发集上好很多，那么你已经过拟合 Eyeball 开发集，并且应该考虑为其获得更多的数据。 4.Eyeball 开发集应该足够大，以便于算法有足够多的错误分类样本供你分析。对很多应用来说，含有1000-10000个样本的 Blackbox 开发集已足够。 5.如果你的开发集不够大到可以按照这种方式进行拆分，那么就不用分出 Blackbox，而是使用 Eyeball 开发集来用于人工误差分析、模型选择和调超参。 Part4 Bias and Variance（偏差和方差）4.1 偏差和方差：误差的两大来源1.不一定获取更多训练数据就一定能提升性能，因为数据不一定如期望那样有帮助。 假设你希望构建一个误差为 5% 的猫识别器。而目前的训练集错误率为 15%，开发集错误率为 16%。在这种情况下，添加数据可能不会有太多帮助。你应该关注其他改变。实际上，在你的训练集上添加更多的样本只会让你的算法难以在训练集上做的更好。 2.分析误差的来源： 如果你在训练集上的错误率是 15%（即 85% 的精度），但你的目标是 5% 错误率（95% 精度），那么第一个要解决的问题是提高算法在训练集上的性能。算法在开发/测试集上的性能通常比在训练集上要差。所以，如果算法在已知样本上只达到 85% 的精度，则不可能在未知的样本上达到 95% 精度。 如上所述，假设你的算法在开发集上有 16% 的错误率（84% 精度），我们将这 16% 的错误率分为两部分： 1）偏差（bias）：算法在大型训练集上的错误率。在本例中，它是 15%。 2）方差（variance）：算法在测试集上的表现低于训练集的程度。在本例中，开发集表现比训练集差 1%。 3.分析要优先解决哪种误差： 优先级：一些学习算法的改变能解决误差来源的第一个部分——偏差，并且提高算法在训练集上的性能；而一些改变能解决第二个部分——方差，并帮助算法从训练集到开发/测试集上更好地泛化。为了选择最有成效的改变，了解二者哪一方更需解决是很有用的。 系统架构改进：还有一些方法能够对系统架构做出较大改变，同时减少偏差和方差。但是这些方法往往难以鉴定和实现。 建立直觉：建立对偏差和方差的良好直觉将帮助你为算法选择出有效的改变。 4.2 偏差和方差举例1.训练错误率=1%，开发错误率=11%。则偏差=1%，方差=10%，说明分类器的训练误差很低但没能成功泛化到开发集上 —— 过拟合（overfitting）。 2.训练错误率=15%，开发错误率=16%。则偏差=15%，方差=1%，说明分类器的训练误差较高（high bias），方差较低 —— 欠拟合（underfitting）。 3.训练错误率 = 15%，开发错误率 = 30%。估计偏差为 15%，方差为 15%。该分类器有高偏差和高方差（high bias and high variance）。说明它在训练集上表现得很差，因此有较高的偏差，而它在开发集上表现更差，因此具有较高的方差。由于分类器同时过拟合和欠拟合，所以过拟合/欠拟合术语很难应用与此。 4.训练错误率 = 0.5%，开发错误率 = 1%。低偏差和低方差，说明该分类器做得很好。 4.3 与最优错误率比较1.与最优错误率比较：如果偏差接近最优错误率，则说明偏差的改善空间就很小了，同时如果方差很大，那么说明在方差造成的误差上还有很大的提升空间。 假设你正在构建一个语音识别系统，并发现 14% 的音频片段有太多的背景噪声，或者太难以理解，导致即使是人类也无法识别出所说的内容。在这种情况下，即使是“最优”的语音识别系统也可能有约为 14% 的误差。 假设在这个语音识别问题上，你的算法达到： 训练错误率 = 15% 开发错误率 = 30% 算法在训练集上的表现已经接近最优错误率 14%，因此在偏差上或者说在训练集表现上没有太大的提升空间。然而，算法没有很好地泛化到开发集上，在方差造成的误差上还有很大的提升空间。 如果最优错误率接近 0%，那么 15% 的训练错误率则留下了很大的提升空间，这表明降低偏差可能有益。但如果最优错误率是 14%，那么 15% 的训练错误率表现告诉我们，在分类器的偏差方面几乎没有改进的余地。 2.误差分解： 1）偏差 = 最佳误差率（“不可避免偏差”）+ 可避免的偏差 最优错误率（“不可避免偏差”）/ 贝叶斯错误率：14%。假设我们决定，即使是世界上最好的语音系统，仍会有 14% 的误差。我们可以将其认为是学习算法的偏差“不可避免”的部分。 可避免偏差：1%。即训练错误率和最优误差率之间的差值。 过拟合训练集：如果可避免偏差值是负的，即你在训练集上的表现比最优错误率要好。这意味着你正在过拟合训练集，并且算法已经过度记忆（over-memorized）训练集。你应该专注于减少方差的方法，而不是进一步减少偏差的方法。 2）方差：15%。即开发错误和训练错误之间的差值。 理论上来说，我们可以通过训练一个大规模训练集将方差减少到接近零。因此只要拥有足够大的数据集，所有的方差都是可以“避免的”，所以不存在所谓的“不可避免方差”。 3.如何知道最优错误率：与人类表现进行比较 对于人类擅长的任务，例如识别图片或转录音频剪辑，可以让普通人提供标签，然后测量这些人为标签相对于训练集标签的精度，这将给出最优错误率的估计 如果你正在解决甚至人也很难解决的问题（例如预测推荐什么电影，或向用户展示什么广告），这将很难去估计最优错误率。 4.4 处理偏差和方差1.处理偏差和方差问题最简单的形式： 高可避免偏差 —— 改进算法：如果具有较高的可避免偏差，那么加大模型的规模（例如通过添加层/神经元数量来增加神经网络的大小）。 高方差 —— 增加数据：如果具有较高的方差，那么向训练集增加数据 注：如果你可以加大神经网络的规模且无限制地增加训练集数据，那么在很多学习问题上都可以做的很好。 2.加大 NN 模型规模 1）作用：通常可减少偏差，但也可能会增加方差和过拟合风险。 这种过拟合问题通常只在你不使用正则化技术的时候出现。如果你的算法含有了一个精心设计的正则化方法，通常可以安全地加大模型的规模，而不会增加过拟合风险。 2）存在的问题： 算力：计算力问题，因为训练大的模型很慢。 数据：你可能会耗尽获取更多训练数据的能力。 3）不同的模型架构（例如不同的神经网络架构）对于你的问题将有不同的偏差/方差值。尝试新架构的结果要比简单地加大模型规模和添加数据的形式更难以预测。 4）总结：假设你正在应用深度学习，使用了 L2 正则化和 dropout 技术，并且设置了在开发集上表现最好的正则化参数。如果你加大模型规模，算法的表现往往会保持不变或提升；它不太可能明显地变差。避免使用更大模型的唯一原因就是这将使得计算代价变大。 4.5 偏差和方差间的权衡1.为什么要权衡 —— 此消彼长的可能性： 大部分对学习算法进行的更改中，有一些能够减少偏差，但代价是增大方差，反之亦然。 例如，加大模型的规模（在神经网络中增加神经元/层，或增加输入特征），通常可以减少偏差，但可能会增加方差。另外，加入正则化一般会增加偏差，但是能减少方差。 2）需要权衡的情况比较少： 我们往往能够获取充足的数据，并且可以使用非常大的神经网络。 现在有更多的选择可以在不损害方差的情况下减少偏差，反之亦然。 例如，你通常可以增加神经网络的规模大小，并调整正则化方法去减少偏差，而不会明显的增加方差。通过增加训练数据，你通常也可以在不影响偏差的情况下减少方差。 注：加入正则化可抵消方差的增加 。 3）策略：如果你选择了一个很适合你任务的模型架构，那么你也可以同时减少偏差和方差。只是选择这样的架构可能有点难度。 4.6 减少可避免偏差的技术如果你的学习算法存在着很高的可避免偏差，你可能可以尝试下面的技术： 加大模型规模（例如神经元/层的数量）：这项技术能够使算法更好地拟合训练集，从而减少偏差。当你发现这样做会增大方差时，加入正则化，这可以抵消方差的增加。 根据误差分析结果修改输入特征：假设误差分析的结果鼓励你创建额外的特征，从而帮助算法消除某个特定类别的误差。这些新的特征对处理偏差和方差都有所帮助。理论上，添加更多的特征将增大方差；然而当你发现这种情况时，加入正则化，这可以抵消方差的增加。 减少或者去除正则化（L2 正则化，L1 正则化，dropout）：这将减少可避免偏差，但会增大方差。 修改模型架构（比如神经网络架构）使之更适用于你的问题：这项技术将同时影响偏差和方差。 有一种方法并不能奏效： 添加更多的训练数据：这项技术可以帮助解决方差问题，但它对于偏差通常没有明显的影响。 4.7 训练集误差分析1.对训练集做误差分析的必要性： 你的算法必须在训练集上表现得很好，才能期望它在开发集和测试集上能够有着良好的表现。 2.策略： 1）类似于在开发集上设置一个 Eyeball 开发集。当你的算法有着高偏差时（例如算法没有很好拟合训练集的时候）这将有所帮助。 2）然后分析 Eyeball 开发集中的各种误差原因的比例，从而发现提升方向。 3）注意检查正常人是否也能很好地识别这些样本。如果正常人都不能正确识别，那么期望算法正确地识别这样的话语就不太合理。 4.8 减少方差的技术1.如果你的学习算法存在着高方差，则可以尝试下面的技术： 添加更多的训练数据：这是最简单也是最可靠的一种方式来处理方差，只要你能访问大量的数据并有足够的计算能力来处理它们。 加入正则化（L2 正则化，L1 正则化，dropout）：这项技术可以降低方差，但却增大了偏差。 加入提前终止（比如根据开发集误差提前终止梯度下降）：这项技术可以降低方差但却增大了偏差。提前终止（Early stopping）有点像正则化理论，一些学者认为它是正则化技术之一。 通过特征选择减少输入特征的数量和种类：这种技术可能有助于解决方差问题，但也可能增加偏差。稍微减少特征的数量（比如从 1000 个特征减少到 900 个）不太可能会对偏差产生很大的影响，但显著地减少它们（比如从 1000 个特征减少到 100 个，10 倍地降低）则很有可能产生很大的影响，你可能排除了太多有用的特征。在现代深度学习中，当数据充足时，特征选择的比重就有所改变，现在我们更有可能将拥有的所有特征提供给算法，并让算法根据数据来确定哪些特征可以使用。但当你的训练集很小的时候，特征选择是非常有用的。 减小模型规模（比如神经元/层的数量）：谨慎使用。这种技术可以减少方差，同时可能增加偏差。然而我不推荐这种处理方差的方法，添加正则化通常会提供更好的分类性能。 减少模型规模的好处是降低了计算成本，从而加快了你对模型进行训练的速度。如果加速模型训练是有用的，那么无论如何都要考虑减少模型的规模。但如果你的目标是减少方差，并且不关心计算成本，那么考虑添加正则化会更好。 2.下面是两种额外的策略，和解决偏差问题章节所提到的方法重复： 根据误差分析结果修改输入特征：假设误差分析的结果鼓励你创建额外的特征，从而帮助算法消除某个特定类别的误差。这些新的特征对处理偏差和方差都有所帮助。理论上，添加更多的特征将增大方差；然而当你发现这种情况时，加入正则化，这可以消除方差的增加。 修改模型架构（比如神经网络架构）使之更适用于你的问题：这项技术将同时影响偏差和方差。 Part5 Learning Curves（学习曲线）5.1 诊断偏差与方差：学习曲线1.学习曲线：可以将开发集的误差与训练集样本的数量进行关联比较 绘制方法：需要设置不同大小的训练集运行算法。假设有 1000 个样本，你可以选择在规模为 100、200、300 … 1000 的样本集中分别运行算法，接着便可以得到开发集误差随训练集大小变化的曲线。 随着训练集大小的增加，开发集误差应该降低。 2.期望错误率：希望算法能达到的值 对标人类错误率：如果希望达到人类水平的表现，那么人类错误率可能就是“期望错误率”。 主观感受：如果学习算法为某些产品提供服务（如提供猫图），我们可能将主观感受到需什么样的水平才能给用户提供出色的体验。 直觉：如果你已经从事一项应用很长时间，那么你可能会有一种直觉，那就是在下一个季度里你会有多大的进步。 3.可以将期望的表现水平添加到你的学习曲线中：来判断趋势和做决策 可以根据红色的“开发误差”曲线的走势来推测，添加一定的数据时曲线距离期望的性能接近了多少。在上面的例子中，将训练集的大小增加一倍可能会让你达到期望的性能，这看起来是合理的。 观察学习曲线可能会帮助你避免花几个月的时间去收集两倍的训练数据，结果却发现这并不管用（如上图）。 4.缺点： 如果只关注开发错误曲线，当数据量变得越来越多时，将很难预测后续红色曲线的走向。因此我们会选择额外的一条曲线来帮助评估添加数据所带来的影响：即训练误差曲线。 5.2 绘制训练误差曲线1.必要性：随着训练集大小的增加，开发集（和测试集）误差应该会降低，但训练集误差往往会同时增加。 举例说明一下这个影响： 假设你的训练集只有两个样本：一张猫图和一张非猫图。学习算法将很容易“记住”训练集中这两个样本，并且训练集错误率为 0%. 即使有一张或两张的样本图片被误标注了，算法也会轻松地记住它们。 现在假设你的训练集有 100 个样本，可能有一些样本是误标记的，或者模棱两可的（图像非常模糊），所以即使是人类也无法分辨是否有一只猫。或许学习算法仍然可以“记住”大部分或全部的训练集，但现在很难获得 100% 的准确率。通过将训练集样本数量从 2 个增加到 100 个，你会发现训练集的准确率会略有下降。 2.将训练误差曲线添加到原有的学习曲线中： 可以发现： 1）蓝色的“训练误差”曲线随着训练集大小的增加而增加； 2）算法在训练集上通常比在开发集上做得更好，故红色的开发误差曲线通常严格位于蓝色训练错误曲线之上。 5.3 解读学习曲线：高偏差1.在同一张图中检查开发误差曲线和训练误差曲线可以让我们更有信心地推测开发误差曲线的走势。 对于上图，可以肯定：添加更多的数据并不奏效。原因： 随着我们添加更多的训练数据，训练误差只会变得更糟。因此蓝色的训练误差曲线只会保持不动或上升，这表明它只会远离期望的性能水平。 红色的开发误差曲线通常要高于蓝色的训练误差曲线。因此只要训练误差高于期望性能水平，通过添加更多数据来让红色开发误差曲线下降到期望性能水平之下也基本没有可能。 看上图可发现： 方差小：因为训练曲线和开发曲线之间的间隙小 可避免偏差大：因为训练误差和期望性能之间有大的间隙 5.4 解读学习曲线：其他情况 例1：蓝色训练误差曲线相对较低，红色的开发误差曲线比蓝色训练误差高得多。故偏差很小，但方差很大。添加更多的训练数据可能有助于缩小开发误差和训练误差之间的差距。 例2：训练误差很大，因为它比期望的性能水平要高得多。开发误差也比训练误差大得多，因此有着明显的偏差和方差。此时你必须找到一种方法来减少算法中的偏差和方差。 5.5 绘制学习曲线1.假设你有一个非常小的训练集，仅有 100 个样本 。那么你可以从中随机选择 10 个样本来训练你的算法，然后是 20 个，30 个，100 个，每次增加 10 个样本。然后使用 10 个数据点来绘制你的学习曲线。你可能会发现，在较小规模的训练集上，曲线看起来带有点噪声（这意味着这些值比预期的要高/低）。 2.当只使用 10 个随机选择的样本进行训练时，你可能会不幸碰到特别“糟糕”的训练集，比如含有很模糊的或者误标记的样本。你当然也有可能会幸运地碰到特别“棒”的训练集。训练集的规模较小意味着开发和训练误差将随机波动。 3.如果你的机器学习应用程序很倾向于某一个类（如猫分类任务的负面样本比例远远大于正面样本），或者说有大量的类（如识别 100 种不同的动物物种），那么选择一个“非代表性”或糟糕的特殊训练集的几率也将更大 。例如，假设你的整个样本中有 80% 是负样本（y=0），只有 20% 是正样本（y=1），那么一个含有 10 个样本的训练集就有可能只包含负样本，因而算法很难从中学到有意义的东西。 4.存在训练集噪声致使难以正确理解曲线的变化时（前置条件），有两种解决方案： 与其只使用 10 个样本训练单个模型，不如从你原来的 100 个样本中进行随机有放回抽样，选择几批（比如 3-10 ）不同的 10 个样本进行组合。在这些数据上训练不同的模型，并计算每个模型的训练和开发错误。最终，计算和绘制平均训练集误差和平均开发集误差。 如果你的训练集偏向于一个类，或者它有许多类，那么选择一个“平衡”子集，而不是从 100 个样本中随机抽取 10 个训练样本。例如，你可以确保这些样本中的 2/10是正样本，8/10 是负样本。更常见的做法是，确保每个类的样本比例尽可能地接近原始训练集的总体比例。 此处有放回抽样的意思是：你会从 100 个样本中随机选择 10 个不同的样本来生成第一个训练集，在生成第二个训练集时，你需要再次选择 10 个样本，且抽样来源仍需包括第一次选择的 10 个样本在内。因此，某一个样本可能在第一个训练集和第二个训练集都有出现。相反，如果你在无放回的情况下进行抽样，那么第二个训练集将从第一次没有被选择的 90 个样本中选出。在实践中，用有放回抽样和无放回抽样的差异不大，但是前者更为常见。 除非你已经尝试过绘制学习曲线，并得出了曲线太过嘈杂且无法看到潜在趋势的结论，否则我将不会考虑使用这两种技术。因为当你的训练集规模很大——比如超过 10000 个样本——而且类分布不是很倾斜时，你可能就不需要这些技巧了。 5.绘制一个学习曲线的成本可能非常高（计算成本、时间成本）： 例如，你可能需要训练 10 个模型，其中样本规模可以是 1000 个，然后是 2000 个，一直到 10000 个。使用小数据集训练模型比使用大型数据集要快得多。因此，你可以用 1000、2000、4000、6000 和 10000 个样本来训练模型，而不是像上面那样将训练集的大小均匀地间隔在一个线性的范围内。这仍然可以让你对学习曲线的变化趋势有一个清晰的认识。当然，这种技术只有在训练所有额外模型所需的计算成本很重要时才有意义。 Part6 Comparing to human-level performance（与人类表现水平相比较）6.1 为何与人类表现水平进行对比1.许多机器学习系统的设计目的：自动化一些人类可以处理得很好的事情，如图像识别、语音识别、垃圾邮件分类等。 2.在处理人类擅长的任务时，构建一个机器学习系统会更加简单： 易于从人为标签中获取数据：如由于人类可以很好地识别图片中的猫，因此让人们为你的学习算法提供高精度的带标签数据也很方便。 可基于人类直觉进行误差分析：假设某个语音识别系统的表现要低于人类的表现水平。比如错误地将音频片段 “This recipe calls for a pear of apples” 中的 “pair” 认为是 “pear”. 此时你可以利用人类的直觉来尝试理解，普通人会利用何种信息来获取正确的转录内容，并且试着修改你的学习算法，使它在相同的知识点上有着更好的表现。 使用人类表现水平来估计最优错误率，并设置可达到的“期望错误率”：假设你的算法在某个任务上达到了 10% 的误差，但普通人所能达到的误差是 2% . 由此我们就可以知道最优错误率是 2% 或更低，这也表明可避免偏差至少是 8% . 所以你应当尝试一下降低偏差的技术。更一般地说，有一个合理可实现的“期望错误率”可以帮助你去估计学习算法的可避免偏差。这反过来也帮你决定是否使用误差降低技术。 3.对于人类不擅长的任务，如广告推荐、书籍推荐、股票预测等，主要面临以下问题： 获取标签数据很难。 比如很难去获取用户数据库，并要求人工标记者使用“最优”的书籍标签对数据库进行注释，从而向用户推荐书籍。如果你正在负责一个书籍销售网站或者是 APP 的运营，你可以通过向用户展示书籍并查看他们的购买记录来获取数据。可当你没有这样一个网站时，就需要去找到一些更具创意的方法来获取数据了。 人类的直觉难以依靠。例如，几乎没有人能准确地预测股票市场。因此当我们的股票预测算法比随机猜测的表现还要差时，很难弄清楚要如何去改进它。 最优错误率和合理的期望错误率难以确定。假设你已经有了一个很好的图书推荐系统。如果没有人类水平作为参考，你怎么知道它还能改善多少呢？ 6.2 如何定义人类表现水平1.假设你正在做一个医学成像应用程序，它可以自动依据X射线图像进行诊断。 除了一些基础的训练外，一个没有任何医学背景的人在该任务上的错误率为 15% . 一名新手医生的错误率为 10% ，而经验丰富的医生可以达到 5% . 如果由小型的医生团队对每一幅图像进行单独的讨论，错误率将降低至 2% . 上述的哪一种错误率可以定义为“人类表现水平”呢？ 在该情景下，我将使用 2% 作为人类表现水平的代表来获得最优错误率。 你还可以将 2% 设置为期望的性能水平。 2.按能力合理分工：当需要获得标签数据时，你可能不希望与整个团队讨论每一张图片，因为他们的时间很宝贵。或许你可以让新手医生给绝大多数的病例贴上标签，而把那些较难分析的病例交给更有经验的医生或医生团队。 3.如果你的系统目前的误差为 40%，那么不论是让初级医生（10% 误差）还是有经验的医生（5% 误差误）来给你的数据贴上标签，那都没有关系。是如果你的系统误差已经是 10%，那么将人类表现水平定义为 2% 将为你提供更好的途径来改进你的系统。 6.3 超越人类表现水平1.当整体算法表现已经优于人类表现水平时：现在你在做一个语音识别项目，并且有一个音频片段数据集。假设数据集里有许多的噪声，导致即使是人类来识别也会有 10% 的误差。同时假设你的算法已经达到了 8% 的误差，你能够使用第 33 章中提到的三种技术来继续取得快速的进展吗？ 2.找到人类表现水平远超系统的数据子集，利用数据子集继续优化算法性能：如果你能找到人类表现水平远超现有系统的数据子集，使用那些技术来驱动进则仍然可行。举个例子，假设你的系统在处理识别含有噪音的音频任务时表现已经优于人类，然而在转录语速很快的语音时人类仍然占有优势。 对于语速很快的语音数据子集： 你仍可以从输出质量比你的算法高的人那儿获取转录数据。 你可以利用人类的直觉来理解，为什么你的系统没能够识别这些数据，而人类做到了。 你可以使用该子集上的人类表现水平作为期望表现目标。 3.只要在开发集上存在着一些人类能正确处理而算法不能的样本，前面提到的技术就能够被应用。即使你的算法在整个开发集或是测试集上的表现已经超过了人类，这样做也是正确的。 Part7 Training and testing on different distributions（在不同的分布上训练和测试）7.1 何时在不同的分布上训练与测试1.选择开发集和测试集的首要原则：服从统一的实际分布 实际分布：即反映你在将来实际应用时想要处理的数据 2.例：假设用户已经向你的猫咪图片程序上传了 10000 张图片，且图片已被人为标记为含有猫与不含猫两类。同时你也从互联网上下载了规模更大的 200000 张图片集，此时训练集、测试集与开发集应该如何定义呢？ 1）分析： 10000张：用户提交的图片，密切反映实际概率分布 200000张：自己从互联网上下载的图片，无法反映实际概率分布 2）策略： 仅使用10000张作为开发集和测试集 整合210000张并随机打乱后再划分：不建议使用这种方法，因为绝大部分数据将来自互联网图片，不能反映实际分布 7.2 如何决定是否使用你所有的数据假设你的猫咪检测器的训练集包括 10000 张用户上传的图片，这些数据来自相同的数据分布且将作为单独的开发/测试集，同时也代表着你关心的将要处理的数据分布。你还从互联网下载了额外的 20000 张图片。此时你是否应该为你的学习算法提供所有的 20000 + 10000 张图片作为它的训练集，或者丢弃这 20000 张网络图片，以免它会影响你的学习算法呢？ 1.在上面的例子中若合并所有数据： 使用传统学习算法，会导致算法表现更差 使用大型NN，风险大大降低，更可能提升算法的性能 2.WHY使用大型 NN 可以不关心数据分布：关键在于算法“大脑”的容量 添加额外的不同分布的数据会迫使 NN 花费部分容量来学习网络图像的特定属性（如更高的分辨率、不同画面结构图像的分布等），若这些属性与移动应用图像有很大的不同，那么它将“耗尽”神经网络的一些表征能力，导致从移动应用图像的分布识别数据的能力就会降低，于是会损害算法性能。 但若你有足够的计算能力来构建一个足够大的神经网络，就有足够的能力从互联网和移动应用图像中学习，而不会存在两种类型的数据在容量上的竞争。 但是，如果你没有足够大的神经网络（或者另一个高度灵活的学习算法），那么你应该更加关注训练数据，需要与开发集/测试集的分布相匹配。 3.使用大型 NN 最好不加无用的样本到训练集里： 因为你的神经网络几乎没有任何东西可以从这些无用的数据中学习，加入它们将会浪费计算资源和神经网络的表征能力，但它们可以应用到开发/测试集中作为负样本。 7.3 如何决定是否添加不一致的数据如果一定要加不一致的数据，请尽量保证开发集和测试集的数据服从实际分布。 7.4 给数据加权重假设你有 20 万张来自互联网的图片，还有来自移动应用用户的 5000 张照片。数据集的大小之间有一个 40:1 的比率。 1.从理论上讲，只要你建立了一个庞大的神经网络，并在所有 205000 张图片上进行足够长的时间训练，那么在网络图像和移动图像上将算法都训练得很好是没有害处的。 2.但在实际操作中，拥有 40 倍的网络图像可能意味着，相比只使用 5000 张图片，你需要花费 40 倍（或更多）的计算资源来对两者进行建模。 3.如果你没有巨大的计算资源，你可以给互联网图片一个较低的权重作为妥协。 例如： 权重为β，若设置β=1/40，则这个算法会对 5000 个移动图像和 20 万个互联网图像给予同等的权重。 4.赋予权重的意义： 通过对额外的网络图像赋予更少的权重，你不需要构建一个庞大的神经网络来确保算法在这两种类型的任务上都能很好地完成。 但注意：只有当你怀疑这些额外的数据（网络图像）与开发/测试集分布不一致，或者额外的数据规模比与相同分布的开发/测试集（手机图像）数据规模大得多时，才需要加这种类型的权重。 7.4 从训练集泛化到开发集1.假设你正在将机器学习应用于不同分布的训练集和开发/测试集上。例如，训练集包含了互联网图像+移动应用图像，而开发/测试集只包含移动应用图像。然而，该算法运行得不太好：它的开发/测试集误差比想要的要高得多。以下是一些可能出现问题的情况： 高偏差问题：在训练集上表现不佳 高方差问题：在训练集上表现很好，但无法很好地泛化到未知数据上 数据不匹配问题：能很好地泛化到与训练集相同分布的未知数据，但不能很好地泛化到与开发/测试集相同分布的未知数据 2.数据不匹配问题的举例：假设人类在猫识别任务上取得近乎完美的表现。你的算法实现了： 1% 的训练集误差 1.5% 的与训练集分布相同的未知数据上的误差 10% 的开发集误差 显然存在数据不匹配问题。 Solu：使训练数据更接近开发和测试数据。 3.诊断一个算法在上面3个问题上受到了多大程度的影响：从训练集中分出一个训练开发集的子集，用于评估是否存在数据不匹配问题。 训练集：这是算法将学习的数据（例如，互联网图像+移动应用图像）。这并不需要我们从与真正关心的相同分布（开发/测试集分布）的数据中提取。 训练开发集：这些数据来自与训练集相同的分布（例如，互联网图像+移动应用图像）。它通常比训练集要小；它只需要足够大到来评估和跟踪我们的学习算法的进展。 开发集：这是从与测试集相同分布的数据中抽取出来的，它反映了我们最终关心的数据的分布（例如，移动应用图像） 。 测试集：这是从与开发集相同分布的数据中抽取出来的（例如，移动应用图像）。 有了这四个独立的数据集，就可以评估： 训练误差：对训练集进行评估。 该算法能够泛化到与训练集相同分布数据的能力，并对训练开发集进行评估。 算法在你实际关心的任务上的性能，通过对开发集 和/或 测试集评估。 7.4 辨别偏差、方差和数据不匹配误差7.5 解决数据不匹配问题1.解决方法： 尝试理解数据属性在训练集和开发集分布之间的差异 尝试找到更多接近实际分布的训练数据，以便更好地匹配你的算法碰到的开发集样本 域适应：研究如何在一个分布上训练算法，并将其推广到不同的分布（但只适用于特殊类型问题，且使用得很少） 2.误差分析：目的是了解训练集和开发集之间的显著差异，这正是导致数据不匹配的原因。 7.6 人工合成数据1.人工合成数据：可以快速创建巨大数据集。 例如： 1）将在安静空间里说话的音频数据与汽车/道路噪音的音频数据合成到一起，从而获取到嘈杂环境中人说话的音频数据，这样比直接在开车时收集大量数据要更节省时间。 2）模拟合成：将模拟的动态模糊加到清晰的图像上，从而获取到模糊图像。 2.人工合成数据的挑战：“过拟合”风险 例如： 1）假设你有 1000 小时的语音训练数据，但只有 1 小时的汽车噪音。如果你反复使用相同的 1 小时的汽车噪音来合成数据，那么虽然听这段音频的人可能无法分辨噪音的区别，但某种学习算法可能会“过拟合”这一小时的汽车噪音，从而可能无法很好地泛化到一个新的音频剪辑片段。 2）假设你有 1000 个小时的汽车噪音片段，但所有的噪音都是从 10 辆不同的车上提取的。在这种情况下，一种算法可能会“过拟合”这 10 辆车，如果在不同的汽车上进行音频测试，性能则会很差。但这些问题通常很难被发现。 3）假设你正在建立一个计算机视觉系统来识别汽车：你正与一家电脑游戏公司合作，该公司拥有20辆汽车的计算机图形模型。如果你的 10 万个训练样本都由这 20 辆车合成而来，那么你的系统将会“过拟合”这 20 款特定的汽车设计，而且它将无法很好地泛化到包含其他汽车设计在内的开发/测试集。 3.合成数据需要考虑：样本的代表性，样本的分布需要尽可能接近实际数据分布。 Part8 Debugging inference algorithms8.1 优化验证测试8.3 强化学习举例假设你正在用机器学习来教直升机复杂的飞行动作。下面是一张延时照片，照片上是一台电脑控制器的直升机正在引擎关闭的情况下执行着陆。 这被称为“自旋”策略，即使引擎意外故障了，它也允许直升机着陆。这也是人类飞行员经常进行的训练。 而你的目标是使用一种学习算法，让直升机通过一个轨迹$T$安全地着陆。 要应用强化学习策略，你必须设计一个 “奖励函数”$R(.)$ ，它给出一个分数来衡量每一个可能轨迹$T$的好坏。例如，如果 $T$导致直升机坠毁，那么奖励也许是$R(T)=-1000$ ，这是一个巨大的负反馈；而一个导致安全着陆的轨迹 $T$ 可能会产生一个正的$R(T)$ 值，它的精确值取决于着陆过程的平稳程度。奖励函数$R(.)$ 通常是人为选择的，以量化不同轨迹 $T$ 的理想程度。它必须权衡考虑着陆的颠簸程度，直升机是否降落在理想的位置，乘客的降落体验等因素。设计一个好的建立函数并非易事。 给定一个奖励函数$R(T)$ ，强化学习算法的工作是控制直升机，使其达到$max_TR(T)$ . 然而，强化学习算法原理内部有许多近似操作，可能无法成功实现这种最大化需求。 假设你已经选择了某些奖励函数$R(T)$ 作为反馈，并运行了学习算法。然而它的表现似乎比人类飞行员要糟糕得多——它更加颠簸，而且似乎不那么安全。你如何判断错误是否由强化学习算法造成——它试图找到一个轨迹$T$ ，满足 $max_TR(T)$ ——或者错误来自于你的奖励函数——它尝试衡量并且指定一种在颠簸程度和着陆精度之间权衡的理想结果。 为了应用优化验证测试，让$T{human}$ 表示人类飞行员所选择的轨迹，并让$T{out}$代表算法所选择的轨迹。根据我们上面的描述， $T{human}$是优于$T{out}$ 的发展轨迹。因此，关键的测试点在于：不等式$R(T{human})&gt;R(T{out})$是否成立？ 情况1：如果不等式成立，奖励函数$R(.)$ 正确地使$T{human}$ 优于$T{out}$ ，但这表明我们的强化学习算法找到的 $T_{out}$ 仍不够好，花时间去改进算法是很值得的。 情况2：如果上面不等式不成立，而是$R(T{human})&lt;=R(T{out})$ . 这表明$R(.)$ 的设计使得理应是更优策略的$T_{human}$ 得到了一个更糟的评分。你应当致力于改进$R(.)$ ，以更好地获得与良好着陆情况相对应的权衡。 许多机器学习应用程序使用这种优化某个近似的 “模式” 来确定得分函数$Score_X(.)$ . 有时没有特定的输入$x$ ，形式简化为 $Score_X(.)$。在上面的例子中，得分函数即是奖励函数，$Score(T)=R(T)$，而采用的优化算法是强化学习算法，目的是找到好的轨迹$T$ . 这和前面的例子有一个区别，那就是，与其比较 “最优” 输出，不如将其与人类水平的表现进行比较。我们认为，即使$T{human}$ 不是最优的，它也是相当不错的。一般而言，只要有一个比当前学习算法性能更好的输出 （在这个例子中即是指$T{human}$ ），即使它不是 “最优” 的，优化验证测试也能够反映改进学习算法与改进得分函数之间哪一个更具前途。 Part9 End-to-end deep learning9.1 端到端学习的兴起1.端到端：要求学习算法直接从输入得到期望的输出，即学习算法将系统的 “输入端” 连接到 “输出端” 。 例如：“情感分类”端到端学习算法 2.上述算法是下面算法的替代： 要实现一个情感分类系统，流水线模块需要2个组件： 解析器（parser）：一种通过识别关键词汇来对文本进行注释的系统。例如，你可以使用解析器对所有的形容词和名词做标记。 情感分类器（sentiment classifier）：一种学习算法，它可以输入带注释的文本，并预测整体的情感。解析器的注释将对这个算法起到极大的帮助：通过给形容词一个较高的权重，你的算法将能很快地找到像 “非常” 这样的重要词汇，并忽视像 “这个” 这样的非关键词。 3.在数据量非常丰富的情况下，端到端系统很有效，但并非总是一个好选择。 9.2 例子1.【非端到端】假设你正在构建一个语音识别系统，你的系统可能需要三个组件： 计算特征（compute features）：提取人工设计的特征，如 MFCC（Mel-frequency cepstrum coefficients，频谱系数）特征，以此来捕捉对话的内容而忽略不太相关的属性，如说话者的音高。 音素识别器（phoneme Recognitizer）：识别出音频中的所有音素，音素是最基本的声音单元。 最终识别器（final Recognitizer）：以已识别音素的序列为序，将它们串在一起，形成转录输出。 【端到端系统】输入一个音频片段，直接输出文字。 2.机器学习流水线：纯线性 &amp; 非纯线性（复杂流水线） 1）下图是一个自动驾驶汽车的简单流水架构： 2）并非所有组件都需要进行学习。 3）端到端方法可能尝试从传感器获取输入并直接输出转向方向： 9.3 端到端学习的优缺点1.【非端到端】人工设计的利弊 缺点—人工设计：限制了语音系统的潜在性能 频谱系数：人工设计的一套特征；合理，但通过抛弃一些信息简化了输入信号 音素：语言学家的发明，本身是对语言声音的不完备表示，是对现实语音的很差的近似，故迫使算法使用音素进行表示将限制语音系统的性能。 优点： 频谱系数的特性对于处理一些不影响内容的音频属性是很有效的，比如说话者的音高。因此它们有助于讲话学习算法面临的问题。 在一定程度上，音素是一种合理的语音表示方法，它们也可以帮助学习算法理解基本的声音成分，从而提高其性能。 拥有更多的人工设计成分通常可以让语音系统学习更少的数据，由频谱系数和音素所捕获的人工设计的知识对算法从数据中获取的知识进行了补充。当我们的数据量不是很多时，这些知识是非常有用的。 2.【端到端】数据量很重要 需要大训练集：缺乏人工设计知识，故当数据集很小时，表现可能比非端到端糟糕；训练集很大时，模型不会受到频谱系数或语音表示方法的限制。若学习算法是一个足够大的神经网络，且喂进去许多的训练数据，就有可能做得更好，甚至达到最优错误率。 需要大量标记数据：端到端学习系统在 “两端” —— 输入端和输出端拥有大量标记数据时，往往做得更好。在这个例子中，我们需要一个大数据集（包含&lt;音频,文本&gt;对）。当这种类型的数据不可用时，使用端到端学习则需非常谨慎。 9.4 流水线组件的选择——数据可用性 + 任务简单性1.流水线系统：对流水线的设计将极大影响整个系统的性能 流水线组件选择的重要因素： 1）数据可用性：是否能够轻松收集到数据来训练每个组件； 2）任务简单性：独立组件使得任务简单了多少？尽可能选择那些易于构建或学习的独立流水线组件。 2.数据可用性 数据集的不同：数据可获得性不同 1）非端到端： 需要大量带车辆和行人标记的计算机视觉数据集，易获取。 2）端到端： 需要一个包含&lt;图像，操纵方向&gt;的大型数据集。 但让人们在驾驶汽车时收集汽车的操纵方向的数据是非常费时费力的，你需要一辆特殊配置的汽车，且需要巨大的驾驶量来涵盖各种可能的场景。这就使得端到端系统难以进行训练。 3）更常见的情况是，如果有大量的数据可以被用来训练流水线的 “中间模块” （例如汽车检测器或行人检测器），你便可以考虑使用多段的流水线架构。因为可以使用所有可用数据进行训练，所以这种结构可能是更优的。 在更多端到端数据变得可用之前，我相信非端到端的方法对于自动驾驶而言是更有希望的——它的体系架构更匹配于数据的可用性。 3.任务简单性： 复杂任务分解：如果你能够完成一个复杂的任务，并将其分解为更简单的子任务，然后显式编写子任务步骤代码，那么你就会给算法一些先验知识，从而帮助它更有效地学习任务，并且每个组件都是相对简单的功能故只需要少量数据来学习。 1）举例：暹罗猫检测器 端到端： 非端到端： Step1：猫咪检测器，检测图像中所有的猫 Step2：将每一块被检测到的猫的图像传送给猫种类分类器（每次一张），如果其中任何一只猫是暹罗猫，则在最后输出 1. 与仅仅使用标签 0/1 来训练一个纯粹的端到端分类器相比，流水线中的两个组件——猫咪检测器和猫种类分类器——似乎更容易进行学习，而且需要更少的数据。 9.5 直接学习更为丰富的输出深度学习拥有更丰富的输出形式： 1）传统的监督学习应用学得一个函数：$h: X—&gt;Y$ ，其中输出$Y$通常是一个整数或者实数。如二分类问题。 2）端到端深度学习：它让我们直接学习比数字复杂得多的$Y$ ，如在图像配字的课题中，你可以使用一个神经网络输入一个图像（$X$），并直接输出一个标题（$Y$）。如图像生成描述，机器翻译，QA，语音识别，文本转语音。 10. Error Analysis by Parts暂无 Reference《Machine Learning Yearning》 文档]]></content>
      <categories>
        <category>Artificial Intelligence</category>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Artificial Intelligence</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阅读笔记：《人生是剧场》]]></title>
    <url>%2F2018%2F04%2F26%2F%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%EF%BC%9A%E3%80%8A%E4%BA%BA%E7%94%9F%E6%98%AF%E5%89%A7%E5%9C%BA%E3%80%8B%2F</url>
    <content type="text"><![CDATA[Abstract：在生命的大电影里，很少人拥有出离的勇气。 // 宗萨仁波切所著，深入浅出，很具智慧的一本佛学书籍。 // 读于2018年4月，我想以后还应该会再读吧。 1.世间八法印度佛教学者阿底峡——有八件事情让人软弱 希望受到赞美 不希望受到批评 希望得到 不希望失去 希望快乐 不希望痛苦 希望声名远播 不希望默默无闻或受到忽视 世间八法：毁誉，得失，苦乐，讥称 修心 出离心 出于了解，我们就不会沉陷，可以根据自我意志轻松抽离而出 明知那是一场电影，却继续观赏，经历个中情绪，但操纵权在我。任何我们控制不了的时刻，任何操之于别人的时刻，都是没有意义的。 失望来源于期待 我们可以做的是：发展明智的期待 我们需要对自己的心有控制力 我们的一切所作所为、所思所想都是来源于期待，包括走路的方式、穿的衣服 … 而非出于真诚。当缺乏真诚时，我们会变得非常软弱，可能会成为自己和他人期待的牺牲品。 2.基本的不安全感修行的目的：得到自我控制的能力，获得某种信心，学会对生活欣赏和感谢 止：使心稳定而专注 焦虑、痛苦：实质是内心的不安全感 我们需要去了解甚至摧毁这种不安全感 3.智慧与慈悲了知自他之间的平等，好坏之间的平等，一切二元对立之间的平等，这就是悲心。但对我们大多数人来说，悲心是很难生起的。有太多强有力的东西，如自私、我执以及助长我执的因素，在阻碍悲心，与悲心做对。 “钦哲”：智慧与慈悲 在大乘中，智慧指的是了知无我的心，或了知自我本空的心 事实上，妄想和无明都不存在，但是由于我们自己的不安全感，我们以为它们存在。我们对自我的存在非常执着，乃至于常忙于成为这个自我的奴隶。 菩提心：有勇气帮助众生，同时有勇气面对现实，面对这个无明、迷惑的人生 获得悲智的最快方法 ：培养虔诚心 4.奢摩他禅修我们总是在做事、想事，我们总是在忙碌。我们因此让自己迷失在无数的执迷和僵固中。当我们禅修，什么也不做时，所有这些僵固就会被揭露出来。你会发现，这些僵固会自动减少，而你什么也没做。 但多半时候,我们无法控制自己,我们的心总是受到一些事物的吸引或干扰——我们的敌人、我们的爱人、我们的朋友,所有这一切,还有希望、恐惧、嫉妒、骄傲、执著、嗔恨。所有这些事物、这些现象,控制了我们的心,而我们无法控制自己。或许我们可以在刹那间控制自己，但我们若是处于极端的情绪当中,我们就会失去控制力。 学会放下：了解执迷、僵固所招致的覆灭 5.十二缘起法【？】十二缘起法：无明，行，识，名色，入，触，受，爱，取，有，生，老死 无明：实际上就是对标签的执着。这个标签就是“自我”、“我的”或“我”；“我”本身是个抽象的概念，对自我的强烈执着是很糟糕的，你会有着魔似的冲动去确认它 行：采取行动——当我们缺乏安全感时，必定会做一些事去证明自己的存在，比如结交朋友、上街购物… 识：行动伴随着识，如眼识、耳识… 名色：关于身份的建构 入：感官对象 触：感官的接触 受：觉受，感官碰触的刹那产生觉受 爱：渴望 取：得到，执着 生：“有”引起“生” 老死：有生必有死 谈自我。我们有自我,它缺乏安全感,同时又非常骄傲,非常的自我本位。因为这个缘故,我们必须去做一些事。当我们做的时候,产生了识、名色,创造出某个我们所归属的身份、趋势或潮流。借由名色、趋势的协助,我们在外遇到了感官的对象,当碰做到的刹那,便产生了觉受—未必是乐受,有时是受。“受”产生了,会引|起渴望,也就是“爱”,这以一个喝酒的人来表示。自我碰到乐受时,会渴望更多;碰到不悦意触或苦受时,则渴望去除它、超越它、战胜它。我们是多么执著于去解决问题,不是吗?在苦受和乐受这两种情况里,不管你是喜欢还是不喜欢这些经验,都产生某种全心全力要摆脱问题、解决问题的感觉,在这两种情况中,你都产生渴望。爱”的渴望会引「生“取”,以一个摘水果的人来表示。我们想要取取得资讯、财产、食物、饮料。一旦取得,会引生执著—你对渴望的和刚刚得到的东西紧握不放,这在十二因缘中被称为“有”,以孕妇的图像为表征。“有”引起“生”,而有了生,自然就有衰老,就有死(亡)。 6.生命之轮正常与异常的界定 正常：当某个事物不依赖于他者时。心完全放下一切对境，放下完全依赖和部分依赖他者的一切实体 从无明产生渴求，渴求是贪欲之母；从无明也产生恐惧，恐惧是嗔恨之母。 地狱道 地狱不是一种实际存在的实体，而是我们内心的一种感知，端赖我们的心，视我们的心理状态而定。 当感知来自嗔恨时，你是在体验地狱道；当感知来自执著、执取或贪吝，你在体验饿鬼道；当你的感知通过无明过滤，你在经验畜生道；当你生出很强的傲慢心，你就投生到天道；生出嫉妒心，投生到阿修罗道；而当你贪欲炽盛时，你投生到人道。 生命之轮：六道轮回 我们生命的目的就是脱离生命之轮，当你脱离这种存在，就获得了解脱。 解脱：从这六种感知解脱 人道比天道好：因为人道有选择，而天人不做选择。当你太安乐时，就没有选择，变得自负。 时间是一切道都无法逃脱的：解脱是超越时间，没有过去，没有现在，没有未来 7.见地，禅修，行为佛法：以智慧而非伦理道德为导向的道，非常强调了解实相，能够在不受文化、社会、教育或个人顾忌所干扰的情况下看待事物。 事物：独立，无常，因缘所生 见地：一切皆是无常，一切相互依存，没有一个东西是整体。这是智慧，也是同理心。 禅修：渡船到彼岸；切断所有顾忌；什么事都不做。 你在自由社会，但你在自己的顾忌中不自由。 行为：同时修持反叛和优雅。 见地 + 禅修 + 行为 8.《金刚经》开示（一）无论谁，哪怕只在一刹那对无相、不可度量等生起信心，甚或只是在一刹那起了疑问，觉得这些也许是真的，这个人就将成为佛的继承人。为什么？因为这样的人，不会被限制在称量和度量“我”、“人”、“众生”、“寿者”之中。这样的众生，即使只有一刹那的信心，也就不会被戒律、道德、法与非法、对与错的概念所困。 我们向往得到简单的指引，如同许多宗教和哲学体系所提供的指导。我们渴望有人告诉我们，如果这样或那样 做——例如拥有正确的见地和行为，就可以获得 某种成就作为奖赏，比如去天堂。 修道本身就是个骗局，是安慰剂，但它却是必要的安慰剂，因为我们有着各种各样的执着、束缚和串习，必须被斩断。 我们的一些习性和执着是如此顽固，就如同因陀罗的金刚，似乎无法被摧毁。而金刚经的智慧就在于能切断这种金刚。 菩萨心：既有度化一切众生的发心，又能了悟：并没有一个真实存在的众生可以被度化。 这不是在否定，因为否定即意味着有众生可被否定；亦不是菩萨要抛弃众生，因为抛弃即意味着有众生可被抛弃 出离心：了知执着是无用的，所以我们应该摆脱它们 佛没有见地，或者说拥有没有见地的见地 佛教倡导不杀生，不偷盗，不妄语；但如果可以利益众生，佛就该杀生，该说谎（灵活） 佛是无相的：没有特定的外在形象 无相 空性为根，无相为道，无愿为果。 佛是没有目的的旅程，随着修道，我们一直在剥除层层外皮，期待找到内在的果实。但实际上没有果实的存在。 我们不需要得到，因为我们已经拥有。我们不是在试图建造，而是试图消除。 9.《金刚经》开示（二）金刚经：最不加修饰地讲“空性”，能摧毁金刚的智慧 佛问须菩提：“你认为我讲过法吗？如果你认为我讲过法，那么你就有邪见。因为没有法可以讲。” 金刚经的关键是摧毁所有的见地，摒弃所有正确和错误的见地。 正确的见地：被称为“传统价值” 佛教：根植于智慧而非常识，常识是基于自我的解释和表现，而智慧则是基于自我的绝对反面 如果你有常识，你就不是个正常人。一个正常人没有染污，不受影响，不受条件制约。 所有的烦恼来源于见地，无论正确or错误。 佛说，我们最大的问题是习惯于收集零件。这些零件本身是和合的，意即它们是短暂和无意义的，由它们组装成了现象，我们却执着于这些现象。有什么比执着于一个标签更愚蠢呢？（无明） 10.如何寻找上师与做个弟子当我们希求的是赞美而非批评，是关注而非忽视，就很难遇到善知识了。你关上了你的门，不让真正的上师进来；他们也很难对你说实话，因为一旦他们对你说实话，你不会喜欢听。可是，一个寻求证悟的人应该是一个追求真理的人，在这条道路上，你必须听真话，尤其是非常严肃的、令人痛苦的真话。 出离心绝不是无意义的、虚无主义的消沉。 11.佛法的智慧众生的本性即是佛。 佛陀说你是自己的老师，自己的救助者，没有人能救你。 佛教的核心是智慧，而不是一般宗教倡导的伦理规范。伦理规范是增强和巩固智慧的工具，但智慧比伦理道德更重要，如果不是为了智慧，所谓的伦理道德是非常危险的。 智慧，指的是超越正确或错误。无论是做正确的事还是不做 错误的事，都不等同于智慧。只有超越善行和不善行，才能得到解脱。 佛家鼓励使用各种方法，但必须伴随智慧。非常普遍的方法是落发出家，但出家只是千万方法中的一种，其他还有供香，咒语，插花，禅宗花园等。 用俗常的语言解释智慧或般若，其意思的“绝对的正常”，即心处在绝对正常的状态。但是，沉迷在嗔恨、嫉妒、愤怒、欲望、傲慢等情绪当中，会使我们的心变得不正常。 证悟—禅定：最好的方法是放下一切，坐着什么也不做。通常我们的心总是寻求娱乐，它不能静止不动，总是被什么事情占据着**，我们不能不去做些什么。当心被占据时，我们就紧紧抓住那个占据我们的东西，心就变得不正常，变得容易激动。 12.内观禅修智慧，就是无分别。 学习忽视任何念头，使自己自由，包括后悔、喜悦。 禅修的持续性很重要，每次时间可以短，但次数要频繁。 内观禅修：非凡的了悟，深透的洞悉 修道，我们不会失去任何东西，却得到所有。得到控制权。 13.临终与死亡的忠告佛教徒相信，转世会持续不断，直至摧毁相续的因缘到来。这个因缘就是“证悟”，证悟意指从轮回相续中觉醒。 悲伤，来自于了悟事实，了解到无论你做什么，无论你多么接近成功… 当下这一刻，我们正在死亡。了悟这种死亡，其实就是在了悟生存的实相。 当你一旦成佛，就会发现自己从未是众生，从未努力修行，从未自始即受苦，你甚至也不是佛。 当你证得初地，你会发现，所有这些转世、新年、圣诞节、国籍变换、投生为各类众生——有时在天上飞，有时在水里游，全都发生在火花崩裂又熄灭的刹那之间。 死亡：毫无意义，并非结束，你的结束时再证悟前的那一刻，而在那一刻之前，有太多假的再见。因为一再重生，我曾经多次做你的丈夫、多次被你抛弃、多次为你而死；我曾多次被活炸,只为满足你的口腹之欲，反之亦然。这就是佛教徒的思维方式。如果你能接受这样的观点，我们才能稍微谈论死亡。 密乘佛教认为死亡是生命中最好的机会，如果懂得运用这机会，它会是个不寻常的加持。因为在死亡状态下，所有东西都在分崩离析，你的眼睛看不到，耳朵听不到，身体无法再有任何感觉，这时你得到一个机会，你的心识有生以来第一次处在一个最自由、最赤裸、最有力量的状态。 佛陀说：心。心无。心是明。 “中阴”：意思是“间隔” 生存中阴：从出生到死亡的过程 不确定性 漂泊性：众生如同风中羽毛，没有自我掌控力 忙碌性：我们害怕不够忙，因为如果不忙，你就是没用的人；贫乏的心态：总觉得拥有的不够 睡梦中阴：从入睡到醒来 类似短暂的死亡，同时享有每日清晨再次醒来的奢侈 禅定中阴 临终中阴 法性中阴 受生中阴 14.如何积聚福德佛法的参照的永远是某件事情是否更接近于实相真理，任何带你更接近于实相的，就是福德；任何带你远离实相的，就是缺乏福德。 诸法因缘生。 福德是可以由你去创造、积聚的东西。 福德是完全相对的。譬如说，你开车去某个地方并顺利抵达，可是你到处都找不到停车位。假设你是去赴一场美妙的约会，结果只因为找不到停车位而把一切都毁了，这就是缺乏福德；但假设你因为找不到停车位，正开车到处转的时候，你本来要进去的那栋大楼突然坍塌了这就是有福德。 福德带你接近实相。 积聚福德的三种训练： 智慧的训练： 开放的心胸，放掉一切顾忌，不受文化、种族、性别、观念、宗教等的束缚，超越一切 在谦卑和自信之间找到平衡，不断对自己所下的结论保持怀疑态度 佛教是最具 批判性、最多疑的哲学系统，最无神论 智慧：一切都是空性，同时，外显不造成任何障碍【？】 禅定的训练 戒律的训练 自己规范自己 15. 如何同时成为修行者与生意人修行即生活，生活即修行。修行不一定是只存在于寺庙之中，不一定要落发出家。 如何同时成为修行者与生意人？ 小乘行者：好好经营你的事业，舍弃伤害别人的因和伤害别人的行为。纯净的动机，就算无法帮助别人，至少也可以不伤害。 做到不伤害很难，比如吃肉是对动物的伤害；杀生只是一种粗重的伤害方式，而生活中许多细微的伤害行为往往被我们忽略。 大乘行者：不仅不伤害人，还要帮助人。 最重要的是：帮助人的动机是否纯净？即慈悲与爱心 心智上的帮助也是帮助，比如由衷的赞美 因乘与果乘 修法的主要目的在清净 因乘： 被洗掉的污垢是我执、傲慢 果乘：衣服从来不需要清洗，衣服永远是衣服，它不是脏的也不是干净的，你无法让它干净，因为你要让一个东西变得干净，它必须原来就是脏的。所以我们不是在洗衣服，而是在洗污垢。 金刚乘行者：最重要的是净观 断除分别心看人：不要以judge的眼光看待任何人，起码要把大家看成不好也不坏 大手印者：须知每件事都是一味，不去分别好坏美丑，减少极端的想法 每样东西都是同样的味道。 每一件事都源自经验与感受，所有这些经验感受不论好坏都是心制造出来的。 假如你从来没有眼睛，你会有视觉吗？会有所谓的美丑之分吗？… 把所有感官摧毁掉，最后就什么也没有了，所有东西就只是存在着，无论美丑。正是因为有感官，所以会有judge，于是制造了美丑。好与坏并非真正存在，而是由不同人捏造出的，这些捏造来源自习惯。 16.四法印我们为什么要佛教？ 人总是在追求乐趣：我们可以有一个很好的、持久的、便宜的、且可随身携带的乐趣。 了解一切和合事物皆无常。 我们之所以无法尽情地享乐，基本上只有一个原因：我们总是看着某个虚假、不可靠、不确定的事物，并且认为那是真实的——绝对的真实。这基本上就是悉达多所谓的“无明”。 佛教的由来 佛陀找到了真理，而后人为了使佛法易于理解和入门故将这个真理分门别类，但之后令人遗憾的事情发生了，这些方法成为所谓的佛教——一种宗教。这真的很令人遗憾，因为我们开始被方法所吸引，而非真理本身。 可能有人会问:“为什么一开始会有这些不同的工具或方法呢?”这是一个颇具挑战性的问题。举个例子。 首先,如果这个杯子一直到杯口都装满了非常纯净的水,那么你是看不到水的,因为水太透明了。为了让你能看到水,我们要怎么做呢?我们把水稍微染黄,这就帮助你看到水了。所以你在佛教里看到的种种方法,不外乎就像染色而已。换句话说,所有所谓佛教的方法,容我坦白地说,都是虚假的,它们不是那个真正的真理。但如果你间我,它们有用吗?我要回答说非常有用,没有它们,你就看不到真理。可是我们的问题在于,我们被颜色转移了注意力,而忘记了水。 见地：是我们的驱动力。 整体和部分的观点 无常：发展、变化的观点；无论什么境地下，都能泰然自若 联系的观点 出家剃光头的目的：是一种提醒你“无常”的方式，而不是佛对长头发过敏。所有这些仪式和制度都是要引领你走向真理，但不幸的是，我们有时对仪式过于着迷，反而忘记背后隐藏的意义——无常。 佛陀的四大见地 一切和合事物皆无常 一切源自自我的情绪皆苦 一切皆空性 涅槃超越边见 我们存在的问题 不了解一切和合事物皆无常 我们一直活在不确定感之中：人类因为得不到自己所求而一直置身于不确定感所带来的痛苦中。带来痛苦的不是外在的恶煞，而是“我执”。我们的一切情绪，诸如爱、恨、嫉妒等基本都来自于我执。无我：并不是因为自我很邪恶，所以说执着于自我导致痛苦，而是自我根本不存在，所以说我执是错误的，这也是我们受苦的原因。 无明：就是当你看着某个东西，你认为它牢不可破，但其实它是由一些短暂、无常的东西组合而成。 比如，13在12、14之间是13，但在A、C之间就变成了B。任何事物都像13或B一样，某些东西被组合在一起会产生某种作用，但这个作用可以因为加入新的元素而改变。 自我其实也是许多元素的组合，当一些元素组合在一起就产生了一个“自我”的 概念，可是它根本不存在。但这正是你所执着的东西，所以很荒谬。 我们认为有一个“自我”，然后我们搭建了很多藏身之处，让自我感到舒适、安全而有力量，比如钱财、权力、影响力、友谊等。 如果没有自我，就没有市场经济。因为不安全感消失了，没有了不安全感，就没有生意可做。 有人试着告诉我们,我们缺少某样东西,应该拥有某样东西,好让这个不存在的自我感到更安全、更舒适一点。而在这个让并不存在的自我更舒适的过程中,从宏观来看,我们摧毁了我们的世界,摧毁了环境,摧毁了一切;从微观来看,当我们对朋友与家人说我们爱他们,我们其实爱的是自己,我们想把他们放在架子上,以便在想用的时候随时可以取到。 佛陀：“若以色相见我，是邪见。”我们所谈的真理就是佛，真理无色无形。佛像之所以是金色，是因为我们要用某种方式让人民对这个真理有兴趣，这是一种引导的方式。佛教的象征都是设计出来的，是为了吸引你引导你走向真理。只要你仍受制于色彩和形状，你就仍受制于和合的现象。 人类的目标是追求乐趣。生命只是个幻相，你的目标应该是知道如何跟这个幻相玩儿。 比如，你梦见身边躺了一只老虎,如果你不知道这是一场梦,你就有麻烦;而一旦你了解这是梦,你就可以跟这只老虎玩儿。你可以骑在它上面,可以抚摸它,拔它的虎须,随便你怎么玩儿。这就是我们的目标。 四大皆空。空性就是，你所见的或事物所展现的，并不是其真实的样子。 空性是什么，举个例子。晚上享受烛光晚餐时，你觉得伴侣看起来很美;但早上起来看到身边的人,你突然心想:“这是谁呀?鬼啊?”所以事物怎么显现,一个人看起来如何,无论美丑,都不是其真实的样子。 我们的价值观、常识反映的都不是事物的真实面貌。比如我们认为钻石很珍贵，但如果钻石太多，反而成了垃圾。比如你认为美或丑的，在别人看来也许是相反的。美与丑并不存在，是空性。 为什么要出离 世间生活是和合的，无常的，徒然无益 执着于世间生活会产生痛苦 因为没有所谓的世间生活可以出离 佛教，我们谈内在佛不谈外在佛。]]></content>
      <categories>
        <category>Product Manager</category>
        <category>Philosophy</category>
      </categories>
      <tags>
        <tag>Reading Notes</tag>
        <tag>Buddhology</tag>
        <tag>Philosophy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[统计数据分析|SPSS操作学习笔记]]></title>
    <url>%2F2018%2F04%2F25%2F%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-SPSS%E6%93%8D%E4%BD%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Abstract：灰常全面的SPSS 学习笔记 ~ // 最近传播统计学课程要进行 SPSS 实操了 w 一、SPSS工具简介1.1 SPSS 介绍 SPSS（Statistical Product and Service Solutions，统计产品与服务解决方案软件） 提供的服务：统计学分析运算、数据挖掘、预测分析和决策支持任务 特点： 图形界面进行统计分析，操作简单 包含几乎全部尖端统计分析方法（包括机器学习、神经网络等），具备完善的数据定义，操作管理和开放的数据接口，灵活美观的统计图表制作 1.2 SPSS 的运行模式 批处理模式 这种模式把已编写好的程序（语句程序）存为一个文件，提交给[开始]菜单上[SPSS for Windows]→[Production Mode Facility]程序运行。 完全窗口菜单运行模式 这种模式通过选择窗口菜单和对话框完成各种操作。用户无须学会编程，简单易用。 程序运行模式 这种模式是在语句（Syntax）窗口中直接运行编写好的程序或者在脚本（script）窗口中运行脚本程序的一种运行方式。这种模式要求掌握SPSS的语句或脚本语言。 混合运行模式 以上各种方法的综合运行方式。 1.3 SPSS主要窗口介绍 数据编辑窗口 构成：标题栏、菜单栏、工具栏、编辑栏、变量名栏、观测序号、窗口切换标签、状态栏 ​ 窗口切换标签：用于“数据视图”和“变量视图”的切换，即数据浏览窗口与变量浏览窗口。数据浏览窗口用于样本数据的查看、录入和修改。变量浏览窗口用于变量属性定义的输入和修改。 SPSS菜单功能简介： 结果输出窗口 作用：在 SPSS 中大多数统计分析结果都将以表和图的形式在结果观察窗口中显示。 构成：窗口右边部分显示统计分析结果，左边是导航窗口，用来显示输出结果的目录，可以通过单击目录来展开右边窗口中的统计分析结果。 调出：当用户对数据进行某项统计分析，结果输出窗口将被自动调出。用户也可以通过双击后缀名为.spv 的 SPSS 输出结果文件来打开该窗口。 语法窗口 用户可以在语句窗口中直接编写SPSS命令程序，也可以使用Paste按钮把菜单运行方式下的各种命令和选项粘贴到命令窗口中，再进行进一步修改，然后通过运行主菜单的运行命令将编写好的程序一次性地提交给计算机执行。 脚本窗口 二、数据文件创建2.1 数据文件介绍 SPSS数据文件是一种结构性数据文件，由数据的结构和数据的内容两部分构成，也可以说由变量和观测两部分构成。一个典型的SPSS数据文件如下表所示： SPSS变量的属性 SPSS中的变量共有11个属性，分别是变量名（Name）、变量类型（Type）、长度（Width）、小数点位置（Decimals）、变量名标签（Label）、变量名值标签（Value）、缺失值（Missing）、数据列的显示宽度（Columns）、对其方式（Align）、度量尺度（Measure）和角色。 定义一个变量至少要定义它的两个属性，即变量名和变量类型，其他属性可以暂时采用系统默认值，待以后分析过程中如果有需要再对其进行设置。 在spss数据编辑窗口中单击变量视图标签，进入变量视窗界面（如图1.5所示）即可对变量的各个属性进行设置。 2.2 数据文件创建操作 创建文件 读取外部数据 数据编辑 对数据进行基本编辑操作的功能集中在Edit菜单中。Edit菜单中的数据编辑功能如下表所示： 数据保存 ​ 数据整理 （1）数据排序（Sort Case） 作用：对数据按照某一个或多个变量的大小排序将有利于对数据的总体浏览。 操作：选择菜单 Data→Sort Case （2）抽样（Select Case） 作用：在统计分析中，有时不需要对所有的观测进行分析，而可能只对某些特定的对象有兴趣。利用SPSS的Select Case命令可以实现这种样本筛选的功能。 操作：选择菜单 Data→Select Case命令 （3）增加样品的数据合并（Merge File→Add cases） 作用：将新数据文件中的观测合并到原数据文件中 操作：选择菜单Data→Merge Files→Add Cases （4）增加变量的数据合并（Merge File→Add variables） 作用：增加变量时指把两个或多个数据文件实现横向对接。例如将不同课程的成绩文件进行合并，收集来的数据被放置在一个新的数据文件中。 操作：选择菜单Data→Merge Files→Add Variables （5）数据拆分（Split File） 作用：对文件中的观测进行分组，按组分别进行分析。 操作：选择菜单Data→Split File （6）计算新变量 作用：在对数据文件中的数据进行统计分析的过程中，为了更有效地处理数据和反映事务的本质，有时需要对数据文件中的变量加工产生新的变量。比如经常需要把几个变量加总或取加权平均数，SPSS中通过Compute命令来产生这样的新变量。 操作：选择菜单Transform→Compute 2.3 实操小练习下表为某大学的一个问卷调查，主要内容是关于教师的基本情况调查，以及对学校科研管理和服务的意向调查。请利用SPSS软件，将问卷调查结果显示成SPSS可识别的数据文件，如图： 注意： 通过编辑变量值标签，使得表中有备选的项目，只需从下拉菜单选择即可。没有备选项的需自行填入数据。 请区分变量类型和变量尺度。 对数据进行排序、转置、拆分操作。 三、描述统计3.1 描述统计介绍 描述性统计分析是统计分析的第一步，做好这一步是进行正确统计推断的先决条件。通过描述性统计分析可以大致了解数据的分布类型和特点、数据分布的集中趋势和离散程度，或对数据进行初步的探索性分析（包括检查数据是否有错误，对数据分布特征和规律进行初步观察）。 ​ 统计原理 描述统计是统计分析的基础，它包括数据的收集、整理、显示，对数据中有用信息的提取和分析，通常用一些描述统计量来进行分析。 集中趋势的特征值：均值、众数、中位数等。其中均值适用于正态分布和对称分布资料，中位数适用于所有分布类型的资料。 离散趋势的特征值：全距、内距、方差、标准差、离散系数等。其中标准差、方差适用于正态分布资料。 分布特征值：偏态系数、峰度系数、他们反映了数据偏离正态分布的程度。 3.2 描述统计操作3.2.1 频数分析（Frequenccies） 基本统计分析往往从频数分析开始。通过频数分析能够了解变量取值的状况，对把握数据的分布特征是非常有用的。比如，在某项调查中，想要知道被调查者的性别分布状况。频数分析的第一个基本任务是编制频数分布表。SPSS中的频数分布表包括的内容有： 频数（Frequency）即变量值落在某个区间中的次数。 百分比（Percent）即各频数占总样本数的百分比。 有效百分比（Valid Percent）即各频数占有效样本数的百分比。这里有效样本数＝总样本－缺失样本数。 累计百分比（Cumulative Percent）即各百分比逐级累加起来的结果。最终取值为百分之百。 频数分析的第二个基本任务是绘制统计图。统计图是一种最为直接的数据刻画方式，能够非常清晰直观地展示变量的取值状况。频数分析中常用的统计图包括：条形图，饼图，直方图等。 操作：选择菜单 分析—&gt;描述统计—&gt;频率 3.2.2 描述统计（Descriptives）操作：选择菜单 分析—&gt;描述统计—&gt;描述 3.2.3 探索分析（Explore） 调用此过程可对变量进行更为深入详尽的描述性统计分析，故称之为探索分析。它在一般描述性统计指标的基础上，增加有关数据其他特征的文字与图形描述，显得更加细致与全面，对数据分析更进一步。 探索分析一般通过数据文件在分组与不分组的情况下获得常用统计量和图形。一般以图形方式输出，直观帮助研究者确定奇异值、影响点、还可以进行假设检验，以及确定研究者要使用的某种统计方式是否合适。 操作：选择菜单 分析—&gt;描述统计—&gt;探索 因变量列表；待分析的变量名称，例如将每股收益率作为研究变量。 因子列表：从源变量框中选择一个或多个变量进入因子列表，分组变量可以将数据按照该观察值进行分组分析。 标准个案：在源变量表中指定一个变量作为观察值的标识变量。 在输出栏中，选择“两者都”，表示输出图形及描述统计量。 输出结果 Case Processing Summary 表 Descriptives 表 直方图 茎叶图描述 茎叶图自左向右可以分为3 大部分：频数（Frequency）、茎（Stem）和叶（Leaf）。茎表示数值的整数部分，叶表示数值的小数部分。每行的茎和每个叶组成的数字相加再乘以茎宽（Stem Width），即茎叶所表示的实际数值的近似值。 箱图 方箱中的中心粗线为中位数。箱图中的触须线是中间的纵向直线，上端截至线为变量的最大值，下端截至线为变量的最小值。 3.3 实操小练习1.打开SPSS自带的Employee data.sav文件，依照上述操作自行完成一次分析。 2.在财经网站调查如下几个公司的业绩报表，依照前面的例子完成描述性分析。 （000725，000735，002507，600016） 3.表2.7为某班级16位学生的身高数据，对其进行频数分析，并对实验结果做出说明。 表2.7 某班16位学生的身高数据 学号 性别 身高（cm） 学号 性别 身高（cm） 1 M 170 9 M 150 2 F 173 10 M 157 3 F 169 11 F 177 4 M 155 12 M 160 5 F 174 13 F 169 6 F 178 14 M 154 7 M 156 15 F 172 8 F 171 16 F 180 4.测量18台电脑笔记重量，见表2.8，对其进行描述统计量分析，并对试验结果做出说明。 表2.8 18台笔记本电脑重量表 序号 1 2 3 4 5 6 7 8 9 重量 1.75 1.92 1.59 1.85 1.83 1.68 1.89 1.70 1.79 序号 10 11 12 13 14 15 16 17 18 重量 1.66 1.80 1.83 2.05 1.91 1.76 1.88 1.83 1.79 四、统计推断统计原理 参数估计的基本原理 假设检验的基本原理 4.1 统计推断操作4.1.1 单个总体均值的区间估计 例子：为研究在黄金时段中,即每晚8:30-9:00 内,电视广告所占时间的多少。美国广告协会抽样调查了20个最佳电视时段中广告所占的时间（单位：分钟）。请给出每晚8:30 开始的半小时内广告所占时间区间估计，给定的置信度为95％。 操作：进行探索分析 4.1.2 两个总体均值之差的区间估计（独立样本 T 检验） 例题：The WallStreet Journal（1994,7）声称在制造业中，参加工会的妇女比未参加工会的妇女的报酬要多2.5 美元。想通过统计方法，对这个观点是否正确给出检验。 假设抽取了7位女性工会会员与8位非工会会员女性报酬数据。要求对制造业中参加工会会员的女性报酬与未参加工会的女性报酬平均工资之差进行区间估计，预设的置信度为95％。 操作：菜单——分析——比较均值——独立样本 T 检验 4.1.3 单个总体均值的假设检验 （单样本T检验） 例子：某种品牌的沐浴肥皂制造程序的设计规格中要求每批平均生产120 块肥皂，高于或低于该数量均被认为是不合理的，在由10 批产品所组成的一个样本中，每批肥皂的产量数据见下表，在0.05 的显著水平下，检验该样本结果能否说明制造过程运行良好？ 操作：菜单——分析——比较均值——单样本 T 检验 4.1.4 两独立样本的假设检验（两独立样本T检验）** 例题：The WallStreet Journal（1994,7）声称在制造业中，参加工会的妇女比未参加工会的妇女的报酬要多2.5 美元。想通过统计方法，对这个观点是否正确给出检验。 假设抽取了7位女性工会会员与8位非工会会员女性报酬数据。要求对制造业中参加工会会员的女性报酬与未参加工会的女性报酬平均工资之差进行区间估计，预设的置信度为95％。 操作：菜单——分析——比较均值——独立样本 T 检验 4.1.5 配对样本T检验 配对样本是对应独立样本而言的，配对样本是指一个样本在不同时间做了两次试验，或者具有两个类似的记录，从而比较其差异；独立样本检验是指不同样本平均数的比较，而配对样本检验往往是对相同样本二次平均数的检验。 配对样本T检验的前提条件为：第一，两样本必须是配对的。即两样本的观察值数目相同，两样本的观察值顺序不随意更改。第二，样本来自的两个总体必须服从正态分布。例如针对试验前学习成绩何智商相同的两组学生，分别进行不同教学方法的训练，进行一段时间试验教学后，比较参与试验的两组学生的学习成绩是否存在显著性差异。 例子：假设某校为了检验进行新式培训前后学生的学习成绩是否有了显著提高，从全校学生中随机抽出30名进行测试，这些学生培训前后的考试成绩放置于数据文件“学生培训.sav”中。 操作：菜单——分析——比较均值——配对样本 T 检验 4.2 实操小练习1．某省大学生四级英语测验平均成绩为65，现从某高校随机抽取20份试卷，其分数为：72、76、68、78、62、59、64、85、70、75、61、74、87、83、54、76、56、66、68、62，问该校英语水平与全区是否基本一致？设α＝0.05 2．分析某班级学生的高考数学成绩是否存在性别上的差异。数据如表所示： 某班级学生的高考数学成绩 性别 数学成绩 男（n＝18） 85 89 75 58 86 80 78 76 84 89 99 95 82 87 60 85 75 80 女（n＝12） 92 96 86 83 78 87 70 65 70 65 70 78 72 56 SPSS自带的数据文件world95.sav中，保存了1995年世界上109个国家和地区的部分指标的数据，其中变量“lifeexpf”,“lifeexpm”分别为各国或地区女性和男性人口的平均寿命。假设将这两个指标数据作为样本，试用配对样本T检验，女性人口的平均寿命是否确实比男性人口的平均寿命长，并给出差异的置信区间。（设α＝0.05） 五、Mac SPSS 破解版安装 SPSS中文版：https://pan.baidu.com/s/1eSOaAII （百度网盘下载，密码：w59n） 激活方法 需安装java环境：apple官方下载地址 https://support.apple.com/kb/DL1572?locale=zh_CN&amp;viewlocale=zh_CN 打开SPSS Statistics License Authorization Wizard,在弹出的界面中输入 SN.txt 中的激活码 所有dmg、zip的解压密码：xclient.info]]></content>
      <categories>
        <category>Computer Science</category>
        <category>Data Mining and Analysis</category>
      </categories>
      <tags>
        <tag>Data Mining and Analysis</tag>
        <tag>SPSS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python Numpy 矩阵运算(附实例和学习材料)]]></title>
    <url>%2F2018%2F04%2F20%2FPython-Numpy-%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97-%E9%99%84%E5%AE%9E%E4%BE%8B%E5%92%8C%E5%AD%A6%E4%B9%A0%E6%9D%90%E6%96%99%2F</url>
    <content type="text"><![CDATA[Abstract：使用numpy进行矩阵的加减乘除、转置、求逆、求特征向量等运算。 1.numpy矩阵运算语法 import numpy库：import numpy as np 创建矩阵：a = np.mat([[x1, x2,…,xn], [y1, y2, …, yn]]) 矩阵乘法： a * b np.dot(a, b )：求两数组点积 矩阵加减：a - b 矩阵转置： a.T a.transpose() 矩阵除法：a/b 矩阵求逆： a.I np.linalg.inv(a) 求特征向量：np.linalg.eig(a) 求迹：np.trace(a) 求行列式的值：numpy.linalg.det() 求两个数组的矩阵乘积：numpy.matual(a, b) 求矩阵形式的线性方程的解：numpy.linalg.solve() 延伸：numpy.linalg 模块里还有很多矩阵运算的方法，感兴趣的可以去了解下 （在文末的Extender里给出了学习材料 Tutorials Numpy 教程）~ 关于矩阵乘法需要注意： 矩阵乘法规定，只有当第一个矩阵的列数（column）和第二个矩阵的行数（row）相同时才有意义。 若a为 m x p 的矩阵，B为 p x n的矩阵，则 a x b 的结果是 m x n的矩阵。 若两矩阵行列数不满足上面的条件，可以对矩阵进行转置。 维度数不符合要求，则不能执行矩阵乘法。 2.numpy 数组语法 随机生成数组：np.random.rand(m, n) 创建数组：np.array([[x1, x2,…,xn], [y1, y2, …, yn]]) 向数组添加元素：np.append(A, a_i) ​数组索引：A[2:5] 表示索引数组 A 中第 3 到第 5 个元素 3. 实例（求多元回归最小二乘估计）多元回归方程参数最小二乘估计的公式：$B=(X^TX)^{-1}X^TY$ 代码实例： 123456789101112import numpy as npX = np.mat([[1,1,1,1,1,1,1,1,1,1,1,1,1,1],[41, 45, 51, 52, 59, 62, 69,72, 78, 80, 90, 92, 98, 103], [49, 58, 62, 71, 62, 74, 71, 74, 79, 84, 85, 94, 91, 95]])X=X.TY = np.mat([28, 39, 41, 44, 43, 50, 51, 57, 63, 66, 70, 76, 80, 84]).TB = (X.T * X).I * X.T * Yprint B 123[[-15.93836228] [ 0.52227044] [ 0.47382726]] Extender【非常全的教程】TutorialsPoint Numpy 教程 运用numpy进行数组、向量、矩阵运算 70个NumPy练习：在Python下一举搞定机器学习矩阵运算 Numpy 小结]]></content>
      <categories>
        <category>Computer Science</category>
        <category>Data Mining and Analysis</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Numpy</tag>
        <tag>Data Mining and Analysis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我的AI PM实习面经(腾讯AILAB，商汤，旷视，网易)]]></title>
    <url>%2F2018%2F04%2F16%2F%E6%88%91%E7%9A%84AI-PM%E5%AE%9E%E4%B9%A0%E9%9D%A2%E7%BB%8F-%E8%85%BE%E8%AE%AFAILAB%EF%BC%8C%E5%95%86%E6%B1%A4%EF%BC%8C%E6%97%B7%E8%A7%86%EF%BC%8C%E7%BD%91%E6%98%93%2F</url>
    <content type="text"><![CDATA[Abstract：2018年3~4月找AI PM暑期实习和面试经验总结 ~ 大概花了1~2周，面了几家公司，拿了商汤和旷视的AI PM实习offer（开心辣） 一、投岗1.岗位需求状况：目前的AI PM实习岗位需求少 今年3、4月份时各大互联网公司、AI公司都启动了春招流程，但我发现互联网公司开放的产品岗基本都是互联网产品岗，基本没有AI PM的公开招聘岗位。只有百度、搜狗、MSRA等有非常少量的AI产品岗，比如搜狗的机器翻译PM岗、MSRA的PM岗、网易的AI产培生。还有少量与AI有一定关系的产品岗，如搜狗和360的搜索引擎PM。 而AI创业公司（我主要看的是CV和语音交互领域的公司）的官网上也很少有招PM实习生的。 2.投岗方式 我采用了3种投岗方式 ： （1）内推：效率最高的方式 通过团长内推，得到了腾讯AI Lab的面试机会 通过学校的一位学长内推，得到了商汤科技的面试机会 （2）直接投递hr邮箱自荐 方法说明：一般公司官网上的“联系我们”页面都会留有公司的hr邮箱，可以给hr邮箱发自荐信，回复率低 自荐信的构成建议：50~100字左右的自荐说明（简要阐述个人亮点） + 个人简历 + 作品集 + 优质输出文档（描述自己在AI PM方向上所做的努力和成果） 我：通过这个方式得到旷视科技的面试机会 （3）官网投递 投了网易PM599-AI领域的产培生项目，做了笔试，不过懒得做素质测评，所以估计凉了 ​ 二、面试2.1 面试前准备1.一个1min的自我介绍：重点抛出自己的亮点，为面试官挖下深入追问的“坑” 2.复盘自己做过的重要项目、产品，思考面试官可能会问哪些问题 3.了解面试公司/部门的主要业务 4.了解面试公司/部门的主要技术的原理和应用，自己思考可用这些技术创新设计什么样的产品 5.思考有什么关于公司/业务/岗位的问题想问面试官 2.2 腾讯AI Lab面试1.一面面试时长：2个多小时 面试官背景：AI基础研究员（技术岗） 面试过程： 1.自我介绍 2.项目经历：针对我做的一个大数据舆情分析的项目，问的非常细致（侧重了解我是如何制定项目框架和流程的，以及项目中用到的AI算法模型和我对这些算法模型的理解） 3.产品经历：让我介绍一个我做过的产品（我挑了一个应用到了NLP技术的产品讲，主要介绍了产品定位、用户需求、开发流程、使用了哪些技术） 3.聊了我对AI音箱的观点、对腾讯AI产品的观点、对整个AI行业和AI产品化的看法 4.面试官给我留了2个“作业”：是2个腾讯AI LAB在实际业务场景的有关AI产品化的问题，让我私下去思考，并建议我在二面时向二面面试官阐述我的方案 2.二面面试时长：1个多小时 面试官背景：AI基础研究员（技术岗） 面试过程： 1.自我介绍 2.主要在聊我对一面时留的2个问题的思考（主要有关对话AI） 3.面试结果面试体验：一面和二面的面试官小哥哥人都非常nice，全程给我充分的主导权和自由表达想法的机会 ，面试过程很轻松愉快，交流了很多想法，并且给了我很多面试反馈和建议。 面试结果：两次面试的面试官都反馈不错，但最后因为我个人的实习时长不能满足他们的要求，所以很遗憾没能拿到实习机会。 2.3 商汤科技面试1.一面面试时长：40多分钟 面试官背景：商汤Senior PM 面试问题： 对人工智能的技术和应用的了解 对CV技术在实际生活中的应用的了解 对AR/VR技术的实际应用的了解 我的大数据舆情分析项目、产品项目 项目中的职责 如果要改进，会怎么改进这个产品 2.二面面试时长：1个小时 面试官背景：商汤Senior PM 面试问题： 为什么我作为PM还会去深入了解技术、甚至自己动手实践技术 深入挖掘了我的舆情分析项目：项目目标，项目职责（作为PM），如何划定分工，如何制定项目框架，如何改进完善这个项目（从PM角度思考）… 面试官向我介绍了一个商汤为某客户开发的一个cv产品，要求我分析这个产品的用户群体、用户市场、用户需求、应用场景、应用场景可以如何拓展（我答的跟面试官之后跟我讲的他们想的应用场景的思路很相似；我还补充了自己之前做过的一个类似的项目） 在最后的提问环节我问的问题 商汤的自我定位、主要业务 商汤 vs 海康威视 vs 旷视 商汤PM的工作内容 3.面试结果拿到了商汤的AI PM暑期实习offer ~ 2.4 旷视科技面试面试时长：40多分钟 面试官背景：有技术背景的AI PM 面试问题 注：面试官把我当做一个AI算法PM来问，所以全程都在考察我对模型、算法的了解 计算文本相似度的技术实现流程细节 针对我的舆情分析项目深入问了一些问题：主要考察对项目中用到的AI技术的理解 python的具体语法 机器学习模型的技术原理：要求随意讲解一种机器学习模型 问到了我的某个应用到了CV技术的产品项目的具体技术实现细节：比如该产品使用的返回了什么参数，以及如何与后面的算法对接起来 后续聊到了我对薪资的要求、对什么类型的PM工作更偏好 面试结果：拿到了旷视的AI PM实习offer ~ 2.5 网易PM599-AI产培生笔试笔试120分钟，三道简答题，其中两道必答题和一道三选一，我做的所有题目如下： 1.设计一款移动场景下的人工智能设备 2.设计一款智能家居产品 3.设计一款未来的智能交通工具 以上三道题主要考察AI产品设计能力，都要求写清楚：产品形态，交互方式 ，用户主体，用户需求，产品功能设计，所用技术和技术实现方案等。 2.5 面试总结1.面试官问什么类型的问题跟面试官的个人经历、业务背景、对AIPM的看法、PM的岗位类型有关，大部分面试官是更侧重考察pm思维的，少数面试官会更看侧重面试者对AI技术原理的了解。 2.不少面试官是AI技术岗转PM或者兼当PM，可能出于是企业业务的发展需求所以兼任PM，也可能是个人有想法懂需求懂行业所以转PM。 3.对AI技术的不了解可能是限制互联网PM转型AI PM的关键因素，也许是在比较大的工作压力下没有充足的时间去学习了解AI技术，也许是已经没有转型的勇气和激情。 4.AI PM需要懂技术但也不能被技术限制了思维，但最关键的是对市场和用户需求的洞察能力，以及基于此发现新的AI应用场景的能力和AI产品应用原始创新能力（这些也是面试官最看重的能力）。 5.如果能在面试中提出对面试公司的技术的产品化方案、或者现有产品的改进方案，会给自己增分很多。 6.讲述自己的项目/产品经历时，有详有略，挑面试官关心的重点讲，简明扼要，别说太多废话；侧重讲自己的实习/项目/产品经历中与面试公司的技术应用方向相似的经历。 7.回答技术性问题时，对自己没有把握的部分可以不用说得太细节（比如说自己对模型算法的理解即可，对具体实现细节可以略过），以防说错被怼。 8.面试快结束时，面试官一般都会问“你有什么问题想问我的吗？”，一定要重视这一part，提出有想法的问题能够提高面试官的印象。]]></content>
      <categories>
        <category>Artificial Intelligence</category>
        <category>AI PM</category>
      </categories>
      <tags>
        <tag>AI PM</tag>
        <tag>Product Manager</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLP|中文分词技术及应用]]></title>
    <url>%2F2018%2F04%2F15%2FNLP-%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E6%8A%80%E6%9C%AF%E5%8F%8A%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[摘要：中文分词是中文信息处理的重要基础，本文详细阐述了目前主要的几种中文分词算法的技术原理 、中文分词目前的瓶颈和评价准则，以及中文分词的具体应用。 中文分词指将一个汉字序列切分成一个个单独的词。现有的中文分词算法有五大类：基于词典的方法，基于统计的方法，基于规则的方法，基于字标注的方法，基于人工智能技术（基于理解）的方法。中文分词目前主要有四个瓶颈，分别是分词歧义、未登录词识别、分词粒度问题、错别字和谐音字规范化。中文分词有五大评价准则：分词正确率，切分速度，功能完备性，易扩充性和可维护性，可移植性。中文信息处理包括三个层次：词法分析，句法分析，语义分析，其中中文分词是词法分析的第一步，非常重要。中文分词是大部分下游应用的基础，这些下游应用小到POS词性标注、NER命名实体识别，大到自动分类、自动摘要、自动校对、语言模型、机器翻译、搜索引擎、语音合成等等。 一、 中文分词是什么中文分词是中文信息处理的基本技术，指将一个汉字序列切分成一个个单独的词。分词就是将连续的字序列按照一定的规范重新组合成词序列的过程。 词是最小的能够独立活动的有意义的语言成分，英文单词之间是以空格作为自然分界符的，而汉语是以字为基本的书写单位，词语之间没有明显的区分标记。 二、中文分词的技术原理2.1 中文分词算法现有的中文分词算法有五大类：基于词典的分词方法，基于统计的分词方法，基于规则的分词方法，基于字标注的分词方法，基于人工智能技术（基于理解）的分词方法。 图1：中文分词算法总结 2.1.1 基于词典的方法基于词典的方法：字符串匹配，机械分词方法 原理：按照一定策略将待分析的汉字串与一个“大机器词典”中的词条进行匹配，若在词典中找到某个字符串，则匹配成功。 按照扫描方向的不同：正向匹配 &amp; 逆向匹配 按照长度的不同：最大匹配 &amp; 最小匹配 按照是否与词性标注过程相结合：单纯分词方法 &amp; 分词与标注相结合 1.正向最大匹配算法（MM）步骤 从左向右取待切分汉语句的m个字符作为匹配字段，m为大机器词典中最长词条个数 查找大机器词典并进行匹配，若匹配成功，则将这个匹配字段作为一个词切分出来；若匹配不成功，则将这个匹配字段的最后一个字去掉，剩下的字符串作为信我的匹配字段，进行再次匹配，重复以上过程直到切分出所有词为止 2.邻近匹配算法邻近匹配算法 对正向最大匹配算法的改进，因为正向正向最大匹配算法对每个不存在的长字符串都要进行一次二分搜索，算法复杂度太高，可以利用同一个首字符下的词条按升序排列这一条件，在找到某个字符串后，在其后增加一个字得到一个新字串，如果新字串在词典中出现，那么新词一定在原字串的后面，且相隔位置不会太远 优点：可以加快匹配进程 3.逆向最大匹配算法（RMM）逆向最大匹配算法 是正向最大匹配的逆向思维（最大匹配的顺序不是从首字母开始，而是从末尾开始，由右向左），匹配不成功，将匹配字段的最前一个字去掉 优点：逆向最大匹配算法要优于正向最大匹配算法（实践证明） 4.双向最大匹配法(Bi-directction Matching method,BM)双向最大匹配法：将正向最大匹配法得到的分词结果和逆向最大匹配法的到的结果进行比较，从而决定正确的分词方法。 中文中90.0％左右的句子，正向最大匹配法和逆向最大匹配法完全重合且正确，只有大概9.0％的句子两种切分方法得到的结果不一样，但其中必有一个是正确的（歧义检测成功），只有不到1.0％的句子，或者正向最大匹配法和逆向最大匹配法的切分虽重合却是错的，或者正向最大匹配法和逆向最大匹配法切分不同但两个都不对（歧义检测失败）。这正是双向最大匹配法在实用中文信息处理系统中得以广泛使用的原因所在。 5.最短路径匹配算法（Shortest path match）最短路径匹配算法 根据词典，找出字串中所有可能的词（也称全分词），然后构造词语切分有向无环图 每一个词对应图中的一条有向边。若赋给相应的边长一个权值（该权值可以是常数，也可以是构成的词的属性值），然后针对该切分图，在起点到终点的所有路径中，求出最短路径，该最短路径上包含的词就是该句子的切分结果 最短路径匹配算法的规则是使切分处理的词数最少，符合汉语自身的语言规律 缺点：在实际应用中，同样不能正确切分出许多不完全符合规则的句子。如果有多条最短路径，往往只能保留其中一个结果，这样对其他同样符合要求的结果不公平，也缺乏理论依据。 6.基于字符串匹配的分词方法的优缺点优点：简单，易于实现 缺点 匹配速度慢 存在交集型和组合型歧义切分问题 词本身没有一个标准的定义，没有统一标准的词集 不同词典产生的歧义也不同 缺乏自学习的智能性 2.1.2 基于统计的分词（无字典分词）主要思想 上下文中，相邻的字同时出现的次数越多，就越可能构成一个词。因此字与字相邻出现的概率或频率能较好的反映词的可信度。 可以对训练文本中相邻出现的各个字的组合的频度进行统计，计算它们之间的互现信息。互现信息体现了汉字之间结合关系的紧密程度。当紧密程度高于某一个阈值时，便可以认为此字组可能构成了一个词。该方法又称为无字典分词。 主要统计模型有：N 元文法模型、隐Markov 模型和最大熵模型等。 在实际应用中一般是将其与基于词典的分词方法结合起来，既发挥匹配分词切分速度快、效率高的特点，又利用了无词典分词结合上下文识别生词、自动消除歧义的优点。 1.N-gram模型思想N-gram模型思想：第n个词的出现只与前面N-1个词相关，而与其它任何词都不相关，整句的概率就是各个词出现概率的乘积 。 2.隐马尔科夫模型（HMM，HiddenMarkov Model）原理：根据观测值序列找到真正的隐藏状态值序列。 2.1.3 基于规则的分词（基于语义）原理：通过模拟人对句子的理解，达到识别词的效果，基本思想是语义分析，句法分析，利用句法信息和语义信息对文本进行分词。 优点：自动推理，并完成对未登录词的补充。 语义分词法引入了语义分析，对自然语言自身的语言信息进行更多的处理，如扩充转移网络法、知识分词语义分析法、邻接约束法、综合匹配法、后缀分词法、特征词库法、矩阵约束法、语法分析法等。 扩充转移网络法：以有限状态机概念为基础。有限状态机只能识别正则语言，对有限状态机作的第一次扩充使其具有递归能力，形成递归转移网络 （RTN）。在RTN 中，弧线上的标志不仅可以是终极符（语言中的单词）或非终极符（词类），还可以调用另外的子网络名字分非终极符（如字或字串的成词条件）。这样，计算机在运行某个子网络时，就可以调用另外的子网络，还可以递归调用。词法扩充转移网络的使用， 使分词处理和语言理解的句法处理阶段交互成为可能，并且有效地解决了汉语分词的歧义。 矩阵约束法：其基本思想是先建立一个语法约束矩阵和一个语义约束矩阵， 其中元素分别表明具有某词性的词和具有另一词性的词相邻是否符合语法规则， 属于某语义类的词和属于另一词义类的词相邻是否符合逻辑，机器在切分时以之约束分词结果。 2.1.4 基于字标注的中文分词方法基于字标注的中文分词方法 实质上是构词方法，即把分词过程视作字在字符串中的标注问题；分词的过程就成为字重组的简单过程 基于规则/统计的分词方法的弊端：一般都依赖于一个事先编制好的词典，而自动分词的过程就是通过词表和相关信息来做出词语切分的决策 本分词法的优点：能够平衡地看待词表词和未登录词的识别问题。因为文本中的词表词和未登录词都是用统一的字标注过程来实现，在学习架构上，既可以不必专门强调词表词信息，也不用专门设计特定的未登录词(如人名、地名、机构名)识别模块。这使得分词系统的设计大大简化。在字标注过程中，所有的字根据预定义的特征进行词位特性的学习，获得一个概率模型。然后，在待分字串上，根据字与字之间的结合紧密程度，得到一个词位的标注结果。最后，根据词位定义直接获得最终的分词结果。 2.1.5 基于人工智能技术的中文分词方法（基于理解）基于人工智能技术的中文分词方法 原理：在分词的同时进行句法、语义分析，利用句法信息和语义信息来处理歧义现象 通常包括三个部分： 分词子系统、句法语义子系统和总控部分。在总控部分的协调下，分词子系统可以获得有关词、句子等的句法和语义信息来对分词歧义进行判断，即它模拟了人对句子的理解过程 条件：需要使用大量的语言知识和信息 目前基于理解的分词方法主要有：专家系统分词法，神经网络分词法，神经网络专家系统集成式分词法等 ​ 1.神经网络分词算法神经网络分词算法 原理：以模拟人脑运行，分布处理和简历数值计算模型工作，将分词知识的隐式方法存入神经网内部，通过自学习和训练内部权值，以达到正确的分词结果 关键在于知识库（权重链表）的组织和网络推理机制的建立 算法的分词过程是一个生成分词动态网的过程，该过程是分步进行的：首先以确定待处理语句的权字串为基础，来确定网络处理单元；然后根据链接权重表激活输入/输出单元之间的链接，该过程可以采用某种激活方式，取一个汉字作为关键字，确定其链接表，不断匹配 优点：神经网络分词法具有自学习、自组织功能，可以进行并行、非线性处理，并且反应迅速、对外界变换敏感 缺点：目前的基于神经网络的分词算法存在着网络模型表达复杂，学习算法收敛速度较慢，训练时间长，并且对已有的知识维护更新困难等不足。 2.专家系统分词算法专家系统分词算法 原理：从模拟人脑功能出发，构造推理网络，将分词过程看做是知识推理过程 该方法将分词所需要的语法、语意以及句法知识从系统的结构和功能上分离处理，将知识的表示、知识库的逻辑结构与维护作为首要考虑的问题。知识库按常识性知识与启发性知识分别进行组织。知识库是专家系统具有“智能”的关键行部件 优点：专家系统分词算法是一种统一的分词算法，不仅使整个分词处理过程简明，也使整个系统的运行效率提高 3.神经网络专家系统集成式分词法神经网络专家系统集成式分词法 原理：首先启动神经网络进行分词，当神经网络对新出现的词不能给出准确切分时，激活专家系统进行分析判断，依据知识库进行推理，得出初步分析，并启动学习机制对神经网络进行训练 优点：可以较充分发挥神经网络与专家系统二者优势，进一步提高分词效率 2.2 中文分词瓶颈1.分词歧义分词歧义：指在一个句子中，一个字串可以有多种不同的切分方法，一个句子经常对应几个合法词序列，因此，汉语分词中的一个重要问题就是在所有这些可能的序列中选出一个正确的结果。 分词歧义是中文分词的主要困难 交集性歧义：可用动态规划来解决 e.g. “北京大学生前来报到”，容易被划分成“北京大学/生前/来/报到” 组合型歧义：指同一个子串既可合又可分；可用统计语言模型来解决 eg.“学生会宣传部”中的“学生会”是一个词，“学生会主动完成作业”里的“学生 会”就必须拆开 统计语言模型：对于任意两个词语 w1、 w2 ，统计在语料库中词语 w1 后面恰好是 w2 的概率 P(w1, w2) 。这样便会生成一个很大的二维表。再定义一个句子的划分方案的得分为 P(∅, w1) · P(w1, w2) · … · P(wn-1, wn) ，其中 w1, w2, …, wn 依次表示分出的词。我们同样可以利用动态规划求出得分最高的分词方案。 2.未登录词识别未登录词识别 未登录词包括：中外人名、中国地名、机构组织名、事件名、货币名、缩略语、派生词、各种专业术语以及在不断发展和约定俗成的一些新词语，是种类繁多，形态组合各异，规模宏大的一个领域。对这些词语的自动辨识，是一件非常困难的事。 中文没有首字母大写，计算机很难分辨人名地名等专有名词 人名刚好与上下文组合成词：比如“高通向人大常委会提交报告” 人名刚好是常用词：比如“汪洋” 品牌名、机构名、地名等专有名词的识别 缩略词的识别 网络新词更难识别：甚至没有固定的生产机制 ​ 3.错别字、谐音字规范化当处理不规范文本（如网络文本和语音转录文本）时，输入的句子中不可避免会存在一些错别字或刻意的谐音字（如香菇—&gt;想哭），这些词对分词系统造成很大干扰。 4.分词粒度问题对“词语的最小单位”的定义存在主观性，导致多人标注的语料存在大量不一致现象，即表达相同意思的同一字串，在语料中存在不同的切分方式。 2.3 中文分词的评价准则中文分词主要有五项评价准则：分词正确率，切分速度，功能完备性，易扩充性和可维护性，可移植性。 1.分词正确率中文分词是中文信息处理的重要基础，因此分词准确性对整体的信息处理任务来说十分重要。 为了获得分词系统切分正确率，应该进行整体测试，歧义测试和专业词测试。自动分词系统的切分正确率的基本公式为： s=\sum^3_{i=1}\beta_iS_i其中，S1，S2，S3。分别为总体测试、歧义测试和专业词测试的正确率；Bi(i=1，2，3)为三种测试加的权值。 2.切分速度切分速度对中文信息处理任务也非常重要。比如对于搜索引擎来说，如果分词速度太慢，即使准确性再高，也是不可用的，因为搜索引擎需要处理数以亿计的网页，如果分词耗用的时间过长，会严重影响搜索引擎内容更新的速度。 切分速度 指单位时间内所处理的汉字个数 在分词正确率基本满足要求的情况下，切分速度是另一个很重要的指标，特别对于算法不单一，使用辅助手段， 诸如联想，基于规则，神经网络，专家系统等方法更应注意这一点 通常中文信息处理的文本数量是相当大的，因此必须考虑方法是否能使系统总开销合理。在人机交互方式下处理歧义问题的策略和人机接口的设计，有时会严重影响切分速度，这也是应考虑的因素 3.功能完备性自动分词方法除了完成分词功能外，还应具备词库增删、修改、查询和批处理等功能。 4.易扩充性和可维护性易扩充性和可维护性是提供数据存储和计算功能扩充要求的软件属性，包括词库的存储结构，输入/输出形式的变化等方面的扩展和完善。这项指标与系统清晰性、模块性、简 单性、结构性、完备性以及自描述性等软件质量准则有直接的联系，对于研究实验性质的软件是非常重要的，因为这类软件需要不断提高与改进，使之适应中文信息 处理的各种应用。 5.可移植性可移植性：指方法能从一个计算机系统或环境转移到另一个系统或环境的容易程度。一个好的分词方法不应该只能在一个环境下运行，而应该稍作修改便可在另一种环境下运行，使它更便于推广。 三、中文分词技术的应用3.1 中文分词是中文信息处理的基础中文分词是中文信息处理的基础，是自然语言处理的基础模块，中文信息处理包括三个层次：词法分析，句法分析，语义分析。中文分词是词法分析的第一步。 图2：自然语言句子级分析技术 Level1：词法分析 （Lexical Analysis） 分词（word segmentation）：将输出的子串切分成单独的词语 词性标注（part-of-speech tag）：为每个词赋予一个类别，如名词、动词、形容词 etc.；一般属于相同词性的词，在句子中承担类似的角色 Level2：句法分析（synactic parsing）：对输入的文本句子进行分析以得到句子的句法结构的处理过程；句法分析的输出结果常作为语义分析的输入 短语结构句法分析（phrase-structure syntactic parsing）：识别出句子中的酸雨结果以及短语间的层次句法关系 依存句法分析（dependency syntactic parsing）：识别句子中词汇与词汇之间的相互依存关系，属于浅层句法分析 深层文法句法分析：利用深层文法，如词汇化树邻接文法、词汇功能文法、组合范畴文法等，对句子进行深层的句法和语义分析 Level3：语义分析（semantic parsing）：理解句子表达的真实语义 语义角色标注（semantic role labeling）：属于浅层语义分析技术 三个层级的联合方式 级联：分词、词性标注、句法分析、语义分析分别训练模型；实际使用时，逐一使用各模块进行分析，最终得到所有结果 联合模型：多任务联合学习和解码，如分词词性联合、词性句法联合、分析词性句法联合、句法语义联合等，联合模型通常可显著提高分析质量（但复杂度高，速度慢） 3.2 具体应用（以搜索引擎为例）中文分词是大部分下游应用的基础，这些下游应用小到POS词性标注、NER命名实体识别，大到自动分类、自动摘要、自动校对、语言模型、机器翻译、搜索引擎、语音合成等等。 下面以搜索引擎为例，具体阐述中文分词在搜索引擎中的应用。 搜索引擎针对用户提交查询的关键词串进行的查询处理后，根据用户的关键词串用各种匹配方法进行分词。 搜索引擎的查询处理 1.首先到数据库里索引相关信息 若用户提交的字符串不超过3个汉字，则直接去数据库索引 分词：若超过4个字符串，则用分隔符（如空格、标点）把用户提交的字符串分割成N个子查询串 2.再检测用户提供的字符串里有无重复词汇 若有，则丢弃，默认为一个词汇 检查用户提交的字符串有无字母和数字，若有则把字母和数字当做一个词 Reference中文分词算法基本介绍 中文分词技术介绍-月光博客 中文信息处理发展报告]]></content>
      <categories>
        <category>Artificial Intelligence</category>
        <category>Natural Language Processing</category>
      </categories>
      <tags>
        <tag>Artificial Intelligence</tag>
        <tag>Natural Language Processing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法推荐带来的“信息茧房”效应研究]]></title>
    <url>%2F2018%2F04%2F12%2F%E7%AE%97%E6%B3%95%E6%8E%A8%E8%8D%90%E5%B8%A6%E6%9D%A5%E7%9A%84%E2%80%9C%E4%BF%A1%E6%81%AF%E8%8C%A7%E6%88%BF%E2%80%9D%E6%95%88%E5%BA%94%E7%A0%94%E7%A9%B6%2F</url>
    <content type="text"><![CDATA[摘要：本文基于桑斯坦的“信息茧房”理论，以今日头条为实例，研究新媒体时代下算法推荐带来的“信息茧房”效应。在此基础上剖析了今日头条的“信息茧房”效应的具体成因以及危害，并给出规避这种危害的建议。 一、信息茧房信息茧房（Information Cocoons），由美国学者凯斯·桑斯坦于2006年提出，指公众会按照个人偏好有选择性地接触媒介信息，会无形中被自己的兴趣所引导，从而将自己的信息获取桎梏在像蚕茧一般的 “茧房”中的现象。桑斯坦认为“信息茧房”主要以“个人日报”的形式展现，个人日报是一个完全私人订制的日报，公众可以按照自己的兴趣选择要关注的话题。信息茧房的出现体现了信息传播从传者本位向受众本位的转变，媒体呈现出不断迎合受众需求的趋势，尤其是如今大数据时代下，为“个人日报”的实现提供了强大的技术和内容保障。但是，在享受个性化推荐内容带来的便利的同时，我们需要警惕陷入“信息茧房”的陷阱。 二、今日头条的“信息茧房”效应随着互联网的发展，手机新闻客户端竞争日益激烈，以用户体验为中心的传播模式成为主导，以今日头条为代表的新闻客户端通过强大的推荐算法根据用户个人喜好不断为用户提供个性化的内容信息，相当于为用户量身定做了一份个人化日报。但在满足用户对信息的个性化需求的同时，也将用户困在了一个个“信息茧房”中。 今日头条“信息茧房”效应的成因1.技术基础：今日头条强大的智能推荐算法以推荐算法为核心的新媒体技术是今日头条的信息茧房形成的技术基础。 今日头条主张“你关心的，才是头条”。与传统新闻媒体相比，今日头条最大的特点就在于为用户打造了“千人千面”的阅读场景 ，这背后是一套强大的个性化推荐系统。推荐算法实质上是一个拟合用户对内容满意度的函数，这个函数需要输入三个维度的变量： 1.内容特征：头条现在俨然已是一个综合性内容媒体平台，图文、视频、UGC短视频、问答、微头条，每种内容有很多自己的特征，需要考虑怎样提取不同内容类型的特征做好推荐。 2.用户特征：包括职业、年龄、性别、兴趣等，以及很多模型刻画出的隐式用户兴趣。 3.环境特征：移动互联网时代推荐的一大特点在于“随时随地”，用户在工作场合、通勤、旅游等不同的场景下的信息偏好会有所差异。 结合三方面的特征，推荐模型会给出一个预估，即预测推荐内容是否适合在这一场景下被推荐给这一用户。 但是，技术是一把双刃剑。虽然推荐算法能计算出用户的信息需求，按照用户的浏览记录、个人兴趣等特点自动为用户设置议程，使用户更精准地接触到符合个人兴趣的信息；但是实际上基于用户表现出的行为特点计算出的用户需求是不够深入的，并不能全面系统地洞察用户最本质真实的需求。一味信奉“算法至上”、机械地利用算法判定用户的信息需求和过分迎合用户喜好，也使得今日头条首页宛如桑斯坦所描述的“个人日报”一样，用户只能接触到自己感兴趣的信息，无形中被自己的喜好所固化，于是不自觉地被禁锢在算法所编织的信息茧房里。 2.用户对信息选择性接触心理受众对信息的选择具有能动性。拉扎斯菲尔德曾通过伊里调査验证了受众的选择性接触理论，即受众在接收大众传播的信息时不是无差别地对待所有媒介和内容，而是更愿意选择接触那些与自己立场和态度一致或类似的媒介或内容，而倾向于回避与自己立场观点对立或冲突的内容。 在现在的新媒体时代，媒体更加注重以用户需求为核心。今日头条的个性化推荐机制实际上也是在迎合受众对信息的选择性接触心理。传统媒体时代，信息的传播主要是“广播机制”，用户是被动地接受信息的一方。而在新媒体时代，信息通过算法技术被过滤，按照用户兴趣进行个性化推送，为用户提供其“选择性接触理”下预期的信息，大大提高了信息分发的效率，但是也更容易产生“个人日报”，导致“信息茧房”效应的形成。 三、“信息茧房”的不良影响1.“回音室”和“拟态环境”：导致信息窄化压缩认知空间 “回声室”效应是个体处于一个相对封闭的虚拟环境中，意见相近的声音不断积聚，分歧的意见被屏蔽在圈外，这些声音被夸张和扭曲，并且封闭环境中的人认为这些扭曲的故事就是事实的全部。。在今日头条为用户构造的“个人日报”里，用户获取到的信息会会越来越符合个人喜好，而屏蔽掉了与个人爱好无关或相异的信息，如此则用户的个人视野在无形中被局限在特定的范围之内，信息来源变得窄化，个体沉浸在自己所建构的信息茧房内，就像身处封闭的“回音室”，自己为自己设置议程，听到的永远只有自己的回音，难以听到外界的声音，从而容易陷入极端主义和盲目自信。 沃尔特·李普曼在“拟态环境”的概念中提出，大众传播所形成的信息环境源于真实环境，却又不尽一致的媒介环境。个人通过对象征性事件的信息加工形成媒介环境，因此个人所接收到的信息环境就是媒介构成的虚拟环境。用户通过“个人日报”所认知的世界就是一个由个人兴趣为主导所营造的“拟态环境”，因为他们认识到的世界是建立在他们所希望见到的图景之上的。沉浸于这种自我认知的环境中，会丧失个人判断能力和对外界的真实了解，造成个体对世界认知的偏差，会加强传播领域的“马太效应”，甚至导致极端的认知现象。 2.公众的意见自由表达受阻，隐性的言论自由被剥夺 网络没有主动给用户提供他们应当获得的内容，但人们意识不到需求这些内容，这是一种对用户隐性的言论自由权利的“合理化剥夺”。 3.泛低质内容的负面影响 今日头条的“蜘蛛抓取”式内容生成方式对泛低质内容的识别效果较差。虚假新闻、黑稿、标题党、低俗涉黄内容等很难被机器识别，需要提供大量的比对信息文本和人工审核。面对大体量的内容，人工编辑产出有限，因而有较大一部分泛低质内容流向用户。这些虚假、低俗的内容给用户带来错误的引导，也造成个性化推荐内容的“越描越黑”。若“信息茧房”中的用户缺乏独立的思考和理性的甄别，则易会受到泛低质内容的误导。 4.价值迷失陷阱：坏内容影响用户的价值观 马克思·韦伯通过理性二分法将理性分解为价值理性和工具理性两个思想维度，并赋予其解释社会现象的功能。价值理性强调行为不计后果地遵从某些价值信念，如“真善美”；而工具理性则从效用最大化的角度考虑实现目的所采取的手段的合适性和有效性。算法推荐充分体现了工具理性，然而由于用户兴趣中充斥着大量的低级趣味，尤其是在当前拜金主义、功利主义、享乐主义等盛行的社会环境下，仅以用户兴趣为主要内容衡量标准往往会造成隐含负面价值取向的内容被大量推送。这很容易为用户造成“很多人都有这种价值取向”的印象。根据“沉默的螺旋”理论，一旦用户认为这种价值取向受到广泛欢迎，那么与之相符的声音就会高涨，与之相反的声音则会陷入沉默。正面价值意见的沉默造成负面价值意见的增势，由此陷人恶性循环，带坏社会风，令越来越多的人迷失正确的价值追求。 5.助推群体极化现象，导致公众理性批判的缺失 群体极化（ Group polarization）是社会心理学里的术语，最早由传媒学者詹姆斯·斯托纳于1961年发现群体讨论时的现象而提出。群体极化指在个人决策由于受到群体的影响从而容易做出比独自一个人决策时更极端的决定，偏离最佳决策。互联网中群体极化现象十分普遍，较常见的就是“水军”和粉丝现象。社交网络通过分享、转发、点赞等方式来划定相同意见的群体，而今日头条作为资讯平台虽然并不具备较高的社交性，但每条新闻的评论、点赞、回复、关注等功能能够将意见同一的人聚集起来，从而独立存在的茧房通过信息平台上的互动成为有外在纽带连结的茧房群。并且匿名的特点加剧了茧房群之间信息交换的畅通性，在突发性事件发生时，极端行为很容易被激发。当公众以相同的立场发表自己的观点，势必会造成强势意见无限膨胀，群情激愤往往会转化为一种舆论压力，但这种舆论压力缺乏理性精神的指引，实际上对社会公共领域的建构起消极作用。 6.削弱社会粘性，破坏共同体维系 社会黏性是由一定的经验、知识和任务分享而来的，处于社会中的人们需要有一些共同的记忆和关心，需要由经验分享而构建的共同联盟。桑斯坦在《网络共和国》中提出，一个容许观点自由表达的完善机制通常包含两个条件：首先，人们必须处在不被主观筛选、没有人为干预的任何信息之中。这种计划之外、难以预期的信息接触，是民主的一个关键条件；其次,是一定程度上为大多数公民所共享的社会经验。倘若经验无法彼此分享，完全异质性的社会将难以相互理解、达成共识。桑斯坦坚信，剥离了共同经验的传播机制，必然会带来一系列问题，最终致使社会黏性的丧失。 但是算法推荐下用户身处各自的“信息茧房”中，共同经验减少了，不同意见群之间的冲突将会上升。缺少共同经验的个体容易沉浸在各自的舆论场中自说自话，脱离整个社会的发展。异见被在用户接触前就已被算法过滤掉，于是用户就很难理解和同情异己的观点和价值观，因此社会黏性便会减少，共同体的维系也会出现问题。 四、如何规避“信息茧房”的危害1.协调工具理性和价值理性，建立“更快更准、更好”的内容分发机制 算法推荐是新媒体时代内容分发方式的革命，信息的精准、快速、个性化投递是其能够获得用户青睐，持续占领市场的根本原因。为了获得长远发展，新媒体产品应继续坚持技术创新，打通与搜索引擎、社交平台、电商平台、生活服务平台、视频音乐游戏平台等网络服务运营商的用户数据壁垒，推进用户网络行为追踪技术和用户兴趣洞察技术的进步，为用户提供更快、更准的个性化信息分发服务。 同时，也必须协调工具理性与价值理性，尤其需要承担起传播正面价值观的媒体责任，关注人的健康发展。拓宽对用户需求的定义，从客观层面将人类的多元化信息需求考虑在内，推送多元优质内容，改善信息茧房带来的信息窄化问题。此外，可以引入人工干预，发挥编辑的信息“把关人”的作用，筛选出符合社会价值追求的高质量内容，剔除低质内容，充分发挥其作为媒体的正面教化作用，推动内容服务向更好的方向发展。 2.用户需要提高自身的媒介素养，培养“多元、开放”思维 对于“信息茧房”现象，个人应努力提升媒介素养，认识到个性化推荐技术的弊端，合理运用推荐技术，理性对待被推荐给自己的信息。做到不轻信一家之言，不把自己局限于具有相同或相似标签的群体。培养多元思维，在获取信息时要有意识地打破自己的固有思维，主动扩展信息获取渠道，多接触多元化的信息。以开放的眼光洞察社会，多倾听一些不同于自己价值观的声音，从而提高认知能力，减少信息的不对称性。 Reference[1]“信息茧房”对网络公共领域建构的影响 [2]喻国明.信息茧房的负面效应及改善之道 [3]邓倩.新媒体的“信息茧房”现象研究 [4]大数据时代的新闻客户端的信息茧房效应 [5]戴维·迈尔斯.《社会心理学》 [6]何婷.新闻客户端个性化推荐引发的信息茧房现象 [7]刘华栋.社交媒体“信息茧房”的隐忧与对策 [8]算法推送：信息私人定制的“个性化”圈套 [9]桑斯坦.《信息乌托邦:众人如何生产知识》 说点个人体会一直以来我都不想使用类似今日头条、抖音等的个性化推荐引擎驱动的产品，尽管它们很火很火。大概是出于内心的自由主义，我不想让自己的行为、兴趣和欲望被外物所决定、牵引、操纵。实际上我也并不认为目前的推荐算法洞悉了我真正的需求，这种基于用户表面行为数据的分析而作出的预测推荐是不够深入的，长期使用的确容易造成个人的信息窄化，无形中被囿于算法所编织的信息茧房中。我想它并没有看到我最本质真实的信息需求，比如获取信息是为了让自己的眼界开阔、滋养和富足精神世界、实现自我提升…但显然，目前的推荐算法仍有很大的提升空间。所谓的“以用户需求为核心”的内容分发机制实质上并没有完全做到“以人为本”。真正“以人为本”的内容分发机制应以人本思维为驱动，算法和数据为“术”，在工具理性之外更多地融入价值理性。那么，到底如何才能设计出这样的推荐系统呢？]]></content>
      <categories>
        <category>Artificial Intelligence</category>
        <category>Recommended System</category>
      </categories>
      <tags>
        <tag>Medium Study</tag>
        <tag>Recommended System</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ofo产品分析|共享单车为何能快速成长]]></title>
    <url>%2F2018%2F04%2F08%2Fofo%E4%BA%A7%E5%93%81%E5%88%86%E6%9E%90-%E5%85%B1%E4%BA%AB%E5%8D%95%E8%BD%A6%E4%B8%BA%E4%BD%95%E8%83%BD%E5%BF%AB%E9%80%9F%E6%88%90%E9%95%BF%2F</url>
    <content type="text"><![CDATA[摘要：本文以产品的角度观ofo全局，从产品本身、用户需求、产品解决方案、产品运营、商业模式、未来发展策略六个维度去分析以ofo为代表的共享单车得以快速成长背后的原因。 一、ofo的快速成长1.共享单车的迅猛发展 根据中国信通院《2017Q3共享单车行业发展指数》，下图是共享单车在2016Q4~2017Q3期间的发展指数，由图可看出共享单车行业的增长速度快，发展势头十分迅猛。 2.ofo小黄车在共享单车领域的领先地位 下图是猎豹大数据发布的中国共享单车app排行榜，在众多共享单车app中，ofo的周活跃渗透率高居第一，可见ofo在发展迅猛的共享单车行业里处于领先地位。 二、ofo产品简介2.1 社会背景每一种成功的产品必定是顺应了或引领了时代的潮流。研究一个产品的成功，一定要去分析其成长的社会背景。那么ofo快速成长的社会背景是： 1.城市化进程加快和汽车工业的发展给我国居民出行结构带来了巨大改变，城市化进程加快和汽车工业的发展给我国居民出行结构带来了巨大改变，网约车的出现虽丰富了用户的出行选择，但仍未解决用户“最后一公里”的出行痛点； 2.我国机动车数量的逐年增长，给交通和环境带来了压力，交通拥堵、空气污染严重等问题亟待解决，这种现状下共享单车以其绿色环保、轻便高效的出行模式赢得了社会的好感； 3.起源于欧洲的“共享单车”于2007年传入中国，2007~2014年中国政府一直倡导“有桩共享单车”，然而一直在取还车便捷度方面存在缺点，成效甚微； 4.ofo共享单车平台以无桩自行车切入市场空白点，解决用户痛点； 5.政策：国家和地方政府对共享单车持正面积极态度并加强相关政策，以带动共享单车的快速发展； 2.2 产品定位ofo的产品定位可用八个字来概括：无桩单车共享系统。 无桩：随停随骑，让用户随时随地有车骑 共享：创新单车分时租赁模式，使得单车资源可在时间、空间上得到合理优化配置 三、用户分析3.1 用户需求共享单车满足的用户最核心的需求是“最后一公里”的短途出行需求，实现“随时随地有车骑”。 其他痛点 私人自行车停放麻烦，且容易被盗 上下班交通工具拥挤，停车困难，上下班高峰期拥堵 满足用户痛点是一个产品能制胜的最根本原因，而ofo非常敏锐地发现了当代中国公民“最后一公里”的短途出行需求，并提出了无桩单车共享系统的解决方案，这是ofo能快速风靡全国、获取大量用户的最本质的原因。 3.2 使用场景“最后一公里”的高频用户出行场景主要集中在地铁站/公交站与家/商区/公司之间的代步 公交车和地铁离家1~3公里之间，打车略贵，走路又略远，共享单车刚好可填补这一部分的空白需求 上班点离家不远，1~8公里之间，不愿意和别人挤公交和地铁的人可能更倾向于骑车上班 需要短途外出（比如去超市买菜、去餐馆吃饭），可以选择骑单车过去 这些是ofo潜在用户群体出行的高频场景，是ofo需要连接用户与场景的地方。ofo能快速成长，也得益于它能明确用户使用场景并针对目标场景使用“车海战术”抢占用户视线、“价格战”引诱用户使用等策略。 3.3 用户群体ofo的主要用户群体是学生、上班族，年龄范围在25~35岁，追求健康生活方式。 下表描述了ofo用户群体的特征： 性别分布 用户中男性居多，占65.7%,女性占34.3% 年龄分布 用户集中在26-35岁之间，占65.0%,36-45岁及25岁以下用户各占21.0% 收入情况 月收入在8001-10000间的占33.2%，其次为月收入10001-20000元的用户,占23.0% 学历情况 用户整体学历偏高，本科学历占71.7%，硕士/MBA占12.0% 生活方式 日均运动时长在小时以上的用户占56.2% 四、产品解决方案分析ofo小黄车能快速成长，也得益于它能开创性地提出的全球第一个无桩共享单车出行解决方案，并能以互联网思维小步快跑、不断更新迭代其解决方案。 ofo不仅有着行业最低的押金与收费标准，还在骑行体验、基于大数据的车辆科学投放等方面具备优势，这也成为了用户选择ofo的原因。 ofo造价低廉，基于此才能大规模生产和投放单车，从而在城市覆盖量、市场占有率等方面占据先发优势。 ofo的开锁方式一开始是手动密码锁，之后在车辆偷盗等问题日益严重后及时换成了智能锁，这也体现ofo产品更新之快 小步迭代：ofo一直秉持“小步迭代，快速创新”的产品开发理念，最初的ofo并不完美成熟，但ofo在合适的实际被推向市场，迅速抢占市场，随后在大规模使用过程中再去调整、优化、迭代。 下表是对ofo出行服务模式的分析： ofo出行服务模式分析 押金 99元 收费标准 城市：1元/半小时，校内：0.5元/半小时 驱动方式 链条传动 重量 初代小黄车：普通自行车 ；3.0版小黄车：约16kg 车锁 手动密码锁，之后换成智能锁 轮胎 普通轮胎 刹车 初代大黄车：普通车刹；3.0版小黄车：夹刹 + 抱闸 使用方式 合规区域内灵活停放 结算方式 需开启APP，手动结算 押金监管 暂无公开信息 投放城市 北、上、广、深、武汉、成都、厦门等46座城市 五、产品运营ofo的戳中用户痛点和产品解决方案出色是它能占领市场的前提，而它的颇具智慧的运营策略则是推动它快速占领市场的“马达”。纵观ofo的发展，我们会发现ofo的主要运营策略可以用7个字来形容：快扩张，密集覆盖。从校园到城市，从一二线城市向三四线城市下沉，ofo以“车海战术”和“免押金、价格战”等战略快速抢占市场。下面是对ofo运营策略的具体分析。 5.1 从校园到城市ofo最初主攻校园市场，在校园市场获得成功后，开始向城市扩张。下面从六个方面分别阐述ofo“从校园到城市”的策略。 1.城市运营策略 以校园作为突破口，覆盖全国200所高校；2016年11月正式进入城市,目前已覆盖46座城市 合作深圳地铁，规划地铁站共享单车停放区 2.用户运营 联合滴滴出行推出红包活动；每周的“免费星骑”免费骑行活动;16年11月，开展“双11不期而遇”的脱单活动；16年11-12月,携手ENOY开展“扩大美食半径”的美食优惠活动 3.车辆运营 一代“大黄车”采用普通自行车、按键式机械锁的设计，3.0小黄车缩小了车辆体积、更新了车锁设计 采取网格化管理政策，将城市划分成网格，每一个网格区域内配备一个维修市傅，维保师傅通过每天定时街面巡查的方式工作，每个网格区域的报修率与维保师傅的奖金进行挂钩 4.行为规范管理 与深圳市交警局、市教育局联合发布《关于未成年人共享单车使用行为的联合声明》 通过实名制禁止未满12岁儿童注册使用；通过公共渠道进行传播宣传,规范未成年人使用行为 5.合作策略 合作第三方供应商进行造车如飞鸽、凤凰等知名自行车企业 合作中国电信、华为共同研发基于物联网NB-oT技术的共亨单车智雏解决方案 6.海外开拓策略 目前已进入新加坡免费运营 海外投放策略将以校园作为入口 目前会将重点放在国内，对于海外市场，由于不同国家策略不同,因此会在前期跑通商业模式后进行大规模扩张 5.2 ofo2017城市策略ofo率先开展服务下沉与市场拓宽，实现了拥有最多城市覆盖数量和最多单车投放数，实现了二三线诸多城市用户也随时随地可以骑上ofo小黄车出行。 2017年1月22日，ofo共享单车启动“2017城市策略”，先后进入合肥、武汉、长沙、南京、重庆等城市，加之2016年开通的北京、上海、广州、深圳等地，目前城市已覆盖46座，且ofo未来还将根据地形、人口、公共交通环境等维度迸进行城市筛选，加大覆盖力度。 5.3 ofo的竞争优势1.数据优势：ofo积攒了大量用户出行数据，在这个数据为王的时代，拥有海量用户出行数据是ofo的一大优势，ofo可从数据中洞悉用户出行习惯，从而成长为一个越来越智能的平台。 2.供应链的壁垒 ：ofo和全球30%的供应链签订了排他协议。 3.品牌势能：ofo已成长为共享单车市场的领军品牌，拥有强大的品牌势能。 4.规模优势：ofo的用户规模和市场覆盖率都是共享单车行业当之无愧的王者。 六、ofo的商业模式实际上虽然经过两三年的野蛮生长，以ofo为代表的共享单车已经成为中国公众出行的一个重要交通工具，但共享单车的商业模式仍然不清晰，已有的盈利方式（租金和押金）并不足以支撑共ofo的巨大生产/运营/维护成本。在没有清晰盈利模式的情况下，ofo能活到现在主要是依赖于投融资的力量。但如果一直无法盈利，那么这个产品迟早会死，所以ofo需要去探索新的盈利方式。对于新盈利方式，我认为可能有四种：大数据服务、增值服务、广告投放和金融服务。 ofo只有拥有强大持久的盈利能力，才能在快速成长后可持续地发展下去。 6.1 运营成本企业运营成本涉及多项维度，研发阶段成本主要为零配件采购、技术研发等费用；运营阶段中，由于车辆的开锁等模块会涉及到通信运营商，期间会随之产生网络费用；维修保护方面，ofo主要通过大数据监管、人工调度车辆以及区域化分配维修师傅进行统一修缮，期间费用涉及零配件置换修理费、劳力成本、运输调度费等，除车辆制造成本以外,运维成本成为共享单车行业另一重要模块，而如何减少运维成本，提高运维效率需根据不同的车辆属性、造价等因素进行策略规划。 下图是对共享单车分阶段（研发/运营/维护）的主要运营成本的拆解： 6.2 盈利模式6.2.1 现有盈利模式ofo现在的主要盈利来源是用户缴纳的单车押金和租金。 6.2.2 盈利模式畅想共享单车的盈利模式主要可能有四块：大数据服务、增值服务、广告投放和金融服务。 1.增值服务 与景点合作：为旅游人群提供景点代步车 与骑行俱乐部合作：为骑行爱好者定制自行车 2.大数据服务：用于支持广告或公共服务 与研究机构合作：制造业和物联网相结合，围绕出行数据挖掘LBS位置信息的商业化可能 与广告商合作：为广告公司提供用户出行数据，以帮助精准投放广告 大数据服务也是被广泛看好的一种变现方式。共享单车内设定位系统，可以收集海量短途出行数据，在不侵犯用户隐私的前提下向第三方出售出行数据，或是为广告业务提供支持，又或是与政府合作构建交通大数据应用于公共服务，都有一定想象空间。从目前共享单车进入世界各个城市的经验中看，地方政府对共享单车运营数据的兴趣也非常强烈，提出让运营商和政府相关部门共享运营数据，帮助政府对骑行基础设施建设进行规划已经成为一个基本要求。 3.广告投放 车身广告：基于地理信息做精准投放 共享单车不能仅靠租金收入，需要开放各类平台资源，和合作伙伴深度合作，结合用户使用场景和地理位置进行广告位的销售和推广。 车身广告可能是目前来看最具前景的盈利模式。广告的关键的在于投放效果，结合共享单车平台本身具备的大数据优势（例如结合LBS联合商户向用户提供附近商业活动优惠），如果能做到既量大又精准，那对广告商的吸引力就很强。 4.金融服务：拿大量租金去做金融投资，靠投资盈利赚钱；但是共享单车押金近来已经受到越来越严格的监管，未来变现空间有限。 七、ofo(共享单车)的未来发展ofo现在已然成为中国共享单车行业的领军者，然而这只是一个开端，它的成长不会止步于此。关于ofo的未来发展，一方面在野草般成长过程中暴露出的许多问题（车辆管理/用户行为）亟待解决，另一方面是在增速放缓的情况下如何去继续拓展市场。这一部分分析的是ofo共享单车如何继续快速成长。 1.拓展市场：向三四线城市下沉，开拓海外市场 一线城市市场需求旺盛但容量有限，三四线城市及海外市场是未来两大拓展方向 目前，ofo市场主要集中在一线及部分发达二线城市，市场需求非常显著，但随着单车数量的急速增加以及用车场景相对单调及同质化的限制，现阶段市场竞争激烈; 一线及部分发达二线城市市场容量有限，单车数量将会较快到达饱和点，向三四线城市拓展成为必然,但市场需求及机会仍有待探索 海外市场自行车售价相对较高，但用户需求旺盛，为共享单车出海提供良好市场机会；同时，海外市场相对高的客单价将会帮助ofo提升盈利能力。 2.车辆智能化管理 单车数量急速增加、车辆停放、后期维护等问题日益凸显，对精细化管理的呼唤迫切。 共享单车行业的快速发展带来城市管理等诸多问题，无桩化特点导致目前乱停车的现象屡见不鲜；另外，部分共享单车由于缺少GPS定位，不能够及时、有效的管理车辆。共享单车的城市管理问题已经初步显现，未来需要共享单车企业与政府相关部门共同努力，建立有效机制，引导共亨单车向更规范、更健康的方向发展。 3.用户行为规范和诚信建设 规范化的用车行为将会极大促进行业发展,反之会限制其规模性发展；用户诚信体系建设能够更加有效地规范用户行为。 用户是共享单车行业快速发展的核心,引导用户规范用车,培养用户的诚信态度及行为,并辅助建立一套社会通用的诚信管理体系，不仅能够解决现阶段共享单车面临的些用户问题,也能极大地促进社会规范化发展,为整体国民素质的提升起到促进作用。 4.基于物联网的大数据管理是未来共享单车行业发展的核心驱动力 共享单车行业未来将面临市场需求不平衡、城市管理难度大以及用户行为不规范等问题，针对以上问题，ofo在企业层面的产品制造升级、智能科技创新能力是为满足消费者需求与提升企业效率的核心竞争力。随着发展，马太效应显著，科技化产品、注重用户体验与城市可持续化发展的企业将占据市场主要地位，因此，企业应加强产品升级迭代速度与质量、合理投放车辆并完善车辆维护措施；而政府层面，应考虑因城施策，要求单车企业建立有效的智能化管理方式，以提升车辆监管与运营效率，彻底改善目前车辆停放、运维、安全等问题。 八、总结近三年中国共享单车野蛮生长，迅速发展，给全国公众的出行方式带来了改变。其中ofo是中国共享单车领域的领军者，它敏锐地洞察了用户“最后一公里”的短途出行需求，并创新性地打造了“无桩共享单车平台”，为解决用户这一需求提供解决方案，这是ofo共享单车快速成长的最本质原因。城市化进程加快和汽车工业发展带来的交通拥堵、空气污染等问题是ofo成长的土壤。ofo能快速成长也得益于它能使用互联网思维小步迭代，不断更新产品解决方案。“快扩张，密集覆盖”的运营策略是推动ofo在短短两年内快速成长的“马达”，从校园到城市，从一二线城市向三四线城市下沉，ofo以“车海战术”和“免押金、价格战”等战略快速成功抢占市场。然而，支撑ofo野蛮成长的背后是投融资的力量，实际上ofo的商业模式是不清晰的，仅靠收取的用户押金和租金并不能支撑其巨大的生产/运营/维护成本，所以ofo必须探索新的商业盈利模式以支撑公司可持续地发展下去。最后关于ofo的未来成长，一方面ofo在野草般成长过程中暴露出的许多亟待解决的问题（车辆管理/用户行为），要求ofo实行车辆智能化管理、规范用户行为和建设诚信；另一方面是在增速放缓的情况下ofo可以通过向三四线城市下沉和进军海外来继续拓展市场。]]></content>
      <categories>
        <category>Product Manager</category>
        <category>Internet &amp; Product</category>
      </categories>
      <tags>
        <tag>Internet &amp; Product</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解析TF-IDF算法原理：关键词提取，自动摘要，文本相似度计算]]></title>
    <url>%2F2018%2F04%2F05%2F%E8%A7%A3%E6%9E%90TF-IDF%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%EF%BC%9A%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96%EF%BC%8C%E8%87%AA%E5%8A%A8%E6%91%98%E8%A6%81%EF%BC%8C%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97%2F</url>
    <content type="text"><![CDATA[Abstract：TF-IDF算法是一种常用的词频统计方法，常被用于关键词提取、文本摘要、文章相似度计算等。 TF-IDF的算法思路 TF词频（Text Frequency）：统计出现次数最多的词 IDF逆文档频率（Inverse Document Frequency）：大小与一个词的常见程度成反比；即给某些词分配“重要性”权重（平时比较少见而在这篇文章里多次出现的词应给予较高权重，而平时也很常见的则分配较低权重（过滤停用词）） TF X IDF = 某个词的TF-IDF值，某个词对文章的重要性越高，其TF-IDF值越大，值最大的几个词即为关键词 词频数：TF=某个词在文章中的出现次数 词频率(标准化，方便不同文章的比较)：TF=某个词在文章中的出现次数/该文出现次数最多的词的出现次数 词频率(标准化，方便不同文章的比较)：TF=某个词在文章中的出现次数/文章的总词数 逆文档频率(需要语料库)：IDF=log(语料库的文档总数/包含该词的文档数+1) TF-IDF=TF × IDF TF-IDF与一个词在文档中的出现次数成正比，与该词在整个语言中的出现次数成反比。 缺点：单纯以“词频”来衡量一个词的重要性不够全面，因为有时重要的词可能出现次数并不多。而且，这种算法无法体现词的位置信息，出现位置靠前的词与出现位置靠后的词，都被视为重要性相同，这是不正确的。（一种解决方法是，对全文的第一段和每一段的第一句话，给予较大的权重。） TF-IDF算法做文本相似度计算【计算余弦相似性】 步骤：分词——列出所有词——计算词频——写出词频向量（于是计算N个文本的相似度变成计算N个向量的相似度） 12345678910111213141516171.分词句子A：我/喜欢/看/电视，不/喜欢/看/电影。句子B：我/不/喜欢/看/电视，也/不/喜欢/看/电影。2.列出所有词我，喜欢，看，电视，电影，不，也。3.计算词频句子A：我 1，喜欢 2，看 2，电视 1，电影 1，不 1，也 0。句子B：我 1，喜欢 2，看 2，电视 1，电影 1，不 2，也 1。4.写出词频向量句子A：[1, 2, 2, 1, 1, 1, 0]句子B：[1, 2, 2, 1, 1, 2, 1] 余弦相似度（如何计算向量间的相似度） 想象两个向量是共原点的射线，两射线形成一个夹角，可通过夹角的大小判断向量的相似度，夹角越小，越相似 余弦定理：cos\theta=(a^2+b^2-c^2)/2ab假定A和B是二维向量，a向量是[x1, y1]，b向量是[x2, y2]，那么可以将余弦定理改写成下面的形式： cos\theta=(x_1x_2+y_1y_2)/\sqrt{x^1+y^1}×\sqrt{x^2+y^2}假定A和B是两个n维向量，A是 [A1, A2, …, An] ，B是 [B1, B2, …, Bn] ，则A与B的夹角θ的余弦等于： cos\theta=\sum^n_{i=1}(A_i×B_i)/\sqrt{\sum^n_{i=1}(A_i)^2}×\sqrt{\sum^n_{i=1}(A_i)^2}=（A·B）/|A|×|B| 余弦值越接近1，就表明夹角越接近0度，也就是两个向量越相似，这就叫”余弦相似性”。 文本相似度计算的步骤 使用TF-IDF算法，找出两篇文章的关键词 每篇文章各取若干个关键词（比如20个），合并成一个集合，计算每篇文章对于这个集合中的词的词频（为避免文章长度的差异，可以使用相对词频） 生成两篇文章各自的词频向量 计算两个向量的余弦相似度，值越大则越相似 TF-IDF——自动摘要自动摘要：即找出那些包含信息最多的句子。而句子的信息量用“关键词”来衡量，若包含的关键词越多，则说明这个句子越重要。 簇：关键词的聚集，即包含多个关键词的句子片段。 上图就是Luhn原始论文的插图，被框起来的部分就是一个”簇”。只要关键词之间的距离小于”门槛值”，它们就被认为处于同一个簇之中。Luhn建议的门槛值是4或5。也就是说，如果两个关键词之间有5个以上的其他词，就可以把这两个关键词分在两个簇。 簇的重要性分值： 簇的重要性=(包含的关键词数量)^2/簇的长度自动摘要的步骤 TF-IDF算法找出关键词 找出簇 找出包含分值最高的簇的句子，把它们合在一起，即构成自动摘要 Reference阮一峰：TF-IDF与余弦相似性的应用]]></content>
      <categories>
        <category>Artificial Intelligence</category>
        <category>Natural Language Processing</category>
      </categories>
      <tags>
        <tag>Artificial Intelligence</tag>
        <tag>Natural Language Processing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[传播统计学作业2|广告效果比较检验]]></title>
    <url>%2F2018%2F03%2F26%2F%E4%BC%A0%E6%92%AD%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%BD%9C%E4%B8%9A2-%E5%B9%BF%E5%91%8A%E6%95%88%E6%9E%9C%E6%AF%94%E8%BE%83%E6%A3%80%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[Abstract：两道传播统计学的作业题，关于广告测试效果比较检验。 Q1一个广告测试比较实验,研究广告A和广告B哪个更能使人对某品牌的评价产生积极的影响。随机抽取40人做研究对象,首先让他们对某品牌打分,然后随机指定20位受试者观看了包含广告A的一组广告。另外20位受试者观看了除把广告A换成广告B其他要素完全与上组相同的一组广告。受试结束后,让这40位受试者重新对该品牌进行评价打分。所有测试结果见下表。 (1) 能否作出结论,广告A比广告B更能使人对该产品的评价产生积极影响? 答：可以做出评价，A广告比B广告更能产生积极影响，定义“产品评价平均增益”：$gain = \frac{1}{n}\sum{i=1}^{n}\frac{P{after}^i-P{before}^i}{P{before}^i}*100\%$ 广告A的产品评价增益：3.8236428828%；广告B的产品评价增益: 1.81104717591% 通过平均评价增益对比得出，对于大多数人来说，广告A相比于广告B更能吸引观众，提高其对产品的评价，并在广告的的驱动下去购买品牌产品。 观看广告A后产品的平均得分：77.7；观看广告B后产品的平均得分：76.85 观看广告A后，观众评价增加最大值：13；观看广告B后，观众评价增加最大值：8 观看广告A后，观众评价降低最大值：0；观看广告B后，观众评价降低最大值：7 由评价得分的波动极值可以得出：广告A对部分受试者有更大的积极影响，使得这一人群通过广告提高了对品牌的认可度和好感度，同时，观看广告后，广告A对产品评价没有损毁作用，即没有受试者因广告而降低对产品的评价。而广告B则在这两方面不如广告A，广告B相比于A没有足够吸引受试者提高对品牌的好感度，此外，在观看广告B后，存在受试者对品牌产生了严重负面评价。 综合来看，广告A较广告B更容易产生积极影响。 (2) 广告A是否有传播效果? 广告B是否有传播效果?** 广告传播效果通常是指具有说服动机的广告信息对受众心理、态度和行为的影响。直接对受试者进行测验，无法判断广告在实际场景中的阅读率、视听率，以及同类产品的广告竞争情况。在实验中，广告的传播效果更多取决于广告本身带给观众的体验，观众的态度变化可以直接体现出广告的传播效果，是一种外在评价指标，观众态度变化积极则意味着购买行为可能发生，而态度变化消极则可能对产品的销售与推广造成负面影响，使得观众降低对产品的喜好程度以及关注程度。通过分析实验数据，广告A和广告B投放后，受试者对品牌产品的评价均有提高，受试者通过广告进一步提高了对产品的感知和认识。但B中存在少数态度变化消极的情况，在实际投放中，可能不利于产品的销售。 Q2假设研究者想测试广告A的效果,于是抽取了40人的随机样本,然后又随机分为两组。第一组不看广告,直接对该品牌评分,第二组先看广告A,再对该品牌评分,即不采用配对样本的方式。仍然使用该表格的数据 (3) 广告A是否有积极的效果? 假设第二列数据为20人不看广告的评价，第三列为受试者观看广告后的评价。 未看广告对产品评价的评分均值：75.049999999999997 未看广告对产品评价的评分最大值：88 未看广告对产品评价的评分最小值：59 观看广告对产品评价的评分均值：77.700000000000003 观看广告对产品评价的评分最大值：88 观看广告对产品评价的评分最小值：62 广告A具有积极效果。在样本均匀的前提下，观看广告后的受试者评价平均水平提高了2.66左右，从整体角度来分析，广告A对品牌产生了积极影响。从局部角度分，最高评价差别不大，但对于产品评分的最低水平，从59提高到了62。在观看广告A后，受试者对产品的评价从整体到局部均比未观看有提高。 (4) 假设手中没有具体的评分值,只知道A组有12人看了A广告之后评分提高,B组有8人看B广告后评分提高。能否根据这两个数字,说明A广告比B广告传播效果好 不能说明，如果需要建立对两者的评价，首先要保证测试样本一致，包括样本的数量，样本的性质。如果A，B两次试验中，受试者的心理状态和对事物的看法截然不同，那么试验结果将无法对比。对于不同数量的受试者的评价，无法确定两者的差别与优劣。 Data两道题的数据见下： 区间估计广告测试数据]]></content>
      <categories>
        <category>Computer Science</category>
        <category>Data Mining and Analysis</category>
      </categories>
      <tags>
        <tag>Data Mining and Analysis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解析今日头条的内容推荐服务]]></title>
    <url>%2F2018%2F03%2F25%2F%E8%A7%A3%E6%9E%90%E4%BB%8A%E6%97%A5%E5%A4%B4%E6%9D%A1%E7%9A%84%E5%86%85%E5%AE%B9%E6%8E%A8%E8%8D%90%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[Abstract：今日头条以推荐算法为引擎在内容领域表现强劲，且其产品从单纯的新闻客户端演变成较大的产品矩阵，内容结构不断完善，可见头条正向综合性信息媒体平台方向发展，甚至其可能更大的野心是做未来互联网的入口。不过，从前坚持“算法至上”的头条，在经历被人民网点名批评和“约谈”后开始采取多项举措力图解决其平台上色情低俗内容泛滥的问题。至于这些举措的效果如何，且待时间检验。 1.今日头条的内容推荐服务今日头条以用户建模为基础，以推荐算法为引擎，以海量数据为依托，以头条号作者和后台强大的爬虫爬取的海量内容为内容源，并通过机器学习感知、理解、判断用户的行为特征（如用户在新闻客户端的滑动、搜索、查询、点击、收藏、评论、分享等动作），综合用户具体环境特征和社交属性来判断用户的个性化兴趣爱好，从而为用户精准推荐个性化的新闻资讯，塑造千人千面的阅读场景。 2.头条产品矩阵与其内容推荐服务 近几年，通过拆分为不同产品的方式，今日头条已由当初单一的新闻资讯客户端往综合信息媒体平台发展，俨然已形成一个头条产品矩阵，多路出击“围剿”各大相关互联网产品。头条产品矩阵涉及的内容领域包含： 新闻资讯：头条客户端主打泛阅读，敌对天天快报、一点资讯等产品；头条极速版主打极速精选阅读，敌对相关精选类阅读产品； 社交媒体：微头条敌对微博，主打阅读场景下的社交（自2017年4月上线已吸引不少明星名人大V入驻）； 短视频：抖音（音乐短视频）、火山小视频（UGC的短视频分享平台）、西瓜视频（PGC的视频新闻）都是国内短视频领域的佼佼者，直接对手为小咖秀、美拍、快手等； 问答：悟空问答在问答类产品领域也发展迅猛，且用户下沉做得好，主要对手是知乎； 直播：头条直播，对标各种直播产品 从这快速构建起的头条系产品矩阵可知，头条的野心很大，它想做的不仅是新闻客户端，更可能是如微信、搜索引擎一样做未来互联网世界的入口。而这整个体系的核心和引擎是头条强大的内容推荐系统，其所有产品都应用了头条引以为傲的推荐系统框架来精准把握用户个性化需求，也许正因如此头条系产品才能在极短时间内迅速获取大量流量、跻身各内容领域的前列。 头条系产品的内容推荐服务的结构有几个特点： 内容生产层面 内容类型丰富且不断拓展：新闻、小说、内涵段子、摄影，等等（丰富的内容配合精准的个性化推荐系统，从而精准定位和充分满足用户需求） 内容形式多元且不断拓展：文字、短视频、直播（一方面头条客户端可为各头条系产品引导流量，另一方面这些产品也可不断为头条输送新鲜内容） OGC+PGC的内容生产模式持续制造内容源：制作者内容生产门槛低；强大的数据爬取能力使头条获取到海量内容资源 内容分发层面 强大的推荐系统使内容被精准分发：一方面通过用户初始信息和用户在头条系产品中的内容消费记录构建精准的用户画像模型；另一方面基于推荐引擎将内容推送给可能对之感兴趣的用户 在传统内容分发渠道上增加社交分发渠道，使内容分发能力和内容对用户的吸引力更强：在微头条板块，用户关注的大V的动态会被推送给用户，且用户可从大V评论的内容链接跳转到源内容 内容阅读场景 增加社交阅读场景：通过微头条，用户可与关注的明星名人和内容创作者进行互动 3.今日头条的推荐系统推荐系统是今日头条的核心引擎，其本质解决的问题是用户、环境和资讯之间的匹配，其实质是一个拟合用户对内容满意度的函数，这个函数是$y=F(x{i},x{u},x_{c})$，其中包含3个维度的变量： $x_{i}$ （内容）：内容形式多元化，不同内容的特征也不同，需要考虑怎样提取不同内容类型的特征做好推荐 $x_{u}$（用户）：怎样提取用户特征 $x_{c}$（环境）：用户在不同场景下的信息偏好不同 其中，这三维度的变量分别被设定了一些不同的用来描述变量的特征： 用户特征：兴趣，职业，年龄，性别，机型，用户行为 环境特征：地理位置，时间，网络，天气 文章特征：主题词，兴趣标签，热度，时效性，质量，作者来源，相似文章 结合这三个维度，推荐模型会给出1个预估：预测推荐内容在这一场景下对用户是否合适。此即头条的推荐系统的基本原理。 头条系的所有产品都是沿用这同一套强大的算法推荐系统来做内容推荐服务的，不过根据业务场景不同，各产品的模型架构会有所调整。 4.今日头条的社会责任履行情况今日头条的社会责任履行情况如何，主要体现在其作为一个公众媒体平台是否肩负起了媒体应承担的社会责任，即内容质量问题。我将这个内容质量问题分为几个维度： 传播的内容类型是否健康、安全、有益？ 传播的价值观是否正确？ 传播的内容质量是否达到合适的需求标准？ 我们可以从已/正在完成、未完成两方面看今日头条的社会责任履行情况。首先是已(正在)履行的责任。就已履行的责任，主要为四方面： 一是严格的风险评估。 据今日头条官方说，其成立之初即专门设有审核团队负责内容安全，当时研发所有客户端、后端、算法的同学一共才不到40人，头条非常重视内容审核。 下图是头条的风险评估流程： 今日头条的内容主要来源于两部分，一是具有成熟内容生产能力的PGC平台，一是UGC用户内容，如问答、用户评论、微头条。这两部分内容需要通过统一的审核机制。如果是数量相对少的PGC内容，会直接进行风险审核，没有问题会大范围推荐。UGC内容需要经过一个风险模型的过滤，有问题的会进入二次风险审核。审核通过后，内容会被真正进行推荐。这时如果收到一定量以上的评论或者举报负向反馈，还会再回到复审环节，有问题直接下架。 二是开发风险内容识别技术鉴别不良不优内容。 头条在风险内容识别技术上也做了一些努力。他们为了开发鉴别色情图片的鉴黄模型，构建了千万张图片数据集，通过深度学习算法(Resnet)训练，最新的召回率为99%。对于鉴别低俗内容的低俗模型，他们对文本和图片同时分析，且对文章和评论都做低俗识别，最新的召回率为90%+，准确率为80%+（低俗模型更注重召回率，可牺牲一定的准确率）。为了净化评论氛围，他们开发了鉴别谩骂言论的谩骂模型，样本库超过百万，最新召回率达95%+，准确率80%+。 除了风险内容识别外，头条还研究泛低质内容识别技术，开发了低质模型，通过对评论做情感分析，并结合用户其它的负反馈信息(举报、不感兴趣、踩)等信息，来解决很多语义上的低质问题，诸如题文不符、有头无尾、拼凑编造、黑稿谣言等。目前低质模型的准确率为70%,召回率为60%，结合人工复审召回率能做到95%。 三是引入人工干预。 以前头条给人的印象一直是算法至上。在人工编辑和运营与机器算法之间，坚定地站在机器算法一侧。如今，头条已经大动作开始引入人工干预，不仅设立了总编辑岗位，还组建了数量可观的内容审核团队。这表明今日头条在逐渐偏离“算法至上”理念，往算法推荐+人工干预方向靠拢。 2017年12月今日头条被发现传播色情低俗信息，而后12月29日被北京市互联网信息办公室约谈后今日头条开始全平台整顿，包括关闭社会频道，将新时代频道设置为默认频道，集中清理涉嫌违规的含低质内容的自媒体账号，共封禁、禁言账号1101个。同时，2017年 12月29日18时至12月30日18时，今日头条手机客户端的“推荐”、“热点”、“社会”、“ 图片 ”、“问答”、“财经”等六个频道还曾暂停更新24小时以便整顿。内容低俗、算法推荐遭狠批后，今日头条今年计划再招聘2000名审核编辑；目前今日头条已建立了国内最大的审核编辑团队，人数超过4000人，且据称头条的内容审核编辑人数预计很快突破10000人。 四是打破黑箱、公开算法。 2018年1月今日头条首度向社会公开其推荐算法原理，主动打破了其算法技术的黑箱，从而可以让用户参与到对算法的检视中。 五是采取举措扭转社会大众对今日头条内容低俗的印象。 2018年3月28日今日头条发布了一款名为“灵犬反低俗助手”的小程序，基于今日头条反低俗模型开发出的。功能很简单，用户输入一段文字或文章链接，即可检测内容健康指数。这个小程序一发布我就进行了几轮测试，测试后发现这个反低俗模型的准确率比较低（也许和测试样本数量太少和样本种类有关）。但实际上这个产品的实际用户需求和使用场景是很少的（它的设计初衷也不是为了用户需求而设计），它所承载的主要是头条的“政治意义”，即作为今日头条使用人工智能技术识别低俗的展示窗口。这个展示对象，既有社会大众，应该也有政府。今日头条想让社会大众和政府看到它反低俗的决心，扭转大家对今日头条内容低俗的印象，我想这才是头条设计这个小程序的初衷。 尽管今日头条已经在履行一个有影响力的媒体该履行的社会责任，但毕竟才刚开始，它的内容服务上仍存在一些问题： 第一点，虽然今日头条在不断强化人工干预权重，但实际内容分发效果并未得到明显改善，用户抱怨低俗化的声音仍不绝于耳。今日头条去低俗化是场硬仗，短期内难见成效，头条仍得再接再厉。 第二点，还未实现内容推荐的真正智能。现在头条的内容推荐仍是内容分发的绝对控制和定向的广告贩卖。但实际上，高质阅读需求的用户想看到的是高质量的内容，能带来更多的知识收获和眼界的开阔，而不是仅仅靠捕捉用户兴趣、迎合用户喜好的内容。 5.对头条色情低俗内容泛滥的原因分析和方案建议头条平台上的色情低俗内容泛滥的根本原因是纯粹算法推荐和没有建立严格的内容审核机制。一方面，色情低俗内容因其猎奇性、感官刺激性故容易吸引人眼球，获得较高的用户点击率，于是这类文章在推荐系统中的热度就高、推荐值也高，所以容易被算法推荐。另一方面，头条之前坚持“算法至上”，平台没有或没有设置足够的人工编辑去审核内容，也没有严格的内容审核机制，于是放任“纯粹算法推荐”的结果就是色情低俗内容泛滥成灾，污染视听。 那么头条如何才能解决这个问题呢？实际上就如上文讲到的，头条近几个月来已经和正在采取相关措施解决这个问题，包括建立严格的风险评估机制，开发风险内容识别技术鉴别不良不优内容，引入人工干预，以及打破黑箱、公开算法。至于这些举措的效果会如何，且待时间告诉我们答案。 除了头条的措施外，我还想提两个建议： 1.建立用户举报机制：海量的内容对有限数量的编辑来说审核难度比较大，且难免会有疏漏，不如让广大用户发挥“火眼金睛”一起发现并举报色情低俗内容，然后头条平台及时采取删稿或封号等举措。 2.研发审稿机器人技术：审稿机器人的计算能力和速度是人类所无法比拟的，也适宜处理海量内容的审核工作。 最后，我想说，企业的本质目的是盈利，这无可厚非。若以优质文化产品取胜，是健康的产业模式。但若通过兜售低俗内容给消费者，就是不负责任的行为。算法推荐赚取巨大流量红利的同时，我们确实应该反思这种创新能给社会带来怎样的进步。如果新的媒体平台，只是为了让低俗化、媚俗化、娱乐化的信息更加便捷传播，如果新的技术，只会降低人的思考能力和审美水平，这样的媒介形态和产品创新必将在社会发展中被淘汰。今日头条作为现在中国最大的新闻资讯分发媒体平台，在获得名声、流量、利润的同时，也应承担起媒体所必须承担的社会责任。]]></content>
      <categories>
        <category>Artificial Intelligence</category>
        <category>Recommended System</category>
      </categories>
      <tags>
        <tag>Medium Study</tag>
        <tag>Recommended System</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[今日头条推荐算法原理解析]]></title>
    <url>%2F2018%2F03%2F25%2F%E4%BB%8A%E6%97%A5%E5%A4%B4%E6%9D%A1%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[Abstract：这篇是1月份头条首次公开的算法原理的阅读笔记。 1.头条推荐算法原理1.1 系统概览1.资讯推荐系统”你关心的，才是头条“ 本质要解决的问题：用户、环境和资讯的匹配，即$y=F(x{i},x{u},x_{c})$ 实质：推荐系统其实是一个拟合用户对内容满意度的函数，这个函数需要输入3个维度的变量。 $x_{i}$ （内容）：内容形式多元化，不同内容的特征也不同，需要考虑怎样提取不同内容类型的特征做好推荐 $x_{u}$（用户）：怎样提取用户特征 $x_{c}$（环境）：用户在不同场景下的信息偏好不同 结合这3个维度，推荐模型会给出1个预估：预测推荐内容在这一场景下对用户是否合适。 2.特征类型 人的特征：兴趣，职业，年龄，性别，机型，用户行为 环境特征：地理位置，时间，网络，天气 文章特征：主题词，兴趣标签，热度，时效性，质量，作者来源，相似文章 3.如何引入无法直接衡量的目标？ 广告&amp;特型内容频控：特型内容比如问答卡片，其推荐目标不完全是让用户浏览，还要考虑吸引用户回答为社区贡献内容，这些内容和普通内容如何混排，怎样控制频控都需要考虑 低俗内容打压&amp;频控 标题党、低质、恶心内容打压 重要新闻置顶&amp;强插&amp;加权 低级别账号内容降权 出于内容生态和社会责任的考量（算法无法完成，需要人工干预） 4.典型推荐算法 Logistic Regression 协同过滤 DNN：深度神经网络 Factorization Machine GBDT $y=F(x{i},x{u},x_{c})$ 是经典的监督学习问题，以上5种算法均可实现 5.推荐系统模型架构 一个优秀的工业级推荐系统需要非常灵活的算法实验平台，可以支持多种算法组合，包括模型结构调整。因为很难有一套通用的模型架构适用于所有的推荐场景 现在很流行将LR和DNN结合 今日头条旗下几款产品都在沿用同一套强大的算法推荐系统，但根据业务场景不同，模型架构会有所调整 6.典型推荐特征 相关性特征：即评估内容的属性与用户是否匹配，包括关键词匹配，分类匹配，主题匹配，来源匹配 环境特征：地理位置，时间 热度特征：全局热搜，分类热度，主题热度，关键词热度（热度信息在冷启动时非常有效） 协同特征：点击相似用户，兴趣分类相似用户，兴趣主题相似用户，兴趣词相似用户（部分程度上解决算法推荐内容窄化问题，通过用户行为分析不同用户间相似性，比如点击相似、兴趣分类相似、主题相似、兴趣词相似，甚至向量相似，从而扩展模型的探索能力） 7.大规模推荐模型的在线训练 基于storm集群实时处理样本数据：包括点击、展现、收藏、分享等动作类型 实时训练模型：省资源且反馈快，用户需要行为信息可以被模型快速捕捉并反馈至下一刷的推荐效果 每收集一定量用户数据就更新推荐模型 模型参数存储在高性能服务器集群，包含几百亿原始特征和数十亿向量特征 整体训练过程： 线上服务器记录实时特征——>导入Kafka文件队列——>导入Storm集群——>拼接用户完整数据，构造样本 ——>根据最新样本，更新模型参数——>线上模型获取新知识8.召回策略设计（match）目的：使用召回策略高效从海量内容中筛选出一小部分符号要求的内容库（选出数千内容后进行大规模机器学习排序，再基于场景去重、多样性控制、加权，生成最终的推荐列表；线上实时产生并更新） 不可能所有内容都由模型评估(计算开销太大)，利用一些召回策略可有效平衡计算成本和效果。 兴趣分类召回：根据用户兴趣标签拉取相应文章，并rank top结果 Tag: 显式兴趣标签 &amp; 隐式兴趣标签 9.推荐系统的数据依赖 特征抽取：需要用户侧和内容侧的各种标签 召回策略：需要内容侧和用户侧的各种标签 内容分析和用户标签挖掘：搭建推荐系统的基石 1.2 内容分析内容分析：文本分析，图片分析，音视频分析 1.文本分析在推荐系统的应用 用户兴趣建模（user profile）：比如，给喜欢阅读“人工智能”文章的用户打上“人工智能”标签 帮助内容推荐：“Apple”的内容推荐给关心“Apple”的用户，“Dota”的内容推荐给“Dota”的用户 生成频道内容：“Dota”的内容分进“Dota频道” 用户标签和内容标签的先后顺序：先有内容标签，然后用户阅读了内容，才给用户打上相应标签； 若出现推荐窄化，则可返回子频道阅读，再回主频道，推荐效果会更好。 子频道的重要性：子频道探索空间较小，更容易满足用户需求。 2.文本特征文本特征对内容推荐的重要性 无文本特征则推荐引擎无法工作：资讯类基本上是消费当天内容，无文本特征的话新内容冷启动会很困难 协同类特征无法解决文章冷启动问题(无文本特征的内容无法被定位推荐给何种用户) 粒度越细的 文本特征。冷启动能力越强（eg.Dota vs 游戏） 文本特征包括 (显式)语义标签类特征：由人预定义的具有明确意义的标签；要求覆盖全，希望每篇文章都找到其合适的分类，精确性要求不高 语义标签需要人持续标注，如网络热词； 有一些产品上的需要，比如频道需要有明确定义的分类内容和容易理解的文本标签体系；语义标签的效果是检测公司NLP水平的试金石。 (隐式)语义特征：topic特征和关键词特征（topic特征是对于词概率分布的描述，无明确含义；关键词特征是基于一些统一特征描述，无明确集合）；不要求覆盖全，但要求精确性（只要求覆盖每个领域的热门文章、机构、产品等即可） 文本相似度特征：包括主题、行文、主体等（根据相似度特征可避免给用户推送重复内容） 时空特征：分析内容的发生地点和时效性（eg.武汉限行的新闻推给北京的用户就没有什么意义） 质量特征：判断内容是否低俗、色情、暴力，或者是否是软文、鸡汤等 语义标签： 特征 使用场景 分类体系 user profile；过滤频道内容；推荐召回；推荐特征 概念体系 过滤频道内容；标签搜索；推荐召回（Like） 实体体系 过滤频道内容；标签搜索；推荐召回（Like） 每个层级粒度不一样，要求也有区别。 3.典型的层次化文本分类算法 层次化：即大类分中类，中类分小类。 有几条“飞线”的作用/目的：？ 实体词识别算法 流程：分词&amp;词性标注——&gt; 抽取候选——&gt;去歧(通过词向量、topic分布、词频本身等)——&gt; 计算相关性模型 1.3 用户标签1.头条用户标签兴趣特征： 感兴趣的类别和主题 感兴趣的关键词 感兴趣的来源 基于兴趣的用户聚类 各种垂直兴趣特征 身份特征 性别：通过第三方登录社交账号获知 年龄：模型预测、通过机型和阅读时间分布等预估 地域：用户授权访问位置信息，然后基于位置信息再通过传统聚类得到常驻点 行为特征：晚上看视频 2.用户标签的主要处理策略 过滤噪声： 过滤停留时间短的点击，打击标题党 惩罚热点：用户在热门文章上的动作做降权处理 时间衰减：随用户动作增加，老的特征权重会随时间衰减，新动作贡献的特征权重会更大 惩罚展现：如果一篇推荐给用户的文章未被点击，则相关特征会被惩罚（类别、关键词、来源） 考虑全局背景：考虑给定特征的人均点击比例（是不是相关内容推送比较多，以及相关的关闭和dislike信号等） 3.用户标签计算用户标签批量计算框架 每天抽取昨天日活用户 抽取这些用户过去2个月的动作数据 在Hadoop集群上批量计算结果 批量计算用户标签的问题 计算量太大：用户增长 &amp; 兴趣种类增加 &amp; 其他批量处理任务增加 导致：集群计算资源紧张，开销增加，用户兴趣标签更新延迟变高 用户标签流式计算框架【头条：2014年上线】 用storm集群实时处理用户动作数据 每收集一定量用户数据就重新计算一次用户兴趣模型 用大规模+高性能存储系统支持用户兴趣模型的读写 流式计算和批量计算混合使用 大部分user profile采用流式计算 各个粒度的兴趣标签 垂直领域profile 对时效性不敏感的user profile采用batch计算 性别、年龄 常驻地点 1.4 评估分析对推荐效果可能产生影响的因素 候选内容集合的变化 召回模块的改进和增加 推荐特征的增加 推荐系统架构的改进 算法参数的优化 规则策略的改变 评估的意义：很多优化最终可能是负向效果 评估需要 完备的评估体系：尝试将尽可能多的综合指标合成唯一的评估指标 强大的实验平台 易用的实验分析工具 评估需要注意 注重短期指标和长期指标 注重用户指标和生态指标 注重协同效应的影响，有时需要做彻底的统计隔离 1.5 内容安全1.头条风险评估流程如果1%的内容出现问题，就会产生较大的社会影响。因此头条从创立伊始就把内容安全放在公司最高优先级队列。 成立之初，已经专门设有审核团队负责内容安全。当时研发所有客户端、后端、算法的同学一共才不到40人，头条非常重视内容审核。 今日头条的内容主要来源于两部分，一是具有成熟内容生产能力的PGC平台，一是UGC用户内容，如问答、用户评论、微头条。这两部分内容需要通过统一的审核机制。如果是数量相对少的PGC内容，会直接进行风险审核，没有问题会大范围推荐。UGC内容需要经过一个风险模型的过滤，有问题的会进入二次风险审核。审核通过后，内容会被真正进行推荐。这时如果收到一定量以上的评论或者举报负向反馈，还会再回到复审环节，有问题直接下架。 2.风险内容识别技术 鉴黄模型：构建了千万张图片数据集，通过深度学习算法(Resnet)训练，召回率99% 低俗模型：对文本和图片同时分析，对文章和评论都做低俗识别；召回率90%+，准确率80%+（更注重召回率，可牺牲一定的准确率） 谩骂模型：净化评论氛围；样本库超过百万，召回率高达95%+，准确率80%+ 3.泛低质内容识别技术 低质模型：通过对评论做情感分析,结合用户其它的负反馈信息(举报、不感兴趣、踩)等信息,来解决很多语义上的低质问题,诸如题文不符、有头无尾、拼凑编造、黑稿谣言等。 目前低质模型的准确率为70%,召回率为60%，结合人工复审召回率能做到95%。 Reference今日头条算法原理全&amp;version=12020810&amp;nettype=WIFI&amp;lang=zh_CN&amp;fontScale=100&amp;pass_ticket=jvoWvMRVU%2BYHiDejcgEmd1GUFu2Q2n1fDkx3UkGD76QJZlqopG9SiXfMtvAA96e8) 2.头条产品相关做最懂用户的信息平台 算法比编辑更懂用户，甚至比用户自己更懂他们真正想要什么 以前把关人是编辑，现在是算法，此即规模化的产品思维。 提升1个编辑的写作水平很难，但提升算法的推荐效率却是一直在进行 盈利：精准需求广告 广告信息化：信息流广告 分发自动化：今日头条可根据大数据给同一个广告做几十版素材，真正做到不同的人给不同的需求 信息视频化：短视频广告 产品思路 推荐引擎 稳固内容生产 垂直内容服务化 内容大拓展：综合信息平台，内容类型、内容形态拓展 社交化 推荐冷启动： 通过对用户微博账号的分析建立一个“兴趣图谱”，即根据用户在微博上发布的内容及其所属类别、用户自标签、社交关系、社交行为、参与的群组、机型、使用时间等来数据源来推断出用户的兴趣点有哪些。社交关系、社交行为即用户和用户之间的交流状况，可以根据二者间的共同好友数、相互评论熟、@数等来做度量。 广泛的信息分发平台： 通过人工智能识别内容最大的难点在于攻克语义的复杂性，涉及到对逻辑推理和因果关系的上下文分析。通俗理解就是，人工智能可以鉴别色情内容，但在区别色情、性感、艺术等照片上还存在很大难度。]]></content>
      <categories>
        <category>Artificial Intelligence</category>
        <category>Recommended System</category>
      </categories>
      <tags>
        <tag>Medium Study</tag>
        <tag>Recommended System</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MyProducts2017]]></title>
    <url>%2F2018%2F03%2F23%2FMyProducts2017%2F</url>
    <content type="text"><![CDATA[Abstract：我在2017年的产品作品集。 1.鸟窝习惯1.简介：群体习惯养成 Android app，通过基于熟人关系链构建用户之间互相监督的打卡窝, 来帮助用户养成习惯。 2.用户分析 用户需求：解决的是用户个人习惯养成的需求，最大特色是基于熟人关系链的群体互相督促和激励机制 用户群体：有习惯养成需求且有每日使用手机的条件的人群，主要为大学生、青年上班族 3.项目说明 项目类型：3人合作项目 开发时间：2017年7-9月 项目状态：已开发完成，未上线. 4.我的职责：提出产品想法；设计产品功能架构；设计产品原型和交互逻辑；与UI设计师对接设计需求、商讨和确定设计风格；与程序员对接开发需求 5.产品功能架构： 6.产品代码仓库：ScarlettYellow/Nest Habit 7.产品设计图 2.Pre-Extraction Helper1.简介：网页信息预抽取 Google插件，可对网页文本进行摘要提取、关键词提取、文本与主题相似度分析和文本情感极性分析。 2.用户分析 用户需求：解决的需求是让用户可以不点开网页即可获知网页文章的关键信息，从而避免被无用信息浪费时间、提高用户网页信息检索的效率。 用户群体：有高效获取网页信息需求的人群，主要为大学生、上班族 3.项目说明 项目类型：Hackday快速开发比赛项目 开发时间：2017年7月 项目状态：未上线. 4.我的职责：提出产品想法；设计产品功能，确定各功能对应使用什么API；设计产品原型和UI视觉设计图；与程序员对接开发需求 5.用户使用流程： 6.技术流程 7.产品代码仓库：ScarlettYellow/Pre-Extraction Helper 8.产品设计图 3.爱彼伴1.简介：亲子互动式的手游监控 Android APP。 2.用户分析 用户需求：解决的是孩子过度玩手游、帮助家长控制孩子玩手游的时间的需求。 用户群体：想约束沉迷手游的孩子的家长群体 3.项目说明 项目类型：Hackday快速开发比赛项目 开发时间：2017年7月 项目状态：未上线 4.我的职责：提出产品想法；设计产品功能结构；设计产品原型；与设计师和程序员对接开发需求 5.功能说明： 6.产品代码仓库：ScarlettYellow/Hackinit20 7.产品设计图 4.Snakesocks1.简介：基于 TCP 实现的代理 (VPN), 用户可以通过自定义模块实现加密细节, 以一定程度上避免 GFW 的侦测, 比现有的 VPN 更安全, 可跨平台移植。 2.用户分析 用户需求：想要一个比shadowsocks更安全的VPN，在“特殊时期”防止端口被封 用户群体：主要是有“科学上网”需求的大学生、科研人士、上班族 3.功能特色 用户自定义模块（防止被墙） 与shadowsocks速度相当 强扩展性：跨平台移植 + 速度优化 + 免流 4.产品代码仓库：ScarlettYellow/snacksocks 5.项目说明 项目类型：4人合作项目 开发时间：2017年7月 项目状态：在GitHub上开源(Star数: 19)，可下载使用 5.表情大作战1.简介：一款应用人脸识别技术识别人脸表情，并自动生成用户专属表情包的 iOS APP。 2.用户分析 用户需求：想把朋友的照片制作成经典表情包 用户群体：主要是学生群体 3.项目说明 项目类型：Hackday快速开发比赛项目 开发时间：2017年5月 项目状态：在GitHub上开源，可下载使用 4.我的职责：设计产品功能架构；设计产品原型和交互逻辑；与UI设计师对接设计需求；与程序员对接开发需求 5.产品代码仓库：ScarlettYellow/EmoPK 使用OC语言进行iOS前端开发，整个APP全部进行本地化部署 引入OpenCV的iOS端SDK和Sensetime人脸识别SDK 使用C++/OpenCV设计算法，自动生成玩家专属表情包进行展示与保存]]></content>
      <categories>
        <category>Artificial Intelligence</category>
        <category>AI PM</category>
      </categories>
      <tags>
        <tag>AI PM</tag>
        <tag>Product Manager</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++求平方根和立方根中遇到的问题]]></title>
    <url>%2F2018%2F03%2F21%2FC-%E6%B1%82%E5%B9%B3%E6%96%B9%E6%A0%B9%E5%92%8C%E7%AB%8B%E6%96%B9%E6%A0%B9%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[Abstract: C++求平方根和立方根中遇到的问题: 函数参数不能为负数，否则会输出Nan。 Problem &amp; Solution平方根函数：sqrt(x) 立方根函数： 1.cbrt(x) 2.pow(x, 1.0/3.0) 注意：平方根和立方根函数的参数都只限于正数和0，若输入的参数为负数则会报错。 改进方法：分段求，对于x&lt;0, 语句可写为：-pow(-x,1.0/3.0) 、-cbrt(-x)、sqrt(-x) Example完成如图所示分段函数: 要求：请使用多分支结构完成该程序；测试数据为 27 、59、 -27、 -120. 1234567891011121314151617181920212223242526#include &lt;iostream&gt;#include &lt;cmath&gt;using namespace std;int main()&#123; double x,y; cout&lt;&lt;"Enter a value of x:"&lt;&lt;endl; cin&gt;&gt;x; if( x &gt;= 59 ) &#123; y=7*x*x + 5; &#125; else if (x&lt;59 &amp;&amp; x&gt;=0) &#123; y=pow(x,1.0/3.0)+x*x; &#125; else if (x&lt;0 &amp;&amp; x&gt;-100) &#123; y=-pow(-x,1.0/3.0)+x*x; &#125; else &#123; y=(2*x+9)/5; &#125; cout &lt;&lt;y&lt;&lt; endl;&#125; ReferenceC++常用数学函数]]></content>
      <categories>
        <category>Computer Science</category>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>Computer Science</tag>
        <tag>C/C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何在Github上托管和预览静态网站？]]></title>
    <url>%2F2018%2F03%2F19%2F%E5%A6%82%E4%BD%95%E5%9C%A8Github%E4%B8%8A%E6%89%98%E7%AE%A1%E5%92%8C%E9%A2%84%E8%A7%88%E9%9D%99%E6%80%81%E7%BD%91%E7%AB%99%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[Abstract：如何把网站托管在GitHub上并能实时预览？本文是Solution ~ 利用GitHub Pages托管网站1.在GitHub上Create a repository 2.Clone the repository to local 1git clone https://github.com/username/username.github.io 3.Enter the project folder and add an index.html 12cd username.github.ioecho &quot;Hello World&quot; &gt; index.html 4.Push it 12345git add --allgit commit -m &quot;Initial commit&quot;git push -u origin master 或者： 3.创建1个gh-pages分支 1git checkout -b gh-pages 4.编辑相应的html/css/js文件，放在gh-pages分支上 5.push gh-pages分支到GitHub上 123git add &lt;filename&gt;git commit -m &quot;xxx&quot;git push -u origin master 预览网站在项目源代码页面链接前缀那加上http://htmlpreview.github.com/?即可，譬如： http://htmlpreview.github.io/?https://github.com/ScarlettYellow/presidentMao.github.io/blob/gh-pages/cvsite_1/index.html Example自己最近做的毛概作业（给毛爷爷做个简历，于是我就做了个网站…）：毛爷爷的个人简历网站 ReferenceGithub Pages 知乎：怎么预览 GitHub 项目里的网页或 Demo？]]></content>
      <categories>
        <category>Computer Science</category>
        <category>Coding</category>
      </categories>
      <tags>
        <tag>Web Development</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[给毛爷爷做了1个个人简历网站]]></title>
    <url>%2F2018%2F03%2F19%2F%E7%BB%99%E6%AF%9B%E7%88%B7%E7%88%B7%E5%81%9A%E4%BA%861%E4%B8%AA%E4%B8%AA%E4%BA%BA%E7%AE%80%E5%8E%86%E7%BD%91%E7%AB%99%2F</url>
    <content type="text"><![CDATA[Abstract：其实是某通识课的课后作业，要求给毛爷爷做一份简历。我想与其做纸质的，不如为毛爷爷设计一个网站，这样更有趣 ~ 网站说明 这个网站是在1个模板上进行模改的（没有全部自己写，太花时间了qaq） 网站托管在我的GitHub上，使用GitHub Pages进行demo预览（加载预览可能会稍微有点慢，需要加载5~10秒左右，可能会先显示几秒无css样式的html页面） 网站地址：毛爷爷的个人简历网站]]></content>
      <categories>
        <category>Computer Science</category>
        <category>Coding</category>
      </categories>
      <tags>
        <tag>Web Development</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[传播统计学作业1|统计学基础概念]]></title>
    <url>%2F2018%2F03%2F17%2F%E4%BC%A0%E6%92%AD%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%BD%9C%E4%B8%9A1-%E7%BB%9F%E8%AE%A1%E5%AD%A6%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[Abstract：10道传播统计学的作业题，主要关于传播统计学基础概念。参考教材是祝建华所著的《传播统计学》。 1.何谓统计调查? 统计调查（Statistical Investigation/Statistical Survey）是根据调查的目的与要求，运用科学的调查方法，有计划、有组织地搜集数据信息资料的统计过程。统计调查是统计工作的基础环节，是认识事物的起点。 按调查对象包括的范围不同，可分为全面调查和抽样调查。 按登记时间是否连续，可分为连续性调查与非连续性调查。 按调查的组织方式不同，可分为统计报表制度和专门调查。 调查研究分为三种类型：探索性研究（Exploration）、描述性研究（Descriptive Research），解释性研究（Explanation）。 2.如何根据调查目的与被调查对象特点, 选择不同的统计数据搜集方法? (1) 直接观察法：指调查人员到现场对调查对象进行观察点数与计量。在所需数据需要调查者去现场对被调查对象进行观察、测量、记录的情况下，适合用直接观察法，比如农作物产量调查。 (2) 报告法：指统计工作机构将调查表格分发或电传给被调查者, 被调查者则根据填报的要求将填好的表格寄回。当统计政府公务数据、被调查对象是政府机关公职人员时，适合用报告法。 (3) 采访法：指由调查人员向被调查者提问，根据被访者的答复来搜集统计资料的一种方法，分为口头询问法、开调查会法和被调查者自填法三种。在所需数据需要通过实际与被调查者沟通才能获得的情况下，适合用采访法。 (4) 登记法：通过让被调查者登记信息来获取统计资料的方式。在需要获取并系统性地留存被调查对象详细信息的情况下，适合用登记法。 (5) 实验设计调查法：即设计分组对照实验。在需要研究控制变量对某些变量的影响(因果关系)的情况下，适合用实验法。 3.何谓统计调查误差?它有几种类型? 统计调查误差，就是调查结果所得的统计数字与调查总体实际数量之间的离差。 类型划分如下： (1) 统计调查误差可分为：登记性误差和代表性误差。 登记性误差：由于错误登记事实而发生的误差，不管是全面调查或是非全面调查都会产生登记性误差；代表性误差：只有非全面调查中才有，全面调查不存在这类误差。 (2) 按产生统计误差的性质来分有：空间误差、时间误差、方法误差和人为误差四种。 空间误差：指统计调查范围所产生的误差，包括重漏统计调查单位，跨区域统计等； 时间误差：指统计调查对象因时期或时点界定不准确所产生的误差； 方法误差：指因使用特定的统计调查方法所产生的误差，如抽样调查中的代表性误差（抽样平均误差）； 人为误差：指在统计设计、调查、整理汇总和推算等过程中因人为过错产生的误差；人为误差是统计误差中产生因素最多的一类，它又分为度量性误差、知识性误差、态度性误差和干扰性误差。 (3) 统计误差按工作环节来分有：源头误差、中间环节误差和最终误差三种。 源头误差：指起报单位或申报者所产生的误差； 中间环节误差：指统计调查数据在逐级上报过程中所产生的误差，包括加工整理、汇总和推算等环节； 最终误差：指下级各基层数据汇总数或规范的方法得到的推算数与最终使用数之间的差异值。 按工作环节划分的统计误差类别是相对的，中间环节误差在不同的场合有可能是源头误差，也可能是最终误差。源头误差在有些场合也叫调查误差，或叫登记误差。 4.何谓统计分组? 统计分组有几种类型? 统计分组（Statistical Grouping）：根据统计研究任务的要求和研究现象总体的内在特点，把现象总体按某一标志划分为若干性质不同但又有联系的几个部分的一种统计方法。 总体的变异性是统计分组的客观依据。统计分组是总体内进行的一种定性分类，它把总体划分为一个个性质不同的范围更小的总体（同一组内的各单位在分组标志的性质相同，不同组之间的性质相异）。 分组种类： (1) 按任务作用的不同，分为：类型分组、结构分组和分析分组； 类型分组：目的是划分经济类型； 结构分组：目的是研究同质总体的构成； 分析分组：目的是研究现象总体内部诸标志间的依从和制约关系。 (2) 按分组标志的多少，分为：简单分组和复合分组； 简单分组：将总体按一个标志进行分组； 复合分组：将总体按两个或两个以上的标志重叠起来进行分组。 (3) 按分组标志的性质，分为品质分组和变量分组。 品质分组：将总体按品质标志进行分组，如企业按经济成份、地理位置分组，职工按性别、文化程度分组等； 变量分组：将总体按数量标志进行分组，如企业按职工人数、劳动生产率分组，职工按工龄、工资分组等。 5.以一实例说明统计分组应遵循的两个原则。 统计分组的原则： (1) 穷尽原则：使总体中的每一个单位都应有组可归，或者说各分组的空间足以容纳总体的所有单位； (2) 互斥原则：即总体任一单位都只能归属于一组，而不能同时或可能归属于几个组。 举例：将100个各不相同的食物进行统计分组，假设需要从以下5个类别中选出最合适的分组方案：水果类、蔬菜类、谷物类、肉类、鱼类。 我们需要排除鱼类，因为鱼类属于肉类的范畴，若分组中同时包含鱼类和肉类两个类别，则违背了互斥原则；我们需要增加1个组，组名为“其他”，因为上面这些组无法穷尽食物的所有类别，加上“其他”则可避免违背穷尽原则。 6.何谓等距分组? 何谓异距分组? 说明它们各自的适用场合。 等距分组和异距分组是组距分组的基本方法。 组距分组：数值型数据分组的基本形式，是将全部变量值依次划分为若干个区间，并将这一区间的变量值作为一组。 (1) 等距分组：各组保持相等的组距。 适用场合：总体中变量值分布比较均衡，各组标志值的变动都限于相同的范围的情况。 在等距分组中，各组单位数的多少不会受到组距大小的影响，便于直接比较各组次数的多少，研究次数分布的特征。 (2) 异距分组：即各组组距并不完全相等的分组。 适用场合： 标志值分布很不均匀的场合 标志值相等的量具有不同意义的场合 标志值按一定比例发展变化的场合 7.说明组距、组限、组数与组中值的含义及它们的计算方法。如何提高组中值代表各组标志值的代表性? (1) 组数$n$：即分组个数。 n=1+3.3lg(N) 其中，n为组数，N为总体单位数. (2) 组距$d$：指每组的最高数值与最低数值之间的距离。 ① 连续组距分组：$d=本组上限-本组下限$ ​ 间断组距分组：$d=本组上限-本组下限+1$ ② 斯特杰斯经验公式确定组距： d=R/n=(Xmax-Xmin)/n $X{max}$：最大变量值；$X{min}$：最小变量值；R：全距/极差. (3) 组限L：表示各组变动范围的两端的数值，其中，每组的最小值称为下组限，每组的最大值称为上组限。 (4) 组中值G：上下限之间的中点数值，以代表各组标志值的一般水平。组中值仅存在于组距数列分组数列中，单项式分组中不存在组中值。 G=\frac{上限+下限}{2}(5) 使变量值在各组内成均匀分布或在组距中点值两侧呈对称分布，可提高组中值代表组内变量值的一般水平的代表性。 8.何谓频数分布?以一实例说明频数分布数列的两个要素的含义。 频数分布（Frequency Distribution）：在分组的基础上，把总体的所有单位按组归并排列，形成总体中各个单位在各组间的分布。 分布数列：将原始资料进行整理,形成的一系列反映总体各组之间单位分布状况的数列。 分布数列包括两个要素： (1) 总体按其标志所分的组 (2) 各组所分布的单位数 举例：在下表中，第一行展示的是总体按播出时长所分的7个组，第二行对应的是各组所分布的单位数。 9.何谓频率密度?为什么要计算频率密度? 频率密度（Frequency Density）：是组频率与组距的比值，指该组内单位距离上的频率，在频率分布直方图中表现为所有矩形的面积之和等于1。 频率密度=频率/组距利用频率分布直方图反映样本的频率分布规律，可清楚显示各组频数分布情况和各组之间频数的差别。主要是为了将我们获取的数据直观、形象地表示出来，让我们能够更好了解数据的分布情况。 10.某新闻节目播出时间统计如下(单位/秒) 886 928 999 946 950 864 1050 927 949 852 1027 928 978 816 1000 918 1040 854 1100 900 866 905 954 890 1006 926 900 999 886 1120 893 900 800 938 864 919 863 981 916 818 946 926 895 967 921 978 821 924 651 850 要求： (1) 试根据上述资料编制次(频)数分布数列 组距$i=70$，组数=7，总体单位数=50. (2) 编制向上和向下累计频数、频率数列 (3) 根据所编制的次数分布数列绘制直方图、折线图与曲线图 由于我的电脑是Mac，Mac上自带的表格制作软件的图表功能里没有曲线图和直方图，所以我就以比较相似的折线图和条形图来代替了，望老师谅解。 (4) 根据所编制的向上(向下)累计频数(频率)数列绘制累计曲线图 (5) 根据累计曲线图, 指出播出时间在1000秒以上的有多少?占多大比重?播出时间在900秒以下的有多少?占多大比重? 由图可知，播出时间在1000秒以上有7个，占比14%；播出时间在900秒以下有20个，占比40% (6) 根据频数分布曲线图说明新闻播出时间的分布是属于哪一种类型? 正态分布；呈钟型，两头低，中间高。 (7) 用直接法计算第5百分位数, 用频数表法计算第95百分位数 第5百分位数: 808 将分布数列按升序排列，第5百分位数在第$(50+1)*0.05 = 2.55 $位，即第2位~3位之间，$P{5}=\frac{P{2}+P_{3}}{2}=\frac{800+816}{2}=808$ 第95百分位数：1063 将分布数列按升序排列，第95百分位数在第$nx\%=50*0.95 = 47.5$ 位，对照向上累计频数分布表可确定$P_{95}$落在第6组段（1000~1069）； 第6组段下限$L=1000$，组距为$i=70$，频数$f=5$，向下累计频数为48，上一组向上累计频数$\sum fl=43$，则$P_{X}=L+i(nx\%)-\sum fl)/f=1000+70*(47.5-43)/5=1063$]]></content>
      <categories>
        <category>Computer Science</category>
        <category>Data Mining and Analysis</category>
      </categories>
      <tags>
        <tag>Data Mining and Analysis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[心向静水流深——我的思想自传]]></title>
    <url>%2F2018%2F03%2F13%2F%E5%BF%83%E5%90%91%E9%9D%99%E6%B0%B4%E6%B5%81%E6%B7%B1%E2%80%94%E2%80%94%E6%88%91%E7%9A%84%E6%80%9D%E6%83%B3%E8%87%AA%E4%BC%A0%2F</url>
    <content type="text"><![CDATA[Abstract：其实某通识课老师布置的课后作业，写一份自己的思想自传。正好趁这个机会，认真把自己的一些零零散散的思绪整理了下，遂成此文，主题是“信仰”。 常有人说现在的国人缺少信仰，人无信仰故心无畏惧，所以爆出的“社会热点事件”一次次挑战道德底线，令人失语。比起狭义的“宗教信仰”，我更愿意将这里的“信仰”理解为一种价值体系。无论是宗教信仰或个人的三观，本质上，信仰是人所创造的，使人敬仰并奉行的、用以约束人并裨益社会和谐运行的行为准则。 记得初中有段时间，国人“无信仰”被批得很热烈，我也曾想过“信仰听起来是个好东西，我是不是也应该尝试去找个宗教来信仰一下呢？”。但信仰不是说信就能信的，那次是我去“信仰”宗教的第一次失败的尝试，现在想来也是年少天真。之后，上大学了，大一时喜好结交外国友人，认识了一对30多岁信仰基督教的西班牙夫妇，那时我们交往还不错，他们许是抱着传教的想法也经常跟我讲些基督教的渊源和教义，看着对方热情解说的样子我不好表现出不感兴趣的样子，但实话说我对基督教并不甚感冒。那是第二次失败的尝试。两次失败后我放弃了去“寻找信仰”的想法。 而后，我的大学生活或紧或慢地过着，看了一些书，交了一些朋友，学了不少知识，去过不少地方，有成就有失意，有欢喜有忧伤，经历也算丰富，个人的价值体系也在不知不觉地构建着，就像徐徐生长的小树苗，我的一切经历、情感、知识、体悟都化作小树苗的养分，滋养着它。然后慢慢我发现，其实我不需要去信仰别人的信仰，取纳百家之言并结合自身的经历体悟，我正成长中的个人价值体系俨然已经成为了独属于我的“信仰”。它约束着我的行为、激励着我前行，使我得以活得坦诚而心安。 世有许多人，信儒则只信儒，信道则只信道，信佛则只信佛，信基督则只信基督…而我更倾向于兼听各家之言，儒、道、法、释、古希腊哲学，取其适合于我之良言以为养分，来滋养我自己的价值体系。况且，各家信仰虽有大大小小的差异，但信仰本就是一套指导人如何生活的行为准则，这套准则在关键道德问题的答案上不会有本质上的互相背离，总归是大同小异。只要是能让自己更好生活，何管信谁家之言呢？ 儒：正心诚意修齐治平儒家学说作为我国三千多年来的“大一统思想”，其虽有弊端，但这套伦理道德规范的确值得沿袭。初高中学了那么多文言文，具体文章内容已然忘却大半，但古文中体现的古人的“爱国、至孝、进取”给我留下了很深的印象。儒家士大夫以“正心诚意修齐治平”为个人毕生追求，这短短八个字蕴含的人生智慧和崇高境界值得人细心体悟。张继也曾有名言：为天地立心，为生民立命，为往圣继绝学，为万世开太平。 作为一名刚成年不久的普通大学生，我大概正处于“修身”阶段，在不断尝试和试错中探索着自己的人生方向和价值体系，在思考中尝试回答自己“我为什么活着，我要去哪儿”的问题。 在“吾日三省吾身”指导下，我会经常去沉思、内省，反思自己的缺点并尝试慢慢去改正。譬如，我曾经胆子很小，不敢在人多的地方讲话，有“演讲恐惧症”。上了大学后我开始多演讲、多抓住在公开场合发表言论的机会，于是这个缺点已经被我克服了。 在“经世致用”的影响下，我不满足于仅仅上课听老师讲课和自己阅读书籍，而是更多寻找将知识活学活用到具体实践中去、并在实践中汲取新的知识，如此周而复始。譬如，我加入启明学院联创团队的这1年以来，与具有不同技能栈的同学合作产出了5个互联网产品，参加了大大小小比赛，其中DF,CCF大数据与计算智能大赛上获得了全国二等奖，这个比赛我们做的是“中印对峙大数据舆情分析”赛题，我得以将新闻传播学和计算机科学所学知识灵活结合运用到比赛中，并从中获得了宝贵的实践经验，也使我更坚信“学以致用”、不做“百无一用的书生”。 儒家以人为中心，其立身立德的思想能使人更加自信。想到初高中时，我是比较典型的“好学生”，学习成绩好且听话，但其实我的内心是不够自信的，不知道自己除了成绩以外的价值在哪里。正因为不自信，所以在面对自己渴望的东西时会产生“求”的匮乏心态。之后在经历了一次情感打击后，才顿悟之前的心态是多么不正确，才认识到：凡欲得之，要证明我是什么，而不是我求什么，求者不得。之后，我在面对人事物时，都能够保持自信的心态，对待喜欢的就去争取，证明自己值得得到，而若不得，也可以很淡然地放下。 诸如“赠人玫瑰，手留余香”、“勿以善小而不为，勿以恶小而为之”之言则启迪我去多行善事，多主动为别人做些我力所能及的事。其实我以前由于性格比较内敛，故并不会主动去助人，顶多是在请求找上门时给予帮助。使我转变的是在大学里遇到的一位位“名师”。此名师非彼名师，而是一些在我困顿失意迷茫时好心无私地给我指导、指明方向的学长学姐们。在我处于是否转专业的迷茫期时求助了一位之前并没有接触过的传播系12级学姐，当时她正在美国CMU读研，听完我的情况后她二话不说就给我打了近2个小时的越洋电话，只为解答我的疑惑。也正是与她的这一番交谈，才使我坚定转专业的决心，才有了现在的已明确未来方向的我。还有在我纠结是否出国读研的时候，同样是一位之前并没有接触过的华科学长在工作间隙抽时间给我打了1个小时的越洋电话；以及在我初进大学时在各方面给我以引导和帮助的一位学姐 … 正是这些我在成长过程中遇到的一位位给予我帮助的人，使我也坚定要像他们一样，去尽我所能帮助更多的人，把这种“赠人玫瑰”的精神传递下去。后来，有学妹对我说“学姐，你真是太好心了”，我回答她“因为学姐以前也被很好心的学长学姐们帮助过呀”。 佛：因果律佛教源于天竺，后唐代传入中国，逐渐本土化，成为中国第一大教。我对佛家思想的认知起始，一是来自我那几十年如一日信佛的奶奶，“阿弥陀佛”“我佛慈悲”“切莫杀生”“戒贪嗔痴”是奶奶经常挂在嘴边念叨的，二是来自电视上年年播放的“西游记”，而其实当时年幼的我对佛道并没有多少感悟。后来随着自己慢慢长大，经历得多了，才逐渐对佛家思想有些许体悟。 因果律应该是佛家思想的一大核心了，物本有末，事有终始，万物皆有因果。我是坚定的无神论者，不相信世界有上帝、佛祖、真神之类的存在，既不存在，也不会有人去执行所谓的上天堂、下地狱。但不知何时开始，我开始相信因果律，相信善恶终有报，相信因果循环的力量。也许是觉得虽无真神，但宇宙中冥冥必然存在一种自然法则（就像“物竞天择，适者生存”的自然铁律），它不由任何人制定，而是自然存在的一种约束，约束人的善恶行为。 若不信因果，不信自己做的恶，有恃无恐的去做某些事情，则恶报最终会返回到自己身上。趋利避害是人之本性，信因果者，会因惧怕“恶果”和期盼“善果”而去多行善事、避免行恶，这是有利于整个社会的和谐的。 果由因生，无因不能生果，有果必有其因。世界上并没有所谓的偶然或随机事件。这个世界有“定数”，但更存在“变数”。这就意味着我们可以去改变一些东西，但关键是，我们得去种我们希望得到的东西的“种子”，“种瓜得瓜种豆得豆”，如果希望什么都不付出而收获，是不可能的。像很多人一样，我小时候也会幻想自己突然得到超能力或拥有超高的智商，这样就可以不用努力就得到一切。但长大后，我意识到“不劳而获”是不可能的，只有种下因，才能得到相应的果，于是再也没有做过那样不切实际的幻想。现在为了得到果，我会努力去付出、耕耘，用自己的能力得到所求。 有俗语云“功过相抵”。但从因果律角度来说，功过是不能简单相抵的。善有善报恶有恶报，说的不仅是单个的人，也可以是单个人身上的不同行为。做了功德，会有好报，但做了坏事，也会受到苦果。打个不恰当的比方，在有智能手机之前，家长和老师还是对年轻人有绝对的影响力；而有了智能手机之后，家长和老师对孩子的主导力就让位给了手机。于是明星比父母重要，网红比老师聪明。有些家长和老师即便愚昧功利，但他们的本心也会尽量为孩子的长远考虑。而对于智能手机里的那些人来说，孩子只是其满足私欲的工具、待宰的羔羊。马云说过一个段子，早上醒来第一件事，以前是摸老婆，现在是摸手机。张志东说半夜起来喝水也忍不住要去刷手机。成人尚且如此，年轻人又能怎样。智能手机，是人类史一大拐点。主导下一代教育的权力从亲人转移给了陌生人。作为智能手机引领推动者的乔布斯，其“功”虽可垂千秋，而他两次患癌壮年离世未必不是其打开智能手机这个潘多拉魔盒的“果”。再如中国互联网的引领者之一的百度，其搜索引擎给中国大众带来方便，这是功；而其“竞价排名、无原则出售广告位”的商业盈利模式则是“过”，“魏则西事件”导致百度被全社会声讨的企业信誉危机则是这一“过”导致的“果”，此功过也不能简单相抵。对因果律的敬畏可以使我们做事时有所顾忌，譬如在做产品设计、商业营销时，需要坚守不触犯道德底线、不损害社会利益和福祉的原则。 在因果面前，我们不能存在侥幸心理，因为只要因缘上参与了，因果法则就不会落空。我们现在的社会生活水平不断提高，仿佛欣欣向荣，可潜在的社会危机呢？整型盛行，网络上微博上一张张自拍都是“蛇精脸”；源源不断生产着的脑残剧侮辱和降低着年轻人的智商、塑造着扭曲的审美和价值观；咪蒙式的言语暴力煽动着大众的情绪 …… 这是这一代年轻人正面临的危机。记得学者刘瑜讲过一个“1%理论”：六十万犹太人被屠杀是谁的罪过？是希特勒的。但希特勒不可能独自屠杀六十万人。所有抓人的，开车的，修隔离墙的，维持秩序的，写告示的，开枪的，放毒气的，搬尸体的，甚至见死不救的，都有一份罪过，都有1%的贡献。虽然这些人都觉得自己很无辜，自己没有扣动扳机，只是做了分内之事，但他们每个人都为六十万犹太人之死做了不可或缺的一部分。这也是今天的文化精英和技术精英要面对的问题。鸦片影视剧的明星和制片人们，视频网站的程序员们，游戏公司的产品经理甚至前台们，咪懵和她的助理以及广告商们，拿美容刀的医生们，黄色直播和约炮交友app以及夜总会的老板和门卫们……每个人都对每一个年轻人的堕落负有1%的责任，因果不会落下任何一个人。 当我深入认知因果律后，发现自己逐渐学会站在更高纬，去观一个事物的“因果”脉络，“由果知因”和“由因知果”——此所谓“欲知前世因，今生受着是；欲知后世果，今生做者事”。“由因知果”可以说是“洞悉”，这种“洞悉”可以帮助我很好地控制情绪，以及有助作出重要决策。比如当我产生了某种不良情绪产生的原因，就会及时去思考其产生的原因，而当洞悉了原因和推测若放任这种情绪滋长可能带来的后果后，我就会及时扼杀这种情绪于忽微。“由果知因”则可以说是“追溯”，这种“追溯”可以帮助我更好地理解和谅解人事物。比如我在与一个同学的相处过程中发现他的性格过于自我甚至自私，只顾自己而不会考虑别人的感受，这一开始让我觉得很不愉快。但当我静下心去思考“他为什么会养成这样‘自我’的性格？也许和他独生子的家庭环境有关？也许和父母过于溺爱有关？以这种推测来延伸，他从小到大的人生状态可能是怎样的？这样的性格是否使得他无法交到能够长年维系的知交友谊？”后，就觉得一个人性格的形成与很多因素有关，而且也是可以改变的，于是我会去尽量谅解和包容他并以同学角度委婉说出我的想法。最后即便他没有听进去我的建议，但我是心安的。 因果律也赋予我们洞察本质的能力，使我能够从事物表象看进其更深层次的东西。譬如，近十几年成功学十分兴盛，当然我小时候也曾或多或少地受到过成功学的“荼毒”，也曾艳羡过成功人士的成就。但殊不知，一个成功人士的脚下踏着无数失败者的尸骨，成功哪有那么简单，成功学可从来不会写失败者的传记，所以给我们一种“好像成功很容易”的概率上的错觉。而且，看一个成功人士，有多大的成就结果不重要，真正有意思的是，他的认知理念的源头在哪里？以及，是哪些因素直接或间接地成就了他的卓越？同样地，去年引起舆论广泛争议的“红黄蓝事件”只是一个结果，如果找原因，每个人都会有自己的分析判断，但如果找源头，会呈现一个完全不同的世界：有没有人打算去深入的了解下，那些猥亵幼童犯是怎么养成的？有没有人想去看一下人渣们的手机，他们订阅了谁的公众号？看着什么影视剧或者综艺节目？喜欢哪一个角色？玩了什么游戏？用什么app看无穷无尽的猥琐色情视频？有没有人打算在这些人落网后去平心静气的跟他们聊聊，你们崇拜谁？受了谁的影响？从什么地方学到了怎样的人生观？遇到过怎样的事情？一个细胞怎样从整个社会获得了源源不断的营养？幼儿园，只是这个恶之花呈现的土壤而已，真正可怕的是背后的源源不断供给的肥料 ….. 总而言之，当我们真的认清了因果法则后，可以更好的去理解这个世界，更好地过好自己的人生。 此外，一直以来激励我的“不忘初心，方得始终”也是出自佛家思想。即：《大方广佛华严经》卷第十七：三世一切诸如来，靡不护念初发心。《大方广佛华严经》卷第十九：如菩萨初心，不与后心俱。从儒释道层面讲的意思：初心就是真如、元神、心、赤子之心；“不忘初心”就是不迷失和忘记初心，安住在真如本性中，才是圆满的境地。对我来说，我的初心是“自由”和“创造价值”。让自己强大到有不受外力束缚的自由选择的机会；为社会创造些价值，使即便百年后归于尘土，这世间仍能留下我曾经来过的足迹，有人记得我。 道：道法自然道家也是中国的主要思想流派之一。小时候，我对道家的最初印象来源于西游记里的“太上老君” ，但西游记里对太上老君着墨较少。之后，高中学了庄子的《逍遥游》，那种绝对自由、忘却物我的主题令我无比心动和向往，也由此对“道法自然”“无为”有了基本的认识。在我的理解中，“道法自然”阐述的是一种自然法则，即万事万物的运行变化都遵循自然规律，且每个事物都有其本身的天性和本质，每个人都有自己独特的思维方式和个性特征，我们应顺应本性、自然规律，不逆天道/自然而行。若反自然法则，则会受到相应的惩罚。这对约束我们的行为、自然地社会交往和自我成长有指导意义。 此外，领悟到道家“术变而道不变”的思想，这对我的成长（诸如如何去构建自己的能力体系）也有指导性作用。我对自己的未来职业规划是成为一名出色的产品经理，即产品的设计者（世界上最出色的产品经理应该是一手缔造了iPhone、Mac、iPad等神级产品的乔布斯）。现在的时代新技术日新月异，作为产品经理需要去不断学习以适应变化，但我们很容易会陷入过于追逐新技术热点、把“术”看得过于重要的误区里，忘记了本质是什么。新技术是“术”，而”术“终究只是更好实现目的的工具，而“道”却是长期稳定不变的，这里不变的“道”指的是产品经理的核心能力，即对市场、用户需求的洞察和把握。譬如用户“解放双手地获取知识”的需求是自古以来都有的，而实现这一需求的产品/方式是每个时代都在变化的，从古代的“听书”到上世纪的“广播”到移动互联网时代的“移动电台”到AI时代的智能音箱，未来必定还会有更多基于新技术的产品，但这些产品满足的本质需求是不变的——解放双手地获取知识。 古希腊：认识你自己“认识你自己”相传是刻在德尔斐的阿波罗]神庙的三句箴言之一，据说是出自苏格拉底，全句是“人呐，认识你自己！”。这句箴言陪伴我良久，我总会以它为镜来观照自我。每当我因取得些许成绩而生起自傲之心，这句话就会及时回响在我耳边，让我能够平静下来、回归淡然。而当我被人批得很惨，太过沮丧甚至有瞬间觉得自己一无是处时，这句话也曾多次拯救我。以前不成熟时，我常常很在意外界对自己的态度、看法，心情情绪因别人的评价、态度而起伏不定。但之后，当我能清晰认知自己后，便不曾再因外界态度评价而影响自己的情绪，虽还未达到“宠辱不惊”的地步，但也能做到淡然处之了。 “认识你自己”也意味着能够发现自己的缺点、愚昧。苏格拉底曾说“除了我的无知，我一无所知”。圣人尚且如此，而我们很多“凡人”却常因自己取得的一点点成绩、懂得的一点点知识而沾沾自喜，不可谓不愚昧。曾经我也是一个容易因成就而喜形于色的人，而现实却给我当头一击，我意识到“总有一天，会败在自己心里这股傲气上”，于是“低调再低调”成为我的行事准则之一。 心向静水流深静水流深，英文的表达是“Still waters run deep”，平静的水面下流淌着深不可测的海水。希望自己可以在未来的时间里，不断丰富自己的价值体系，同时不忘初心。但愿有一天能达到“静水流深”的境界，能洞察一切而不被矛盾束缚，不被欲望捆绑，拥有长久的快乐。也希望自己能在这个繁荣的时代创造些价值，留下独属的印记。]]></content>
      <categories>
        <category>Artificial Intelligence</category>
        <category>AI PM</category>
      </categories>
      <tags>
        <tag>AI PM</tag>
        <tag>Product Manager</tag>
        <tag>PM Ability</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈云计算和云服务]]></title>
    <url>%2F2018%2F03%2F12%2F%E6%B5%85%E8%B0%88%E4%BA%91%E8%AE%A1%E7%AE%97%E5%92%8C%E4%BA%91%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[Abstract：云计算是相对于本地计算来说的，可以解决复杂高性能计算与硬件性能之间的矛盾，使本地设备可通过接口接入数据中心使用云计算服务。云计算的核心是“共享”，其发展有助于优化社会上的计算资源配置。云的层次架构为：云设施—(up)—&gt;云平台—(up)—&gt;云服务。 “服务”是云计算的本质，云服务实际上是一种“共享”的信息技术资源。云计算的技术的发展促进云服务的发展，促进媒介变革。一方面是制作媒介变革，我们可通过接入第三方云服务（各种API/SDK，模块化云资源[计算、存储、验证、监控]）来快速实现产品开发和内容生产；另一方面是传输媒介变革，即云分发和云终端的发展对传统内容分发和传统终端的变革。 1.云计算的基本原理云 vs 本地 解决复杂计算与硬件性能的矛盾：需要强大的硬件性能支撑复杂计算，本地个人设备要求的便携性和性价比牺牲硬件性能；把计算放在云端，使得个人设备兼具便携性与高性能计算 优化计算资源配置：合理分配和共享计算资源和提高利用率 本质原因：资源的时空分配不均；硬件性能与设备便携性、性价比的矛盾 云计算 通过互联网来提供动态易扩展且经常是虚拟化的资源（信息技术资源），数据存在云端 云：网络、互联网（比喻说法） 本质：服务 核心：共享 “云”的层次架构：云设施—(up)—&gt;云平台—(up)—&gt;云服务 基于私有云或公有云，通过公开接口，为各种终端提供云服务 特征 以用户为中心：共享 智能的：数据挖掘和语义分析 可编程的：数据备份自动化 廉价性：低成本使用高性能计算资源 优点 计算能力 无限存储 数据安全 高可伸缩性：操作系统、个人设备无关性 群组协作 文档普适访问 按需服务 极其廉价 高可靠性，通用性 缺点 必须在线：持续互联网连接 低速连接效果差、慢 功能有限 数据不绝对安全 2.云计算的层次模型 虚拟化技术：云计算的重要技术（将硬件资源虚拟化成虚拟资源，如计算资源池、存储资源池） 云计算PM 分布式存储产品管理 企业级存储行业研究、市场调研、竞品分析、产品规划 文档撰写：技术白皮书，解决方案，营销 产品培训 3.云计算的应用模式——云服务3.1 云服务云服务：共享的信息技术资源 终端重要性变革：$手机、PC——&gt;云终端$ 特定设备是浮云，信息、数据是王道 3.2 云计算的服务类型 云制造，云视频，云教育… （本质：资源的时空分配不均；硬件性能与设备便携性、性价比的矛盾） 3.3 媒介变革1.SaaS（软件即服务）+ PaaS（平台即服务）：导致制作媒介变革 Abstract：我们可通过接入第三方云服务（各种API/SDK，模块化云资源[计算、存储、验证、监控]）来快速实现产品开发和内容生产 SaaS：基于web提供软件应用；提供直接的内容生产服务 eg.喜马拉雅开放内容接口，AI音箱可直接接入接口、使用喜马拉雅的内容为自己的用户提供内容服务 PaaS：开放、计算的平台 提供第三方技术接口服务，提供应用系统模块，使新产品可快速开发出来 将可标准化的应用堆栈层的功能抽象出来，封装成API/SDK，通过接口方式提供给外界，eg.人脸识别API； 新产品的大部分标准化模块的功能都可通过接入第三方服务实现，比如用户注册身份验证模块 提供云端大数据挖掘和分析服务（大数据处理平台），使新产品可实现个性化生产（通过数据分析了解用户兴趣需求，建立全新内容采编和个性化推荐模式） 制作媒介变革：基于媒介混合云资源平台，采用模块化云服务架构，业务模块按需组合，实现新产品的快速开发 e.g..CCTV的中央厨房内容生产和新媒体互动云 用户数据的构成： 网络行为数据 网站内行为数据 用户内容偏好数据 用户交易数据 制作媒介变革的影响 全渠道的内容汇聚 全云端核心业务生产 应用场景（以电视台为例）： 综艺节目制作外场(外场拍摄，实时生产发布)——>云平台(云剪辑，云存储，云发布，云运营)——>终端(微信，微博，网站，APP等) 2.IaaS（构架即服务）+ NaaS（网络即服务）：导致传输媒介变革（云终端，云分发） 提供内容分发网络（CDN）服务，能将源站内容分发至最接近用户的节点，使内容最快触达用户 提高用户响应速度和效率，增加流量]]></content>
      <categories>
        <category>Artificial Intelligence</category>
        <category>AI Application</category>
      </categories>
      <tags>
        <tag>Artificial Intelligence</tag>
        <tag>AI Application</tag>
        <tag>Distributed Computing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[总结|产品设计的四大原则]]></title>
    <url>%2F2018%2F03%2F12%2F%E6%80%BB%E7%BB%93-%E4%BA%A7%E5%93%81%E8%AE%BE%E8%AE%A1%E7%9A%84%E5%9B%9B%E5%A4%A7%E5%8E%9F%E5%88%99%2F</url>
    <content type="text"><![CDATA[Abstract：产品创新设计时有一些通用原则，要设计出有用有意义、健康安全的产品需要PM遵守这些原则。以用户为中心，站在用户角度去考虑用户利益与诉求，为社会谋福祉，注重用户体验的细节设计；保护个人隐私，在征求用户同意后再采集用户信息，给予用户拒绝的权力，并对用户提供一定的技术透明性以提高用户对产品的信任度；在确定产品理念和需求时，需要check法律和伦理道德底线；具备长远眼光，兼顾商业利益和社会利益，在二者出现激烈冲突时，以社会利益为先。 1.网络应用创新设计的四大原则1.以用户为中心 以人为本：一切为了提高人类的福祉，应当是为社会做贡献，而不是损害他人利益为己任 以用户为中心：以用户的诉求和利益为核心 当用户利益与公司商业利益产生冲突时，如何选择？ eg.谷歌从不出售搜索结果展示位 用户体验细节设计 充分考虑女性、儿童、残疾人、少数族群等易被忽视群体的利益,并对道德和法律的极端情况设置特别的判断规则 考虑使用群体文化规范的多样性 透明性：对使用者提供一定程度的技术透明性 2.保护个人隐私 隐私保护 提醒、征求用户同意：体验流程中, 如果涉及隐私,需要“明确提醒用户并征得用户同意”,以及“告知用户收集ⅩX隐私信息的期限和方式” 用户有权拒绝：用户有权(有操作入口)拒绝企业对其进行画像等自动化决策(即,不能先斩后奏,让用户先使用、然后再关闭相关功能) 承诺用户不泄露隐私 透明性：对使用者提供一定程度的技术透明性 应用设计者对用户隐私数据保护需要做什么？ 严格限定 采集什么：严格限定数据采集的范围 如何采集 被采集数据的保存周期多久 越界抓取用户隐私；若出问题则可能危及产品信誉 3.产品理念和需求讨论时，需要check法律和伦理道德底线 AI训练出的结果可能放大偏差或某种非中立特征，比如歧视；也可能被人为教坏 2016年3月23日,微软的人工智能聊天机器人Tay上线一天就被紧急下线,因为她被用户“教坏”了—她成为一个集反犹太人、性别歧视、种族歧视等于一身的“不良少女”。 法律、伦理、自由、隐私、尊严 4.兼顾社会利益和商业利益 可持续发展 下面是我归纳的网络产品冲击道德底线的一些情况。 2.冲击道德底线的情况将这些冲击道德底线的情况按是否是产品本身设计的问题进行二分类： 1.产品本身设计不合理/不正当导致的【5个】 虚假宣传：搜索引擎竞价排名，百度魏则西 灰色产业链：披着正经产品的皮干灰色色情交易，探探、陌陌； 用户隐私泄露：未征求用户同意就采集样本和使用 2011年, Facebook就曾因其”人脸识别和标记功能未按伊利诺伊州《生物信息隐私法案》(B|PA)要求告知用户收集面部识别信息的期限和方式“被诉,随后又因”釆集面部特征前未能明确提醒用户并征得用户同意〃而遭到爱尔兰和德国有关部门的调查。 谷歌街景地图 版权问题：书籍、文章、视频、音乐的版权侵犯；电子书网站，网盘（免费下载和阅读） 算法设计得不完善导致种族、性别、身份歧视：比如谷歌公司的图片软件曾错将黑人的照片标记为大猩猩（图像识别算法不够完善） ​ 2.非产品本身设计导致的 （被用户的不合理使用等原因）【5个】 网络暴力：人肉搜索，恶意引导舆论 直播不良现象：教唆粉丝，辱骂 污言秽语污染网络环境：社区，贴吧，微博，游戏 诚信问题：游戏外挂 AI被人为教坏 微软的人工智能聊天机器人Tay上线一天就被紧急下线,因为她被用户“教坏”了—她成为一个集反犹太人、性别歧视、种族歧视等于一身的“不良少女”。]]></content>
      <categories>
        <category>Product Manager</category>
        <category>Internet &amp; Product</category>
      </categories>
      <tags>
        <tag>Internet &amp; Product</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阅读笔记：《Python机器学习及实战》]]></title>
    <url>%2F2018%2F03%2F05%2F%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%EF%BC%9A%E3%80%8APython%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8F%8A%E5%AE%9E%E6%88%98%E3%80%8B%2F</url>
    <content type="text"><![CDATA[Abstract：这本书面向的是对机器学习和数据挖掘实践及竞赛感兴趣的读者，以python为基础从零开始，在不涉及数学模型和复杂编程知识的前提下，逐步学习和掌握机器学习、数据挖掘、自然语言处理工具，如scikit-learn、NLTK、Pandas、TensorFlow等。 全书分为4章： 简介篇：机器学习概念和python编程知识 基础篇：如何使用scikit-Learn作为基础机器学习工具 进阶篇：怎样借助高级技术或模型提高机器学习系统的性能 竞赛篇：如何完成kaggle竞赛 1.简介篇1.1 机器学习任务： 监督学习 （supervised Learning）：预测事物未知表现，包括分类问题(classification)、回归问题(regression)，根据目标预测变量类型的不同可分为： 分类问题：预测类别（已知数量，类别离散），如已知一个人的身高、体重和三围等数据预测其性别 回归问题：预测连续变量，如根据房屋的面积、地理位置、建筑年代等预测销售价格(C.V) 无监督学习（Unsupervised Learning）：倾向于分析事物本身特性，常用技术：降维(Dimensionality Reduction), 聚类 (Clustering) 数据降维：对事物的特性进行压缩和筛选，如对图像进行降维以保留最有区分度的像素组合 聚类：依赖于数据的相似性从而把相似的数据样本划分为一个簇（区别于分类，簇的数量和含义非已知），如电子商务中对用户信息和购买习惯进行聚类分析，一旦找到数量多且背景相似客户群，则可针对其投放广告促销 ​ ​ 经验： 特征（feature）：反映数据内在规律的信息 监督学习中的经验：特征、标记(label) 用一个特征向量描述一个数据样本 label的表现形式取决于监督学习的种类 数据标注需耗费大量资源，故数据量少 训练集（training set）：带label的数据集，用来训练学习系统 无监督学习中的经验：无label故无法做预测，但适合对数据结构作分析 原始数据转化为特征向量的过程中会遭遇多种数据类型（需全部转化为具体数值运算）： 类别型特征（categorical） 数值型特征（numerical） 缺失数据（missing value） 性能（performance）： 评价学习模型完成任务质量的指标 分类问题：准确性（accuracy）——预测正确类别的百分比 回归问题：衡量预测值与实际值之间的偏差大小 测试集（testing set）：与TS具备相同特征，没有被用于训练 how：用测试集测试预测的准确率（用具备相同特征的数据，模型在测试集上的预测结果与正确结果进行比对） 1.2 python编程库 python numpy：高级数学运算机制，高效向量与矩阵运算 scipy：在numpy基础上更强大、应用更广泛的科学计算包，依赖numpy Matplotlib：数据分析与可视化的绘图工具包 scikit-learn：封装了大量ML模型 pandas：数据处理和分析的工具包 1.3 python基础常用数据类型 数字 布尔值 字符串 元组（turple）：以()表征，元组数据不允许修改 列表：以[]表征，允许修改列表数据 字典（dict）：以key-value构成，以{}表征 数据运算 流程控制 函数设计 2.基础篇Abstract：机器学习模型的使用方法、性能评价和优缺点。 模型阐述角度：模型简介、数据描述 、编程实践、性能测评、特点分析。 2.1 监督学习经典模型2.1.1 监督学习任务的基本流程 准备训练数据——抽取所需特征、形成特征向量(Feature Vectors)——把特征向量连同labels一起送入学习算法(Machine Learning Algorithm)中——训练出一个预测模型(Predictive Model) 采用同样特征抽取方法作用于测试集、得到用于测试的特征向量——使用预测模型对待测试的特征向量进行预测并得到结果(Expected Label) ​ 2.1.2 分类学习分类问题： 二分类（binary classification）：二选一 多分类（multiclass classification）：判断一个样本是否同时属于多个不同类别 2.1.2.1 线性分类器（linear classifiers）线性分类器：假设特征与分裂结构存在线性关系的模型，通过累加计算每个维度的特征与各自权重的乘积来帮助类别决策（基于线性假设的分类器） logistics regression模型： x = ：n维特征向量w = : 特征向量对应的权重/系数(coefficient)b:截距(intercept),为避免过原点 线性关系表示为： f(w,x,b) = w^T x + b, f∈R我们所需处理的二分类问题中f∈(0,1)，故需将函数的f∈R映射到(0,1)，故有logistics函数： g(z) = \frac{1}{1+e^{-z}}, ,z∈R，g∈(0,1) 综上，若将z替换为f，则logistics regression模型如下： h_{w,b}(x) = g(f(w,x,b)) =\frac{1}{1+e^{-f}} = \frac{1}{1+e^{-(w^Tx+b)}}该模型处理一个待分类的特征向量：若z=0，则g=0.5；若z&lt;0,则g&lt;0.5,此FV被判别为一类，反之则为另一类。 当使用一组m个用于训练的FV $X=$ 和其对应的分类目标$y=$ ，我们希望该模型能在这组训练集上取得最大似然估计(Maximum Likelihood)的概率 $L(w,b)$, 或者说至少要在训练集上表现为： argmax_{w,b} L(w,b) = argmax \prod_{i=1} (h_{w,b}(i)^{y^i} (1-h_{w,b}(i)))^{1-y^i}为学习到决定模型的参数(parameters)，即系数w和截距b，我们普遍使用一种精确计算的解析算法和一种快速估计的随机梯度上升算法(stochasitic gradient ascend)。 1.任何模型在训练集上的表现都不一定能代表其最终在未知待测数据集上的性能，但至少要先保证模型可以被训练集优化。 2.SGA和SGD都属于梯度法迭代渐进估计参数的过程，梯度上升(SGA)用于目标最大化，梯度下降(SGD)用于目标最小化。 数据集说明： 数据集地址：Address 描述：良/恶性肿瘤预测数据 目的：分类预测，并使用精细的测评指标评价模型性能 1.数据预处理 123456789import pandas as pd import numpy as np # create feature listcolumn_names = ['Sample code number', 'Clump Thickness', 'Uniformity of Cell Size', 'Uniformity of Cell Shape', 'Marginal Adhesion','Single Epithelical Cell Size','Bare Nuclei','Bland Chromatin','Normal Nucleoli','Mitoses','Class']# read datadata = pd.read_csv('/Users/scarlett/repository/projects/breast_cancer/breast_cancer.csv',names=column_names) 12345# clean datadata = data.replace(to_replace='?', value=np.nan)data = data.dropna(how='any')print data.shapeprint data.info() output： 123456789101112131415161718(683, 11)&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;Int64Index: 683 entries, 0 to 698Data columns (total 11 columns):Sample code number 683 non-null int64Clump Thickness 683 non-null int64Uniformity of Cell Size 683 non-null int64Uniformity of Cell Shape 683 non-null int64Marginal Adhesion 683 non-null int64Single Epithelical Cell Size 683 non-null int64Bare Nuclei 683 non-null objectBland Chromatin 683 non-null int64Normal Nucleoli 683 non-null int64Mitoses 683 non-null int64Class 683 non-null int64dtypes: int64(10), object(1)memory usage: 64.0+ KBNone 由于原始数据没有提供对应的测试样本，故需要对带有label的样本进行分割，一般是25%作为测试集，75%作为训练集。 2.准备训练、测试数据 1234567891011# prepare training set and testing set# use train_test_split in sklearn to split datafrom sklearn.cross_validation import train_test_split# randomly sample 25% for testing,75% for trainingX_train,X_test,y_train,y_test = train_test_split(data[column_names[1:10]],data[column_names[10]],test_size=0.25,random_state=33)# check number and classprint y_train.value_counts()print y_test.value_counts() 1234562 3444 168Name: Class, dtype: int642 1004 71Name: Class, dtype: int64 训练样本：512条（344条良性肿瘤数据+168恶性肿瘤数据），测试样本171条（100+71） sklearn.model_selection.train_test_split解释 from sklearn.cross_validation import train_test_split 一般形式：X_train,X_test, y_train, y_test = cross_validation.train_test_split(train_data,train_target,test_size=0.4, random_state=0) 参数解释： train_data：所要划分的样本特征集 train_target：所要划分的样本结果 test_size：样本占比，如果是整数的话就是样本的数量 random_state：是随机数的种子 随机数种子：其实就是该组随机数的编号，在需要重复试验的时候，保证得到一组一样的随机数。比如你每次都填1，其他参数一样的情况下你得到的随机数组是一样的。但填0或不填，每次都会不一样。 随机数的产生取决于种子，随机数和种子之间的关系遵从以下两个规则： 种子不同，产生不同的随机数；种子相同，即使实例不同也产生相同的随机数。 3.使用线性分类模型进行分类预测 123456789101112131415161718192021222324from sklearn.preprocessing import StandardScalerfrom sklearn.linear_model import LogisticRegressionfrom sklearn.linear_model import SGDClassifier# standarlize data,make sure DE=1,EX=0,so that the outcome wont be influenced by big featuress = StandardScaler()X_train=ss.fit_transform(X_train)X_test=ss.transform(X_test)# init logisticregression and SGDClassifierlr = LogisticRegression()sgdc = SGDClassifier()# use fit() of LR to train paraslr.fit(X_train,y_train)# use trained lr to predict X_testlr_y_predict = lr.predict(X_test)# use fit() of SGDC to train paras, use trained lr to predict X_testsgdc.fit(X_train,y_train)sgdc_y_predict=sgdc.predict(X_test)print sgdc_y_predictprint lr_y_predict output： 12345678910[4 2 4 4 2 2 2 4 2 2 2 2 4 2 4 4 4 4 4 2 2 4 4 2 4 4 2 2 4 4 4 4 4 4 4 4 2 4 4 4 4 4 2 4 2 2 4 2 2 4 4 2 2 2 4 2 2 2 2 2 4 4 2 2 2 4 2 2 2 2 4 2 2 4 2 2 2 2 4 2 2 2 4 2 2 2 4 2 4 2 4 4 2 2 2 2 4 4 2 2 2 4 2 2 4 2 2 2 2 2 4 2 2 2 2 2 2 4 2 2 4 4 2 4 2 2 2 4 2 2 4 4 2 4 4 2 2 2 2 4 2 4 2 4 2 2 2 2 2 4 4 2 4 4 2 4 2 2 2 2 4 4 4 2 4 2 2 4 2 4 4][2 2 4 4 2 2 2 4 2 2 2 2 4 2 4 4 4 4 4 2 2 4 4 2 4 4 2 2 4 4 4 4 4 4 4 4 2 4 4 4 4 4 2 4 2 2 4 2 2 4 4 2 2 2 4 2 2 2 2 2 4 4 2 2 2 4 2 2 2 2 4 2 2 2 2 2 2 4 4 2 2 2 4 2 2 2 4 2 4 2 4 4 2 2 2 2 4 4 2 2 2 4 2 2 4 2 2 2 2 2 4 2 2 2 2 2 2 4 2 2 4 4 2 4 2 2 2 4 2 2 4 4 2 4 4 2 2 2 2 4 2 4 2 4 2 2 2 2 2 4 4 2 4 4 2 4 2 2 2 2 4 4 4 2 4 2 2 4 2 4 4] 混淆矩阵：二分类任务中，预测结果(predicted condition)与正确标记(true condition)之间存在4种不同的组合： 真阳性(true positive)：预测正确的恶性肿瘤 真阴性 假阳性(false positive)：误判为恶性肿瘤 假阴性 4.性能评价 性能评价指标 评价指标1：准确率 $ Accuracy = \frac{TP + TN}{TP+TN+FP+FN}$ 评价指标2：召回率(Recall)和精确率(Precision),F1 指标(F1 measure) Precision = \frac{TP}{TP+FP}Recall = \frac{TP}{TP+FN}F1 measure = \frac{2}{\frac{1}{Precision}+\frac{1}{Recall}}F1指标：两指标的调和平均数，以综合考量两指标 对肿瘤识别，我们更关心召回率，即应该被正确识别的恶性肿瘤的百分比。 使用线性分类模型进行肿瘤预测任务的性能分析 123456from sklearn.metrics import classification_report# 使用逻辑回归模型自带的评分函数score获得模型在测试集上的准确性结果print 'Acurracy of LR Classifier:', lr.score(X_test,y_test)# use classification_report to get the other 3 measures of LRprint classification_report(y_test,lr_y_predict,target_names=['Benign','Malignant']) output： 1234567Acurracy of LR Classifier: 0.9883040935672515 precision recall f1-score support Benign 0.99 0.99 0.99 100 Malignant 0.99 0.99 0.99 71avg / total 0.99 0.99 0.99 171 1234# 使用随机梯度下降模型自带的score评分函数模型在测试集上的准确性结果print 'Acurracy of SDG Classifier:', sgdc.score(X_test,y_test)# use classification_report to get the other 3 measures of SGDCprint classification_report(y_test,lr_y_predict,target_names=['Benign','Malignant']) output： 1234567Acurracy of SDG Classifier: 0.9824561403508771 precision recall f1-score support Benign 0.99 0.99 0.99 100 Malignant 0.99 0.99 0.99 71avg / total 0.99 0.99 0.99 171 综上，发现，LR比SGDC在测试集上有更高的准确性，因为sklearn中采用解析的方式精确计算LR的参数，而使用梯度法估计SGDC的参数 特点分析： LR model：精确解析参数，计算时间长但模型性能略高 SGDC model：随机梯度上升算法估计参数，计算时间短但模型性能略低 训练数据规模在10万量级以上的数据，考虑到时间耗用，推荐使用随机梯度算法对模型参数进行估计 2.1.2.2 支持向量机（Suport Vector Classifier）（分类）模型介绍： 1.不是在所有数据集上SVM的表现一定都优于普通线性模型或其他模型，而是假设未知待测数据也如训练数据一样分布，则SVM可帮助找到最佳分类器；实际应用数据总是有偏差的。 2.上图，H1表现不佳（有分类错误）；H2与H3都表现完美。 3.但，分类模型的选取中我们需要更加关注如何最大限度为未知分布的数据集提供足够的待预测空间。如有一个黑色样本稍偏离H2，则会很可能被误判为白色，造成误差，而H3则可为样本提供更多的容忍度，故H3优于H2. 数据描述： 应用场景：邮政系统对收信人邮编进行识别与分类，以便确定信件的投送地；邮编多数为手写 任务：手写数字图片识别与分类 数据集：利用SVM处理sklearn内部集成的手写体数字图片数据集 1.读取数据 1234567from sklearn.datasets import load_digits# 将数据存储在digits变量中digits=load_digits()# 检查数据规模与特征维度print digits.data.shape Output: 1(1797, 64) (1797,64):有1797条图像数据，每幅图片由8*8=64的像素矩阵表示 2.分割数据集 1234from sklearn.cross_validation import train_test_splitX_train,X_test,y_train,y_test=train_test_split(digits.data,digits.target,test_size=0.25,random_state=33)print y_train.shapeprint y_test.shape Output: 12(1347,)(450,) 3.使用SVM对手写数字图像进行识别 1234567891011121314151617from sklearn.preprocessing import StandardScaler# 从SVM里导入基于线性假设的SVM分类器LinearSVCfrom sklearn.svm import LinearSVC# 对训练和测试的特征数据进行标准化ss=StandardScaler()X_train=ss.fit_transform(X_train)X_test=ss.transform(X_test)# 初始化LinearSVClsvc=LinearSVC()# 进行模型训练lsvc.fit(X_train,y_train)# 利用训练好的模型对测试样本的数字类别进行预测，预测结果存在y_predict中y_predict=lsvc.predict(X_test)print y_predict Output: 12345678910111213[1 3 7 3 2 4 6 1 4 0 4 7 9 5 2 8 3 6 7 0 6 0 8 3 0 6 2 3 0 9 0 2 0 6 9 1 1 5 8 0 6 1 5 8 9 5 1 6 2 6 6 7 6 7 7 2 7 8 0 7 3 6 3 9 6 6 5 5 4 2 9 3 7 6 5 7 2 8 1 2 2 8 1 1 6 3 5 0 0 1 6 7 6 8 9 7 0 0 9 8 0 8 2 3 6 1 9 9 1 7 3 9 8 8 5 9 5 1 1 7 9 3 3 2 8 1 3 8 6 4 0 0 0 7 1 5 5 1 8 5 1 8 1 6 9 9 4 5 7 5 2 1 2 5 8 7 7 5 1 9 6 9 8 0 6 1 2 1 5 7 8 9 6 8 4 1 0 0 9 8 7 2 8 6 4 8 9 4 2 6 1 8 5 6 7 5 1 9 2 8 3 2 9 4 3 5 5 6 2 4 3 2 6 4 8 5 8 0 8 8 6 3 2 3 0 5 7 1 3 9 3 2 1 6 6 5 1 9 7 2 4 5 2 1 3 1 1 2 1 7 0 1 2 2 1 2 4 9 6 6 3 9 2 8 1 5 5 1 8 6 2 5 6 0 1 4 2 1 8 9 4 3 0 6 8 3 3 2 0 2 0 6 5 6 6 4 6 1 8 3 4 1 3 5 1 4 9 8 7 5 1 1 3 7 8 8 3 7 4 0 7 2 8 7 1 9 4 5 3 5 2 5 1 3 0 5 8 4 7 6 9 9 3 3 4 0 6 4 7 0 6 1 2 3 3 4 5 3 3 5 2 0 9 7 1 5 5 8 4 4 3 6 2 5 1 0 6 1 5 8 4 7 6 4 3 4 0 3 0 1 2 8 0 5 4 5 2 2 9 6 9 8 0 8 8 2 4 6 5 6 4 3 9 8 9 7 1 7 9 4 1 9 9 5 9 8 0 8 2 5 1 4 2 6 3 7 9 3 7 4 3 7 1 8 8 9 5 3 6 6] 4.性能测评 同样使用precision、recall、accuracy、F1这四个测度来评价性能 12# 使用模型自带的评估函数进行准确性测评print 'The Accuracy of Linear SVC is:',lsvc.score(X_test,y_test) Output: 1The Accuracy of Linear SVC is: 0.9533333333333334 123# 使用模型自带的classification_report模块对预测结果做更精细的分析from sklearn.metrics import classification_reportprint classification_report(y_test,y_predict,target_names=digits.target_names.astype(str)) Output: 1precision recall f1-score support 123456789101112 0 0.92 1.00 0.96 35 1 0.96 0.98 0.97 54 2 0.98 1.00 0.99 44 3 0.93 0.93 0.93 46 4 0.97 1.00 0.99 35 5 0.94 0.94 0.94 48 6 0.96 0.98 0.97 51 7 0.92 1.00 0.96 35 8 0.98 0.84 0.91 58 9 0.95 0.91 0.93 44avg / total 0.95 0.95 0.95 450 由上可知，SVM可提供较高的手写数字识别性能，平均各项指标都在95%左右。 多分类：判断一个样本是否同时属于多个不同类别；将多分类看成N个二分类任务。 如本例的分类目标有10个类别，即0—9这10个数字，因此无法直接计算三指标。故我们逐一评估每个类别的这三指标，把所有其他类别统一看做阴性(负)样本，则创造了10个二分类任务。 特点分析： 可帮助在海量甚至高维度数据中筛选对预测任务最有效的少数训练样本，节省数据内存，提高模型预测性能 但计算代价高（CPU资源与计算时间） 2.1.2.3 朴素贝叶斯（Naive Bayes）（分类）模型介绍：基于贝叶斯理论的分类器 会单独考量每一维度特征被分类的条件概率，进而综合这些概率并对其所在的特征向量做出分类预测 基本数学假设：各个维度上的特征被分类的条件概率之间是相互独立的 若采用概率模型来表述，则定义$x=$为某一n维特征向量，$y\in{c1,c_2,…,c_k}$ 为改特征向量x所有k种可能的类别，记$P(y=c_i|x) $ 为特征向量x属于类别$c_i$的概率，根据贝叶斯概率公式： P(y|x)=\frac{P(x|y)P(y)}{P(x)}我们的目标是寻找所有$y \in {c_1,c_2,…,c_k} $ 中$P(y|x)$ 最大的，即$argmaxP(y|x)$；并考虑到$P(x)$ 对于同一样本都是相同的，因此可忽略不计。故： argmaxP(y|x)=argmaxP(x|y)P(y)=argmaxP(x_1,x_2,..,x_n|y)P(y)若每一种特征可能的取值均为0或1，在无特殊假设的条件下，计算$P(x_1,x_2,..,x_n|y)$ 需要对$k*2^n$ 个可能的参数进行估计： P(x_1,x_2,..,x_n|y)=P(x_i|y)P(x_2|x1,y)P(x_3|x_1,x_2.y)...P(x_n|x_1,x_2,...,x_{n-1},y)但由于朴素贝叶斯模型的特征类别独立假设，故$P(xn|x_1,x_2,…,x{n-1},y)=P(x_n|y)$ ;若依然每种特征可能的取值只有2种，则只需要估计$2kn$个参数，即$P(x_1=0|y=c_1),P(x_1=1|y=c_1),…,P(x_n=1|y=c_1)$ . 为估计每个参数的概率，采用如下公式，且改用频率比近似计算概率： P(x_n=1|y=c_k)=\frac{P(x_n=1,y=c_k)}{P(y=c_k)}=\frac{\#x_n=1,y=c_k)}{\#(y=c_k)}数据描述 应用场景：互联网新闻文本分类 数据集：20类新闻文本 1.读取数据 12345678from sklearn.datasets import fetch_20newsgroups# 即时从网上下载数据news=fetch_20newsgroups(subset='all')print len(news.data)print news.data[0] output： 12345618846From: Mamatha Devineni Ratnam &lt;mr47+@andrew.cmu.edu&gt;Subject: Pens fans reactionsOrganization: Post Office, Carnegie Mellon, Pittsburgh, PALines: 12NNTP-Posting-Host: po4.andrew.cmu.edu 123456789I am sure some bashers of Pens fans are pretty confused about the lackof any kind of posts about the recent Pens massacre of the Devils. Actually,I am bit puzzled too and a bit relieved. However, I am going to put an endto non-PIttsburghers&apos; relief with a bit of praise for the Pens. Man, theyare killing those Devils worse than I thought. Jagr just showed you whyhe is much better than his regular season stats. He is also a lotfo fun to watch in the playoffs. Bowman should let JAgr have a lot offun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the finalregular season game. PENS RULE!!! ​ 数据没有被设定特征，也无数字化的量度，因此需要在被训练前对数据做进一步处理。 2.分割数据 12from sklearn.cross_validation import train_test_splitX_train,X_test,y_train,y_test=train_test_split(news.data,news.target,test_size=0.25,random_state=33) 3.使用朴素贝叶斯分类器对新闻文本数据进行类别预测 先将文本转化为特征向量 再利用朴素贝叶斯模型从训练数据中估计参数 最后利用这些概率参数对同样转化为特征向量的测试集进行类别预测 1234567891011121314# 导入用于文本特征向量转换模块from sklearn.feature_extraction.text import CountVectorizervec=CountVectorizer()X_train=vec.fit_transform(X_train)X_test=vec.transform(X_test)# 导入naive bayesfrom sklearn.naive_bayes import MultinomialNB# 初始化NBmnb=MultinomialNB()# 利用训练数据对模型参数进行估计mnb.fit(X_train,y_train)# 对测试样本进行类别预测，结果存储在变量y_predict中y_predict=mnb.predict(X_test) 4.性能评估 123from sklearn.metrics import classification_reportprint 'The Accuracy of NBC is:',mnb.score(X_test,y_test)print classification_report(y_test,y_predict,target_names=news.target_names) output： 12345678910111213141516171819202122232425The Accuracy of NBC is: 0.8397707979626485 precision recall f1-score support alt.atheism 0.86 0.86 0.86 201 comp.graphics 0.59 0.86 0.70 250 comp.os.ms-windows.misc 0.89 0.10 0.17 248comp.sys.ibm.pc.hardware 0.60 0.88 0.72 240 comp.sys.mac.hardware 0.93 0.78 0.85 242 comp.windows.x 0.82 0.84 0.83 263 misc.forsale 0.91 0.70 0.79 257 rec.autos 0.89 0.89 0.89 238 rec.motorcycles 0.98 0.92 0.95 276 rec.sport.baseball 0.98 0.91 0.95 251 rec.sport.hockey 0.93 0.99 0.96 233 sci.crypt 0.86 0.98 0.91 238 sci.electronics 0.85 0.88 0.86 249 sci.med 0.92 0.94 0.93 245 sci.space 0.89 0.96 0.92 221 soc.religion.christian 0.78 0.96 0.86 232 talk.politics.guns 0.88 0.96 0.92 251 talk.politics.mideast 0.90 0.98 0.94 231 talk.politics.misc 0.79 0.89 0.84 188 talk.religion.misc 0.93 0.44 0.60 158 avg / total 0.86 0.84 0.82 4712 由上评估结果可知，NBC对4712条新闻文本测试样本分类的准确性约为83.977%，平均精确率、召回率、F1指标分别为86%、84%、82%。 特点分析： 朴素贝叶斯模型广泛应用在互联网文本分类任务 优点：由于其较强的特征条件独立假设，使得模型预测所需估计的参数规模从幂指数量级向线性量级减少，极大节约内存消耗和计算时间 缺点：同样由于这种强假设的限制，模型训练时无法将各个特征之间的联系考量在内，使该模型在其他数据特征关联性强的分类任务上性能不佳 2.1.2.4 K近邻(k-Nearest Neighbor，KNN)（分类）模型介绍： 最简单的ML算法之一 假设有一些携带分类标记的训练样本，分布于特征空间中；蓝色、绿色样本点各自代表其类别；对一个待分类的红色测试样本点，未知其类别，按照“近朱者赤近墨者黑”的说法，我们需要寻找与这个待分类的样本在特征空间中距离最近的K个已标记样本作为参考，来帮助做出分类决策 思路:如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。 ​ ​ 数据描述： 应用场景：使用K近邻算法对生物物种进行分类 数据集：Iris 1.读取数据集 1234567from sklearn.datasets import load_irisiris=load_iris()print iris.data.shapeprint iris.DESCR output： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263(150, 4)Iris Plants Database====================Notes-----Data Set Characteristics: :Number of Instances: 150 (50 in each of three classes) :Number of Attributes: 4 numeric, predictive attributes and the class :Attribute Information: - sepal length in cm - sepal width in cm - petal length in cm - petal width in cm - class: - Iris-Setosa - Iris-Versicolour - Iris-Virginica :Summary Statistics: ============== ==== ==== ======= ===== ==================== Min Max Mean SD Class Correlation ============== ==== ==== ======= ===== ==================== sepal length: 4.3 7.9 5.84 0.83 0.7826 sepal width: 2.0 4.4 3.05 0.43 -0.4194 petal length: 1.0 6.9 3.76 1.76 0.9490 (high!) petal width: 0.1 2.5 1.20 0.76 0.9565 (high!) ============== ==== ==== ======= ===== ==================== :Missing Attribute Values: None :Class Distribution: 33.3% for each of 3 classes. :Creator: R.A. Fisher :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov) :Date: July, 1988This is a copy of UCI ML iris datasets.http://archive.ics.uci.edu/ml/datasets/IrisThe famous Iris database, first used by Sir R.A FisherThis is perhaps the best known database to be found in thepattern recognition literature. Fisher&apos;s paper is a classic in the field andis referenced frequently to this day. (See Duda &amp; Hart, for example.) Thedata set contains 3 classes of 50 instances each, where each class refers to atype of iris plant. One class is linearly separable from the other 2; thelatter are NOT linearly separable from each other.References---------- - Fisher,R.A. &quot;The use of multiple measurements in taxonomic problems&quot; Annual Eugenics, 7, Part II, 179-188 (1936); also in &quot;Contributions to Mathematical Statistics&quot; (John Wiley, NY, 1950). - Duda,R.O., &amp; Hart,P.E. (1973) Pattern Classification and Scene Analysis. (Q327.D83) John Wiley &amp; Sons. ISBN 0-471-22361-1. See page 218. - Dasarathy, B.V. (1980) &quot;Nosing Around the Neighborhood: A New System Structure and Classification Rule for Recognition in Partially Exposed Environments&quot;. IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. PAMI-2, No. 1, 67-71. - Gates, G.W. (1972) &quot;The Reduced Nearest Neighbor Rule&quot;. IEEE Transactions on Information Theory, May 1972, 431-433. - See also: 1988 MLC Proceedings, 54-64. Cheeseman et al&quot;s AUTOCLASS II conceptual clustering system finds 3 classes in the data. - Many, many more ... 2.分割数据集 12from sklearn.cross_validation import train_test_splitX_train,X_test,y_train,y_test=train_test_split(iris.data,iris.target,test_size=0.25,random_state=33) 3.使用K近邻算法对iris数据进行类别预测 123456789101112from sklearn.preprocessing import StandardScalerfrom sklearn.neighbors import KNeighborsClassifierss=StandardScaler()X_train=ss.fit_transform(X_train)X_test=ss.transform(X_test)knc=KNeighborsClassifier()knc.fit(X_train,y_train)y_predict=knc.predict(X_test)print y_predict output： 12[1 1 0 1 1 2 0 0 2 2 2 0 2 1 2 1 1 0 1 2 0 0 2 0 1 2 1 1 2 1 1 1 2 2 2 2 2 1] 4.性能评估 1234print 'The Accuracy of KNC is:',knc.score(X_test,y_test)from sklearn.metrics import classification_reportprint classification_report(y_test,y_predict,target_names=iris.target_names) output： 12345678The Accuracy of KNC is: 0.8947368421052632 precision recall f1-score support setosa 1.00 1.00 1.00 8 versicolor 0.73 1.00 0.85 11 virginica 1.00 0.79 0.88 19avg / total 0.92 0.89 0.90 38 特点分析： k近邻算法是非常直观简单的模型 是无参数模型：没有参数训练过程，即未通过任何学习算法分析数据，而只是根据测试样本在训练数据中的分布做出分类 缺点：高计算复杂度和内存消耗；平方级别的算法复杂度（每处理一个测试样本就要对所有训练样本进行遍历，逐一计算相似度、排序且选取K个最近邻训练样本的标记，进而做出分类决策） 当然也有KD-Tree这样的数据结构通过“空间换时间”思想节省KNN的决策时间 2.1.2.5 决策树（Decision Tree）（分类）模型介绍： 描述非线性关系 LR和SVM都要求被学习的数据特征和目标之间遵照线性假设，但现实场景下这种假设不存在。如用年龄预测流感死亡率，年龄与死亡率之间不存在线性关系。 决策树节点(node)——数据特征 各节点下的分支——特征值的分类 决策树的所有叶子节点——显示模型的决策结果 使用多种不同特征组合搭建多层决策树时，需考虑特征节点的选取顺序，常用的度量方式有信息熵(Information Gain)和基尼不纯性(Gini Impurity) ​ 数据描述 借助决策树模型预测泰坦尼克号乘客生还情况 数据集：乘客信息 Address 1.数据查验 1234import pandas as pdtitanic=pd.read_csv('/Users/scarlett/repository/projects/titanic/titanic.csv')print titanic.head() 1234567891011121314151617181920 row.names pclass survived \0 1 1st 1 1 2 1st 0 2 3 1st 0 3 4 1st 0 4 5 1st 1 name age embarked \0 Allen, Miss Elisabeth Walton 29.0000 Southampton 1 Allison, Miss Helen Loraine 2.0000 Southampton 2 Allison, Mr Hudson Joshua Creighton 30.0000 Southampton 3 Allison, Mrs Hudson J.C. (Bessie Waldo Daniels) 25.0000 Southampton 4 Allison, Master Hudson Trevor 0.9167 Southampton home.dest room ticket boat sex 0 St Louis, MO B-5 24160 L221 2 female 1 Montreal, PQ / Chesterville, ON C26 NaN NaN female 2 Montreal, PQ / Chesterville, ON C26 NaN (135) male 3 Montreal, PQ / Chesterville, ON C26 NaN NaN female 4 Montreal, PQ / Chesterville, ON C22 NaN 11 male 12print titanic.info()print titanic.shape 123456789101112131415161718&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;RangeIndex: 1313 entries, 0 to 1312Data columns (total 11 columns):row.names 1313 non-null int64pclass 1313 non-null objectsurvived 1313 non-null int64name 1313 non-null objectage 633 non-null float64embarked 821 non-null objecthome.dest 754 non-null objectroom 77 non-null objectticket 69 non-null objectboat 347 non-null objectsex 1313 non-null objectdtypes: float64(1), int64(2), object(8)memory usage: 112.9+ KBNone(1313, 11) 2.数据预处理 123456# 特征选择X=titanic[['pclass','sex','age']]y=titanic['survived']# 探查所选特征print X.info() 123456789&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;RangeIndex: 1313 entries, 0 to 1312Data columns (total 3 columns):pclass 1313 non-null objectsex 1313 non-null objectage 633 non-null float64dtypes: float64(1), object(2)memory usage: 30.8+ KBNone 数据处理任务： age只有714个，有缺失项，需要补全（使用平均数或中位数来补全，造成的影响最小） sex与pclass是int/object型，需要转换成数值特征，用0/1代替 12X['age'].fillna(X['age'].mean(),inplace=True)print X.info() 123456789&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;RangeIndex: 1313 entries, 0 to 1312Data columns (total 3 columns):pclass 1313 non-null objectsex 1313 non-null objectage 1313 non-null float64dtypes: float64(1), object(2)memory usage: 30.8+ KBNone 由此可知Age特征得到了补充。 4.数据分割 12from sklearn.cross_validation import train_test_splitX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=33) 5.特征转换 123456789# 使用特征转换器from sklearn.feature_extraction import DictVectorizervec=DictVectorizer(sparse=False)# 转换特征后发现：凡是类别型的特征都单独被剥离出来独立成一列特征，数值型则保持不变X_train=vec.fit_transform(X_train.to_dict(orient='record'))X_test=vec.transform(X_test.to_dict(orient='record')) 6.训练模型 1234from sklearn.tree import DecisionTreeClassifierdtc=DecisionTreeClassifier()dtc.fit(X_train,y_train)y_predict=dtc.predict(X_test) 7.性能评估 12345from sklearn.metrics import classification_reportprint dtc.score(X_test,y_test)print classification_report(y_predict,y_test,target_names=['died','survived']) 12345670.7811550151975684 precision recall f1-score support died 0.91 0.78 0.84 236 survived 0.58 0.80 0.67 93avg / total 0.81 0.78 0.79 329 2.1.2.6 集成模型（Ensemble）（分类）模型介绍 综合考量多个分类器的预测结果，从而做出分类决策，综合考量方式有2种： 1.利用相同的训练数据同时搭建多个独立的分类模型，然后通过投票以少数服从多数的原则做出最终分类决策，如： 随机森林分类器(Random Forest Classifier)：在相同训练数据上同时搭建多棵决策树（每棵树都随机选取特征） 2.按照一定词序搭建多个分类模型，模型间彼此存在依赖关系（每个后续模型的加入都需要对现有集成模型的综合性能有所贡献，进而不断提升更新过后的集成模型的性能，并最终期望借助整合多个分类能力较弱的分类器，搭建出具有更强分类能力的模型），如: 梯度提升决策树(Gradient Tree Boosting)：每棵树在生成过程中都会尽可能降低整体集成模型在训练集上的拟合误差 数据描述 对比单一决策树、随机森林、梯度提升决策树三者性能差异 数据集：Titanic乘客数据 1.集成模型对titanic乘客是否生还的预测 123456789101112131415161718import pandas as pdtitanic=pd.read_csv('/Users/scarlett/repository/projects/titanic/titanic.csv')X=titanic[['pclass','age','sex']]y=titanic['survived']X['age'].fillna(X['age'].mean(),inplace=True)from sklearn.cross_validation import train_test_splitX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=33)from sklearn.feature_extraction import DictVectorizervec=DictVectorizer(sparse=False)X_train=vec.fit_transform(X_train.to_dict(orient='record'))X_test=vec.transform(X_test.to_dict(orient='record')) 1234567# 使用单一决策树训练模型from sklearn.tree import DecisionTreeClassifierdtc=DecisionTreeClassifier()dtc.fit(X_train,y_train)dtc_y_pred=dtc.predict(X_test)print dtc_y_pred 123456789[0 1 0 0 0 1 1 0 0 1 0 1 1 0 0 1 0 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 1 1 0 0 0 1 1 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0] 1234567# 使用随机森林分类器进行集成模型训练from sklearn.ensemble import RandomForestClassifierrfc=RandomForestClassifier()rfc.fit(X_train,y_train)rfc_y_pred=rfc.predict(X_test)print rfc_y_pred 123456789[0 1 0 0 0 1 1 0 0 1 0 1 1 0 0 1 0 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 1 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 1 1 0 0 0 1 1 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0] 1234567# 使用梯度提升模型训练集成模型from sklearn.ensemble import GradientBoostingClassifiergbc=GradientBoostingClassifier()gbc.fit(X_train,y_train)gbc_y_pred=gbc.predict(X_test)print gbc_y_pred 123456789[0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0] 2.性能测评 12345678910111213from sklearn.metrics import classification_report# 输出单一决策树的评估指标print 'The accuracy of DTC is',dtc.score(X_test,y_test)print classification_report(dtc_y_pred,y_test)# 输出随机森林的评估指标print 'The accuracy of RFC is',rfc.score(X_test,y_test)print classification_report(rfc_y_pred,y_test)# 输出梯度提升决策树的评估指标print 'The accuracy of GBC is',gbc.score(X_test,y_test)print classification_report(gbc_y_pred,y_test) 1234567891011121314151617181920212223The accuracy of DTC is 0.7811550151975684 precision recall f1-score support 0 0.91 0.78 0.84 236 1 0.58 0.80 0.67 93avg / total 0.81 0.78 0.79 329The accuracy of RFC is 0.7781155015197568 precision recall f1-score support 0 0.90 0.78 0.83 233 1 0.59 0.78 0.67 96avg / total 0.81 0.78 0.79 329The accuracy of GBC is 0.790273556231003 precision recall f1-score support 0 0.92 0.78 0.84 239 1 0.58 0.82 0.68 90avg / total 0.83 0.79 0.80 329 上面的输出表明：在相同的训练和测试数据条件下，仅使用模型默认配置，梯度上升决策树具有最佳预测性能。一般工业界为追求更强的预测性能，会把随机森林作为基线系统(Baseline System) 集成模型： 最常见的应用；可整合多种模型 缺点：模型估计参数的过程受概率影响，具有不确定性 优点：虽然模型训练需要耗费更多时间，但得到的综合模型会具有更高的性能和稳定性 2.1.3 回归预测回归 vs 分类：区别在于其待预测目标是连续变量 2.1.3.1 线性回归器模型介绍 最小二乘法：数学优化方法；通过最小化误差的平方和寻找数据的最佳函数匹配。利用最小二乘法可以简便地求得未知的数据，并使得这些求得的数据与实际数据之间误差的平方和为最小。 线性回归问题中：优化目标即最小化预测结果与真实值之间的差异（因为预测目标直接是实数域上的数值） 当使用一组m个用于训练的特征向量$X=$ 和其对应的回归目标$y=$ 时，我们希望线性回归模型可以最小二乘(Generalized Least Squares)预测的损失$L(w,b)$ ，则线性回归器的常见优化目标为：* argminL(w,b)=argmin\sum_{m}^{k=1}\qquad(f(w,x,b)-y^k)^2同样为学习到决定模型的参数$w，b$ ，仍可使用一种精确计算的解析算法和一种快速的随机梯度下降(Stochastic Gradient Descend)估计算法. 数据描述 美国波士顿地区房价预测 性能评估指标： 假设测试数据有m个目标数值$y= $ 且记$\overline{y}$ 为回归模型的预测结果，则： MAE： $SS{abs}=\sum{m}^{i=1}\qquad|y^i-\overline{y}|$ ,$ $MAE=\frac{SS_{abs}}{m}$ MSE： $SS{tot}=\sum{m}^{i=1}\qquad(y^i-\overline{y})^2$ Missing close brace MSE=\frac{SS_{tot}{m} R-squared： $SS{res}=\sum{m}^{i=1}\qquad(y^i-(f(x^i))^2$ Missing close braceR^2=1-\frac{SS_{res}{tot} 其中，$SS{tot}$ 代表测试数据真实值的方差(内部差异)；$SS{res}$ 代表回归值与真实值之阿金的平方差异（回归差异） R-squared（拟合度）：比较预测结果与真实值的吻合程度，既考量了回归值与真实值的差异，又兼顾了问题本身真实值的变动；而MAE、MSE(差值的绝对值或平方)则会随不同预测问题而变化巨大，欠缺在不同问题中的可比性 1.导入和查验数据 1234from sklearn.datasets import load_bostonboston=load_boston()# 输出数据描述print boston.DESCR 1234567891011121314151617181920212223242526272829303132333435 Boston House Prices dataset===========================Notes------Data Set Characteristics: :Number of Instances: 506 :Number of Attributes: 13 numeric/categorical predictive :Median Value (attribute 14) is usually the target :Attribute Information (in order): - CRIM per capita crime rate by town - ZN proportion of residential land zoned for lots over 25,000 sq.ft. - INDUS proportion of non-retail business acres per town - CHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) - NOX nitric oxides concentration (parts per 10 million) - RM average number of rooms per dwelling - AGE proportion of owner-occupied units built prior to 1940 - DIS weighted distances to five Boston employment centres - RAD index of accessibility to radial highways - TAX full-value property-tax rate per $10,000 - PTRATIO pupil-teacher ratio by town - B 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town - LSTAT % lower status of the population - MEDV Median value of owner-occupied homes in $1000&apos;s :Missing Attribute Values: None :Creator: Harrison, D. and Rubinfeld, D.L.This is a copy of UCI ML housing dataset.http://archive.ics.uci.edu/ml/datasets/Housing 1This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University. 1234567891011121314The Boston house-price data of Harrison, D. and Rubinfeld, D.L. &apos;Hedonicprices and the demand for clean air&apos;, J. Environ. Economics &amp; Management,vol.5, 81-102, 1978. Used in Belsley, Kuh &amp; Welsch, &apos;Regression diagnostics...&apos;, Wiley, 1980. N.B. Various transformations are used in the table onpages 244-261 of the latter.The Boston house-price data has been used in many machine learning papers that address regressionproblems. **References** - Belsley, Kuh &amp; Welsch, &apos;Regression diagnostics: Identifying Influential Data and Sources of Collinearity&apos;, Wiley, 1980. 244-261. - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann. - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing) 2.数据分割 123456789from sklearn.cross_validation import train_test_splitimport numpy as npX=boston.datay=boston.targetX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=33)# 分析回归目标值的差异print np.max(y),np.min(y),np.mean(y) 150.0 5.0 22.532806324110677 由上发现目标房价之间的差异较大，故需要对特征和目标值进行标准化处理。 1234567891011from sklearn.preprocessing import StandardScaler# 分别初始化feature和target的标准化器ss_X=StandardScaler()ss_y=StandardScaler()# 分别对训练和测试数据的feature和target进行标准化处理X_train=ss_X.fit_transform(X_train)X_test=ss_X.transform(X_test)y_train=ss_y.fit_transform(y_train)y_test=ss_y.transform(y_test) ​ 3.使用线性回归模型和SGDRegressor分别对房价进行预测 123456789from sklearn.linear_model import LinearRegressionlr=LinearRegression()lr.fit(X_train,y_train)lr_y_predict=lr.predict(X_test)from sklearn.linear_model import SGDRegressorsgdr=SGDRegressor()sgdr.fit(X_train,y_train)sgdr_y_predict=sgdr.predict(X_test) 4.性能测评 测量目的：衡量预测值与真实值之间的差距 测评指标： 平均绝对误差（Mean Absolute Error,MAE) 均方误差(Mean Squared Error,MSE) 拟合度(R-squared, R平方)：拟合度检验是对已制作好的预测模型进行检验，比较它们的预测结果与实际发生情况的吻合程度 使用三种回归评价机制和两种调用R-squared评价模块的方法，评价此模型的回归性能 1234567891011121314151617181920212223242526# 使用LR模型自带的评估模块print 'The value of default measurement of LR is',lr.score(X_test,y_test)# 导入MAE和MSE评估回归模型from sklearn.metrics import r2_score,mean_squared_error,mean_absolute_error# 使用r2_score模块print 'The value of R-squared of LR is',r2_score(y_test,lr_y_predict)# 使用mean_squared_error模块print 'The MSE of LR is',mean_squared_error(ss_y.inverse_transform(y_test),ss_y.inverse_transform(lr_y_predict))# 使用mean_absolute_error模块print 'The MAE of LR is',mean_absolute_error(ss_y.inverse_transform(y_test),ss_y.inverse_transform(lr_y_predict))# 使用SGDR自带评估模块print 'The value of default measurement of SGDR is',sgdr.score(X_test,y_test)# 使用r2_score模块print 'The value of R-squared of SGDR is',r2_score(y_test,sgdr_y_predict)# 使用mean_squared_error模块print 'The MSE of SGDR is',mean_squared_error(ss_y.inverse_transform(y_test),ss_y.inverse_transform(sgdr_y_predict))# 使用mean_absolute_error模块print 'The MAE of SGDR is',mean_absolute_error(ss_y.inverse_transform(y_test),ss_y.inverse_transform(sgdr_y_predict)) 特点分析： 数据规模超10万，使用随机梯度法估计参数 在不清楚特征之间关系的前提下，可使用线性回归模型作为基线系统（baseline system） 2.1.3.2 支持向量机(回归)模型介绍 同样是从训练数据中选取一部分更加有效的支持向量，只是这少部分训练样本所提供的并不是类别目标，而是具体的预测数值 继续使用2.1.3.1中的训练集和测试集进行不同核函数配置的SVM回归模型训练，且分别对测试数据做出越策，会发现： 不同配置下的模型在相同测试集上存在非常大的性能差异，且使用径向基(Radical basis function)核函数对特征进行非线性映射后，SVM展现最佳回归性能 可以多尝试几种配置，以活动最佳预测性能 核函数：一种特征映射技巧，即通过某种函数计算，将原有的线性不可分的低维特征映射到更高维度的空间，从而尽可能达到新的高维度特征线性可分的程度。 2.1.2.3 K近邻(回归)模型介绍 在回归任务中，K近邻(回归)模型同样只是借助周围K个距离最近的训练样本的目标数值，对待测样本的回归值进行决策。 1.使用2种不同配置的K近邻回归模型对美国波士顿放假数据进行回归预测 1234567891011121314151617181920from sklearn.datasets import load_bostonboston=load_boston()from sklearn.cross_validation import train_test_splitimport numpy as npX=boston.datay=boston.targetX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=33)from sklearn.neighbors import KNeighborsRegressor# 初始化回归器，调整配置，使预测方式为平均回归，weights='uniform'uni_knr=KNeighborsRegressor(weights='uniform')uni_knr.fit(X_train,y_train)uni_knr_y_predict=uni_knr.predict(X_test)# 初始化回归器，调整配置，使预测方式为根据距离加权回归，weights='distance'dis_knr=KNeighborsRegressor(weights='distance')dis_knr.fit(X_train,y_train)dis_knr_y_predict=dis_knr.predict(X_test) 2.性能测评 12345# 使用R-squared、MSE、MAE三指标分别对两种不同配置的模型进行性能评估from sklearn.metrics import r2_score,mean_squared_error,mean_absolute_errorprint 'Uniform',uni_knr.score(X_test,y_test),mean_squared_error(y_test,uni_knr_y_predict),mean_absolute_error(y_test,uni_knr_y_predict)print 'Distance',dis_knr.score(X_test,y_test),mean_squared_error(y_test,dis_knr_y_predict),mean_absolute_error(y_test,dis_knr_y_predict) 12Uniform 0.6418225886716102 27.773540157480316 3.7645669291338586Distance 0.6565370125979323 26.63256467749057 3.6251742046017417 由上可知，K近邻加权平均的回归策略具有更好的预测性能。 2.1.3.4 回归树模型介绍 在选择不同特征作为分裂节点的策略上，与决策树类似 不同：回归树叶节点的数据类型为连续型非离散型；决策树每个叶子节点依照训练数据表现的概率倾向决定其最终的预测类别，而回归树叶子节点是一个个具体数值，从预测值连续的意义上严格讲，回归树不能称为回归算法（因为回归树叶子节点返回的是“一团”训练数值的均值，而非具体连续的预测值） 1.使用回归树对波士顿房价训练数据进行学习，并对测试数据预测 12345678910111213141516from sklearn.datasets import load_bostonboston=load_boston()from sklearn.cross_validation import train_test_splitimport numpy as npX=boston.datay=boston.targetX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=33)# 训练模型from sklearn.tree import DecisionTreeRegressordtr=DecisionTreeRegressor()dtr.fit(X_train,y_train)dtr_y_predict=dtr.predict(X_test) 2.性能测评 123from sklearn.metrics import r2_score,mean_squared_error,mean_absolute_errorprint dtr.score(X_test,y_test),mean_squared_error(y_test,dtr_y_predict),mean_absolute_error(y_test,dtr_y_predict) 10.5223911380673973 37.034409448818906 3.4700787401574806 树模型（回归树，决策树） 优点：可解决非线性拟合问题；不要求对特征标准化和统一量化（即数值型、类别型特征都可直接被训练）；可直观输出决策过程，使决策结果具有可解释性 缺点：容易因为模型搭建得过于复杂而丧失对新数据的精确预测能力（泛化能力）；树模型从上至下的预测流程会因为数据细微的更改而发生较大的结构变化，故预测稳定性较差；在有限时间内无法找到最优解（而只是次优解） 2.1.3.5 集成模型（回归）补充：极端随机森林(Extremely Randomized Trees) 每构建一棵树的分裂节点时，不会任意选取特征，而是先随机选取一部分特征，然后利用信息熵(Information Gain)和基尼不纯性(Gini Impurity)等指标挑选出最佳节点特征 1.使用三种集成回归模型对波士顿房间训练数据进行学习，并对测试数据进行预测 1234567891011121314151617181920212223from sklearn.datasets import load_bostonboston=load_boston()from sklearn.cross_validation import train_test_splitimport numpy as npX=boston.datay=boston.targetX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=33)from sklearn.ensemble import RandomForestRegressor,ExtraTreesRegressor,GradientBoostingRegressorrfr=RandomForestRegressor()rfr=rfr.fit(X_train,y_train)rfr_y_predict=rfr.predict(X_test)etr=ExtraTreesRegressor()etr=etr.fit(X_train,y_train)etr_y_predict=etr.predict(X_test)gbr=GradientBoostingRegressor()gbr=gbr.fit(X_train,y_train)gbr_y_predict=gbr.predict(X_test) 2.性能评估 12345from sklearn.metrics import r2_score,mean_squared_error,mean_absolute_errorprint 'RFR',rfr.score(X_test,y_test),mean_squared_error(y_test,rfr_y_predict),mean_absolute_error(y_test,rfr_y_predict)print 'ETR',etr.score(X_test,y_test),mean_squared_error(y_test,etr_y_predict),mean_absolute_error(y_test,etr_y_predict)print 'GBR',gbr.score(X_test,y_test),mean_squared_error(y_test,gbr_y_predict),mean_absolute_error(y_test,gbr_y_predict) 123RFR 0.8515156020299433 11.513672440944882 2.257322834645669ETR 0.8003897320777417 15.478038582677165 2.416299212598424GBR 0.8430286082992219 12.171764921769585 2.277247326989519 2.2 无监督学习经典模型无监督学习（Unsupervised Learning） 着重发现数据本身的特点 无需标记数据 2.2.1 数据聚类数据聚类： 无监督学习的主流应用之一 2.2.1.1 K-means算法模型介绍 最经典易用的聚类模型；要求预先设定聚类个数，然后不断更新聚类中心，经过几轮迭代，最后的目标是让所有数据点到其所属聚类中心距离的平方和趋于稳定 算法执行的过程分4个阶段： 1.随机布设K个特征空间内的点作为初始的聚类中心； 2.根据每个数据的特征向量，从K个聚类中心中寻找距离最近的一个，并且把该数据标记为从属于这个聚类中心； 3.在所有数据都被标记过聚类中心之后，根据这些数据新分配的类簇，重新对K个聚类中心做计算； 4.若一轮下来，所有数据点从属的聚类中心与上一次分配的类簇没有变化，则迭代可停止，否则回到步骤2继续循环 数据描述 手写体数字图像识别数据集 Address 聚类算法的性能评估指标： 1.若被评估数据已被标注正确的类别，则使用2个指标： ARI指标（Adjusted Rand Index） Accuracy（准确性，同分类问题） 2.若被评估数据无所属类别，则使用轮廓系数（Silhouette Coefficient）来度量聚类结果的质量，说明： 轮廓系数兼顾聚类的凝聚度（Cohesion）和分离度（Separation） 取值范围：[-1,1]，轮廓系数值越大，则聚类效果越好 具体计算步骤： 1.对已聚类数据中第$i$个样本$x^i$ ,计算$x^i$与其同一个类簇内的所有其他样本距离的平均值，记作$a^i$ ,用于量化簇内的凝聚度； 2.选取$x^i$ 外的一个簇$b$，计算$x^i$与簇$b$中所有样本的平均距离，遍历所有其他簇，找到最近的这个平均距离，记作$b^i$ ，用于量化簇之间分离度； 3.对于样本$x^i$ ，轮廓系数为$sc^i=\frac{b^i-a^i}{max(b^i,a^i)}$ ; 4.最后对所有样本$X$求出平均值，即为当前聚类结果的整体轮廓系数 衡量效果： 若$sc^i &lt; 0$ ,则说明$x^i$ 与其簇内元素的平均距离大于最近的其他簇，表示聚类效果不好； 若$a^i$ 趋于0，或$b^i$ 足够大，则$sc^i $ 趋于1 ,表示聚类效果好； 1.导入和查验数据 123456789import pandas as pddigits_train=pd.read_csv('/Users/scarlett/repository/projects/digits/optdigits.tra',header=None)digits_test=pd.read_csv('/Users/scarlett/repository/projects/digits/optdigits.tes',header=None)# header=None 表示第一行是数据而非文件第一行print digits_train.shape,digits_test.shapeprint digits_train.head()print digits_train.info() 2.使用K-means算法识别手写体图像数据 图像数据由8*8像素矩阵表示，64个像素维度；1个目标维度用来标记每个图像样本代表的数字类别 12345678910111213141516import numpy as npimport matplotlib.pyplot as plt# 从训练集合测试集上都分离出64维度的像素特征和1维度的数字目标X_train=digits_train[np.arange(64)]y_train=digits_train[64]X_test=digits_test[np.arange(64)]y_test=digits_test[64]from sklearn.cluster import KMeans# 初始化模型，并设置聚类中心数量为10kmeans=KMeans(n_clusters=10)kmeans.fit(X_train)# 逐条判断每个测试图像所属的聚类中心y_pred=kmeans.predict(X_test) 3.性能测评 123# 使用ARI进行K-means聚类性能评估from sklearn import metricsprint metrics.adjusted_rand_score(y_test,y_pred) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# 使用轮廓系数评估不同类簇数量的from sklearn.cluster import KMeans# 导入计算轮廓系数的模块from sklearn.metrics import silhouette_score# 分割出3*2=6个子图，并在1号子图作图；subplot(m,n,p)是将多个图画到一个平面上的工具,m行n列，p=1代表从左到右从上到下的第一个位置plt.subplot(3,2,1)# 初始化原始数据点x1=np.array([1,2,3,1,5,6,5,5,6,7,8,9,7,9])x2=np.array([1,3,2,2,8,6,7,6,7,1,2,1,1,3])X=np.array(zip(x1,x2)).reshape(len(x1),2)# 在1号子图做出原始数据点阵的分布plt.xlim([0,10])plt.ylim([0,10])plt.title('Instance')plt.scatter(x1,x2)colors=['b','g','r','c','m','y','k','b']markers=['o','s','D','v','^','p','*','+']clusters=[2,3,4,5,8]subplot_counter=1sc_scores=[]for t in clusters: subplot_counter += 1 plt.subplot(3,2,subplot_counter) kmeans_model=KMeans(n_clusters=t).fit(X) for i,l in enumerate(kmeans_model.labels_): plt.plot(x1[i],x2[i],color=colors[l],marker=markers[l],ls='None') plt.xlim([0,10])plt.ylim([0,10])sc_score=silhouette_score(X,kmeans_model.labels_,metric='euclidean')sc_scores.append(sc_score)plt.title('K=%s,silhouette coefficient=%0.03f' %(t,sc_score))plt.figure()plt.plot(clusters,sc_scores,'*-')plt.xlabel('Number of Clusters')plt.ylabel('silhouette coefficient score')plt.show() 由图可知，当聚类中心数量k=3时，轮廓系数最大；由轮廓系数与不同类簇数量的关系曲线可知，聚类中心数量为3也符合数据分布特点。 特点分析： K-means聚类模型采取的是迭代式算法 缺点：容易收敛到局部最优解；需要预先设定簇的数量 局部最优解： 最优化：在复杂环境中遇到的许多可能的决策中，挑选“最好”的决策 局部最优：指对于一个问题的解在一定范围或区域内最优，或者说解决问题或达成目标的手段在一定范围或限制内最优（和全局最优不同，局部最优不要求在所有决策中是最好的） 全局最优：针对一定条件/环境下的一个问题/目标，若一项决策和所有解决该问题的决策相比是最优的，就可以被称为全局最优 如下图：左边是实际数据和正确的所属类簇；右下的局部最优情况导致无法继续更新聚类中心，使聚类结果与正确结果相差很大 “容易收敛到局部最优解”是算法自身的缺陷，但可通过执行多次kmeans算法来挑选性能最好的初始中心点 肘部观察法： 作用：粗略估计相对合理的类簇个数 思路：因为K-means模型最终期望所有数据点到其所属的类簇举例的平方和趋于稳定，所以我们可以通过观察这个数值随K的走势来找出最佳的类簇数量；理想条件下，这个折线在不断下降且趋于平缓的过程中会有斜率的拐点，即从这个拐点对应的K值开始，类簇中心的增加不会过于破坏数据聚类的结构（进一步增加K值不会再有利于算法的收敛），则此拐点K=n是相对最佳的类簇数量。 肘部观察法示例： 1234567891011121314151617181920212223242526272829303132import numpy as npfrom sklearn.cluster import KMeansfrom scipy.spatial.distance import cdistimport matplotlib.pyplot as plt# 使用均匀分布函数随机三个簇，每个簇周围10个数据样本cluster1=np.random.uniform(0.5,1.5,(2,10))cluster2=np.random.uniform(5.5,6.5,(2,10))cluster3=np.random.uniform(10.5,11.5,(2,10))# 绘制30个数据样本的分布图像X=np.hstack((cluster1,cluster2,cluster3)).Tplt.scatter(X[:,0],X[:,1])plt.xlabel('x1')plt.ylabel('x2')plt.show()# 测试9种不同聚类中心数量下，每种情况的聚类质量K=range(1,10)meandistortions=[]for k in K: kmeans=KMeans(n_clusters=k) kmeans.fit() meandistortions.append(sum(np.min(cdist(X,kmeans.cluster_centers_,'euclidean'),axis=1))/X.shape[0]) plt.plot(K,meandistortions,'bx-')plt.xlabel('k')plt.ylabel('Average Dispersion')plt.title('Selecting k with the Elbow Method')plt.show() 2.2.2 特征降维特征降维 特征维度过高，无法构建有效特征；无法肉眼观测超过三个维度的特征 重构有效的低维特征向量，为数据拓展提供可能 ​ 2.2.2.1 主成分分析(Principle Component Analysis)模型介绍 最经典使用的特征降维技术；辅助图像识别 举例：若我们有一组2*2的数据[(1,2),(2,4)]，假设这两个数据都反映到一个类别或类簇；若我们的学习模型是线性模型，则这两个模型只能帮助权重参数更新1次，因为他们线性相关，所有特征值只是扩张了相同背书；若使用PCA分析，则此矩阵的“秩”=1，即在多样性程度上，此矩阵只有1个自由度。 可把PCA当做特征选择，这种特征选择是先把原来的特征空间作了映射，使得新的映射后特征空间数据彼此正交；则我们通过主成分分析就尽可能保留下具备区分性的低维数据特征。 矩阵的秩：一个矩阵A的列秩是A的线性独立的纵列的极大数目，通常表示为r(A)或rank A。 自由度：统计学上，指当以样本的统计量来估计总体的参数时，样本中独立或能自由变化的数据的个数；数学上，自由度是一个随机向量的维度数，即一个向量能被完整描述所需的最少单位向量数。如从电脑屏幕到厨房的位移能够用三维向量$\widehat{ai}+\widehat{bj}+\widehat{ck}$来描述，因此这个位移向量的自由度是3。自由度也通常与这些向量的座标平方和，以及卡方分布中的参数有所关联。 求线性相关矩阵的秩： 123import numpy as np test = np.array([[1,2],[2,4]])print np.linalg.matrix_rank(test,tol=None) 11 数据描述 数据集：digits 展示经PCA处理后，这些数字图像映射在二维空间的分布情况；结果会发现把64维度的图像压缩到2维空间后，依然可发现绝大多数数字之间的区分性 1.显示手写体数字图片经PCA压缩后的二维空间分布 12345678910111213141516171819202122232425262728293031import pandas as pd digits_train=pd.read_csv('/Users/scarlett/repository/projects/digits/optdigits.tra',header=None)digits_test=pd.read_csv('/Users/scarlett/repository/projects/digits/optdigits.tes',header=None)# 分割训练数据的特征向量和标记，前64维是feature vector，第65维是标记X_digits=digits_train[np.arange(64)]y_digits=digits_train[64]# 导入PCAfrom sklearn.decomposition import PCA# 初始化一个可将高维向量压缩到二维的PCAestimator=PCA(n_components=2)X_pca=estimator.fit_transform(X_digits)# 显示10类图像经PCA压缩后的二维空间分布from matplotlib import pyplot as pltdef plot_pca_scatter(): colors=['black','blue','purple','yellow','white','red','lime','cyan','orange','gray'] for i in xrange(len(colors)): px=X_pca[:,0][y_digits.as_matrix()==i] py=X_pca[:,1][y_digits.as_matrix()==i] plt.scatter(px,py,c=colors[i]) plt.legend(np.arange(0,10).astype(str)) plt.xlabel('First Principle Component') plt.ylabel('Second Principle Component') plt.show() plt_pca_scatter() 2.使用原始像素特征和经PCA压缩重建的低维特征，在相同配置的SVM上分别进行图像识别 12345678910111213141516171819202122X_train=digits_train[np.arange(64)]y_train=digits_train[64]X_test=digits_test[np.arange(64)]y_test=digits_test[64]# 导入基于线性核的SVM分类器,建模，预测from sklearn.svm import LinearSVCsvc=LinearSVC()svc.fit(X_train,y_train)y_predict=svc.predict(X_test)# 特征压缩到20维,并转化原训练特征estimator=PCA(n_components=20)pca_X_train=estimator.fit_transform(X_train)pca_X_test=estimator.transform(X_test)# 对压缩后的20维特征的训练数据进行建模，并对测试集预测pca_svc=LinearSVC()pca_svc.fit(pca_X_train,y_train)pca_y_predict=pca_svc.predict(pca_X_test)print pca_y_predict,y_predict 1[0 1 1 ... 8 9 8] [0 1 2 ... 8 9 8] 3.性能评估 1234567from sklearn.metrics import classification_reportprint svc.score(X_test,y_test)print classification_report(y_test,y_predict,target_names=np.arange(10).astype(str))print pca_svc.score(pca_X_test,y_test)print classification_report(y_test,pca_y_predict,target_names=np.arange(10).astype(str)) 123456789101112131415161718192021222324252627282930310.9309961046188091 precision recall f1-score support 0 0.98 0.98 0.98 178 1 0.85 0.95 0.90 182 2 0.99 0.97 0.98 177 3 0.92 0.95 0.93 183 4 0.95 0.97 0.96 181 5 0.90 0.96 0.93 182 6 0.99 0.98 0.99 181 7 0.98 0.91 0.94 179 8 0.96 0.74 0.83 174 9 0.82 0.91 0.86 180avg / total 0.93 0.93 0.93 17970.9081803005008348 precision recall f1-score support 0 0.97 0.96 0.96 178 1 0.80 0.91 0.85 182 2 0.96 0.94 0.95 177 3 0.96 0.91 0.94 183 4 0.94 0.96 0.95 181 5 0.86 0.97 0.91 182 6 0.98 0.96 0.97 181 7 0.96 0.88 0.92 179 8 0.82 0.83 0.83 174 9 0.87 0.75 0.80 180avg / total 0.91 0.91 0.91 1797 由上发现，经过PCA处理后会损失2%左右的预测准确性，但相比原始数据64维度的特征，使用PCA可降低68.75%的维度、 特点分析： 降维/压缩是选取数据具有代表性的特征，在保持数据多样性(Variance)的基础上，规避掉大量的特征冗余和噪声；并可节省模型训练时间，提高综合效率 但容易损失一些有用的模式信息 3.进阶篇前一节使用的数据集都是经过规范化处理的的规整数据集，使用的模型也都是默认配置，但现实生活中我们得到的数据集不会如此规整，默认配置也不一定最佳。 本章目的：掌握如何通过抽取或筛选数据特征、优化模型配置，以进一步提升经典模型的性能表现。 3.1 模型实用技巧依靠默认配置学习到模型所需的参数，不能保证： 所有用于训练的数据特征都是最好的 学习到的参数一定是最优的 默认配置下的模型总是最佳的 本节技巧：预处理数据，控制参数训练、优化模型配置,etc 3.1.1 特征提升特征抽取：逐条将原始数据转化为特征向量的形式，这个过程同时涉及到对数据特征的量化表示； 特征筛选：(更进一步)在高维度、已量化的特征向量中选择对指定任务更有效的特征组合，进一步提升模型性能 3.1.1.1 特征抽取原始数据的种类有很多：数字化的信号数据(声纹、图像)，符号化的文本；而我们无法直接将符号化的文本用于计算，而需要通过某些处理手段预先将文本良华为特征向量。 1.DictVectorizer对使用字典存储的数据进行特征抽取和向量化 有些符号化的数据特征已相对结构化，并以字典这种数据结构进行存储，故可使用DictVectorizer对特征进行抽取和向量化。 123456789101112M=[&#123;'city':'Dubai','temperature':33.&#125;,&#123;'city':'London','temperature':12.&#125;,&#123;'city':'Beijing','temperature':40.&#125;]from sklearn.feature_extraction import DictVectorizer# 初始化特征抽取器vec=DictVectorizer()# 输出转化后的特征矩阵print vec.fit_transform(M).toarray()# 输出各维度特征的含义print vec.get_feature_names() 1234[[ 0. 1. 0. 33.] [ 0. 0. 1. 12.] [ 1. 0. 0. 40.]][&apos;city=Beijing&apos;, &apos;city=Dubai&apos;, &apos;city=London&apos;, &apos;temperature&apos;] 由输出可知，特征向量化过程中。DictVectorizer对类别型和数值型特征的处理方式不同。 类别型(categorical)特征：借助原特征名称组合产生新特征，并用0/1二值方式进行量化 数值型(numerical)：维持原始特征值 2.使用CountVectorizer且在不去掉停用词的条件下，对文本特征进行量化的朴素贝叶斯分类性能测试 处理文本数据的方法：词袋法(Bag of Words) 词袋法：不考虑词语出现的顺序，只将训练文本中的每个出现过的词汇单独视作一列特征；词表：不重复的词汇的集合；每条训练文本都可在高维度词表上映射出一个特征向量； 特征数值的常见计算方式：CountVectorizer &amp; TfidfVectorizer CountVectorizer：只考虑每种词汇(Term)在该条训练文本中出现的频率(Term Frequency) TfidfVectorizer：既考量某一次会在当前文本中出现的频率，又考虑包含这个词汇的文本条数的倒数(Inverse Document Frequency),即训练的条目越多，TfidfVectorizer的特征量化就越有优势；可剔除在每条文本中都出现的常用词汇，以减少它们对模型分类决策的影响 停用词(Stop Words)：在每条文本中都出现的常用词汇，如the,a；停用词常在特征抽取中以黑名单的方式过滤掉，以提高模型的性能表现 区别：CountVectorizer只统计词频，而TfidfVectorizer还过滤掉了停用词 1234567891011121314151617181920212223from sklearn.datasets import fetch_20newsgroupsnews=fetch_20newsgroups(subset='all')from sklearn.cross_validation import train_test_splitX_train,y_train,X_test,y_test=train_test_split(news.data,news.target,test_size=0.25,random_state=33)from sklearn.feature_extraction.text import CountVectorizercount_vec=CountVectorizer()# 只使用词频统计将原始训练和测试文本转化为特征向量X_count_train=count_vec.fit_transform(X_train)X_count_test=count_vec.transform(X_test)# 导入naive bayes,默认配置初始化，使用CountVectorizer(未剔除停用词的)后的训练样本进行学习from sklearn.naive_bayes import MultinomialNBmnb_count=MultinomialNB()mnb_count.fit(X_count_train,y_train)y_count_predict=mnb_count.predict(X_test)# 输出性能评估结果from sklearn.metrics import classification_reportprint 'Accuracy:',mnb_count.score(X_count_train,y_train)print classification_report(y_test,y_count_predict,target_names=news.target_names) output： 12345678910111213141516171819202122232425The Accuracy of NBC is: 0.8397707979626485 precision recall f1-score support alt.atheism 0.86 0.86 0.86 201 comp.graphics 0.59 0.86 0.70 250 comp.os.ms-windows.misc 0.89 0.10 0.17 248comp.sys.ibm.pc.hardware 0.60 0.88 0.72 240 comp.sys.mac.hardware 0.93 0.78 0.85 242 comp.windows.x 0.82 0.84 0.83 263 misc.forsale 0.91 0.70 0.79 257 rec.autos 0.89 0.89 0.89 238 rec.motorcycles 0.98 0.92 0.95 276 rec.sport.baseball 0.98 0.91 0.95 251 rec.sport.hockey 0.93 0.99 0.96 233 sci.crypt 0.86 0.98 0.91 238 sci.electronics 0.85 0.88 0.86 249 sci.med 0.92 0.94 0.93 245 sci.space 0.89 0.96 0.92 221 soc.religion.christian 0.78 0.96 0.86 232 talk.politics.guns 0.88 0.96 0.92 251 talk.politics.mideast 0.90 0.98 0.94 231 talk.politics.misc 0.79 0.89 0.84 188 talk.religion.misc 0.93 0.44 0.60 158 avg / total 0.86 0.84 0.82 4712 3.使用TfidfVectorizer且在不去掉停用词的条件下，对文本特征进行量化的朴素贝叶斯分类性能测试 12345678910from sklearn.datasets import fetch_20newsgroups# 即时从网上下载数据news=fetch_20newsgroups(subset='all')print len(news.data)print news.data[0]from sklearn.cross_validation import train_test_splitX_train,X_test,y_train,y_test=train_test_split(news.data,news.target,test_size=0.25,random_state=33) 12345678910111213141516from sklearn.feature_extraction.text import TfidfVectorizertfidf_vec=TfidfVectorizer()# 转化为特征向量X_tfidf_train=tfidf_vec.fit_transform(X_train)X_tfidf_test=tfidf_vec.transform(X_test)from sklearn.naive_bayes import MultinomialNBmnb_tfidf=MultinomialNB()mnb_tfidf.fit(X_tfidf_train,y_train)y_tfidf_predict=mnb_tfidf.predict(X_tfidf_test)# 性能评估print 'Accuracy:',mnb_tfidf.score(X_tfidf_test,y_test)from sklearn.metrics import classification_reportprint classification_report(y_test,y_tfidf_predict,target_names=news.target_names) 12345678910111213141516171819202122232425Accuracy: 0.8463497453310697 precision recall f1-score support alt.atheism 0.84 0.67 0.75 201 comp.graphics 0.85 0.74 0.79 250 comp.os.ms-windows.misc 0.82 0.85 0.83 248comp.sys.ibm.pc.hardware 0.76 0.88 0.82 240 comp.sys.mac.hardware 0.94 0.84 0.89 242 comp.windows.x 0.96 0.84 0.89 263 misc.forsale 0.93 0.69 0.79 257 rec.autos 0.84 0.92 0.88 238 rec.motorcycles 0.98 0.92 0.95 276 rec.sport.baseball 0.96 0.91 0.94 251 rec.sport.hockey 0.88 0.99 0.93 233 sci.crypt 0.73 0.98 0.83 238 sci.electronics 0.91 0.83 0.87 249 sci.med 0.97 0.92 0.95 245 sci.space 0.89 0.96 0.93 221 soc.religion.christian 0.51 0.97 0.67 232 talk.politics.guns 0.83 0.96 0.89 251 talk.politics.mideast 0.92 0.97 0.95 231 talk.politics.misc 0.98 0.62 0.76 188 talk.religion.misc 0.93 0.16 0.28 158 avg / total 0.87 0.85 0.84 4712 由输出可知，在使用TfidfVectorizer而不去掉停用词的条件下，对训练和测试文本进行特征量化，并利用默认配置的naive bayes，在测试文本上可得到比CountVectorizer更高的预测准确性。证明：在训练文本量较多时，使用TfidfVectorizer压制常用词汇对分类决策的干扰，可提升模型性能。 4.分别使用CountVectorizer和TfidfVectorizer，并在去掉停用词的条件下，对文本特征进行量化的Naive Bayes分类性能测试 123456789101112131415161718192021222324252627282930313233# 分别使用停用词过滤器配置初始化CountVectorizer和TfidfVectorizerfrom sklearn.feature_extraction.text import CountVectorizerfrom sklearn.feature_extraction.text import TfidfVectorizercount_filter_vec,tfidf_filter_vec=CountVectorizer(analyzer='word',stop_words='english'),TfidfVectorizer(analyzer='word',stop_words='english')# 使用带停用词过滤的CountVectorizer对训练和测试文本进行量化处理X_count_filter_train=count_filter_vec.fit_transform(X_train)X_count_filter_test=count_filter_vec.transform(X_test)# 使用带停用词过滤的TfidfVectorizer对训练和测试文本进行量化处理X_tfidf_filter_train=tfidf_filter_vec.fit_transform(X_train)X_tfidf_filter_test=tfidf_filter_vec.transform(X_test)# 初始化默认配置的朴素贝叶斯，并对CountVectorizer后的数据进行预测和性能评估from sklearn.naive_bayes import MultinomialNBmnb_count_filter=MultinomialNB()mnb_count_filter.fit(X_count_filter_train,y_train)y_count_predict=mnb_count_filter.predict(X_count_filter_test)# 初始化另一个默认配置的朴素贝叶斯，并对TfidfVectorizer后的数据进行预测和性能评估mnb_tfidf_filter=MultinomialNB()mnb_tfidf_filter.fit(X_tfidf_filter_train,y_train)y_tfidf_predict=mnb_tfidf_filter.predict(X_tfidf_filter_test)# CountVectorizer性能评估from sklearn.metrics import classification_reportprint 'Count_Accuracy',mnb_count_filter.score(X_count_filter_train,y_train)print classification_report(y_test,y_count_predict,target_names=news.target_names)# TfidfVectorizer性能评估print 'Tfidf_Accuracy',mnb_tfidf_filter.score(X_tfidf_filter_train,y_train)print classification_report(y_test,y_tfidf_predict,target_names=news.target_names) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051Count_Accuracy 0.9439649073156926 precision recall f1-score support alt.atheism 0.85 0.89 0.87 201 comp.graphics 0.62 0.88 0.73 250 comp.os.ms-windows.misc 0.93 0.22 0.36 248comp.sys.ibm.pc.hardware 0.62 0.88 0.73 240 comp.sys.mac.hardware 0.93 0.85 0.89 242 comp.windows.x 0.82 0.85 0.84 263 misc.forsale 0.90 0.79 0.84 257 rec.autos 0.91 0.91 0.91 238 rec.motorcycles 0.98 0.94 0.96 276 rec.sport.baseball 0.98 0.92 0.95 251 rec.sport.hockey 0.92 0.99 0.95 233 sci.crypt 0.91 0.97 0.93 238 sci.electronics 0.87 0.89 0.88 249 sci.med 0.94 0.95 0.95 245 sci.space 0.91 0.96 0.93 221 soc.religion.christian 0.87 0.94 0.90 232 talk.politics.guns 0.89 0.96 0.93 251 talk.politics.mideast 0.95 0.98 0.97 231 talk.politics.misc 0.84 0.90 0.87 188 talk.religion.misc 0.91 0.53 0.67 158 avg / total 0.88 0.86 0.85 4712Tfidf_Accuracy 0.9479977359558511 precision recall f1-score support alt.atheism 0.86 0.81 0.83 201 comp.graphics 0.85 0.81 0.83 250 comp.os.ms-windows.misc 0.84 0.87 0.86 248comp.sys.ibm.pc.hardware 0.78 0.88 0.83 240 comp.sys.mac.hardware 0.92 0.90 0.91 242 comp.windows.x 0.95 0.88 0.91 263 misc.forsale 0.90 0.80 0.85 257 rec.autos 0.89 0.92 0.90 238 rec.motorcycles 0.98 0.94 0.96 276 rec.sport.baseball 0.97 0.93 0.95 251 rec.sport.hockey 0.88 0.99 0.93 233 sci.crypt 0.85 0.98 0.91 238 sci.electronics 0.93 0.86 0.89 249 sci.med 0.96 0.93 0.95 245 sci.space 0.90 0.97 0.93 221 soc.religion.christian 0.70 0.96 0.81 232 talk.politics.guns 0.84 0.98 0.90 251 talk.politics.mideast 0.92 0.99 0.95 231 talk.politics.misc 0.97 0.74 0.84 188 talk.religion.misc 0.96 0.29 0.45 158 avg / total 0.89 0.88 0.88 4712 由输出可知，TfidfVectorizer的特征抽取和量化方法更具备优势，对停用词进行过滤后的模型性能比未过滤高3%—4%。 3.1.1.2 特征筛选良好的数据特征组合可提高模型性能，冗余特征会浪费CPU计算资源，不良特征会降低模型精度。 主成分分析(PCA)：用于去除线性相关的特征组合 特征筛选：不是修改特征值，而是寻找对模型性能提升大的少量特征 使用Titanic数据集，通过特征筛选法一步步提升决策树的预测性能 1234567891011121314151617# 导入数据import pandas as pdtitanic=pd.read_csv('/Users/scarlett/repository/projects/titanic/titanic.csv')print titanic.shapeprint titanic.info()# 分离数据特征与预测目标y=titanic['survived']X=titanic.drop(['row.names','name','survived'],axis=1)# 填充缺失数据X['age'].fillna(X['age'].mean(),inplace=True)X.fillna('UNKNOWN',inplace=True)# 分割数据from sklearn.cross_validation import train_test_splitX_train,y_train,X_test,y_test=train_test_split(X,y,test_size=0.25,random_state=33) 123456789101112131415161718(1313, 11)&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;RangeIndex: 1313 entries, 0 to 1312Data columns (total 11 columns):row.names 1313 non-null int64pclass 1313 non-null objectsurvived 1313 non-null int64name 1313 non-null objectage 633 non-null float64embarked 821 non-null objecthome.dest 754 non-null objectroom 77 non-null objectticket 69 non-null objectboat 347 non-null objectsex 1313 non-null objectdtypes: float64(1), int64(2), object(8)memory usage: 112.9+ KBNone 123456# 类别型特征向量化from sklearn.feature_extraction import DictVectorizervec=DictVectorizer()X_train=vec.fit_transform(X_train.to_dict(orient='record'))X_test=vec.transform(X_test.to_dict(orient='record'))print len(vec.feature_names_) 1234from sklearn.tree import DecisionTreeClassifierdt=DecisionTreeClassifier()dt.fit(X_train,y_train)dt.score(X_test,y_test) 1234567# 导入特征筛选器from sklearn import feature_selectionfs=feature_selection.SelectPercentile(feature_selection.chi2,percentile=20)X_train_fs=fs.fit_transform(X_train,y_train)dt.fit(X_train_fs,y_train)X_test_fs=fs.transform(X_test)dt.score(X_test_fs,y_test) 12345678910111213141516# 通过交叉验证法，按照固定间隔的百分比筛选特征，并作图展示性能岁特征筛选比例的变化from sklearn.cross_validation import cross_val_scoreimport numpy as nppercentile=range(1,100,2)results=[]for i in percentile: fs=feature_selection.SelectPercentile(feature_selection.chi2,percentile=i) X_train_fs=fs.fit_transform(X_train,y_train) scores=cross_val_score(dt,X_train_fs,y_train,cv=5) results=np.append(results,scores.mean()) print results # 找到提现最佳性能的特征筛选的百分比opt=np.where(results==results.max())[0]print 'Optimal number of features %d'%percentiles[opt] 3.1.2 模型正则化任何机器学习模型在训练集上的性能表现都不能作为其对未知测试数据预测能力的评估。 本节：模型泛化力(Generalization)，和如何保证模型泛化力 3.1.2.1 欠拟合和过拟合(Underfitting &amp; Overfitting)拟合：机器学习模型在训练过程中，通过更新参数，使模型不断契合可观测数据(训练集)的过程。 阐述：模型复杂度与泛化力的关系 数据描述 披萨饼价格预测 每种直径(Diameter)对应一个报价 需要设计一个学习模型，可根据披萨的直径特征来预测售价 由上表：5组训练数据，4组测试数据且报价未知；只考虑直径与售价的关系，则适合用线性回归模型。 1.使用线性回归模型在披萨训练样本上进行拟合 123456789101112131415161718192021222324252627X_train=[[6],[8],[10],[14],[18]]y_train=[[7],[9],[13],[17.5],[18]]# 导入线性回归模型from sklearn.linear_model import LinearRegressionregressor=LinearRegression()regressor.fit(X_train,y_train)# 导入numpyimport numpy as np# 在x轴上从0-25均匀采样100个数据点,并以100个数据点为基准，预测回归直线xx=np.linspace(0,26,100)xx=xx.reshape(xx.shape[0],1)yy=regressor.predict(xx)# 对预测到的直线作图,import matplotlib.pyplot as pltplt.scatter(X_train,y_train)# 使用plt.plot()画(x,y)曲线,degree=1表示特征是一维的，做个标记plt1,=plt.plot(xx,yy,label="Degree=1")plt.axis([0,25,0,25]) # axis表示坐标的极值范围plt.xlabel('Diameter of pizza')plt.ylabel('Price') plt.show()# 输出模型在训练样本上的R-squared值print regressor.score(X_train,y_train) 10.9100015964240102 接下来我们尝试将原特征提高一个维度，用2次多项式回归来拟合训练样本 2.使用2次多项式回归模型在训练样本上进行拟合 12345678910111213141516171819202122232425from sklearn.preprocessing import PolynomialFeatures# 使用PolynomialFeatures(degree=2)映射出2次多项式特征poly2=PolynomialFeatures(degree=2)X_train_poly2=poly2.fit_transform(X_train)# 以线性回归模型为基础，初始化模型（特征维度提升，但模型仍是线性回归模型）regressor_poly2=LinearRegression()regressor_poly2.fit(X_train_poly2,y_train)# 从新映射绘图用x轴采样数据xx_poly2=poly2.transform(xx)# 预测yy_poly2=regressor_poly2.predict(xx_poly2)# 作图plt.scatter(X_train,y_train)plt1,=plt.plot(xx,yy,label='degree=1')plt2,=plt.plot(xx,yy_poly2,label='degree=2')plt.axis([0,25,0,25])plt.xlabel('diameter')plt.ylabel('price')plt.show()print regressor_poly2.score(X_train_poly2,y_train) 10.9816421639597428 果然在升高特征维度后，模型性能更高，对训练数据的拟合程度更好。接下来我们进一步提高特征维度。 1234567891011121314151617181920212223242526from sklearn.preprocessing import PolynomialFeatures# 使用PolynomialFeatures(degree=4)映射出2次多项式特征poly4=PolynomialFeatures(degree=4)X_train_poly4=poly4.fit_transform(X_train)# 以线性回归模型为基础，初始化模型（特征维度提升，但模型仍是线性回归模型）regressor_poly4=LinearRegression()regressor_poly4.fit(X_train_poly4,y_train)# 从新映射绘图用x轴采样数据xx_poly4=poly4.transform(xx)# 预测yy_poly4=regressor_poly4.predict(xx_poly4)# 作图plt.scatter(X_train,y_train)plt1,=plt.plot(xx,yy,label='degree=1')plt2,=plt.plot(xx,yy_poly2,label='degree=2')plt3,=plt.plot(xx,yy_poly4,label='degree=4')plt.axis([0,25,0,25])plt.xlabel('diameter')plt.ylabel('price')plt.show()print regressor_poly4.score(X_train_poly4,y_train) 11.0 由图和R平方指标可见，4次多项式曲线几乎完全拟合了所有训练样本点。接下来我们看着三种特征维度下的模型分别在测试集上的性能表现。 3.评估3种回归模型在测试集上的性能表现 12X_test=[[6],[8],[11],[16]]y_test=[[8],[12],[15],[18]] 12345678910# degree=1print regressor.score(X_test,y_test)# degree=2X_test_poly2=poly2.transform(X_test)print regressor_poly2.score(X_test_poly2,y_test)# degree=4X_test_poly4=poly4.transform(X_test)print regressor_poly4.score(X_test_poly4,y_test) 1230.8097267977076650.86754436563451080.8095880795788558 特征多项式次数 训练集R-squared值 测试集R-squared值 degree=1 0.9100 0.8097 degree=2 0.9816 0.8675 degree=4 1.0000 0.8096 由输出可见 欠拟合：当模型复杂度很低时(degree=1)，模型既在训练集上拟合不好，又在测试集上表现一般 过拟合：一味追求高模型复杂度(degree=4)，尽管模型完美拟合了几乎所有训练数据，但模型会变得非常波动，几乎丧失了对未知数据的预测能力 这两种都是模型缺乏泛化力的表现。 要求我们在增加模型复杂度、提高在可观测数据上的性能表现的同时，需要兼顾模型的泛化力，防止发生过拟合。为了平衡这两种选择，我们通常采用2种模型正则化方法：L1范数正则化 &amp; L2范数正则化 3.1.2.2 L1范数正则化正则化(Regularization) 目的：提高模型在位置测试数据上的泛化力，避免过拟合 常见方法：在原模型优化目标的基础上，增加对参数的惩罚项(Penalty) 以最小二乘优化目标为例： 最小二乘优化目标: $argminL(w,b)=argmin\sum_{m}^{k=1}\qquad(f(w,x,b)-y^k)^2$ 若加入对模型的L1范数正则化，则新的线性回归目标为： argminL(w,b)=argmin\sum_{m}^{k=1}\qquad(f(w,x,b)-y^k)^2 + \lambda \Arrowvert{w}\Arrowvert_{1}即在原优化目标的基础上，增加了参数向量的L1范数，则在新目标优化过程中需要考虑L1惩罚项的影响。 为使目标最小化，这种正则化方法的结果是让参数向量中的许多元素趋向于0，使大部分特征失去对优化目标的贡献。而这种让有效特征变得稀疏的L1正则化模型，称为Lasso。 Lasso模型在4次多项式特征上的拟合表现 123456from sklearn.linear_model import Lassolasso_poly4=Lasso()lasso_poly4.fit(X_train_poly4,y_train)print lasso_poly4.score(X_test_poly4,y_test)# 输出lasso模型的参数列表print lasso_poly4.coef_ 1230.8388926873604382[ 0.00000000e+00 0.00000000e+00 1.17900534e-01 5.42646770e-05 -2.23027128e-04] 123# 回顾普通4次多项式回归模型拟合后的性能和参数列表print regressor_poly4.score(X_test_poly4,y_test)print regressor_poly4.coef_ 1230.8095880795788558[[ 0.00000000e+00 -2.51739583e+01 3.68906250e+00 -2.12760417e-01 4.29687500e-03]] 由上可见，默认配置的lasso模型性能提高了约3%。lasso模型拟合后的参数列表中，4次与3次特征的参数均为0.0，使得特征更加稀疏。 3.1.2.3 L2范数正则化与L1范数正则化略有不同，L2范数正则化在原优化目标上增加了参数向量的L2范数的惩罚项，公式如下： argminL(w,b)=argmin\sum_{m}^{k=1}\qquad(f(w,x,b)-y^k)^2 + \lambda \Arrowvert{w}\Arrowvert_{2}为使新优化目标最小化，这种正则化方法的结果会让参数向量中的大部分元素都变得很小，压制了参数之间的差异性，这种压制参数间差异性的L2正则化模型被称为Ridge。 Ridge模型在4次多项式特征上的拟合表现 1234# 输出普通4次多项式回归模型的参数列表print regressor_poly4.coef_# 输出上述参数的平方和，验证参数间的巨大差异print np.sum(regressor_poly4.coef_ **2) 123[[ 0.00000000e+00 -2.51739583e+01 3.68906250e+00 -2.12760417e-01 4.29687500e-03]]647.3826457370965 123456from sklearn.linear_model import Ridgeridge_poly4=Ridge()ridge_poly4.fit(X_train_poly4,y_train)print ridge_poly4.score(X_test_poly4,y_test)print ridge_poly4.coef_print np.sum(ridge_poly4.coef_ **2) 1230.8374201759366331[[ 0. -0.00492536 0.12439632 -0.00046471 -0.00021205]]0.01549896520353474 由输出可见，相比普通4次多项式回归模型，默认配置下的Ridge模型性能提高了约3%，且模型拟合后的参数间差异非常小。 λ是调节因子。 3.1.3 模型检验错误的做法：拿测试集的正确结果反复调优模型与特征。 正确的做法： 充分使用现有数据，对现有数据进行采样分割，一部分用于模型参数训练（训练集），另一部分用于调优模型配置和特征选择，并对未知测试性能作出估计（开发集Development Set/验证集Validation Set） 根据验证流程复杂度的不同，模型验证方式分为：留一验证 &amp; 交叉验证 3.1.3.1 留一验证(Leave-one-out cross validation)留一验证 最简单，即从任务提供的数据中，随机采样一定比例作为训练集，剩下的“留作”验证 通常比例为：7：3 缺点：模型性能不稳定（由于对验证集合随机采样的不确定性） 适用：计算能力较弱、而相对数据规模较大的机器学习发展早期（现在应该很少用了） 3.1.3.2 交叉验证(K-fold cross-validation)交叉验证 执行了多次留一验证的过程 每次检验所使用的验证集之间互斥，且需保证每一条可用数据都被模型验证过 优点：保证所有数据都有被训练和验证的机会，尽最大可能让优化的模型性能表现可信 ​ 以5折(five-fold)交叉验证为例： 全部可用数据被随机分割为平均数量的5组，每次迭代都选取其中的1组数据作为验证集，其余4组作为训练集 ​ 3.1.4 超参数搜索(hyperparameter)超参数 指开始学习之前设置值的参数（模型配置），而非通过训练得到的参数 如K近邻算法中的K值，SVM中不同的核函数(Kernal) 多数情况下，超参数的选择是无限的；故在有限时间内，除了可验证人工预设的几种超参数组合外，还可通过启发式的搜索方法对超参数组合进行调优，这种启发式的超参数搜索法称为网络搜索 超参数的验证过程之间彼此独立，故适合并行计算 3.1.4.1 网格搜索(GridSearch)由于超参数的空间无尽，故超参数组合配置只能是更优解，没有最优解。 网格搜索 依赖网格搜索对多种超参数组合的空间进行暴力搜索，每一套超参数组合被代入到学习函数中作为新的模型，且为比较新模型间的性能，每个模型都会采用交叉验证法在多组相同的训练和开发集下进行评估 使用单线程对文本分类的Naive Bayes模型的超参数组合执行网格搜索 12345from sklearn.datasets import fetch_20newsgroupsimport numpy as npnews=fetch_20newsgroups(subset='all')print len(news.data)print news.data[0] 12345618846From: Mamatha Devineni Ratnam &lt;mr47+@andrew.cmu.edu&gt;Subject: Pens fans reactionsOrganization: Post Office, Carnegie Mellon, Pittsburgh, PALines: 12NNTP-Posting-Host: po4.andrew.cmu.edu 123456789I am sure some bashers of Pens fans are pretty confused about the lackof any kind of posts about the recent Pens massacre of the Devils. Actually,I am bit puzzled too and a bit relieved. However, I am going to put an endto non-PIttsburghers&apos; relief with a bit of praise for the Pens. Man, theyare killing those Devils worse than I thought. Jagr just showed you whyhe is much better than his regular season stats. He is also a lotfo fun to watch in the playoffs. Bowman should let JAgr have a lot offun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the finalregular season game. PENS RULE!!! ​ 123456789101112131415161718192021222324from sklearn.cross_validation import train_test_split# 选取前3000条新闻文本进行分割X_train,X_test,y_train,y_test=train_test_split(news.data[:3000],news.target[:3000],test_size=0.25,random_state=33)from sklearn.svm import SVCfrom sklearn.feature_extraction.text import TfidfVectorizer# 导入pipeline，使用pipeline简化系统搭建流程（简化代码），将文本抽取与分类器模型串联起来from sklearn.pipeline import Pipelineclf=Pipeline([('vect',TfidfVectorizer(stop_words='english',analyzer='word')),('svc',SVC())])# 这里需要试验的2个超参数的个数分别是4、3，svc_gamma的参数共有10^-2,10^-1...，则共有12种超参数组合，12种不同参数下的模型parameters=&#123;'svc_gamma':np.logspace(-2,1,4),'svc_C':np.logspace(-1,1,3)&#125;# 导入网络搜索模块GridSearchCVfrom sklearn.grid_search import GridSearchCV# 将12组参数组合、初始化的Pipeline和3折交叉验证的要求全部告诉GridSearchCV，注意refit=True的设定gs=GridSearchCV(clf,parameters,verbose=2,refit=True,cv=3)# 执行单线程网络搜索%time_=gs.fit(X_train,y_train)gs.best_params_,gs.best_score_print gs.score(X_test,y_test) 1UsageError: Line magic function `%time_` not found. 注： np.logspace(a,b,c):创建等比数列，a为起始点，b为终点，c为元素个数;np.linspace(a,b,c):创建等差数列 refit=True: 使程序以交叉验证训练集得到的最佳超参数重新对所有可用的训练集合开发集进行，作为最终用于性能评估的最佳模型参数 结果分析： 使用单线程网格搜索技术对朴素贝叶斯模型在文本分类任务中的超参数组合进行调优，共有12组超参数X3折交叉验证=36项独立运行的计算任务。寻找到的最佳超参数组合在测试集上所能达成的最高分类准确性为82.27%。 缺点：耗时 优点：一旦获取到好超参数组合，则可以保持一段时间使用，是一劳永逸提高模型性能的方法 3.1.4.2 并行搜索(Parallel Grid Search)由于各新模型在执行交叉验证过程中相互独立，故我们可充分利用多核处理器甚至是分布式计算资源来从事并行搜索，则可成倍节省运算时间。 使用多线程对文本分类的Naive Bayes模型的超参数组合执行并行化的网络搜索 12345678910111213141516171819202122232425262728293031from sklearn.datasets import fetch_20newsgroupsimport numpy as npnews=fetch_20newsgroups(subset='all')from sklearn.cross_validation import train_test_split# 选取前3000条新闻文本进行分割X_train,X_test,y_train,y_test=train_test_split(news.data[:3000],news.target[:3000],test_size=0.25,random_state=33)from sklearn.svm import SVCfrom sklearn.feature_extraction.text import TfidfVectorizer# 导入pipeline，使用pipeline简化系统搭建流程（简化代码），将文本抽取与分类器模型串联起来from sklearn.pipeline import Pipelineclf=Pipeline([('vect',TfidfVectorizer(stop_words='english',analyzer='word')),('svc',SVC())])# 这里需要试验的2个超参数的个数分别是4、3，svc_gamma的参数共有10^-2,10^-1...，则共有12种超参数组合，12种不同参数下的模型parameters=&#123;'svc_gamma':np.logspace(-2,1,4),'svc_C':np.logspace(-1,1,3)&#125;# 导入网络搜索模块GridSearchCVfrom sklearn.grid_search import GridSearchCV# 将12组参数组合、初始化的Pipeline和3折交叉验证的要求全部告诉GridSearchCV，注意refit=True的设定# 初始化配置并行网络搜索，n_jobs=-1代表使用该计算机的全部CPUgs=GridSearchCV(clf,parameters,verbose=2,refit=True,cv=3,n_jobs=-1)# 执行多线程并行网络搜索%time_=gs.fit(X_train,y_train)gs.best_params_,gs.best_score_# 输出最佳模型在测试集上的准确性print gs.score(X_test,y_test) 1UsageError: Line magic function `%time_` not found. 结果分析： 相较于单线程，多线程的计算时间仅为51.8秒，且准确性仍为82.27% 并行搜索：有效利用多核处理器的计算资源，几乎成倍提升运算速度，节省最佳超参数组合的搜索时间 3.2 流行库/模型实践3.2.1 NLTK自然语言处理包计算语言学 借助计算机强大的运算能力和海量的互联网文本，来提高自然语言处理能力 如何让计算机处理、生成、理解人类的自然语言 NLTK(Natural Language Toolkit) 高效的语言学家，快速完成对自然语言文本的深层处理和分析 1.使用词袋法(bag of words)对文本进行特征向量化 12345678910111213sent1='The dog is walking on the street.'sent2='A cat was running across the room.'from sklearn.feature_extraction.text import CountVectorizercvr=CountVectorizer()sent=[sent1,sent2]print cvr.fit_transform(sent).toarray()print cvr.get_feature_names()from sklearn.feature_extraction.text import TfidfVectorizertfidf=TfidfVectorizer()print tfidf.fit_transform(sent).toarray()print tfidf.get_feature_names() 12345678[[0 0 1 1 1 0 0 1 2 1 0] [1 1 0 0 0 1 1 0 1 0 1]][u&apos;across&apos;, u&apos;cat&apos;, u&apos;dog&apos;, u&apos;is&apos;, u&apos;on&apos;, u&apos;room&apos;, u&apos;running&apos;, u&apos;street&apos;, u&apos;the&apos;, u&apos;walking&apos;, u&apos;was&apos;][[0. 0. 0.37729199 0.37729199 0.37729199 0. 0. 0.37729199 0.53689271 0.37729199 0. ] [0.4261596 0.4261596 0. 0. 0. 0.4261596 0.4261596 0. 0.30321606 0. 0.4261596 ]][u&apos;across&apos;, u&apos;cat&apos;, u&apos;dog&apos;, u&apos;is&apos;, u&apos;on&apos;, u&apos;room&apos;, u&apos;running&apos;, u&apos;street&apos;, u&apos;the&apos;, u&apos;walking&apos;, u&apos;was&apos;] 2.使用NLTK对文本进行语言学分析 1234567891011121314import nltk# 对句子进行分词和正规化tokens_1=nltk.word_tokenize(sent1)print tokens_1# 初始化词性标注器，对每个词汇进行标注pos_tag_1=nltk.tag.pos_tag(tokens_1)print pos_tag_1# 初始化stemmer，寻找各个词汇最原始的词根stemmer=nltk.stem.PorterStemmer()stem_1=[stemmer.stem(t) for t in tokens_1]print stem_1 3.2.2 词向量技术(Word2Vec)词袋法可对文本向量化，但无法计算两段文本间的相似性。如sent1与sent2在词袋法看来唯一相同的词汇是the，找不到任何语义层面的联系。 Word2Vec模型： 用来产生词向量的相关模型，为浅层神经网络，是监督学习系统 网络以词表现，并需猜测相邻位置的输入词；训练完成后，Word2vec模型可用来映射每个词到一个向量，表示词对词之间的关系（该向量为神经网络之隐藏层） 词汇间的联系通过上下文(context)建立 如”A cat was running across the room.”,若需要上下文数量为4的连续词汇片段，则有A cat was running、cat was running across、was running across the、running across the room. 每个连续词汇片段的最后一个单词有可能是什么都是受到前面3个词汇的制约，故形成由前3个词汇预测最后一个单词的监督学习系统 当上下文数量为n时，提供给网络的输入(Input)都是前n-1个连续词汇$w{t-m+1},…,w{t-1}$，指向的目标输出(Output)就是最后一个单词$w{t}$；而在网络中用于计算的都是这些词汇的向量表示，如$C(w{t-1})$，每一个实心红色圆都代表词向量中的元素；每个词汇红色实心圆的个数代表词向量的维度(dimension),且所有词向量的维度一致。神经网络的训练也是一个通过不断迭代、更新参数循环往复的过程，从网络中最终获得的即每个词汇独特的向量表示。 实践：用20类新闻文本进行词向量训练 3.2.3 XGBoost模型(extreme gradient boosting)XGBoost模型 提升(Boosting)分类器隶属于集成学习模型，基本思想：把成千上万个分类准确率较低的树模型组合起来，成为一个准确率很高的模型 特点：不断迭代，每次迭代都生成一棵新的树;能自动利用CPU的多线程进行并行计算 如何生成合理的树？如梯度提升树 实践：对比随机决策森林和XGBoost模型对titanic上的乘客是否生还的预测能力 12345678910111213141516171819202122232425262728293031323334# coding:utf-8import pandas as pd titanic=pd.read_csv('/Users/scarlett/repository/projects/titanic/titanic.csv')print (titanic.info())# 选取训练特征X=titanic[['pclass','age','sex']]y=titanic['survived']# 补全缺失值X['age'].fillna(X['age'].mean(),inplace=True)# 数据分割from sklearn.cross_validation import train_test_splitX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=33)# 导入文本特征向量化的模块DictVectorizerfrom sklearn.feature_extraction import DictVectorizervec=DictVectorizer(sparse=False)# 对原数据进行特征向量化处理X_train=vec.fit_transform(X_train.to_dict(orient='record'))X_test=vec.transform(X_test.to_dict(orient='record'))# 采用默认配置下的随机森林分类器对测试集进行预测from sklearn.ensemble import RandomForestClassifierrfc=RandomForestClassifier()rfc.fit(X_train,y_train)print ('rfc accuracy:',rfc.score(X_test,y_test))# 采用默认配置的xgboost模型对相同测试集进行预测from xgboost import XGBClassifierxgbc=XGBClassifier()xgbc.fit(X_train,y_train)print 'xgbc accuracy:',xgbc.score(X_test,y_test) Output: 12rfc accuracy:0.775075987842xgbc accuracy:0.78234042553 由输出可知：XGBoost分类模型的确可发挥出更好的预测能力. 3.2.4 TensorFlow框架TensorFlow 一个完整的编码框架 使用图(Graph)来表示计算任务，使用会话(Session)来执行图 1.使用TensorFlow输出一句话 123456789101112131415# 使用TensorFlow输出一句话import tensorflow as tf import numpy as np # 初始化一个TensorFlow常量，使greeting作为一个计算模块greeting=tf.constant("Hello Google!")# 启动一个会话sess=tf.Session()# 使用会话执行greeting计算模块result=sess.run(greeting)# 输出会话执行结果print result# 关闭会话sess.close() 1Hello Google! 2.使用TensorFlow完成一次线性函数的运算 12345678910111213# 使用TensorFlow完成一次线性函数的运算# 声明matrix1为一个1*2的行向量，matrix2为一个2*1的列向量matrix1=tf.constant([[3.,3.]])matrix2=tf.constant([[2.],[2.]])# Product将上面两个算子相乘，作为新算例product=tf.matmul(matrix1, matrix2)# 继续讲Product与一个标量2.0求和拼接，作为最终linear算例linear=tf.add(product, tf.constant(2.0))# 直接在会话中执行linear算例，相当于将上面所有单独的算例拼接成流程图来执行with tf.Session() as sess: result=sess.run(linear) print result 1[[14.]] Skflow sklearn X tensorflow 人工神经网络(Artificial Neural Network) 神经信息传递的大致工作原理：神经元的树突(Dendrite)接收其他神经元传递过来的信息，然后神经细胞体对信息进行加工后，会通过轴突(Axon)把加工后的信息传递到轴突终端(Axon Terminal)然后再传递给其他神经元的树突。如此，则大量神经元就连接成了一个结构复杂的神经网络。 感知机 模拟神经元 n维输入信号(Input) $x=$，对应的参数向量$w=$，和截距b，输出信号y(Output)等 感知机在具体运算上采用线性加权求和的方式处理输入信号，即 y=sign(w^Tx+b)为模拟神经元行为，定义如下激活(符号)函数，由此可知感知机最终会产生两种离散数值的输出(output)信号: \begin{eqnarray}sign(z)= \begin{cases} +1, &z\ge0\cr \cr -1, &z]]></content>
      <categories>
        <category>Artificial Intelligence</category>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Artificial Intelligence</tag>
        <tag>Machine Learning</tag>
        <tag>Reading Notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[统计学| 电视收视率是如何获得的？]]></title>
    <url>%2F2018%2F03%2F05%2F%E7%BB%9F%E8%AE%A1%E5%AD%A6-%E7%94%B5%E8%A7%86%E6%94%B6%E8%A7%86%E7%8E%87%E6%98%AF%E5%A6%82%E4%BD%95%E8%8E%B7%E5%BE%97%E7%9A%84%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[摘要：电视收视率是指某一时段内收看某电视频道（或某电视节目）的人数（或家户数）占电视观众总人数（或家户数）的百分比。电视收视率的获取过程运用到了很多统计学的方法，其流程为基础研究——&gt;抽样和建立固定样组——&gt; 数据采集——&gt; 数据处理。 基础研究是为了得到被调查地区的详细资料而进行的抽样调查，是保证收视率数据质量的重要环节。目的是为固定样组的抽取提供一个抽样框，以及为固定样本的轮换提供依据。具体操作是对各项人口统计学特征和可能对用户收视行为产生影响的因素进行抽样调查。 抽样和建立固定样组：抽样采取固定样组的方式进行概率抽样；收视率调查是一种成数（比率）调查，样本量的设定需考虑允许误差的大小、所能承受调查成本的高低和所测量目标的个体行为差异；需要保证样本的代表性。建立固定样组需保证固定样组的代表性，以及采取一些措施维护和更替固定样本。 数据采集主要使用日记卡法或测量仪法。 收视数据处理流程包括三个环节：数据输入、数据清洗、数据分析。数据输入是将采集到的数据转换为二进制代码并传输到每个地区的服务器上进行全方位封闭式计算，数据安全性有保障。数据清洗时需要严格检验回收的日记卡数据的可用性，清洗后的收视数据与样本背景资料库及节目资料库合并形成一个更全面的“收视率资料库”。数据分析时，首先需要通过数据推及修正无回复误差；然后通过一整套的收视率分析指标体系，对受众的媒介使用情况进行全方位的分析和预测。收视率数据分析包括总体测量分析、累积测量分析和比较分析。收视率预测包括传统预测方法和机器学习模型预测。 关键词：收视率获取流程，收视率调查，统计分析方法，机器学习模型预测 1.收视率1.1 收视率电视收视率是指某一时段内收看某电视频道（或某电视节目）的人数（或家户数）占电视观众总人数（或家户数）的百分比。作为“注意力经济”时代的重要量化指标，它是深入分析电视收视市场的科学基础，是节目制作、编排及调整的重要参考，是节目评估的主要指标，是制定与评估媒介计划、提高广告投放效益的有力工具。 1.2 收视率调查收视率调查是一种抽样调查，因而如何根据电视人口的总体特征，科学合理地设计抽样方案，以获得具有代表性的样本，就成为收视数据是否准确的决定性因素。 2.基础研究2.1 基础研究的含义基础研究是为了得到被调查地区的详细资料而进行的抽样调查，是保证收视率数据质量的重要环节。 2.2 基础研究的目的（1）为固定样组的抽取提供一个抽样框. 用一个例子说明在收视率调查中，基础研究为什么能够为固定样组的抽取提供一个抽样框： 假定某个城市有100万户居民，现在要抽取300户进行收视率调查，那么如何抽取样本，才能保证这300户居民对整个城市的100万户居民具有代表性呢？ 很显然，如果抽取这300户样本时已经考虑了100万户居民中每一户关于对收视行为有重要影响作用的指标，如频道入户情况、家庭电视机拥有情况、家庭规模、家庭成员的性别、年龄、文化程度等，那么这300户样本对该城市总体肯定具有较好的代表性。但是问题是这100万户居民中每户居民的上述指标谁也不知道，这样就产生了一个抽样需求与信息短缺的矛盾。 基础研究为解决这个矛盾提供了一个可行的办法，即先从这100万户居民中抽取一个大样本（比如3000户），调查每户有关对收视行为有重要影响作用的指标，而后在考虑这些指标的情况下，从3000户中再抽取300户作为固定样本进行连续的收视率调查。由于这300户是在考虑了对收视行为有重要影响作用因素的基础上抽取出来的，因而其对总体具有较好的代表性。在上述例子中，从城市总体中抽取3000户并进行调查，就是基础研究（基础调查），而进行收视率调查的300户固定样本是从基础研究样本（3000户）中抽取出来的，所以基础研究为固定样组的抽取提供了一个抽样框。 （2）为固定样本的轮换提供依据. 同其他大规模、连续性调查一样，收视率调查中同样也要进行样本轮换。 样本轮换主要是为解决样本老化所造成的代表性降低和由于被访者厌倦造成的数据质量降低这两个问题。在样本轮换过程中，退出固定样组的样本户一般根据已参加调查的时间确定，即挑选那些被调查时间最长的样本户先退出，而进入固定样组的样本户就不是随便选取了，而必须要挑选那些能够保证固定样组的配额指标结构与总体指标结构相一致的户进入固定样组，这样才能保证固定样本组对总体的代表性。而这些配额指标的具体数值就来源于基础研究。 2.3 基础研究的过程基础研究的过程包括：在选取较大的样本量的前提下，对被调查区域内人群的各项人口统计学特征（如当地居民的性别比例、年龄分布、职业和收入情况等）以及可能对用户收视行为产生影响的因素（如收视设备的拥有情况、是否为有线用户、电视频道覆盖率情况及被调查者的常用语言及生活习惯等）进行抽样调查，为调查样本的抽取提供一个基础。基础研究的样本是固定样组的抽样框，基础研究的结果研究报告是对固定样组进行轮换和控制的重要依据依据。 2.4 收视率的调查对象收视率的调查对象一般被界定为目标区域内4岁及以上的电视家庭人口。这个界定包括以下三个要素： 1.目标区域：目标区域由所要调查收视率的范围所决定，可以为全国、一个省，也可以为一个市或一个县； 2.电视家庭人口：电视家庭人口是指拥有电视机的家庭人口。被调查者拥有电视机是进行收视调查的基本条件，而之所以必须还是家庭人口，主要是因为收视率调查是一种连续性调查，要求被调查者基本保持稳定，家庭人口的稳定性较强； 3.年龄下限：在收视率调查中一般要求被调查者必须是4岁及以上，主要是考虑到收视率调查的两种主要方式――日记卡法和测量仪法，均要求被调查者具有一定的行为能力，如日记卡法要求填卡，而测量仪法也要求被访者在开始看电视和结束看电视时要按手控器，如果年龄太小，无法做到这一点。 3.抽样和建立固定样组3.1 抽样抽样是收视率调查的基础环节，对样本是否具有代表性，能否保证收视率数据的准确具有十分重要的意义。电视收视率的调查一般采用固定样组的方式进行，除进行一定比例的样本轮换外，样本相对比较固定。一般采用多阶段、PPS、整群抽样等抽样方法，来抽取样本、建立固定样组。 具体过程包括：在基础研究的大样本中，按照随机原则抽取若干家庭作为固定样本，建立固定样组，参与收视率的调查。对电视收视率调查的合作样本1年365天，每天24小时的收视行为进行不间断检测，获得电视收视率数据。 3.1.1 概率抽样抽样调查按照样本抽取时所遵循原则的不同分为概率抽样与非概率抽样两大类。 概率抽样：又称随机抽样，是指调查者按照随机原则从总体中抽取样本的方法； 非概率抽样：又称为非随机抽样，是指调查者根据自己的主观判断从总体中抽取样本的方法。 概率抽样的优点 1.样本按随机原则抽取，避免了抽样者主观偏好等因素的影响，使样本能够较大可能地反映总体情况，也即采用概率抽样方法抽取出来的样本对总体的代表性较好； 2.基于概率抽样抽取出来的样本，可以对总体指标进行统计推断，而非概率抽样抽取出来的样本不能对总体指标进行统计推断； 3.概率抽样可以计算所要调查指标的抽样误差，以明确反映该项抽样调查的精度，同时也可根据抽样调查所要达到的抽样精度进行样本量和具体抽样方案的设计，而非概率抽样无法做到这一点. 正是由于概率抽样具有上述三大优点，同时收视率调查又是一种连续性调查，并且要通过对样本观众收视行为的调查数据，来对总体观众（如某一城市观众）的收视情况进行统计推断，所以收视率调查中样本的选取要采用概率抽样方法。 3.1.2 样本量的设定收视率是某个地区、某个时间段中收看某一频道（或节目）的人数占电视观众总人数的百分比，故收视率调查是一种成数（比率）调查。 在成数（比率）调查中，样本量的确定主要考虑两个因素： （1）允许误差的大小：允许误差越小，所需调查的样本量就越大； （2）所能承受调查成本（包括人力、物力和财力）的高低：因为样本量越大，也就意味着调查成本越高；同其他抽样调查一样，收视率调查样本量是允许误差与调查成本之间平衡的结果； （3）所测量目标的个体行为差异：个体行为差异大，一般会抽取较大样本量；个体行为差异小，则可以抽取较小的样本量。 根据抽样理论，允许误差M、样本量n及成数（比率）P有如下关系式： 其中：M为允许误差, t为概率度, P为成数（收视率），n为样本量，N为总体单位数（如某一城市观众总数）. 由于收视率调查的区域一般至少是一个城市（或县），城市（或县）的人口规模N相对于收视率调查样本数量n而言很大，n/N非常小，故可忽略n/N，推导出收视率调查中所需样本量的计算公式： 由上式可知，样本量与总体数量（观众规模）无关。 在收视率调查样本量的计算中，收视率P一般取50%，这是因为此时P(1-P)在所有可能的收视水平中数值达到最大；置信水平（把握程度）一般取95％，此时对应的概率度t为1.96，这样在上述公式中，样本量n的多少就取决于允许误差M的大小。 随着允许误差的降低，所需样本数量增大，但是样本数量的增加与允许误差的降低二者之间的关系并不是简单的线性关系。当允许误差较大时，随着允许误差的降低，所需样本量增加，但增加幅度不大；当允许误差较小时，随着允许误差的降低，所需样本量大幅上升。如将收视调查的允许误差由4％降为3％，所需样本量由600人增加到1067人，但如果将允许误差由3％降为2％，所需样本量则由1067人增加到2401人（增加了一倍多），如果将允许误差由2％降为1％，所需样本量由2401人增加为9604人（增加了三倍）。 上述样本量与允许误差的变动关系说明，当允许误差降低到3％时，再降低允许误差，所需要的样本量成倍增加，样本量的成倍增加就意味着调查所需成本的大幅增加，换言之，样本量和成本的大幅增加所能带来的抽样误差的降低非常有限，这时再增加样本量是不经济的。权衡抽样误差（精度）和成本之间的关系，在收视率调查中，一个比较合理的样本规模是1067人，按约3.5左右的户规模计算，即300户。 以CSM为例，其媒介研究在城市收视调查网中的样本规模一般为300户。对于部分中小城市，考虑到客户的经济负担能力，且个体收视行为差异较小，样本规模确定为100户或200户；对于部分大城市，由于客户经济实力较强，并且个体收视行为差异较大，为了取得精度更高的收视数据，样本规模确定为400户、500户或600户。 3.1.2 保证样本的代表性获得一个有代表性的固定样本组的关键： （1）采用一个可靠的概率样本设计，确保样本户随机地来自于总体； （2）保证固定样本组的一些关键特征与总体的分布保持一致。这些关键特征叫控制变量，在固定样本组组建以及维护过程中控制变量的选取非常重要。 控制变量 1.控制变量应是总体分布已知的一些特征变量； 2.应选择跟测量目标（即收视率水平）高度关联的特征变量作为控制变量； 3.2 建立固定样组3.2.1建立固定样组建立固定样组的两阶段： （1）进行一个大样本的基础调查，用以收集所需要的电视市场信息； （2）通过分析基础调查数据以及其他来源的有关数据，就可以挑选相关控制变量，采用合适的抽样方法组建固定样本组 3.2.2 保证固定样本的代表性保证固定样本代表性的方式：在现场抽取固定样本时，保持固定样组中样本的各个重要特征(如户规模、电视机数目、收入水平、日用品购买者年龄、有小孩家庭比例以及有线户比例)结构与基础研究的结果尽可能地相一致，以保证固定样本的代表性。 3.2.3 固定样本的维护和更替措施以CSM为例： 1.样本轮换：平均每周轮换2%的样本户，以防止被调查户由于长期填写日记卡产生疲劳而导致的填写质量不稳或下降； 2.样本结构的监测与调整：每周对样本的控制目标进行监测，看样本的结构特征是否与总体结构特征仍保持一致，如果样本结构与总体结构发生了偏离，则马上对固定样本进行调整； 3.大规模的基础研究：每年进行一次基础研究，以便为样本轮换与调整提供最新的总体结构特征。 4.数据采集4.1 日记卡法4.1.1 简介所谓日记卡法是指由样本户中所有4岁及以上的家庭成员，将每天收看电视的频道、时间段随时记录在日记卡上，以获取电视观众收视信息的方法。 4.1.2 记录单位以15分钟为一个记录单位，当样本人员在15分钟内收看某一频道的累计时间超过8分钟时才可记录。 4.1.3 记录项目记录用户属性，如年龄、性别、职业、爱好等；所收看电视频道的代码和收视时段；还有可能记录用户对一些电视节目的评价 4.1.4 测量过程以CSM使用的日记卡为例，日记卡中最重要的两个部分是收视率调查日记卡和日记卡专用夹。在收视率调查日记卡上有记录样本人员所收看电视频道的代码和收视时段的地方，一张日记卡可以记录一名样本人员一周七天的收视情况。 访问员每周一次上门收取已填好的日记卡，并给样本户留下下一周的空白日记卡，以记录下一周的收视情况。 4.2 测量仪法4.2.1 简介测量仪法是指用测量仪来详细记录样本户中所有4岁及以上家庭成员收看电视的情况，从而获取电视观众收视信息的方法。 4.2.2 测量过程电视频道变化直接通过测量仪采集，没有任何影响。数据可以精确到秒，可以准确反映收视变化。 4.2.3 三种主要测量技术DFM；Si-code；声音匹配(Audio Matching)。 DFM(Direct Frequency Measurement)：用于模拟电视信号进行测量的技术。收看不同频道时，电视机高频头的本振频率不同，测量仪通过测量电视机高频头输入的本振频率结合频道对应关系掌握收看的频道。 Si-code：用于数字电视信号的一种操作性较强的方法，同时，准确度也是最高的。测量仪通过监测机顶盒输出的频道系统信息代码(Si-code)结合频道对应关系掌握收看的频道。由于是机顶盒直接给出的工作状态信息，不需要其它任何的转换或识别，因此这种方式是最准确的。 声音匹配(Audio Matching)：同时可以用于模拟电视和数字电视的测量技术。通过记录样本户家庭电视机播放的声音信号特征，与服务器记录的播出声音信号特征库进行比对，掌握样本户正在收看的频道。 4.2.4 日记卡法与测量仪法比较 比较项目 测量仪 日记法 数据采集方式 测量仪跟踪记录，通过机顶盒、电话线或者电脑传输数据 人员记录，访问员上门收取日记卡 记录时间单位 1分钟（或1秒） 15分钟 数据准确性 好 一般（可能受用户的记录状态、主观因素影响） 资料丰富性 一般（只能得到收视信息） 较好（除收视信息外还可记录用户属性和用户评价） 收视信息反馈速度 迅速 较慢 样本稳定性 高 高 样本排除性 较大（数据获取前提时受访者家中有相应设备） 小 5.数据处理收视数据处理流程包括三个环节：数据输入、数据清洗、数据分析。 5.1 数据输入通过日记卡或测量仪采集到的数据产生二进制代码，代码会传输到分布在不同地区的服务器，进行全封闭自动化计算，形成加密格式的观众数据包、节目数据包和广告数据包，分别记录观众的性别、年龄、职业、观看的节目和对应的观看时间、广告播出时间等等。然后将收视调查原始数据输入计算机（仅限日记卡法，测量仪法不需要数据录入）。 数据的安全性保障： 二进制代码表示，原始数据不可见，就无法被修改； 服务器自带毁灭系统； 三种数据包统一上传，所有用户只能从服务器下载，且只能用特殊软件打开数据包、查看软件计算后的结果; 通过三组数据对比，可了解观看节目的人数、时长、哪种观众更多收看哪种节目、节目的观众构成比例、节目间的竞争情况。 5.2 数据清洗原始数据输入后，要进行数据清洗，以确保原始数据的完整及合理。其中日记卡数据的加工整理过程最繁琐。需要严格检验回收的日记卡数据的可用性，由人或机器进行，主要包括以下工序： 找出信息失效、不完整、缺乏前后一致性和逻辑性的日记卡 在数据转换成电子形式的过程中，操作人员利用计算机程序辅助检查每个市场内的电视台代码、节目名称、电视台的宣传语及其他信息的正确性，纠正错误信息 对于含缺失值的数据，采用“归因法（ascription）”填补缺失值，即利用计算机程序生成与缺失数据最接近的值 清洗后的收视数据与样本背景资料库及节目资料库合并形成一个更全面的“收视率资料库”。 5.3 数据分析5.3.1 数据推及：修正无回复误差收视率的计算根据样本信息对实际人口进行估计并提供观众规模及组成信息。但这只有在完全随机抽样（调查对象中各色人等均以适当的比例代表）的前提下才有效。无回复误差会使一部分人被过高估计，而对另一部分人估计不足。修正无回复误差最普遍的方式是对依据特定变量（如性别、年龄）对样本进行加权处理，赋予一部分样本以较高的权重，而另一部分以较低的权重。 假如18至24岁的男性在有效样本中的数量不足,他们在有效样本的比例是4%而在实际人口中的比例是8%,那么这些男性的权重就是2.0(即8%/4%=2.0)。因此为了推断这部分观众的规模,每个年轻男性的每日记人值(PPDv)为2,000人(即1000x2.0)而非1,000人。相反，样本中数量过多的样本的每日记人值(PPDV)应为1,000人以下。 5.3.2 收视率分析指标体系1.收视率分析指标体系在对观众收视行为、电视收视市场进行分析以及对电视节目的评估中，所采用的收视指标往往不止一个，而是一个完整的收视率分析的指标体系。 就观众收看电视节目这一行为而言，包括两个基本要素，一是谁在看？二是看了多长时间？二者是构成收视率这一概念最基本的要素。因此，在描述收视率各指标之间的关系时，把“到达率”（反映谁在看）和“人均接触分钟数”（反映看了多长时间）作为收视率指标体系的“源头”，由它们得出收视率指标，并派生出更多的收视率分析指标。关于收视率指标体系以及各指标之间的关系如下图所示。 收视率分析指标体系本质上是对受众媒介使用情况（exposure to media）的分析。依据是否对受众个体的媒体使用情况进行连续跟踪的标准，将测量和量化受众的方法分为两种，“总体测量（gross measures）”和“累计测量（cumulative measures）”。若不跟踪测量个体行为，则为总体测量；反之则为累计测量。 2.总体测量分析总体测量的定义：指对受众使用媒体情况的总体测量包括在某一时间对受众规模和构成的估计，是对受众群体的一个概括，不涉及受众个体对某个媒体的重复使用情况。 总体测量数据包括： （1）原始数据： 家庭开机率推及的开机户数总电视户数 个人开机率推及的个人开机数总电视个数 （2）由原始数据计算生成的数据： A.收视率：根据抽样调查所估计的，某个特定时段内收看电视的人口占所有电视渗透人口的平均百分比。其中电视渗透人口是指拥有电视收视手段或工具的人口（通常具有年龄下限）。 收视率收视时长分钟权重该时段总时长分钟总体推及人口 推及受众人口 = 收视率 x 总体人口 B.衍生收视率： 基本收视率收看节目或电视台的家户数总电视户数 分钟收视率在分钟内看电视分钟以上的户数总电视户数 分钟平均收视率分钟收视率总和分钟的个数 平均收视率所有家庭户收看某节目的总分钟数节目时长总电视户数 总收视率收看节目超过分钟的户数总电视户数 C.市场份额： 家庭市场份额收看某一频道或节目的家庭数家庭开机率 节目推及受众数市场份额开机家庭户 节目收视率市场份额家庭开机率 平均分钟市场份额平均分钟视听率平均分钟家庭开机率 D.成本计算： 每千人成本插播点成本美元推及目标受众人数 平均千人成本播放总成本美元目标总体影响 每收视点成本广告插播成本目标受众收视率 3.累计测量分析累计测量数据包括： 电视台的累积受众（cume）：通过每个人在1周内使用媒体的情况，计算出有多少人至少接触过该媒体1次 到达率（reach） 非重叠受众（unduplicated audience） 暴露频次（frequency）：受众个体在某段时期内接触某一广告信息的次数 4.比较分析（1）到达率：分析观众规模 观众规模是从观众数量的角度来反映所有频道/节目或一个频道/节目播出时获得的最大观众数量。在收视率分析中，可利用到达率这个指标来表示在某个时间段内收看所有频道/节目或某频道/某节目观众规模的大小，到达率高，观众规模大；到达率低，观众规模小。 到达率（Rch%）：指在特定时段内符合到达条件的接触总人数占总体电视推及人口的百分比，计算公式为： 接触人特定时段权重总体推及人口 在收视率分析中，到达率考虑的是人数而不是人次，不管观众在特定时间段内收看过这个频道或节目几次，到达率只计算观看过该节目的不重复人数的百分比。使用到达率时，最重要的一点是所考察的时间范围，是一周、一个月的到达率，还是黄金时间一小时内的到达率。不同时间范围内的到达率数值反映了不同阶段的观众规模。 （2）分析观众构成 观众构成：指对于特定频道、时段或节目，目标观众平均每分钟的收视人数（千人）占参照观众平均每分钟收视人数（千人）的百分比。其中，参照观众一般为4岁及以上所有人，目标观众可按性别、年龄、职业、收入水平、教育程度等来进行划分。观众构成的计算公式为： 观众构成目标观众收视时长分钟权重参照观众收视时长分钟权重 观众构成反映的是某一时段内特定频道（节目）的观众收视结构，回答了“谁在看该频道（节目）”的问题，可以用来很好地描述频道（节目）的收视观众特征。目标观众比例较高的为多数观众，是构成频道（节目）收视的主要群体。 （3）分析观众人均收视时长 人均收视时长（AvAud(All)）：收看电视观众的日平均收视时间（分钟）与总体电视推及人口的比值，可针对特定频道、时段或节目进行计算。人均收视时长是一个反映观众收看电视时间长度的指标，通常也用来描述一个电视收视市场的大小。计算公式为： 人均收视时长收视分钟数权重总天数总体推及人口 由于人均收视时长显示的是观众接触节目的时间长短，反映观众接触节目的深度，所以也是体现观众忠诚度的重要指标之一。对于播出时间长度相似的节目或节目类型，可以通过人均收视时长来比较它们获取观众收视的能力。对于播出长度相同或不同的节目和节目类型，也可以通过收视比重（基于收视时长计算）与播出比重的对比，来分析不同节目和节目类型的资源利用效率。 （4）分析观众收视倾向 观众收视倾向：观众对于特定节目或者频道的收视倾向，可反映观众对该节目或频道的喜爱与热衷程度。对于观众收视倾向的分析，通常使用观众集中度这个指标。 观众集中度（TgAfin%）：指对于特定频道（或节目），目标观众（如15-34岁人群）收视率（%）与参照观众（如4岁及以上所有人）收视率（%）的比值，表示目标观众相对于参照观众的收视集中程度，以此来反映目标观众对特定频道（节目）的收看选择倾向。目标观众收视率和参照观众收视率须对应同一时段和同一频道（节目），两组观众均可自定义。计算公式为： 观众集中度收视率目标观众收视率参照观众 5.3.3 收视率预测收视率预测：指基于历史收视率数据，对未来收视率趋势进行预测。 1.传统预测方法Step1.预测广告插播时段的总体受众规模：即家庭开机率（HUT）或个人开机率（PUT）。最简单的方法是假设它会和1年前的这一天一样；复杂方法：根据几个月或几年的HUT、PUT分析受众规模的长期趋势或反常情况，找出其与未来开机率水平的影响关系。 Step2.预测媒体或节目能达到的市场份额：最简单是假设它与上一调查时期的市场份额相同；复杂一点则需要考虑各种因素对市场份额的影响，如节目编排的变化。 Step3：求出预估收视率 预估收视率预估开机率预估市场份额 2.机器学习&amp;深度学习模型预测电视节目收视率预测是一种典型的非线性预测，收视率在短时间内相对稳定。人工神经网络具有很好的容错性、自适应学习能力和非线性映射能力，采用人工神经网络做收视率预测精度较高。以BP神经网络为例。 BP神经网络：一种多层感知器结构，按照误差逆向传播算法训练的多层前馈神经网络。 如上所示，X为输入向量，O为隐含层向量，Z为输出层向量。V为输入层到隐含层的权值矩阵，W为隐含层到输出层的权值矩阵，Y为输出层的期望输出向量。 BP神经网络通过对样本的学习训练，不断改变连接网络的权值,从而使得实际的输出结果逐渐趋近期望结果。经过多次迭代学习，直到达到所要的精度为止。]]></content>
      <categories>
        <category>Computer Science</category>
        <category>Data Mining and Analysis</category>
      </categories>
      <tags>
        <tag>Data Mining and Analysis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[My AI PM Learning Path Summary]]></title>
    <url>%2F2018%2F03%2F04%2FMy-AI-PM-Learning-Path-Summary%2F</url>
    <content type="text"><![CDATA[Abstract：本文是我这大半年来的AI PM学习之路的总结，主要是关于学习路径和成果。 自17年9月至今，我已在AI PM的学习之路上行进了大半年。基于之前已有产品策划设计的经历，又通过这大半年的学习计划，我已经对AI技术、AI行业、AI PM职业有了较为全面的了解。然而现在已处于仅靠阅读和在校的资源无法使我继续快速成长的瓶颈阶段，只有实打实的业务实践才是AI PM最快的成长方式，所以打算暑假去实习，在真实业务中得到锻炼。 引用图灵机器人公司的AI PM黄钊学长所描述的AI PM能力模型： AI PM = AI + PM + X其中右式各项分别包括 AI：AI技术理解力、类机器学习思维方式、多模态人机交互设计 PM：逻辑，沟通，需求把握，快速学习 X：垂直场景认知，跨领域协作，人文素养和精神境界 我基本上是以这个能力模型为参照，一步步去浇灌自己的技能树的。 1.AI1.1 AI全局观1.阅读AI领域通识类书籍，输出读书笔记已阅读的书籍的阅读笔记 阅读笔记《漫谈人工智能》 《走近2050：注意力、互联网和人工智能》阅读笔记 阅读笔记：《智能时代》 阅读笔记：《奇点临近》 正在阅读的书籍 《如何创造思维》 《终极算法》 待阅读的书籍：(正躺在我的文件夹里…） 2.持续关注AI资讯和内容，输出文章每天都会保持浏览AI资讯的习惯，主要通过微信公众号获取信息。 关注的AI资讯类公众号有：量子位，机器之心，全球人工智能，PaperWeekly，智能玩咖，THU数据派，智东西，AI科技大本营 等等 1.2 AI技术理解力1.2.1 自学机器学习技术1.阅读机器学习理论与实战书籍精读《Python机器学习实战》这本书，并复现了书内的所有代码，输出了1份三万字的读书笔记：阅读笔记：《Python机器学习及实战》 2.上Coursera斯坦福机器学习网课精看这门Coursera上的斯坦福机器学习经典网课，并输出课程笔记（5篇）：Notes：Andrew Ng Coursera Machine Learning 3.自学Python(机器学习开发语言)1.自学廖雪峰的Python教程 2.自学《笨办法学Python》这本书，并手打全部练习 3.做Github上Show-me-the-code里的所有python小练习：Github：Python 练习册，每天一个小程序 4.输出Python相关文章（共14篇）：ScarlettHuang: Python 4.自学机器学习流行库和计算框架1.自学pandas（python数据分析基础软件库），输出2篇pandas学习文章：ScarlettHuang：Pandas 2.自学Numpy（Python数据分析的科学计算库），输出3篇numpy学习文章：ScarlettHuang：Numpy 3.自学Matplotlib（Python数据可视化软件库），输出1篇学习文章：ScarlettHuang：Matplotlib 5.搭建机器学习开发环境配置机器学习开发环境踩了不少坑，花了不少时间才配好。 正在使用TensorFlow、Keras，准备尝试下PyTorch；有时使用Jupyter Notebook进行交互式编程。 代码编译器用的是Sublime Text，鼓捣了不少时间终于把我的Sublime配置成了“十项全能”的编译器，我现在写C语言/C++/Java/Python/LaTex/Html等代码都是用这个编译器。 我的电脑是Mac Unix系统，日常使用Mac终端命令进行操作。 关于开发环境配置也输出了几篇文章，放在Machine Learning目录里：ScarlettHuang：Machine Learning 6.进行机器学习实战项目开发1.实战了几个Kaggle上的机器学习新手项目，其中Titanic罹难乘客预测项目准确率为0.779，排名前50%。项目代码在我的GitHub仓库里：Github：ScarlettYellow/Machine_Learning_Projects 2.输出了一些机器学习学习文章(12篇)：ScarlettHuang：Machine Learning 1.2.2 了解深度学习和强化学习现在我还处于机器学习的学习阶段，想先打好机器学习基础，故现在对深度学习和强化学习技术仅有一些了解，还未有具体技术实践，下面是我输出的一些文章： 1篇深度学习文章：ScarlettHuang：Deep Learning 2篇强化学习文章：ScarlettHuang：Reinforcement Learning 1.2.3 了解AI技术的问题领域1.计算机视觉输出了2篇计算机视觉相关的文章：ScarlettHuang：Machine Vision &amp; Computer Vision 2.语音识别与合成花了不少时间输出了这篇将近五千字的TTS语音合成技术的综述文章：TTS语音合成技术综述 3.自然语言处理个人对自然语言处理领域中的一大难点——情感分析比较感兴趣，在这方面看了一些论文，输出了3篇文章：ScarlettHuang：Sentiment Analysis 2.AI PM2.1 AI PM1.输出AI PM相关文章输出AI PM相关文章（共5篇）：ScarlettHuang：AI PM 2.输出AI产品应用相关文章输出AI 产品应用相关文章（共4篇） ScarlettHuang：AI Application ScarlettHuang：AI Thoughts 3.成为AI产品经理大本营的首批团员AI产品经理大本营是由图灵机器人人才战略官黄钊创建的行业内第一个AI产品经理的成长交流付费社区。我已经加入这个社区大半年了，里面每天都会分享优质AI PM相关内容，我每天都会保持阅读，有时会在社群里参与讨论。 4.正在担任联创团队AI Lab负责人2.2 PM能力1.PM能力输出了5篇有关PM能力的文章：ScarlettHuang：PM Ability 2.PM的沟通力输出了1篇文章：ScarlettHuang：PM Communication 3.对产品、互联网的思考输出了3篇文章，发表在人人都是产品经理社区上，总阅读量2.1万，订阅量736：人人都是产品经理社区：我的文章 输出了一些对互联网的思考（共4篇）：ScarlettHuang：Internet Thoughts 4.PM的系统逻辑和必备技术知识输出了13篇有关PM的系统逻辑、必备技术知识的文章，其中11篇是《给产品经理讲技术》这本书的阅读笔记：ScarlettHuang：System Logic 5.对UI/UX设计的了解1.输出了4篇UI/UX设计相关文章，其中1篇是《术与道：移动应用UI设计》的阅读笔记：ScarlettHuang：Design 2.自学Photoshop和Sketch，细心临摹过一些UI设计图，了解基本的UI/UX设计规范 3.有时候出去打比赛缺设计师，我也会充当设计师画UI设计图 3.计算机技术3.1 高等数学高等数学(微积分、线代、概率论)是研究AI技术的必备数学知识，我已经修学了全套高数课程。不过笔记都是做在纸质笔记本上，所以这里不能贴出链接了。以下是唯一一篇线代的学习笔记：ScarlettHuang：Linear Algebra 3.2 计算机专业课程计算机专业方面目前修学了7门专业课：C语言程序设计，计算机程序设计基础C++，VB程序设计，数据结构，计算机组成原理，离散数学，数据库系统概论。本学期正在修学5门专业课：Java程序设计，C++程序设计，计算机网络，多媒体技术，数据挖掘导论。 以下是我输出的各门课程的学习笔记： 离散数学（6篇）：ScarlettHuang：Discrete Mathematics 计算机组成原理（2篇）：ScarlettHuang：Basis of Computer Engineering C/C++程序设计（2篇）：ScarlettHuang：C/C++ 数据挖掘与数据分析（12篇）：ScarlettHuang：Data Mining and Analysis 数据结构（15篇）：ScarlettHuang：Data Structure 数据库系统（5篇）：ScarlettHuang：Database System 其他计算机相关知识： 并行计算（1篇）：ScarlettHuang：Distributed Computing 社交网络分析（1篇）：ScarlettHuang：Social Network Analysis 4.传播学科学传播学专业文工交叉，课程学得比较广泛，旨在培养文工交叉的新媒体复合型人才。 文科方面涉及：新闻学、传播学、广告公关营销、社会学、心理学、经济学、文学；理工科方面涉及：高等数学、计算机科学。 文科方面的核心课程：传播学原理，传播心理学，传播统计学，新媒体用户分析，媒介经营管理，Web信息框架，网络信息管理，网络传播功能设计，互联网应用模式，新媒体营销；学科通识课程：新闻学、广告学、公关学、广播电视学、社会学相关课程。理工科方面见上一板块。 下面是我输出的传播学相关文章、笔记。 4.1 新媒体传播新媒体媒介研究（6篇）：ScarlettHuang：Medium Study 组织传播研究（2篇）：ScarlettHuang：Organizational Communication 舆情分析相关文章（2篇），其中1篇是中印对峙事件大数据舆情分析报告（2017年下半年参加DF,CCF大数据与计算智能大赛的成果，全国二等奖，单赛题第2名）：[ScarlettHuang：Public Opinion Analysis](http://scarletthuang.cn/categories/Communication-Science/Public-Opinion-Analysis/ 社交网络传播（1篇）：ScarlettHuang：Social Network Communication 新闻学研究（1篇）：ScarlettHuang：Journalism 批判性思维（1篇），是《批判性思维原理与方法》的阅读笔记：ScarlettHuang：Critical Thinking 4.2 广告、公关、营销公共关系研究（4篇）：ScarlettHuang：Public Relationship 广告与营销（5篇）：ScarlettHuang：Advertising and Marketing 新媒体创意写作（4篇）：ScarlettHuang：Creative Writting and Planning 5.我的详细简历5.1 教育背景华中科技大学，传播学 &amp; 计算机（双专业） 专业成绩：专业前20% 双专业培养方向：懂技术、懂产品、会设计、通传播的一专多通跨界复合型产品设计人才 5.2 项目经历1.中印对峙事件大数据舆情分析项目描述：DF, CCF全国大数据与计算智能大赛的中印对峙事件舆情分析赛题（获得全国二等奖,单赛题第二名） 时间：2017年9月-2017年12月 个人职责：项目管理, 舆情数据分析 设计舆情分析框架和流程 分布式爬取2017.6-2017.8中印对峙事件全网舆情数据 全方位舆情统计数据分析, 并使用LSTM深度学习模型进行情感分析 撰写长达46页的中印对峙事件舆情分析报告 2.产品项目时间：2017年5月-2017年11月项目职责：产品经理（产品概念模型设计, 产品功能设计, 产品原型设计, 与设计师和程序员进行需求对接, 统筹产品管理） 1.鸟窝习惯：群体习惯养成 Android app, 通过基于熟人关系链构建用户之间互相监督的打卡窝, 来帮助用户养成习惯 2.Snacksocks: 基于TCP实现的代理(VPN,用户可以通过自定义模块实现加密细节,以一定程度上避免GFW的侦测,比现有的VPN更安全,可跨平台移植3.pre-extraction Helper: 网页信息预抽取 Google插件,可对网页文本进行摘要提取,关键词提取,文本与主题相似度分析,文本情感极性分析4.爱彼伴: 亲子互动式的手游监控 Android AP5.表情大作战: 一款应用人脸识别技术识别人脸表情, 并生成用户专属表情包的 IOS APP 产品作品集：https://scarletthuang.cn/2018/03/23/MyProducts2017/) 3.Kaggle机器学习项目 Titanic: Machine Learning from Disaste项目描述：Titanic罹难乘客预测（个人机器学习入门项目），2018年2月 成果：Titanic罹难乘客预测项目准确率为0.779，排名前50%。项目代码在我的GitHub仓库里：Github：ScarlettYellow/Machine_Learning_Projects 分别使用随机森林分类器和 Xgboost分类器模型进行预测 使用并行网络搜索寻找更优参数组合,提高 Xgboost预测性能 使用5折交叉验证法分别对两个模型进行性能评估 5.3 课外经历1.华中科技大学联创团队的Unique AI Lab初创者和现任负责人 简介：一个基于人工智能技术的旨在培养AI精英人才的平台 个人职责 于2017年9月牵头建立起Unique AI Lab，联系导师、服务器、人才、资金等资源 制定AI Lab的规章制度，拟定团队理念，搭建起Unique AI Lab技术博客，负责Lab日常管理运营 组织2次面向全校的招新，总报名人数逾200人，经多轮筛选共录取6人；在全校范围内引领AI学习潮流 2.AI产品经理大本营首批活跃队员2017年9月至今，已加入这个社区大半年，里面每天都会分享优质AI PM相关内容，我每天都会保持阅读，有时会在社群里参与讨论。 AI产品经理大本营是由图灵机器人人才战略官黄钊创建的行业内第一个AI产品经理的成长交流付费社区 5.4 专业技能IT技能: Python, CC++, Java, 机器学习, 数据挖掘与分析,爬虫, SQL, Html, CSS 工具软件: Axure, SPSS, Sketch, Photoshop, Xmind 外语: 雅思(7),英语六级(550),德语四级(优秀) 5.5 自我认知与未来规划1.自我认知：富有责任感，逻辑思维强，快速自学能力强，喜欢广泛阅读，热爱产品策划与管理，擅长与人沟通，追求创新和卓越 2.职业目标: 成为优秀的AI产品经理, 研发AI产品, 推动AI产品化商业化 3.未来学习规划： 1.通过AI PM或互联网PM岗位的实习，积累AI产品研发经验； 2.通过合作参与AI和大数据比赛，提高AI开发技术和积累项目经验； 3.在AI领域选择1个细分领域进行深入学习和研究，比如计算机视觉、自然语言处理、人机交互； 4.本科毕业后，会选择国内保研(加入AI相关实验室)，或去美国进行硕士深造(申请AI或软件管理相关硕士项目)，或直接工作(AI PM岗位)。 5.6 更多了解 &amp; Contact MeGithub：https://github.com/ScarlettYellow Blog：http://scarletthuang.cn/（目前共输出170+篇博文，主要涉及人工智能、产品经理、计算机技术、传播学科学四个领域） WoShiPM：http://www.woshipm.com/u/192348 Contact Me Email：scarlett.huang.hust@foxmail.com Gmail: sijiahuang@hustunique.com 引用阿基米德之言：“给我一个支点，我可以翘起整个地球。” 数风流人物，还看今朝。这个时代会因我们而改变。 致人工智能终将创造的所有美好。]]></content>
      <categories>
        <category>Artificial Intelligence</category>
        <category>AI PM</category>
      </categories>
      <tags>
        <tag>AI PM</tag>
        <tag>Product Manager</tag>
        <tag>PM Ability</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SEO|解析关键词密度与分布]]></title>
    <url>%2F2018%2F03%2F03%2FSEO-%E8%A7%A3%E6%9E%90%E5%85%B3%E9%94%AE%E8%AF%8D%E5%AF%86%E5%BA%A6%E4%B8%8E%E5%88%86%E5%B8%83%2F</url>
    <content type="text"><![CDATA[Abstract：关键词密度和分布是SEO中的一对基础概念，是搜索引擎决定网页排名的重要衡量标准。网页合理的关键词密度可以让网页在搜索引擎中获得较高的排名位置，但是如果操作不当，也会引起搜索引擎对网页的降权处理。合理的关键词布局也有助于网页在搜索引擎中排名的提升。本文从含义、意义、测量方法、应用四个方面分别阐释关键词密度和关键词分布是什么。 1.关键词密度（Keyword Density）1.涵义：关键词密度是用来量度关键词在网页上出现的总次数与其他文字的比例的一个概念，一般用百分比表示。相对于页面总字数而言，关键词出现的频率越高，关键词密度也就越大。 2.意义：有助于搜索引擎优化，也被称“搜索项密度”，是关于特定搜索请求的项在网页上所有项中的比率。 3.测量方法： (1) 工作流程 对网页进行索引 按关键词对网页进行分类归档，其中搜索引擎把网页内容中的符合关键词密度标准的词，确定为该网页的关键词 对某个关键词归档内的所有网页进行排序 对于热门的网页，结果的第一页生成静态页面 (2) 计算方法 关键词密度=关键词在网页中出现的次数/网页上的文字总数比如，如果网页上有1000个词，你的关键词是“中国”，而网页上这个词出现了12 次，你网页上这个词的关键词密度是5%（50/1000）。搜索引擎通常认为有 6~7%关键词密度的网页是很高质量的网页。 (3) 用户的测量工具 我们可用一些网页关键词密度检测工具进行检测，比如网页关键词密度检测-站长工具。这些工具的计算是以字为标准的，即用关键词的字数除以文章字数。比如一篇文章有100个字，假设你的一个关键词是“冬奥会”，出现了1次,占用了4个字，那么4除以100等于0.04，0.04就是关键词的密度。 (4) 搜索引擎计算关键词密度的方法 搜索引擎计算密度的算法与上面说的免费在线测密度的软件算法不一样，它是以词为标准计算的。搜索引擎抓取完文章以后进行切词，将文章分解成多个词语，用关键词出现的次数除以整篇文章的词数。比如假设一篇文章切词完毕之后一共有50个词，关键词“冬奥会”出现了1次，那么密度是1除以50等于0.02，这样算出来的密度比软件计算出来的密度稍微低点。 2.关键词分布1.涵义：关键字在网页上的位置。这个位置可以是title 标签、链接、headings、文本主体，或任何有文字出现的地方。 2.意义：研究关键词分布可告诉我们关键词出现在哪里最好、最容易被搜索引擎抓取，方便做搜索引擎优化。 3.测量方法： 关键词分布技术的“四处一词”原理： 网页的标题和链接中分布关键词：在搜索引擎对网页的排名上和SEO站内优化上，标题占据重要地位。此处的优化需要即保证“出现关键词”和“吸引用户点击”。 在Meta页面的description和Keywords中进行：这一处随着SEO策略的改良和技术的进步，必要性已经不大，做SEO时可省去。 在页面中多处分布关键词，密度保持在5%-8%，并在第一次出现时加粗：作用是利用“强调”和“密集”让文章内容和标题以及用户的搜索行为产生相关性，这是一般建设网站内容中必须用到的一个SEO点。 在网站（站内和站外）的其他页面内容中，分布这个关键词，并将它以锚链接的形式指向之前的内容：即用A页面优化“云平台”这个词，做到了以上三点，然后在B页面出现几次“云平台”这个词，然后将它链接到了A页面。这样做是告诉搜索引擎，这个链接是与这个关键词相关的，蜘蛛会沿着链接去A页面抓取内容，发现质量优秀的内容，从而给予比较高的排名。 3.关键词密度和分布的应用关键词密度和分布在SEO中十分重要，设置好这两点可使搜索引擎更容易发现和抓取到网站内容，提高网站内容的曝光率。 关键词密度和分布是搜索引擎决定网页排名的重要衡量标准。网页合理的关键词密度可以让网页在搜索引擎中获得较高的排名位置，但是如果操作不当，也会引起搜索引擎对网页的降权处理。一般来说密度控制在2％到8％左右的比例最为合适，有利于网页在搜索引擎中排名。不过如果网站年龄够长，已经形成品牌，搜索引擎会对关键词密度放宽限制。 至于关键词分布，合理的关键词布局也有助于网页在搜索引擎中排名的提升。常用方法有：在正文标题和在整个网站的内页标题里使用关键词；在Meta页面的description和Keywords中分布关键词；在网站（站内和站外）的其他页面内容中，分布这个关键词，并将它以锚链接的形式指向之前的内容；等等。]]></content>
      <categories>
        <category>Communication Science</category>
      </categories>
      <tags>
        <tag>SEO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阅读笔记：《奇点临近》]]></title>
    <url>%2F2018%2F02%2F12%2F%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%EF%BC%9A%E3%80%8A%E5%A5%87%E7%82%B9%E4%B8%B4%E8%BF%91%E3%80%8B%2F</url>
    <content type="text"><![CDATA[Abstract：我们正身处信息化高速发展、机遇无限的时代，时代正以指数级速度向知识型经济迈进。适者生存是自然万古之定律，在奇点临近下，顺应时代勇于做潮流的弄潮儿是最佳选择。 各类技术普遍朝小型化方向发展 各类技术的关键部件的尺寸正以指数速度缩小，如磁存储 这种小型化趋势由摩尔定律驱动 奇点是一项经济命令 理智者适应世界，不理智者试图让世界适应自己，然而世界的进步总是取决于那些不理智者（乔治.伯纳德.肖） 世人莫不怀着一种与生俱来的欲望，要把支出超过收入，此乃一切进步之动力（萨缪尔.巴特勒） 我们正以指数级的速度向知识型经济迈进 技术的实际进步与表面的繁荣或萧条无关 消费者的愿望和需求正成为商业关系中的一种驱动力 人脑与计算机的区别 大脑的电路十分缓慢 大脑是大规模并行的 大脑是模拟和数字相结合的 大脑自身线路重铺 大脑中大部分细节都是随机的 大脑运用浮现特性 大脑是不完善的 大脑的思考自我违背：我们依靠内部的多样性而蓬勃发展 大脑运用进化 模式很重要 大脑是全息的 大脑是密切联系的 影响的盛装 “扮演上帝”是人性的最高体现，我们有强烈的愿望不断完善自我，掌控环境，为我们的孩子们铺就最好的道路。这些与喊我可能是所有人类历史中最基本的驱动力。（瑞米兹.南姆） 由于奇点的临近，我们将不得不重新思考人类生活性质的概念，重新设计我们人类的习俗 人生要么是一次大胆的冒险，要么什么都不是（海伦凯勒）]]></content>
      <categories>
        <category>Artificial Intelligence</category>
      </categories>
      <tags>
        <tag>Artificial Intelligence</tag>
        <tag>Reading Notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[报告阅读笔记：bilibili, 网综, 旅游, 新消费洞察, 品牌传播, 41stCNNIC]]></title>
    <url>%2F2018%2F02%2F12%2F%E6%8A%A5%E5%91%8A%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%EF%BC%9Abilibili-%E7%BD%91%E7%BB%BC-%E6%97%85%E6%B8%B8-%E6%96%B0%E6%B6%88%E8%B4%B9%E6%B4%9E%E5%AF%9F-%E5%93%81%E7%89%8C%E4%BC%A0%E6%92%AD-41stCNNIC%2F</url>
    <content type="text"><![CDATA[Abstract：本文是我对近期7份互联网行业相关报告的总结笔记，内容涉及中国互联网行业、新消费者洞察、品牌传播、bilibili,、网综、旅游。 0.报告列表 1.bilibili变形记定位：在线弹幕分享视频网站 优势 用户忠诚度最高(群体文化认同)，二次元文化人群聚集地 内容原创性：丰富的原创内容生态 UP主：输送原创视频 B站用户：生产弹幕评论 内容风格：二次元，鬼畜 用户体验：一入B站深似海（用户在B站可体验到三次元世界所不能体验到的自由、群体认同） 商业模式：内容独特导致用户独特，决定不同的商业模式 会员付费：在保证用户优雅流畅的观看体验(无广告)的基础上，会员可享受到更多福利(有表情的评论、粉丝昵称etc) 商业化的两难：过于商业化易导致用户流失 开辟游戏营收路线：游戏代理与分发，B站庞大的用户群体为游戏引流(游戏营收占比95%，主业务，2017) 未来发展 不断扩展边界和用户群体：线上—线下，二次元小众—大众 粉丝经济：粉丝为喜好买单 2.2018中国在线综艺用户洞察报告综艺之大变：电视时代到网络时代 视频网站 用户群体 年轻化：90后为主 高学历用户占比较大 偏好弹幕、互动社区（表达欲） 播放场景 播放场景移动化 伴随性价值：消遣娱乐 内容 真人秀减少；脱口秀、选秀增加 广告成为节目的一部分 嘉宾选择对年轻人是否看一档综艺的影响很大，用户很关注嘉宾动态(对明星“路转粉”很普遍) 搞笑内容最受欢迎 细分人群偏好：男性——喜剧类；女性——情感类、明星真人秀 好节目生命力3-5年 艺人 用户的偶像选择标准多元化，以节目价值定位匹配艺人为佳，跳出流量牢笼 艺人新要求：幽默有趣，有才华 未来发展 聚焦细分人群需求 研究细分偏好 用户群按内容消费程度划分：核心用户 &amp; 普通观众 把握需求随潮流的变化 不忽视中年人娱乐解压需求 节目引起人群价值观共鸣：身份认同，共同回忆 打造优质口碑 流量时代——&gt;口碑时代 传播：社交传播 分发：长综艺——短分发 找到用户中的KOL，帮助传播 话题、热点 延展商业链条 丰富商业植入形式，提高节目变现能力 建立完整产品和商业链条 内容付费 打广告的形式 广告即内容 3.互联网原住民：移动场景图谱—90后旅游锦移动场景：更多场景化广告机会 旅游动机：说走就走 梦想 美景 旅游目的地 日本、泰国 饮食：90后更相信口碑推荐 境外支付：现金为主；电子支付后来居上 金融类广告主：渠道之争；与餐厅、酒店合作，抢占支付场景 4.新青年Lifestyle洞察报告消费升级趋势下用户需求洞察 消费升级 消费结构多元化、品质化 垂直领域：旅游、运动健身、亲子、教育培训、宠物(品质化) 新青年定义：需求多元，品质为先 生活方式 深度体验：新鲜感、个性；游如土著 自我颜控：颜值管理 活在未来：注重自我投资 壕而不败：金钱换时间（eg.外卖），巨资爱宠 外向消费：种草，网红 5.2017年中国消费品市场解读新生代消费者 特点：自我彰显，女性崛起，超级消费(重度使用者&amp;重度参与者) ​ 市场 趋势：消费升级、健康绿色、零售改革 要点 消费者趋势：洞察消费者特征 市场趋势 营销趋势：内容为王，IP当道 变化 购买力：趋于年轻化，品牌忠诚度分散 6.品牌传播：信息社会的时代选择A.由信息碎片走向价值聚合：品牌传播 信息碎片：资讯超载；媒体繁多 价值聚合：通过各种营销手段、传播通道等消费者接触点所接触到的资讯碎片进行整合，形成一致性诉求 品牌传播 品牌定位创建：品牌价值，定位，文化 广告传播/非广告传播：公关；营销 品牌传播管理：基于新媒体的关系管理，品牌实态监控与更新 B.信息社会与广告传播 媒体环境的变化，促使广告传播模式变革 “搜索-满足”广告模式 品牌传播 品牌：符号化过程 传播 C.实时数据.实时传播：信息时代企业新常态广告提示+自媒体矩阵品牌传播模式： 7.第41st CNNIC 中国互联网发展报告【重点】A.移动网络促进万物互联（设备基础） 移动支付不断深入，P2P用户增长明显 网络娱乐；文娱产业进入全面繁荣 共单；网约车监管政策逐步落地(出租车) 政务服务智能化 电商 网络安全法规完善，用户安全体验提升 B.各类应用发展 1.基础应用类 即时通信 差异化：QQ(注重对阅读、游戏、直播等娱乐功能的连接；主打年轻群体)；微信(探索内容分发)；陌陌(转型泛娱乐社交平台；直播营收) 办公场景应用：TIM；钉钉 搜索引擎 移动化趋势 AI：推荐算法 &amp; 多输入方式搜索(图像识别，语音输入；基于NLP的问答技术) 网络新闻：内容(原创优质内容)、形式(短视频、直播、VR)、技术(AI技术成为核心竞争力) 社交应用 社交网络：“连接一切”的生态平台；功能丰富化 网络营销：基于社交圈、位置服务 ；移动广告 社交媒体传播影响力提升 2.商务交易类 网络购物 高质量、高效能过渡：网易严选、米家等品质电商，直接连接制造商和消费者两端，促进流通领域供给侧改革；服务性网络消费；绿色电商、二手电商 线上线下融合纵深发展：资源融合带动流通领域供应链的数字化升级，形成从供应商、销售渠道、仓储到门店各环节的协同效用（盒马生鲜混业经营、无人值守零售） 网上外卖 市场格局进一步集中：美团、饿了么双雄；平台资源整合有利于企业精细化经营 高频市场需求形成：市场补贴培育使外卖成为网民的常态化就餐方式 开始重视打造外卖品牌：优质餐饮联合；统一食材、包装；外卖IP 智能化程度：语音技术；智能调度 旅行预订 酒店直销，共享民宿 3.网络金融类应用 互联网理财：多元化发展 网上支付 深入绑定个人生活，并进一步向公共服务领域延伸（水费、电费——&gt;公共交通、高速收费、医疗etc.） 向农村、老龄网民渗透 技术提升支付安全性和便捷性 4.网络娱乐类应用 网络音乐：版权是重点；音乐+社交+短视频可能成为新增长点 网络文学：扶持原创内容 &amp; 发展听书业务(付费) 网络游戏：用户偏好的游戏类型变化；微信小游戏（手机端H5游戏新型入口）；精品游戏在海外市场成功；行业规范化提升 网络视频：移动化；网络视频内容正规化和精品化；生态化程度加深（内部：全产业生态链；外部：与相关内容行业联动） 网络直播：行业规范化 5.公共服务类应用 共享单车 蓬勃发展——淘汰整合，市场集中度提升 行业规范化 网约车 谋求转型和跨界融合提升盈利能力 跨界资源整合：接入铁路订票系统，打造一站式出行服务；与旅行、招聘等企业合作，分享客户资源 多元业务扩展：摩拜X滴滴 C.新兴技术趋势网络通讯5G技术标准研发引领全球 开展5G标准研究 构建5G商用网络 推动5G支撑移动互联网、物联网应用融合创新 开启万物互联和人机深度交互新时代 量子通信和量子计算快速发展 人工智能产业生态正加速构建 区块链技术探索和研发正在起步 探索期 落地需要 应用场景 技术（运算效率和存储）]]></content>
      <categories>
        <category>Communication Science</category>
        <category>Advertising and Marketing</category>
      </categories>
      <tags>
        <tag>Reading Notes</tag>
        <tag>Communication Science</tag>
        <tag>Advertising and Marketing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解析数字化新营销“4R原则”]]></title>
    <url>%2F2018%2F02%2F11%2F%E8%A7%A3%E6%9E%90%E6%95%B0%E5%AD%97%E5%8C%96%E6%96%B0%E8%90%A5%E9%94%80%E2%80%9C4R%E5%8E%9F%E5%88%99%E2%80%9D%2F</url>
    <content type="text"><![CDATA[Abstract：新营销是数字时代的营销，核心为4R，Recognize——Reach——Relationship——Return。营销者需采用数据化方式构建用户动态画像，通过数字媒介全方位覆盖和深层次触达消费者，与消费者建立连接、构建社群并做好社会化客户关系管理，以及充分挖掘社群价值以实现交易回报。 R1：Recognize 线下数据化，对用户进行动态画像 流量：人流量，入店量，入店率 顾客：来访周期，新老顾客，顾客关联度 访问：入店时长，探访率，深访率 R2：Reach 数字化覆盖与到达 数字广告：数字显示，视频广告，广告再定向，富媒体广告，文本关联广告，原生广告，赞助社交广告 数字媒体：社区网站，户外数字媒体，金融门户网站，新闻， 移动设备及其他设备：APP，手机，平板电脑，移动广告，短信 搜索：SEO，付费搜索，本地搜索列表，搜索引擎营销，博客搜索引擎 Email：电邮通讯简报，账单广告email，事件触发器，潜在需求培养 网站：AB.com广告，登录页，公关/媒体室，在线广告 社交计算：百科，社会化活动，UGC， 社交媒体：社群聆听，社交网络，视觉社交，视频，博客，影响者推广，社会化商业 窄播：加V博客，数字广播 动机设计：行为触发，消费者决策历程 R3：Relationship 社群（建立持续交易的基础） 建立连接 连接平台：建立与外界进行连接的平台 连接客户：通过提供信息与产品连接客户 连接企业：基于客户资源寻求与其他企业连接的可能 构建社群 评价客户基础 确定社群定位 聚拢线上线下成员 开展社群活动 关系连接点管理 SCRM 社会化客户关系管理 营销模块 数据模块 内容模块 用户模块 优秀案例：小红书 R4：Return 实现交易回报 社群资格商品化：付费会员 社群价值商品化 延伸自有品牌产品和服务 转介联合品牌产品和服务 品牌社群的自我产品提供 社群关注媒体化 基于关系圈的扩散式传播 线上线下的购买流量引导 社群成员渠道化 发展新会员 二次产品销售 人才招聘作用 社群信任市场化：信息推荐/产品评测推荐合作]]></content>
      <categories>
        <category>Communication Science</category>
        <category>Advertising and Marketing</category>
      </categories>
      <tags>
        <tag>Communication Science</tag>
        <tag>Advertising and Marketing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[失败的预测|08经济危机]]></title>
    <url>%2F2018%2F02%2F10%2F%E5%A4%B1%E8%B4%A5%E7%9A%84%E9%A2%84%E6%B5%8B-08%E7%BB%8F%E6%B5%8E%E5%8D%B1%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[Abstract：当我们做预测时倾向于绕开不确定性，而只关注符合我们期许的信息。失败的预测都是非样本预测——非样本，无思考，而用不含类似样本的先前经验预测未来是毫无根据的。 失败的预测 只关注符合我们对这个世界的期许的信息，而不在乎真实性，倾向于绕开不确定性 经济学家克鲁格曼：“房地产泡沫是内置在经济系统里的，房市崩盘并非是黑天鹅，而是房间里的大象，看上去显而易见却总是被人们忽略。”（2005，预测2008年的全球金融危机） “他们只是不想让“音乐”停下来罢了”：不是真的无知，而是不想让音乐停止(利润财富) 风险是自由市场经济发展的助力，而不确定性则是阻力 一旦房地产行业的发展超出人们的负担能力，房地产市场必将崩盘 贪婪和恐惧是经济发展的两个非常不稳定的因素 负债经营的高风险 08金融危机至少存在4大失败之处 人们本可发现这次房地产泡沫是次错误的预测，但房主和投资者却错误地认为：不断走高的房价表明房屋价值还会不断提高；事实上，房价走高时，其价值反而容易走低 各大评级机构的预测模式存在很多错误假设，对房价暴跌的风险预估不足 高负债经营 金融危机发生后，人们没能预测到这次危机会引发巨大的经济问题 失败的预测都是非样本预测——非样本，无思考 非样本：用不含类似样本的先前经验预测未来是毫无根据的 信息时代下，全球知识总量在增加，而我们实际掌握的知识和自认为掌握的知识之间的鸿沟越来越宽 盲目自信]]></content>
      <categories>
        <category>Artificial Intelligence</category>
      </categories>
      <tags>
        <tag>Artificial Intelligence</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文献理解| 新媒体概念解析]]></title>
    <url>%2F2018%2F02%2F09%2F%E6%96%87%E7%8C%AE%E7%90%86%E8%A7%A3-%E6%96%B0%E5%AA%92%E4%BD%93%E6%A6%82%E5%BF%B5%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[Abstract：现在人人都在说新媒体，但实际上，鲜少人能说出新媒体的确切定义。学界对于新媒体的定义也是众说纷纭，至今尚无统一、被广泛认同的解释。个人通过阅读和思考四篇经典的关于“新媒体概念”的论文（这些论文都是通过对学界中新媒体概念的历史、内涵和外延的各家之言的分析，探讨何为新媒体的问题），试图给出个人对新媒体概念的理解。 1.廖祥忠：何为新媒体？0.新媒体 最重要的信息集散枢纽，以数字媒体为核心的新媒体（ 通过数字化交互性的固定或移动的多媒体终端向用户提供信息和服务的传播形态 ） 背景：基于数字技术的internet的发展 特点：即时交互，无限兼容 本质：以数字技术为核心 1.传统媒体时代的“新媒体” 概念缘起：19c50s，麦克卢汉的演讲（“新媒介的革命影响”） 媒介即讯息，社会依靠集体行动开发出一种新媒介，即可表达新讯息 新旧媒介共存，每一种媒介 都有其独特的固有的力量和讯息 新：相对于“旧” 媒介在各个发展阶段的特定倾向对人的感知都是一种奴役，电子媒介解放人类于奴役状态【？】 现在的“新”：不只是一个Adj，焕发数字时代的生机与活力 2.当今时代的“新媒体” 不同定义 数字媒体 利用数字技术、网络技术，通过网络基础设施和电子终端，向用户提供信息服务的传播形态；具有消解边界之力 所有人对所有人的传播 原创性，前所未见，创新 快准全易 特征 鲜明的时代特征：数字媒体，传统媒体的数字化融合 本质特征：媒体互动的新方式，媒体技术的新融合，媒体产品的互相依赖与交叠等维度 无限包容性 以数字媒体为核心的新媒体，通过数字化交互性的固定或即时移动的多媒体终端向用户提供信息服务的传播形态 传统媒体与新媒体 二者依存度和扩增性越来越强 各种媒体间具有协同关系 旧媒体不死，死的是用来获取媒体内容的工具 旧媒体为新媒体提供智力资源 新媒体形式不断扩散，很难从传播介质上定义新媒体，新媒体形式在不同传播介质平台上具有可复制性 相对论：“新媒体是什么”伴随媒体的发展而不断变化（随时推进）；当今的新媒体指基于计算机信息处理技术的媒体形态 3.走向无限融合的新媒体 第四、第五媒体论（false、有争议）：网络，手机（代表当下新媒体的发展方向） 无限融合：传统媒体与新媒体融合，三网融合，有线网与无线宽带网融合 My Opinion 新媒体：以数字技术为核心的新型媒介体系，万物皆媒的生态环境 2.匡文波：“新媒体”概念辨析新媒体的不同定义 以数字技术为基础，以网络为载体进行信息传播的媒介总和 特征：交互，即时，延展，融合 用户：信息的发布者 兼 接收者 大众传播、组织传播、人际传播的全方位立体化融合 概念界定的问题：界定过宽，分类混乱的逻辑错误 特征 即时性，开放性，个性化，分众性，信息海量性，低成本传播性，检索便捷，融合性 根本特征：数字化，互动性 传播过程：非线性，信息发送和接收可同步可异步 “新” 内涵随传媒技术的进步而发展；特指“今日之新” 国际标准：各国的媒体发展程度不同 ​ 哪些不应当属于新媒体 非新出现的媒体都可称为新媒体 互动性/交互性（interactive）是本质特征 传统媒体两分法：传播者 &amp; 受众 两大阵营，但新媒体：传者受者的界限变模糊 eg.楼宇广告具有强制性，非互动性，故非新媒体 新媒体的外延 作者将新媒体分为网络类、数字广播电视类、移动类 但个人认为这是不全的，VR/AR/物联网以及未来可能出现的新形态媒体都未算进去 3.匡文波：关于新媒体核心概念的厘清到底什么是新媒体 借助数字设备传播信息的载体 数字化互动式新媒体 数字电视是否是新媒体？ 国内的数字电视缺乏互动性，只是提高清晰度和增加电视频道，故尚不属于新媒体 应采用双向信息传输技术，增加交互性 未来的数字电视属于新媒体 手机媒体 手机 + 互联网：重要的大众传播媒体 手机媒体是借助手机进行信息传播的工具，非独立媒体形态，非第五媒体，而是网络媒体的延伸 手机短信非手机媒体：信息传播内容匮乏，信息承载量有限 4.我对新媒体的理解广义上的新媒体之“新”是相对于“旧”而言的一种历时性的概念，如广播之于纸质、电视之于广播都是新媒体；而狭义上的新媒体指当今数字时代的新媒体，是数字化、交互式、具有创新形态的媒体，是利用数字技术、网络技术，通过网络基础设施和电子终端，向用户提供信息服务的传播形态，其区别于传统媒体的本质特征是数字化（即以数字技术为核心）和交互性（这也是我们用来判断是否为新媒体的标准）。 并非所有新出现的媒介都是新媒体，如楼宇广告；并非所有数字媒体都是新媒体。不具有交互性的数字化的传统媒体不是新媒体，如现在中国的数字电视。学界有“手机媒体是第五媒体”之论，这是不恰当的，因为手机并不是一个独立的媒体形态，而只是网络媒体的延伸。]]></content>
      <categories>
        <category>Communication Science</category>
        <category>Medium Study</category>
      </categories>
      <tags>
        <tag>Medium Study</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[详解| Sklearn—GridSearch 调参函数]]></title>
    <url>%2F2018%2F02%2F08%2F%E8%AF%A6%E8%A7%A3-Sklearn%E2%80%94GridSearch-%E8%B0%83%E5%8F%82%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[Abstract：GridSearch是Sklearn里的一个调参函数。本文是对此函数的详细解释。 1.参数搜索参数并非从estimators中直接学到的，可以通过设置一个参数搜索空间来找到最佳的cross-validation score。通常示例包括的参数有：SVM分类器的中C、kernel和gamma，Lasso中的alpha等。 当构建一个estimator时，提供的参数可以以这种方式进行优化。更特别的是，可以使用如下的方法来给给定estimator的所有参数来找到对应的参数名和当前值： 1estimator.get_params() 这些参数称被提到：“超参数（hyperparameters）”，尤其在Bayesian learning中，它们与机器学习过程中的参数优化是有区别的。 一个这样的参数search包含： 一个estimator(regressor/classifier) 一个参数空间 一个用于searching/sampling候选参数的方法 一个cross-validation的scheme 一个score function 这样的模型允许你指定有效的搜索参数策略，如下。在sklearn中，有两种通用方法进行sampling搜索候选参数： GridSearch: 暴力搜索所有参数组合 RandomizedSearchCV: 在指定参数空间内抽样一部分候选参数 2.GridSearchCVgrid search提供了GridSearchCV，相应的参数空间param_grid设置如下： 1234param_grid = [ &#123;&apos;C&apos;: [1, 10, 100, 1000], &apos;kernel&apos;: [&apos;linear&apos;]&#125;, &#123;&apos;C&apos;: [1, 10, 100, 1000], &apos;gamma&apos;: [0.001, 0.0001], &apos;kernel&apos;: [&apos;rbf&apos;]&#125;, ] 上例指定了两个要搜索的参数空间：一个是线性kernel，其中C值为[1,10,100,1000]；另一个则使用RBF kernel，对应的C值为[1,10,100,1000]，对应的gamma值为 [0.001, 0.0001]. GridSearchCV实例实现了通用的estimator API: 当在数据集的所有可能参数组合上进行”fitting”时，所有参数组都会被评测，并保留最优的参数组合。 3.随机参数优化使用GridSearch进行参数搜索是目前最广泛使用的参数优化方法，还有另一些方法存在。RandomizedSearchCV实现了在参数上的随机搜索，每个设置都会以可能的参数值分布进行抽样。对比穷举法，它具有两个优势： 1.budget的选择与参数个数和可能的值独立 2.增加参数不会影响性能，不会降低效果 参数设定部分和GridSearchCV类似，使用一个字典表来进行参数抽样。另外，计算开销（computation budget）, 抽取的样本数，抽样迭代次数，可以由n_iter来指定。对于每个参数，都可以指定在可能值上的分布，或者是一个离散值列表（它可以被均匀采样）。 例如： 1234[&#123;‘C’: scipy.stats.expon(scale=100), ‘gamma’: scipy.stats.expon(scale=.1), ‘kernel’: [‘rbf’], ‘class_weight’:[‘auto’, None]&#125;] 这个例子使用scipy.stats模块，该模块包含了许多分布方法可以用来进行抽样，包括：指数分布（expon），gamma分布(gamma)，均匀分布（uniform），或randint分布。通常每个函数都可以提供一个rvs（随机变量抽样）方法进行抽样。 注意： scipy.stats的分布不允许以随机方式指定。作为替代，我们可以使用一个全局的numpy 随机态，它可以通过np.random.seed或np.random.set_state来设定。 对于连续的参数，比如上面的C，指定一个连续的分布十分重要，它可以完全利用随机化（randomization）。这种情况下，增加n_iter将产生一个更好的搜索。 4.参数搜索tips4.1 指定一个目标metric缺省的，参数搜索会使用estimator的缺省score函数来评估参数设置。其中，分类使用sklearn.metrics.accuracy_score，回归使用sklearn.metrics.r2_score。对于其它应用，可能需要使用一个可合适的scoring函数（例如：对于unbalanced分类问题，accuracy的score是不合适的）。可选择的scoring函数可以通过GridSearchCV/RandomizedSearchCV以及其它CV工具类的scoring参数来设置。详见。 4.2 将estimators与参数空间组合详见：Pipeline: chaining estimators 4.3 模型选择：开发集与评测集通过评估多种参数设置来进行模型选择，可以认为是使用labeled数据集来训练这些参数空间。 当评估产生的模型时，在留存样本（held-out samples）上做模型评测，不会在参数搜索过程看到：推荐你将数据划分成两部分： 1.开发集（development set）：对它进行GridSearchCV 2.评测集（evaluation set）：计算性能metrics 可以通过cross_validation.train_test_split来进行划分。 4.4 并列化n_jobs参数进行设置。 4.5 容错一些参数设置可能会导致fit1或多个folds的数据时失败。缺省的，它会引起整个搜索的失败，即使有些参数设置已经被评测过了。通过设置error_score=0 (or =np.NaN), 可以让该过程更加具有容错性，对于那个存在0(或NaN)的fold数据集来说会继续进行下去。 5.可选择的其它暴力参数搜索（brute force parameter search）5.1 模型指定的cv一些模型可以在一些参数值范围内拟合数据，与单个参数值的拟合一样有效。这种特性可以执行一个更有效的cv来进行参数的模型选择。 一种最常用的参数策略的方式是，将正则项参数化。这种情况下，我们可以计算estimator的正则化path（regularization path）。 模型如下： linear_model.ElasticNetCV linear_model.LarsCV linear_model.LassoCV linear_model.LassoLarsCV linear_model.LogisticRegressionCV linear_model.MultiTaskElasticNetCV linear_model.MultiTaskLassoCV linear_model.OrthogonalMatchingPursuitCV linear_model.RidgeCV linear_model.RidgeClassifierCV 5.2 Information Criterion一些模型经常提供一个information-theoretic、closed-form的公式来进行正则项参数的估计优化，通过计算单个正则项path（而非使用cv）。 比如： linear_model.LassoLarsIC 5.3 带外估计（Out of Bag Estimates）当我们使用基于bagging的ensemble方法时，比如：使用有放回抽样来生成新的数据集，训练集中的部分仍然是见不到的。对于ensemble中的每个分类器，训练集都会遗留下另一部分数据。 这部分遗留下来的数据，可以被用于估计泛化错误（generalization error），而无需依赖于一个独立的验证集。这种估计是“免费（for free）”的，因为，不需要额外的数据就可以进行模型选择。 这些类当中实现了该方法： ensemble.RandomForestClassifier ensemble.RandomForestRegressor ensemble.ExtraTreesClassifier ensemble.ExtraTreesRegressor ensemble.GradientBoostingClassifier ensemble.GradientBoostingRegressor Referencehttp://scikit-learn.org/stable/modules/grid_search.html]]></content>
      <categories>
        <category>Artificial Intelligence</category>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Artificial Intelligence</tag>
        <tag>Machine Learning</tag>
        <tag>Sklearn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[新时代传媒人才去往何方？]]></title>
    <url>%2F2018%2F02%2F07%2F%E6%96%B0%E6%97%B6%E4%BB%A3%E4%BC%A0%E5%AA%92%E4%BA%BA%E6%89%8D%E5%8E%BB%E5%BE%80%E4%BD%95%E6%96%B9%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[Abstract：新媒体时代对传媒人才提出更高的要求，市场更多需要的是一专多通的跨界混搭复合型传媒人才。 传媒人才能力要求一专多通 通：不是兼通，而应是贯通 通识为“体”，专业为“用” 跨界&amp;混搭复合型传媒人才 跨学科：心理学，计算机，经济学，金融学，法学，社会学，统计学，文学， 跨媒体：大数据，全媒体 跨文化：国际传播人才，跨文化思维方式 跨行业：跨界 ​ 传媒人才能力要求 传统专业能力：新闻采访能力，阐释新闻的能力，事实核查能力，真相获取能力 技术背景下新要求：多媒体叙事能力，跨界合作能力，用户画像能力，数据收集能力，社交媒体运用能力，内容经营能力，数据收集分析能力 传媒行业新兴岗位 应用技术引领员，新闻游戏设计师，PGC短视频编辑，社群运营专员，虚拟现实编辑，移动直播全能记者，社会发现总监，AR内容开发师，VR影片架构师，数据分析师，网红主播经纪人，受众分析员 AI重塑传媒版图 内容生产智能化：新闻写作机器人 内容分发智能化：个性化推荐 内容监测智能化：智能监测内容 内容核实智能化：假新闻甄别 Reference2018传媒业需要什么样的人才？腾讯新闻发布首份传媒人能力需求报告]]></content>
      <categories>
        <category>Product Manager</category>
        <category>PM Ability</category>
      </categories>
      <tags>
        <tag>Communication</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阅读笔记：《智能时代》]]></title>
    <url>%2F2018%2F02%2F06%2F%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%EF%BC%9A%E3%80%8A%E6%99%BA%E8%83%BD%E6%97%B6%E4%BB%A3%E3%80%8B%2F</url>
    <content type="text"><![CDATA[Abstract：《智能时代》是吴军老师所著，书中主要围绕大数据与机器智能展开阐述。 数据 狭义：所有能输入计算机并被计算机程序处理的符号介质的总称 广义：能被处理以表示编码的信息或知识，可被测量、收集、报告、分析、可视化 范式（paradigm，科学学概念） 一个共同体成员所共享的信仰、价值、技术等等的集合。指常规科学所赖以运作的理论基础和实践规范，是从事某一科学的研究者群体所共同遵从的世界观和行为方式。 科学研究发展的四个范式：描述自然现象的实验科学，以牛顿定律和麦克斯韦方程等为代表的理论科学，模拟复杂现象的计算科学，数据密集型科学 每一次技术革命都会围绕一个核心技术展开：蒸汽机——&gt;电——&gt;计算机和半导体芯片——&gt;大数据与机器智能 ​ 数据密集型科学 产生背景：多维度和多变量导致很大的不确定性，虽还不能解释其因果关系，但可从足够多的数据中发现相关性从而把握事物的发展轨迹 大数据：源于需求，得益于技术的发展 数据的产生：互联网宽带化、移动互联网和物联网技术与应用 智能化时代 大数据与机器智能相伴而生，促进物联网从感知到认知并只能决策的升华 计算无所不在，软件定义一切，数据驱动发展 时代特征：以大数据应用、智能化为标志 如何在智能时代跨越思维的不连续性？ 大数据解决问题的本质：用不确定的眼光看待世界，再用信息来消除这种不确定性 世界的不确定性来自两方面： 影响世界的变量太多以至于无法用数学模型来描述 来自客观世界本身的不确定性（宇宙的特性） 解决智能问题：将问题转化为消除不确定性的问题，而大数据则是消除不确定性的关键 现有产业 + 新技术 = 新产业 ​ 信息论：建立在不确定性上的理论 信息熵：将世界的不确定性与信息相联系 信息熵（C.E.）：信息的度量，描述信源的不确定度 研究大数据与机器智能的基石 要消除不确定性，就要引入信息，而引入多少信息取决于系统中的不确定性有多大（——&gt;谁掌握信息，就能获得财富） 互信息(Mutual Information):信息的相关性 香农第一定律(信源编码定律)：对信源发出的所有信息设计一种编码，则编码的平均长度一定大于该信源的信息熵；且一定存在一种编码方式，使得编码的平均长度无限接近于它的信息熵 霍夫曼最优编码：把最短的编码分配给最常见的汉字 香农第二定律：信息的传播速率不可能超过信道的容量 最大熵原理：当我们要对未知事件寻找一个概率模型时，这个模型应当满足我们所有已看到的数据，但是对未知的情况不要做任何主观假设（应用于机器学习） ​ 数据 vs 数字 数据的范畴大得多，文字、图片、音视频等等 范畴随人类文明的进程不断变化、扩大 语料库：专门针对语音、文字的数据库 数据是人造物 信息 关于世界、人、事物的描述 信息可以是客观存在的，也可以是人造的 使用数据的标准流程 获取数据——&gt;分析数据——&gt;建立模型——&gt;预测未知 数据驱动方法 先有大量数据，而不是预设的模型，然后用很多简单的模型去契合数据(fit data) 即只要数据量足够，就可以用若干个简单模型取代一个复杂模型 切比雪夫大叔定律：当样本足够多时，一个随机变量和它的数学期望值之间的误差可以任意小 建立数学模型要解决2个问题 找到合适的模型 模型参数 大数据的特征 体量大Vast 多维度variety 完备性 变智能问题为数据问题 计算机自动回答 7类问题：What,when,where,which,who,why,how 前5类已经可以回答的很好，难的是why、how 思维方式决定科学成就 工业革命：机械思维的结果 机械思维的核心思想：确定性（可预测性）和因果关系 爱因斯坦和牛顿的思维方式是一致的：建立在确定性（绝对时空）的基础上 机械思维的局限性：否认不确定性和不可知性 张首晟教授用3个公式概况人类科学文明的最高成就： 爱因斯坦质能转换公式：$E=me^2$ 量子力学测不准原理 熵的定义 ​ 从因果关系到强相关关系 技术改变商业模式 技术革命导致商业模式的变化，尤其是新商业模式的诞生 技术的拐点 拐点：重大科技图片常常需要酝酿很长时间，技术进步是个缓慢的量的积累，当量积累到一定程度就会在短时间内取得质的突破，然后新科技全面迸发，此即拐点 大数据形成的技术条件：从数据的产生、存储、传输、处理四维度分析 数据的产生：电脑 &amp; 传感器 &amp; 已有信息数字化 信息的存储：存储技术的进步，如SSD 传输(从采集端到存储端)： 移动通信 处理：算力 &amp; 并行计算 机器学习 不断迭代进步的过程，即“期望最大化(Expectation Maximization)”，只要事先定出一个学习目标，这些算法就会不断优化模型，以越来越接近真实情况；算法迭代次数越多，学习得越深入，则得到的模型效果越好 机器学习方法不可能每家公司都自己去研究，最终会由专业公司为大众提供机器学习服务 数据安全与隐私保护 对数据安全性和隐私保护的诉求 数据安全：保证用户数据不损坏 &amp; 保证数据不被偷走或盗用 大数据应用 体育 农业 医疗 律师 记者、编辑 ​ 计算机写作 计算机写作的层次 书写完整的句子 组织几个句子构成符合逻辑的段落 给予特定格式或写作模板，能清晰传递信息 能不限定格式地写作内容，达到一般人写作水平 能达到专业记者、作家、学者水平 目前计算机已达到第3层次]]></content>
      <categories>
        <category>Artificial Intelligence</category>
      </categories>
      <tags>
        <tag>Artificial Intelligence</tag>
        <tag>Reading Notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解决方法| “conda command not found”]]></title>
    <url>%2F2018%2F02%2F05%2F%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95-%E2%80%9Cconda-command-not-found%E2%80%9D%2F</url>
    <content type="text"><![CDATA[Abstract：解决“conda:command not found”的方法。 Error安装anaconda后，在终端里运行conda命令，出现Error： 1conda:command not found Solution1.打开.zshrc文件 123ls -a # 显示根目录下所有文件(包括隐藏文件)vim .zshrc 2.在.zshrc里添加一行代码，并保存文件 1export PATH=&quot;/Users/scarlett/anaconda3/bin:$PATH&quot; 3.返回终端，输入命令： 1source ~/.zshrc]]></content>
      <categories>
        <category>Artificial Intelligence</category>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Anaconda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AI概念初导(小白版)]]></title>
    <url>%2F2018%2F02%2F05%2FAI%E6%A6%82%E5%BF%B5%E5%88%9D%E5%AF%BC-%E5%B0%8F%E7%99%BD%E7%89%88%2F</url>
    <content type="text"><![CDATA[Abstract：本文以简短通俗的语言阐释AI的几个基本概念。AI本质是函数。 1.人工智能 AI：让机器具备类似人的智能，从而替代人类去完成某些工作和任务 强AI：像人类一样去思考和推理，且具备知觉和自我意识（发展处于停滞状态） 弱AI：看起来像是智能的，但不会具备知觉和意识 AI实现： 基于规则进行编程：让机器能按照程序中存在的逻辑处理特点任务，从结果上看机器是智能的 机器学习：喂给机器大量针对某一任务的数据，让机器自己去学习，继而挖掘出规律，从而具备完成某一任务的智能 如识别狗： 1.基于规则：告诉机器狗的特征，机器去识别出满足这些规则的东西 2.机器学习：不告诉机器狗的特征，但喂给机器10万张狗的图片，机器自己从图片中学习到狗的特征，从而具备识别狗的智能 2.机器学习从模型层次结构角度划分：浅层学习和深度学习 浅层学习（shadow Learning） 模型层次较浅，通常无隐藏层或只有一层隐藏层(hidden layer) 常见算法：线性回归，逻辑回归，随机森林，SVM，K-means，RBM，AutoEncoder，PCV，SOM等 ​ 深度学习（Deep Learning） 深：有较多的隐藏层（使得深度学习网络拥有表达更复杂函数的能力，能够识别更复杂的特征，完成更复杂的任务） CNN与RNN CNN(Convolutional Neural Networks): 计算机视觉最主要的算法 Prisma：图像风格迁移 美颜相机：滤镜 交通监控视频识别：识别车牌号 商场监控视频：识别人脸 无人车：用CV去观察和理解世界 RNN(Recurrent Neural NetWorks): 循环神经网络 RNN衍生算法：LSTM、GRU(Gated Recurrent Unit)，其拥有时间记忆功能，可用来处理一些具有时间序列属性的数据，适合处理语言、文字 应用：语音识别、机器翻译、合成音乐 如：对话机器人、讯飞输入法(TTS) 3.AI的本质——函数AI其实就是我们喂给机器目前已有的数据，机器就会从这些数据里去找出一个最能满足（此处用“拟合”或可提升逼格）这些数据的函数，当有新的数据需要预测的时候，机器就可以通过这个函数去预测出这个新数据对应的结果是什么。 一个具备智能的模型具备三要素： 数据：大数据量（只有数据量足够大，模型才能够学习到足够多且准确的区分猫和狗的特征，才能在区分猫狗这个任务上表现出足够高的准确性） 算法：神经网络架构的设计（构建模型时我们打算用浅层的网络还是深层的，如果是深层的话我们要用多少层，每层有多少神经元、功能是什么，etc） 相当于我们确定了我们的预测函数应该大致结构是什么样的，我们用Y=f(W，X，b)来表示这一函数，X是已有的用来训练的数据（猫和狗的图片），Y是已有的图片数据的标签（该图片是猫还是狗），函数里的W（权重）和b（偏差）是需要机器学习后自己找出来的,找的过程也就是模型训练的过程。 模型: 我们把数据带入到算法中进行训练，机器就会去不断地学习，当机器找到最优W（权重）和b（偏差）后，我们就说这个模型是train成功了，这个时候我们的函数Y=f(W，X，b)就完全确定下来了。]]></content>
      <categories>
        <category>Artificial Intelligence</category>
      </categories>
      <tags>
        <tag>Artificial Intelligence</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac上使用sublime编辑LaTex]]></title>
    <url>%2F2018%2F02%2F05%2FMac%E4%B8%8A%E4%BD%BF%E7%94%A8sublime%E7%BC%96%E8%BE%91LaTex%2F</url>
    <content type="text"><![CDATA[Abstract：LaTex是科技论文排版利器，sublime是一款轻量级编辑器，本文是关于如何在Mac上使用sublime编辑LaTex。 介绍sublime Text：一款简洁优雅的跨平台编辑器 LaTex 一款权威的科技论文排版软件，可写论文、处理文档工作、做幻灯片 相比Word，LaTeX最大的优势是对于复杂公式的编辑与排版非常漂亮。并且用简单的命令就可以生成脚注、索引、目录和参考文献等复杂的结构 skim：一款免费轻量的PDF阅读、标注工具 操作步骤1.安装MacTeX 在MacTeX官网上下载MacTeX.pkg文件，约2GB 下载完成后，进行安装 2.安装Sublime Text 在Sublime Text官网](http://www.sublimetext.com/)上下载最新版本的Sublime Text 3.在Sublime Text中安装Package Control 具体方法可google 4.安装LaTexTools 按cmd+shift+P打开命令托盘 在命令托盘输入install package，按enter回车 输入LaTexTools，找到这一项并再次回车安装 重启sublime Text 在菜单栏选择：Tools &gt; Build System &gt; LaTex 5.安装和配置skim 在Skim上下载Skim并安装 打开Skim，在菜单栏中Skim &gt; Preference(选项) &gt; Sync(同步) 在预设菜单中选择Sublime Text 6.编译LaTex文件 打开sublime，cmd+N新建文件，编辑LaTex代码 完成编辑后，cmd+S保存文件 cmd+B编译并运行，即可看到PDF预览 在预览的 PDF 中，若需要修改某部分内容，先在键盘上按下 shift + ⌘ ，然后鼠标点击需要修改的部分即可跳转到 Subime Text 中对应的内容；当你修改后再次 build 后 Skim 会询问你是否自动刷新，选择自动即可]]></content>
      <categories>
        <category>Artificial Intelligence</category>
      </categories>
      <tags>
        <tag>LaTex</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac系统安装xgboost]]></title>
    <url>%2F2018%2F02%2F04%2FMac%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85xgboost%2F</url>
    <content type="text"><![CDATA[Abstract：Xgboost是大规模并行boosted tree的工具，本文是关于在Mac系统下如何安装XGBoost。 XgboosteXtreme Gradient Boosting Xgboost是大规模并行boosted tree的工具，它是目前最快最好的开源boosted tree工具包，比常见的工具包快10倍以上。 安装步骤1.安装homebrew (Mac系统下一个非常优秀的包管理工具) 1/usr/bin/ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot; 2.安装最新版本的gcc Mac上是没有gcc和g++的, 默认的是clang(安装XCode之后会有).但是XCode自带的clang是不支持OpenMP的.所以我们要自己安装gcc 1brew install gcc --without-multilib 3.下载xgboost的git源码 12git clone --recursive https://github.com/dmlc/xgboost cd xgboost 4.修改配置文件，用于编译 1cp make/config.mk ./config.mk 用vim打开config.mk，修改下面两行： 12export CC = gcc-6export CXX = g++-6 5.编译 1./build.sh 6.安装python版Xgboost 12cd python-packagesudo python setup.py install 7.打开python解释器进行验证，若无报错则安装成功 1&gt;&gt;&gt;import xgboost as xgb 使用12345678910import xgboost as xgb# read in datadtrain = xgb.DMatrix('demo/data/agaricus.txt.train')dtest = xgb.DMatrix('demo/data/agaricus.txt.test')# specify parameters via mapparam = &#123;'max_depth':2, 'eta':1, 'silent':1, 'objective':'binary:logistic' &#125;num_round = 2bst = xgb.train(param, dtrain, num_round)# make predictionpreds = bst.predict(dtest) ReferenceXgboost官网 Xgboost Mac系统安装Xgboost]]></content>
      <categories>
        <category>Artificial Intelligence</category>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Artificial Intelligence</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac系统下Anaconda安装及使用]]></title>
    <url>%2F2018%2F02%2F03%2FMac%E7%B3%BB%E7%BB%9F%E4%B8%8BAnaconda%E5%AE%89%E8%A3%85%E5%8F%8A%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Abstract：anaconda是一个可用于科学计算的python发行版，使用conda进行包管理和环境管理。本文是关于Mac系统下如何进行Anaconda的安装及使用。 Anacondaanaconda是一个可用于科学计算的python发行版，支持Linux、Unix、Windows系统，内置常用科学计算包。anaconda使用conda进行包管理和环境管理，解决官方python的2大痛点： 包管理功能：功能类似pip 环境管理功能：功能类似 Virtualenv，解决多版本python并存、切换的问题 软件发行版: 在系统上提前编译和配置好的软件包集合， 装好了后就可以直接用。 包管理器是自动化软件安装，更新，卸载的一种工具。 Anaconda内置多项应用 Anaconda Navigator：用于管理工具包和环境的图形用户界面，众多管理命令也可以在 Navigator 中手工实现 Jupyter notebook ：基于web的交互式计算环境，可以编辑易于人们阅读的文档，用于展示数据分析的过程 qtconsole ：一个可执行 IPython 的仿终端图形界面程序，相比 Python Shell 界面，qtconsole 可以直接显示代码生成的图形，实现多行代码输入执行，以及内置许多有用的功能和函数 spyder ：一个使用Python语言、跨平台的、科学运算集成开发环境 ​ pip与conda Conda 和 pip 目标不相同， 只有小部分子集有交集有竞争关系：比如python包的安装和环境隔离 pip可以允许你在任何环境中安装python包，而conda允许你在conda环境中安装任何语言包（包括c语言或者python） 如果你想在一个已有系统快速管理python包，那你应该选择pip，因为conda应该在conda环境中使用，而pip鼓励在任何环境中使用 。而如果，你想要让许多依赖库一起很好地工作（比如数据分析中的Numpy，scipy，Matplotlib等等）那你就应该使用conda，conda很好地整合了包之间的互相依赖 pip只是一个包管理器，所以它不能为你管理环境，pip甚至不能升级python，因为它不像conda一样把python当做包来处理；但是它可以安装一些conda安装不了的包 安装直接在官网下载安装包，选择对应的python3.6 / python2.7的安装包进行下载，下载完成后直接安装，安装时选择默认配置。 conda使用安装成功后，conda会默认加入到环境变量中，故可直接在命令行界面运行命令conda conda的环境管理操作类似 Virtualenv 1234567891011121314151617181920212223242526# 查看帮助 conda -h# 基于python3.6版本创建一个名字为python36的环境conda create --name python36 python=3.6 # 激活此环境activate python36 source activate python36 # linux/mac# 再来检查python版本，显示是 3.6python -V # 退出当前环境deactivate python36 source deactivate python36 # linux/mac# 删除该环境conda remove -n python36 --all# 或者 conda env remove -n python36# 查看所有安装的环境conda info -epython36 * D:\Programs\Anaconda3\envs\python36root D:\Programs\Anaconda3 conda的包管理操作类似pip, 一定要在当前python下安装工具包 1234567891011121314151617181920# (在当前活跃环境下)安装 matplotlib conda install matplotlib# (在指定环境下)安装packageconda install -n python36 numpy# 查看已安装的包conda list # 包更新conda update matplotlib# 删除包conda remove matplotlib# 查找package信息conda search numpy# 查看某个指定环境下的已安装包conda list -n python36 在 conda 中 anything is a package。conda 本身可以看作是一个包，python 环境可以看作是一个包，anaconda 也可以看作是一个包，因此除了普通的第三方包支持更新之外，这3个包也支持。比如： 12345678# 更新conda本身conda update conda# 更新anaconda 应用conda update anaconda# 更新python，假设当前python环境是3.6.1，而最新版本是3.6.2，那么就会升级到3.6.2conda update python 修改镜像地址Anaconda 的镜像地址默认在国外，用 conda 安装包的时候会很慢，目前可用的国内镜像源地址有清华大学的。修改 ~/.condarc (Linux/Mac) 或 C:\Users\当前用户名.condarc (Windows) 配置： 1234channels: - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ - defaultsshow_channel_urls: true 环境搭建好之后就可以开始愉快地玩数据分析了。]]></content>
      <categories>
        <category>Artificial Intelligence</category>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Anaconda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PyTorch安装、使用、学习指南]]></title>
    <url>%2F2018%2F02%2F02%2FPyTorch%E5%AE%89%E8%A3%85%E3%80%81%E4%BD%BF%E7%94%A8%E3%80%81%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[Abstract：PyTorch是一个python优先的非常优秀的深度学习框架，本文是个人总结的关于如何安装、使用和学习PyTorch的指南。 1.PyTorch介绍Torch是一个科学计算框架，广泛支持机器学习算法，将GPU放在第一位。由于使用简单快速的脚本语言LuaJIT以及底层的C / CUDA实现，因此易于使用和高效。 核心功能： 一个强大的N维数组 许多例行索引，切片，转置，… 通过LuaJIT向C提供了惊人的界面 线性代数程序 神经网络和能量模型 数字优化例程 快速高效的GPU支持 可嵌入，具有iOS，Android和FPGA后端的端口 ​ Pytorch python优先的深度学习框架，使用CPU和GPU优化的深度学习张量库，能在强大GPU加速基础上实现张量和动态神经网络 PyTorch 和 TensorFlow、MXNet、Caffe2 一样，是非常底层的框架；也正如 TensorFlow 是谷歌官方框架，MXNet 是亚马逊官方框架，背后支持 PyTorch 的则是 Facebook python软件包：提供了两种高层面的功能： 1.使用强大的 GPU 加速的 Tensor 计算（类似 numpy） 2.构建于基于 tape 的 autograd 系统的深度神经网络。通常，人们使用 PyTorch 的原因通常有二：1. 作为 numpy 的替代，以便使用强大的 GPU；2. 将其作为一个能提供最大的灵活性和速度的深度学习研究平台 社区驱动的项目：由经验丰富的工程师和研究者组成的 Torch7 团队开发 特点：快速成形、代码可读和支持最广泛的深度学习模型 2.安装步骤1.在终端里运行以下代码： 123sudo pip install http://download.pytorch.org/whl/torch-0.3.0.post4-cp27-none-macosx_10_6_x86_64.whl sudo pip install torchvision 其他系统参见PyTorch官网上，在官网上找到下面图片的对应处，勾选好系统、包管理工具、python版本、是否支持CUDA，再在终端运行其下出现的代码即可安装。 安装PyTorch会安装2个模块，一个是torch，是主模块，用来搭建神经网络；另一个是torchversion，是辅模块，有数据库和一些已经训练好的神经网络（如VGG、AlexNet、ResNet）。 2.测试安装是否成功：在python编译器里输入以下代码 1import torch 3.学习步骤Step1：文档 60分钟的入门tutorials + jcjohnson 的Simple examples to introduce PyTorch Step2：example 参考 pytorch/examples 实现一个最简单的例子(比如训练mnist ) Step3：通读doc PyTorch doc 尤其是autograd的机制，和nn.module ,optim 等相关内容。文档现在已经很完善，而且绝大部分文档都是作者亲自写的，质量很高 Step4：论坛讨论 PyTorch Forums 。论坛很活跃，而且质量很高，pytorch的维护者(作者)回帖很及时的。每天刷一刷帖可以少走很多弯路，避开许多陷阱,消除很多思维惯性.尤其看看那些阅读量高的贴，刷帖能从作者那里学会如何写出bug-free clean and elegant 的代码。如果自己遇到问题可以先搜索一下，一般都能找到解决方案，找不到的话大胆提问，大家都很热心的。 Step5：阅读源代码 fork pytorch，pytorch-vision等。相比其他框架，pytorch代码量不大，而且抽象层次没有那么多，很容易读懂的。通过阅读代码可以了解函数和类的机制，此外它的很多函数,模型,模块的实现方法都如教科书般经典。还可以关注官方仓库的issue/pull request, 了解pytorch开发进展，以及避坑。 Others Tipps： 还可以加入 slack群组讨论，e-mail订阅 chenyuntc/pytorch-book 写的教程，里面还有很多有趣的例子，比如用GAN生成动漫头像，用CharRNN写唐诗，类Prisma的滤镜（风格迁移）和图像描述等 关于如何照着example实现简单的例子, 我的做法是认真看几遍example的实现代码.理解透,然后自己从头写, 实现相同的模型, 实在卡住了写不下去可以看一下, 但是绝对不能copy and paste. 当你实现了一个简单的例子(比如tutorial 的 mnist) 基本上对pytorch的主要内容都有了大概的了解. 写的时候会涉及 dataset,nn.module, optim, loss等许多模块, 也算是加深理解. 一些其它的例子: 50行实现GAN devnag/pytorch pytorch 资源合集 The Incredible PyTorch 加强版pytorch tutorial侧重NLP spro/practical-pytorch 利用LSTM学习梯度下降法等优化方法：ikostrikov/pytorch-meta-optimizer: A PyTorch implementation of Learning to learn by gradient descent by gradient descent WGAN的官方实现 martinarjovsky/WassersteinGAN ​]]></content>
      <categories>
        <category>Artificial Intelligence</category>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Artificial Intelligence</tag>
        <tag>Machine Learning</tag>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sublime-x-Anancoda-x-机器学习开发配置]]></title>
    <url>%2F2018%2F02%2F01%2FSublime-x-Anancoda-x-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%BC%80%E5%8F%91%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[Abstract: 关于如何使用Sublime和Anaconda插件进行机器学习开发配置。 Error在使用sublime编译python文件时，出现无法import已安装在系统中的tensorflow等工具包的情况，报错如下： 1ModuleNotFoundError: No module named &apos;tensorflow&apos; Solution报错缘由应该是sublime原有的python编译环境使用的python版本与当前活跃的python版本不是同一个，故无法import当前python版本里的工具包。解决办法是新配置一个当前python版本的编译环境，操作如下： 1.在终端里当前活跃python环境里输入以下命令，以获取当前python的路径： 1which python 获得的路径形式如下: 1/Users/scarlett/anaconda3/envs/python36/bin/python 2.打开Sublime-Tools——&gt;BuildSystem——&gt;New Build System, 打开窗口，将窗口里的信息改成如下代码，然后保存为Python36.sublime-build 123&#123; &quot;cmd&quot;: [&quot;/Users/scarlett/anaconda3/envs/python36/bin/python&quot;,&quot;-u&quot;,&quot;$file&quot;],&#125; 3.查看Build System列表，选择python36，然后再重新编译原报错的python代码，若无报错，则新编译环境配置成功。]]></content>
      <categories>
        <category>Artificial Intelligence</category>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Anaconda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IMDB电影数据分析实践]]></title>
    <url>%2F2018%2F01%2F31%2FIMDB%E7%94%B5%E5%BD%B1%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[Abstract: IMDB电影数据分析练习。 1.项目简介数据集 包含来自MovieLens 电影推荐服务的5星评分和文本标记数据和来自IMDB1950-2012年IMDB TOP10000排行榜数据 MovieLens数据集包含27278部电影的20000263份评分和465564次标签应用 ​ 实践内容 1.什么样题材的电影评分会相对较高（较低） 2.电影时长对评分是否有影响 3.不同年代什么类型电影较受欢迎 4.其他自选角度 2.数据整理步骤1.构建数据框：理想情况下，把所有数据放入这个数据框中 2.清洗数据：对构建的数据框进行数据清理，它应该具有以下属性: Each row describes a single object Each column describes a property of that object Columns are numeric whenever appropriate ​ 3.探索全局特征：通过直方图，散点图，聚合函数等获得一个数据的全局的了解 4.探索分组特征。通过一些分组操作分析数据集 3.实践3.1 数据整理与探索12345678910111213141516171819202122232425262728293031323334%matplotlib inlineimport matplotlib.pyplot as pltimport pandas as pdimport numpy as np#tell pandas to display wide tables as pretty HTML tablespd.set_option('display.width', 500)pd.set_option('display.max_columns', 100)def remove_border(axes=None, top=False, right=False, left=True, bottom=True): """ Minimize chartjunk by stripping out unnecesasry plot borders and axis ticks The top/right/left/bottom keywords toggle whether the corresponding plot border is drawn """ ax = axes or plt.gca() ax.spines['top'].set_visible(top) ax.spines['right'].set_visible(right) ax.spines['left'].set_visible(left) ax.spines['bottom'].set_visible(bottom) #turn off all ticks ax.yaxis.set_ticks_position('none') ax.xaxis.set_ticks_position('none') #now re-enable visibles if top: ax.xaxis.tick_top() if bottom: ax.xaxis.tick_bottom() if left: ax.yaxis.tick_left() if right: ax.yaxis.tick_right() 3.2 数据分析123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120# 1.构建数据框data=pd.read_csv("../input/movielens/imdb10000.csv")print data.head() # print the first 5 rows# 2.清洗数据# 数据的问题：1.电影时长：改字符串类型为数值；2.电影流派不是原子数据，很难提取特定的流派做分析；3.上映年份在名称和时间上是重复的# 2.1 修正电影时长(列表解析法)clean_runtime = [float(r.split(' ')[0] for r in data.runtime)]data['runtime'] = clean_runtimeprint data.head()# 2.2 分割流派信息(使用指示变量的概念将流派列分割成许多列。每个新的列将对应于一个单一的流派，每个单元格将为True或False)# determine the unique genresgenres = set()for m in data.genres: genres.update(g for g in m.split('|')) genres = sorted(genres)# make a column for each genrefor genre in genres: data[genre] = [genre in movie.split('|') for movie in data.genres]print data.head()genres = set()# 2.3 从名称中移除年份信息data['title']=[t[0:-6] for t in data.title]print data.head()# 3.探索全局特征print data[['score','runtime','year','votes']].describe()# 4.发现损坏数据并清洗# 看电影时长为0的有多少个print ((len(data[data.runtime==0])))# 标记为nandata.runtime[data.runtime==0]=np.nan# 5.探索局部特征(通过一些基本的可视化)# 5.1 分数与年份的变化关系plt.hist(data.score, bins=30, color='#000000')plt.xlabel('Release Year')remove_border()plt.show()# 5.2 Runtime distributionplt.hist(data.runtime, bins=50, color='#000000')plt.xlabel('Runtime distribution')plt.show()# 5.3 IMDB Rating# plt.scatter():绘制散点图plt.scatter(data.year,data.score,lw=0,alpha=.08,color='k')plt.xlabel("Year")plt.ylabel("IMDB Rating")remove_border()# 6.寻找异常点# 6.1 评价较低但投票数高print data[(data.votes &gt; 9e4)] &amp; data[(data.score &lt; 5)][['title','year','score','votes','genres']]# 6.2 最低评分电影print data[data.score == data.score.min()][['title','year','score','votes','genres']]# 6.3 最高评分电影print data[data.score == data.score.max()][['title','year','score','votes','genres']]# 7.对一些行或列，使用聚合函数如 sum 进行分析# 7.1 哪些流派出现频次最高？print genre_count = np.sort(data[genres].sum())[::-1]pd.DataFrame(&#123;'genre count':genre_count&#125;)# 7.2 平均一部电影有多少个流派标记？genre_count = data[genres].sum(axis=1)print (("average movie has %0.2f genres" % genre_count.mean()))print genre_count.describe()# 8.探索分组特征# 8.1 将电影按年代划分decade = (data.year // 10)*10tyd = data[['title','year']]tyd['decade'] = decadeprint tyd.head()# 8.2 将电影按年代分组(groupby)decade_mean = data.groupby(decade).score.mean()decade_mean.name = 'Decade Mean'print(decade_mean)plt.plot(decade_mean.index, decade_mean.values, 'o-', color='r', lw=3, label='Decade Average')plt.scatter(data.year, data.score, alpha=.04, lw=0, color='k')plt.xlabel("Year")plt.ylabel("Score")plt.legend(frameon=False)remove_border()# 8.3 看每年评分的分散情况grouped_scores = data.groupby(decade).scoremean = grouped_scores.mean()std = grouped_scores.std()plt.plot(decade_mean.index, decade_mean.values, 'o-',color='r', lw=3, label='Decade Average')plt.fill_between(decade_mean.index, (decade_mean + std).values, (decade_mean - std).values, color='r', alpha=.2)plt.scatter(data.year, data.score, alpha=.04, lw=0, color='k')plt.xlabel("Year")plt.ylabel("Score")plt.legend(frameon=False)remove_border()# 9.small multiples# 9.1 按流派划分数据，看发行时间、时长、评分如何分布fig, axes = plt.subplots(nrows=4,ncols=6,figsize=(12,8),tight_layout=True)bins = np.arange(1950, 2013, 3)for ax, genre in zip(axes.ravel(), genres): ax.hist(data[data[genre] == 1].year,bins=bins, histtype='stepfilled', normed=True, color='r', alpha=.3, ec='none') ax.hist(data.year, bins=bins, histtype='stepfilled', ec='None', normed=True, zorder=0, color='#cccccc') ax.annotate(genre, xy=(1955, 3e-2), fontsize=14) ax.xaxis.set_ticks(np.arange(1950, 2013, 30)) ax.set_yticks([]) remove_border(ax, left=False) ax.set_xlabel('Year')]]></content>
      <categories>
        <category>Computer Science</category>
        <category>Data Mining and Analysis</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jupyter Notebook新手使用教程]]></title>
    <url>%2F2018%2F01%2F29%2FJupyter-Notebook%E6%96%B0%E6%89%8B%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[Abstract：jupyter notebook新手入门指南。 入门指南1.终端运行jupyter notebook命令，会自动打开网页 2.新建一个notebook：点击new，选择你希望启动的notebook类型（本文新建一个.py文件为例） 3.在单元格中可输入任意代码并执行，如输入1+2并按下shift+Enter，则单元格中的代码会被计算，光标也会被移动到一个新的单元格中 4.插入其他类型的单元格：Insert——&gt;Insert Cell Above; Cell——&gt;Cell Type——&gt;Markdown，输入markdown格式的文本，然后Cell——&gt;Run Cells，即可看到渲染后的解释文本 5.单元格操作： 删除：Edit——&gt;Delete Cells 移动：Edit——&gt;Move cell [up|down] 剪贴：Edit——&gt;Cut Cell / Paste Cell 合并：Edit——&gt;Merge Cell[Above|below] 6.markdown高级用法： 执行HTML代码： 1&lt;img src=&quot;https://gss2.bdstatic.com/-fo3dSag_xI4khGkpoWK1HF6hhy/baike/w%3D400/sign=d1db22fb45a7d933bfa8e5739d4ad194/4bed2e738bd4b31c59698c5480d6277f9f2ff88c.jpg&quot;&gt; 支持LaTex语法： 1$$\int_0^&#123;+\infty&#125; x^2 dx$$ 7.导出功能：File——&gt;Download as（可导出成多种格式：HTML，Markdown，ReST，PDF，Raw Python） 8.Matplotlib集成： 1pip install matplotlib 1%matplotlib inline 1234567import matplotlib.pyplot as pltimport numpy as npx = np.arange(20)y = x**2plt.plot(x, y) 以上是绘制方程式y=x_2的代码。 9.远程访问 默认jupyter是本地访问的，若要远程访问，可通过修改配置文件的方式实现，具体操作可参照Jupyter notebook在mac:linux上的配置和远程访问 Referencejupyter快速入门]]></content>
      <categories>
        <category>Artificial Intelligence</category>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Artificial Intelligence</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac系统jupyter notebook报错解决方法]]></title>
    <url>%2F2018%2F01%2F29%2FMac%E7%B3%BB%E7%BB%9Fjupyter-notebook%E6%8A%A5%E9%94%99%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[Abstract：Mac系统jupyter notebook报错解决方法，Error为execution error。 Error: execution error安装jupyter notebook，执行jupyter notebook命令后，出现报错信息： 10:42:execution error: “&quot;http://localhost:8891/tree&quot;”不理解“open location”信息。 (-1708) 解决方法1.终端进入jupyter的配置目录 12345Users/scarlett/.jupyter# or~/.jupyter 2.进入jupyter_notebook_config.py的vim编辑模式 1vim jupyter_notebook_config.py 3.在jupyter_notebook_config.py文件中加入3行代码 123c.NotebookApp.browser = u&apos;Safari&apos;c.NotebookApp.token = &apos;&apos;c.NotebookApp.password = &apos;&apos; 4.保存文件，重新执行 1jupyter notebook]]></content>
      <categories>
        <category>Artificial Intelligence</category>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Artificial Intelligence</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac系统jupyter notebook安装指南]]></title>
    <url>%2F2018%2F01%2F29%2FMac%E7%B3%BB%E7%BB%9Fjupyter-notebook%E5%AE%89%E8%A3%85%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[Abstract：Mac系统jupyter notebook安装指南。 jupyter notebookJupyter Notebook（此前被称为 IPython notebook）是一个交互式笔记本，支持运行 40 多种编程语言。 安装步骤前提：系统里已安装python和pip。 1.pip安装jupyter 12345# pipsudo pip install jupyter# pip2sudo pip3 install jupyter 2.安装完成后输入以下指令启动jupyter网页服务；或在浏览器地址栏输入localhost:8888访问主页 1jupyter notebook]]></content>
      <categories>
        <category>Artificial Intelligence</category>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Artificial Intelligence</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习开发环境：PyTorch安装指南]]></title>
    <url>%2F2018%2F01%2F29%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%EF%BC%9APyTorch%E5%AE%89%E8%A3%85%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[Abstract：机器学习开发环境：PyTorch安装指南。 PyTorchTorch是一个科学计算框架，广泛支持机器学习算法，将GPU放在第一位。由于使用简单快速的脚本语言LuaJIT以及底层的C / CUDA实现，因此易于使用和高效。 核心功能： 一个强大的N维数组 许多例行索引，切片，转置，… 通过LuaJIT向C提供了惊人的界面 线性代数程序 神经网络和能量模型 数字优化例程 快速高效的GPU支持 可嵌入，具有iOS，Android和FPGA后端的端口 安装步骤1.在终端里运行以下代码： 123sudo pip install http://download.pytorch.org/whl/torch-0.3.0.post4-cp27-none-macosx_10_6_x86_64.whl sudo pip install torchvision 其他系统参见PyTorch官网上，在官网上找到下面图片的对应处，勾选好系统、包管理工具、python版本、是否支持CUDA，再在终端运行其下出现的代码即可安装。 安装PyTorch会安装2个模块，一个是torch，是主模块，用来搭建神经网络；另一个是torchversion，是辅模块，有数据库和一些已经训练好的神经网络（如VGG、AlexNet、ResNet）。 2.测试安装是否成功：在python编译器里输入以下代码 1import torch 学习资料Torch中文文档]]></content>
      <categories>
        <category>Artificial Intelligence</category>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Artificial Intelligence</tag>
        <tag>Machine Learning</tag>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib入门实战全教程]]></title>
    <url>%2F2018%2F01%2F29%2Fmatplotlib%E5%85%A5%E9%97%A8%E5%AE%9E%E6%88%98%E5%85%A8%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[Abstract：matplotlib入门实战全教程，教你如何绘制条形图、饼图、箱线图，附代码和相关学习资料。 1.MatplotlibMatplotlib的设计理念是能够用轻松简单的方式生成强大的可视化效果，是Python学习过程中核心库之一。 用在python中绘制数组的2D图形库 matplotlib代码在概念上分为3个部分： 1.pylab接口是由matplotlib.pylab提供的函数集，允许用户使用非常类似于MATLAB图生成代码的代码创建绘图 2.matplotlib前端或API是一组重要的类，可创建和管理图形，文本，线条，图表等（艺术家教程），是一个对输出无所了解的抽象接口 3.后端是设备相关的绘图设备，也称为渲染器，将前端表示转换为打印件或显示设备；后端示例：PS 创建 PostScript® 打印件，SVG 创建可缩放矢量图形打印件，Agg 使用 Matplotlib 附带的高质量反颗粒几何库创建 PNG 输出，GTK 在 Gtk+ 应用程序中嵌入 Matplotlib，GTKAgg 使用反颗粒渲染器创建图形并将其嵌入到 Gtk+ 应用程序中，以及用于 PDF，WxWidgets，Tkinter 等 2.绘制条形图（Bar Chart）条形图实际上是用来表示分组（或离散）变量的可视化，可以使用matplotlib模块中的bar函数完成条形图的绘制。 2.1 简单垂直条形图Example1: 2017世界国家GDP排名（见下图），我们需要用matplotlib绘制展现以下数据的条形图。后端示例：PS 创建 PostScript® 打印件，SVG 创建可缩放矢量图形打印件，Agg 使用 Matplotlib 附带的高质量反颗粒几何库创建 PNG 输出，GTK 在 Gtk+ 应用程序中嵌入 Matplotlib，GTKAgg 使用反颗粒渲染器创建图形并将其嵌入到 Gtk+ 应用程序中，以及用于 PDF，WxWidgets，Tkinter 等 1234567891011121314151617181920212223242526# 导入绘图模块import matplotlib.pyplot as plt# 构建数据GDP = [12406.8,13908.57,9386.87,9143.64]# 中文乱码的处理plt.rcParams['font.sans-serif'] =['Microsoft YaHei']plt.rcParams['axes.unicode_minus'] = False# 绘图plt.bar(range(4), GDP, align = 'center',color='steelblue', alpha = 0.8)# 添加轴标签plt.ylabel('GDP')# 添加标题plt.title('四个直辖市GDP大比拼')# 添加刻度标签plt.xticks(range(4),['北京市','上海市','天津市','重庆市'])# 设置Y轴的刻度范围plt.ylim([5000,15000])# 为每个条形图添加数值标签for x,y in enumerate(GDP): plt.text(x,y+100,'%s' %round(y,1),ha='center') # 显示图形plt.show() 以上是绘制图形的代码，运行代码得到的条形图如下： Example2：2017中国城市GDP排名（见下图），同样我们用matplotlib绘制展现其数据的条形图。 123456789101112# World GDP RankGDP = [185691, 112182.8, 49386.4, 34666.3]plt.rcParams['font.sans-serif'] = ['Microsoft YaHei']plt.rcParams['axes.unicode_minus'] = Falseplt.bar(range(4), GDP, align='center',color='purple',alpha=0.5)plt.ylabel('GDP')plt.title('2017 World GDP Rank')plt.xticks(range(4), ['USA', 'China', 'Japan', 'Germany'])plt.ylim([10000, 200000])for x,y in enumerate(GDP): plt.text(x,y+100,'%s' %round(y,1),ha='center')plt.show() 以上是绘制图形的代码，运行代码得到的条形图如下： 代码解释： rcParams：由于matplotlib对中文的支持并不是很友好，所以需要提前对绘图进行字体的设置，即通过rcParams来设置字体，这里将字体设置为微软雅黑，同时为了避免坐标轴不能正常的显示负号，也需要进行设置； bar函数指定了条形图的x轴、y轴值，设置x轴刻度标签为水平居中，条形图的填充色color为铁蓝色，同时设置透明度alpha为0.8； 添加y轴标签、标题、x轴刻度标签值，为了让条形图显示各柱体之间的差异，将y轴范围设置在5000~15000； 通过循环的方式，添加条形图的数值标签； 2.2 简单水平条形图Example3：同一本书不同平台最低价比较 很多人在买一本书的时候，都比较喜欢货比三家，例如《python数据分析实战》在亚马逊、当当网、中国图书网、京东和天猫的最低价格分别为39.5、39.9、45.4、38.9、33.34。针对这个数据，我们也可以通过条形图来完成，这里使用水平条形图来显示： 123456789101112# 3.Book Price Comparsionprice = [39.5, 39.9, 45.4, 38.9, 33.34]plt.rcParams['font.sans-serif'] = ['Microsoft YaHei']plt.rcParams['axes.unicode_minus'] = Falseplt.barh(range(5),price,align='center',color='black',alpha=0.5)plt.xlabel('price')plt.yticks(range(5),['Amazon','Dangdang','BooksChina','Jingdong','Tianmao'])plt.xlim([32,47])plt.title('Book Price Comparsion')for x,y in enumerate(price): plt.text(y+0.1,x,'%s' %y,va='center')plt.show() 绘制结果： 代码解读： 水平条形图的绘制与垂直条形图的绘制步骤一致，只是调用了barh函数来完成。需要注意的是，条形图的数值标签设置有一些不一样，需要将标签垂直居中显示，使用va参数即可。 Example4: 同家宠物店不同品种犬价格比较 Breed Husky（哈士奇） 39099 French Bulldog（法国斗牛犬） 5899 Golden Retriever（金毛寻回犬） 13599 Labrador Retriever（拉布拉多犬） 12399 Shiba Inu（日本柴犬） 68599 123456789101112# 4.Dog Price Comparsionprice = [39099, 5899, 13599, 12399, 68599]plt.rcParams['font.sans-serif'] = ['Microsoft YaHei']plt.rcParams['axes.unicode_minus'] = Falseplt.barh(range(5),price,align='center',color='red',alpha=0.5)plt.title('Dog Price Comparsion')plt.xlabel(price)plt.yticks(range(5),['Husky','French Bulldog','Golden Retriever','Labrador Retriever','Shiba Inu'])plt.xlim([3000,80000])for x,y in enumerate(price): plt.text(y+100,x,'%s' %y,va='center')plt.show() 绘制结果： 2.3 水平交错条形图以上的简单垂直和水平条形图是基于离散变量的情况，针对两种离散变量的条形图我们可以用水平交错条形图和堆叠条形图，下面看如何绘制： Example5：胡润财富榜：亿万资产超高净值家庭数 利用水平交错条形图对比2016年和2017年亿万资产超高净值家庭数（top5） 1234567891011121314151617181920212223242526# 5.胡润财富榜：亿万资产超高净值家庭数/Millions Family Amount Top5 City Distributionimport numpy as np Y2016 = [15600,12700,11300,4270,3620]Y2017 = [17400,14800,12000,5200,4020]labels = ['Beijing','Shanghai','Hongkong','Shenzhen','Guangzhou']bar_width = 0.5plt.rcParams['font.sans-serif'] =['Microsoft YaHei']plt.rcParams['axes.unicode_minus'] = Falseplt.bar(np.arange(5),Y2016,label='2016',color='steelblue',alpha=0.8,width=bar_width)plt.bar(np.arange(5) + bar_width, Y2017, label='2017', color='indianred',alpha=0.8,width=bar_width)plt.xlabel('Top5 City')plt.ylabel('Family Amount')plt.xticks(np.arange(5)+bar_width,labels)plt.ylim([2500,20000])plt.title('Millions Family Amount Top5 City Distribution')# 为每个条形图添加数值标签for x2016,y2016 in enumerate(Y2016): plt.text(x2016,y2016+100,'%s'%y2016)for x2017,y2017 in enumerate(Y2017): plt.text(x2017+bar_width, y2017+100, '%s' %y2017)# 显示图例plt.legend()plt.show() 代码解读 水平交错条形图绘制的思想很简单，就是在第一个条形图绘制好的基础上，往左移一定的距离，再去绘制第二个条形图，所以在代码中会出现两个bar函数； 图例的绘制需要在bar函数中添加label参数；color和alpha参数分别代表条形图的填充色和透明度； 给条形图添加数值标签，同样需要使用两次for循环的方式实现； 绘制图形： 3.饼图（Pie Chart）3.1 饼图条形图：离散变量的分布呈现 饼图：离散变量各水平占比情况（使用matplotlib库里的pie函数绘制） 3.2 pie函数参数解读1plt.pie(x, explode=None, labels=None, colors=None, pctdistance=0.6, shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, wedgeprops=None, textprops=None, center=(0,0), frame=False) x: 指定绘图的数据 explode:指定饼图某些部分的突出显示，即呈现爆炸式 labels：为饼图添加标签说明，类似于图例说明 colors：指定饼图的填充色 autopct：设置百分比格式，如’%.1f%%’为保留一位小数 shadow：是否添加饼图的阴影效果 pctdistance:设置百分比标签与圆心的距离 labeldistance：设置各扇形标签（图例）与圆心的距离； startangle：设置饼图的初始摆放角度, 180为水平； radius：设置饼图的半径大小； counterclock：是否让饼图按逆时针顺序呈现, True / False； wedgeprops：设置饼图内外边界的属性，如边界线的粗细、颜色等, 如wedgeprops = {‘linewidth’: 1.5, ‘edgecolor’:’green’} textprops：设置饼图中文本的属性，如字体大小、颜色等； center：指定饼图的中心点位置，默认为原点 frame：是否要显示饼图背后的图框，如果设置为True的话，需要同时控制图框x轴、y轴的范围和饼图的中心位置； 3.3 绘制饼图Example6：芝麻信用失信用户分析 我们使用芝麻信用近300万失信人群的样本统计数据来绘制饼图，该数据显示，从受教育水平上来看，中专占比25.15%，大专占比37.24%，本科占比33.36%，硕士占比3.68%，剩余的其他学历占比0.57%。对于这样一组数据，我们该如何使用饼图来呈现呢？ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# 6.Zhima Credit Discredited User Analysisimport matplotlib.pyplot as plt# 设置绘图的主题风格（不妨使用R中的ggplot分隔）plt.style.use('ggplot')# 构造数据edu = [0.2515,0.3724,0.3336,0.0368,0.0057]labels = ['Secondary','Junior College','Bachelor','Master','Others']explode = [0,0.1,0,0,0] # 用于突出显示大专学历人群colors=['#FEB748','#EDD25D','#FE4F54','#51B4FF','#dd5555'] # 自定义颜色# 中文乱码和坐标轴负号的处理plt.rcParams['font.sans-serif'] = ['Microsoft YaHei']plt.rcParams['axes.unicode_minus'] = False# 将横、纵坐标轴标准化处理，保证饼图是一个正圆，否则为椭圆plt.axes(aspect='equal')# 控制x轴和y轴的范围plt.xlim(0,4)plt.ylim(0,4)# 绘制饼图plt.pie(x = edu, # 绘图数据 explode=explode, # 突出显示大专人群 labels=labels, # 添加教育水平标签 colors=colors, # 设置饼图的自定义填充色 autopct='%.1f%%', # 设置百分比的格式，这里保留一位小数 pctdistance=0.8, # 设置百分比标签与圆心的距离 labeldistance = 1.15, # 设置教育水平标签与圆心的距离 startangle = 180, # 设置饼图的初始角度 radius = 1.5, # 设置饼图的半径 counterclock = False, # 是否逆时针，这里设置为顺时针方向 wedgeprops = &#123;'linewidth': 1.5, 'edgecolor':'green'&#125;,# 设置饼图内外边界的属性值 textprops = &#123;'fontsize':12, 'color':'k'&#125;, # 设置文本标签的属性值 center = (1.8,1.8), # 设置饼图的原点 frame = 1)# 是否显示饼图的图框，这里设置显示# 删除x轴和y轴的刻度plt.xticks(())plt.yticks(())# 添加图标题plt.title('Zhima Credit Discredited User Analysis')# 显示图形plt.show() 绘制图形： 4.箱线图（Boxplot）4.1 箱线图数值型变量可视化：箱线图、直方图、折线图、面积图、散点图等等。 箱线图一般用来展现数据的分布（如上下四分位值、中位数等），同时，也可以用箱线图来反映数据的异常情况。 4.2 boxplot函数的参数解读matplotlib包中boxplot函数的参数含义及使用方法： 1plt.boxplot(x, notch=None, sym=None, vert=None, whis=None, positions=None, widths=None, patch_artist=None, meanline=None, showmeans=None, showcaps=None, showbox=None, showfliers=None, boxprops=None, labels=None, flierprops=None, medianprops=None, meanprops=None, capprops=None, whiskerprops=None) x：指定要绘制箱线图的数据； notch：是否是凹口的形式展现箱线图，默认非凹口； sym：指定异常点的形状，默认为+号显示； vert：是否需要将箱线图垂直摆放，默认垂直摆放； whis：指定上下须与上下四分位的距离，默认为1.5倍的四分位差； positions：指定箱线图的位置，默认为[0,1,2…]； widths：指定箱线图的宽度，默认为0.5； patch_artist：是否填充箱体的颜色； meanline：是否用线的形式表示均值，默认用点来表示； showmeans：是否显示均值，默认不显示； showcaps：是否显示箱线图顶端和末端的两条线，默认显示； showbox：是否显示箱线图的箱体，默认显示； showfliers：是否显示异常值，默认显示； boxprops：设置箱体的属性，如边框色，填充色等； labels：为箱线图添加标签，类似于图例的作用； filerprops：设置异常值的属性，如异常点的形状、大小、填充色等； medianprops：设置中位数的属性，如线的类型、粗细等； meanprops：设置均值的属性，如点的大小、颜色等； capprops：设置箱线图顶端和末端线条的属性，如颜色、粗细等； whiskerprops：设置须的属性，如颜色、粗细、线的类型等； 4.3 箱线图的绘制Example7: titanic不同等级仓位的年龄箱线图 1.整体乘客的年龄箱线图 123456789101112131415161718192021# 按舱级排序，为了后面正常显示分组盒形图的顺序titanic.sort_values(by = 'Pclass', inplace=True)# 通过for循环将不同仓位的年龄人群分别存储到列表Age变量中Age = []Levels = titanic.Pclass.unique()for Pclass in Levels: Age.append(titanic.loc[titanic.Pclass==Pclass,'Age'])# 绘图plt.boxplot(x = Age, patch_artist=True, labels = ['一等舱','二等舱','三等舱'], # 添加具体的标签名称 showmeans=True, boxprops = &#123;'color':'black','facecolor':'#9999ff'&#125;, flierprops = &#123;'marker':'o','markerfacecolor':'red','color':'black'&#125;, meanprops = &#123;'marker':'D','markerfacecolor':'indianred'&#125;, medianprops = &#123;'linestyle':'--','color':'orange'&#125;)# 显示图形plt.show() 绘制图形： 结果分析： 对于所有乘客而言，从图中容易发现，乘客的平均年龄在30岁，有四分之一的人低于20岁，另有四分之一的人超过38岁，换句话说，有一半的人，年龄落在20~38岁之间；从均值（红色的菱形）略高于中位数（黄色虚线）来看，说明年龄是有偏的，并且是右偏；同时，我们也会发现一些红色的异常值，这些异常值的年龄均在64岁以上。 2.不同等级仓的年龄箱线图 123456789101112131415161718192021222324252627282930313233343536# 8.Different Levels of Cabintitanic.sort_values(bu='Pclass',inplace=True)Age = []Levels = titanic.Pclass.unique()for Pclass in Levels: Age.append(titanic.loc[titanic.Pclass==Pclass,'Age'])plt.boxplot(x=Age, patch_artist=True, labels=['First Class','Second Class','Third Class'], boxprops = &#123;'color':'black','facecolor':'#9999ff'&#125;, flierprops = &#123;'marker':'o','markerfacecolor':'red','color':'black'&#125;, meanprops = &#123;'marker':'D','markerfacecolor':'indianred'&#125;, medianprops = &#123;'linestyle':'--','color':'orange'&#125;)plt.show()# 按舱级排序，为了后面正常显示分组盒形图的顺序titanic.sort_values(by = 'Pclass', inplace=True)# 通过for循环将不同仓位的年龄人群分别存储到列表Age变量中Age = []Levels = titanic.Pclass.unique()for Pclass in Levels: Age.append(titanic.loc[titanic.Pclass==Pclass,'Age'])# 绘图plt.boxplot(x = Age, patch_artist=True, labels = ['First Class','Second Class','Third Class'], # 添加具体的标签名称 showmeans=True, boxprops = &#123;'color':'black','facecolor':'#9999ff'&#125;, flierprops = &#123;'marker':'o','markerfacecolor':'red','color':'black'&#125;, meanprops = &#123;'marker':'D','markerfacecolor':'indianred'&#125;, medianprops = &#123;'linestyle':'--','color':'orange'&#125;)# 显示图形plt.show() 绘制图形： 结果分析：如果对人群的年龄按不同的舱位来看，会发现一个明显的趋势，就是舱位等级越高的乘客，他们的年龄越高，三种舱位的平均年龄为38、30和25，说明年龄越是偏大一点，他们的经济能力会越强一些，所买的舱位等级可能就会越高一些。同时，在二等舱和三等舱内，乘客的年龄上存在一些异常用户。 学习资料Gitbook：pyplot教程 matplotlib图像教程 matplotlib艺术家教程 始终：Matplotlib 教程 Reference从零开始学Python【1】—matplotlib(条形图) 从零开始学Python【2】—matplotlib(饼图) 从零开始学Python【3】—matplotlib(箱线图)]]></content>
      <categories>
        <category>Computer Science</category>
        <category>Data Mining and Analysis</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac上TensorFlow安装指南]]></title>
    <url>%2F2018%2F01%2F29%2FMac%E4%B8%8ATensorFlow%E5%AE%89%E8%A3%85%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[Abstract: 本文手把手教你如何在Mac上安装TensorFlow，笔者使用的是pip安装方式。 TensorFlowTensorFlow是谷歌爸爸开发出的一个开源软件库，用于各种感知和语言理解任务的机器学习。 安装步骤1.安装pip：在终端里运行以下代码 12sudo easy_install --upgrade pipsudo easy_install --upgrade six 2.安装TensorFlow 12345# python2:$ pip install tensorflow# python3:$ pip3 install tensorflow 3.测试: 打开你的python编译器运行下面代码，检查是否有正确安装 1import tensorflow 4.在python编译器或终端里运行一个TensorFlow小程序 123456789# input:import tensorflowhello = tf.constant('Hello TensorFlow!')sess = tf.Session()print (sess.run(hello))# output:Hello TensorFlow! 5.若你需要更新TensorFlow版本，可先根据python版本在终端删除原有版本，然后运行以下代码，再重复以上步骤再次安装。 12345# python2:sudo pip uninstall tensorflow# python3:sudo pip3 uninstall tensorflow]]></content>
      <categories>
        <category>Artificial Intelligence</category>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Artificial Intelligence</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[numpy实战小练习集锦附代码]]></title>
    <url>%2F2018%2F01%2F29%2Fnumpy%E5%AE%9E%E6%88%98%E5%B0%8F%E7%BB%83%E4%B9%A0%E9%9B%86%E9%94%A6%E9%99%84%E4%BB%A3%E7%A0%81%2F</url>
    <content type="text"><![CDATA[Abstract：Numpy是Python做数据分析所必须要掌握的基础库之一。这篇练习通过89道题目带你快速玩转Numpy。 介绍Numpy是Python做数据分析所必须要掌握的基础库之一。这篇练习通过89道题目带你快速玩转Numpy。 练习123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648# coding:utf-8import numpy as np import pandas as pd # 1.Print the numpy version and the configurationprint (np.__version__)print np.show_config()# 2. Create a null vector of size 10Z = np.zeros(10)print Z # 3.Create a null vector of size 10 but the fifth value which is 1A = np.zeros(10)A[4] = 1print A # 4.Create a vector with values ranging from 10 to 49 A = np.arange(50)print A # 5.Reverse a vector (first element becomes last)A = np.arange(50)A = A[::-1]print A # 6.Create a 3x3 matrix with values ranging from 0 to 8Z = np.arange(9).reshape(3,3)print Z # 7.Find indices of non-zero elements from [1,2,0,0,4,0]nz = np.nonzero([1,2,0,0,4,0])print (nz)# 8.Create a 3x3 identity matrixA = np.eye(3)print A # 9.create a 3*3*3 array with random valuesA = np.random.random((3,3,3))print (Z)# 10.Create a 10x10 array with random values and find the minimum and maximum valuesZ = np.random.random((10,10))Zmin,Zmax = Z.min(), Z.max()print (Zmin,Zmax)# 11.Create a random vector of size 30 and find the mean valueA = np.random.random(30)m = A.mean()print (m)# 12.Create a 2d array with 1 on the border and 0 insideZ = np.ones((10,10))Z[1:-1,1:-1] = 0print(Z)# 13.How to add a border (filled with 0's) around an existing array?A = np.ones((5,5)) # ones(): 返回给定形状和类型的新数组，用数字填充A = np.pad(A,pad_width=1, mode='constant', constant_values=0) # pad():filling the arrayprint A # 14.What is the result of the following expression?print (0 * np.nan)print (np.nan == np.nan)print (np.nan - np.nan)print (0.3 == 3 * 0.1)# 15.Create a 5x5 matrix with values 1,2,3,4 just below the diagonalA = np.diag(1+np.arange(4),k=-1)print A # 16.Create a 8x8 matrix and fill it with a checkerboard patternA = np.zeros((8,8),dtype=int)A[1::2,::2] = 1A[::2,1::2] = 1print A # 17.Consider a (6,7,8) shape array, what is the index (x,y,z) of the 100th elementprint (np.unravel_index(100,(6,7,8))) # unravel_index(): 将平面索引的平面索引或数组转换为坐标数组的元组# 18.Create a checkerboard 8x8 matrix using the tile functionZ = np.tile( np.array([[0,1],[1,0]]), (4,4))print(Z)# 19.Normalize a 5x5 random matrixZ = np.random.random((5,5))Zmax, Zmin = Z.max(), Z.min()Z = (Z - Zmin)/(Zmax - Zmin)print(Z)# 20. Create a custom dtype that describes a color as four unsigned bytes (RGBA)color = np.dtype([("r", np.ubyte, 1), ("g", np.ubyte, 1), ("b", np.ubyte, 1), ("a", np.ubyte, 1)])# 21.Multiply a 5x3 matrix by a 3x2 matrix (real matrix product)A = np.dot(np.ones((5,3)),np.ones((3,2)))print A B = np.dot(np.ones((1,2)),np.ones((2,2)))print B # 22.Given a 1D array, negate all elements which are between 3 and 8, in place.A = np.arange(11)A[(3 &lt; A) &amp; (A &lt;= 8)] *= -1print A # 23.What is the output of the following script?print (sum(range(5),-1))# 24.What are the result of the following expressions?print(np.array(0) / np.array(0))print(np.array(0) // np.array(0))print(np.array([np.nan]).astype(int).astype(float))# 25.How to round away from zero a float array ?Z = np.random.uniform(-10,+10,10)print (np.copysign(np.ceil(np.abs(Z)), Z))# 26.How to find common values between two arrays?A1 = np.random.randint(0,10,10)A2 = np.random.randint(0,10,10)print A1,A2print (np.intersect1d(A1,A2))# 27.How to ignore all numpy warnings (not recommended)?defaults = np.seterr(all="ignore")Z = np.ones(1) / 0_ = np.seterr(**defaults)with np.errstate(divide='ignore'): Z = np.ones(1) / 0# 28. Is the following expressions true? # numpy.sqrt():按元素方式返回数组的正平方根print np.sqrt(-1) == np.emath.sqrt(-1)# 29. How to get the dates of yesterday, today and tomorrow?yesterday = np.datetime64('today', 'D') - np.timedelta64(1, 'D')today = np.datetime64('today', 'D')tomorrow = np.datetime64('today', 'D') + np.timedelta64(1, 'D')print yesterday,today,tomorrow# 30.How to get all the dates corresponding to the month of July 2016? A = np.arange('2018-01','2018-02',dtype='datetime64[D]')print A A = np.arange('2019-03','2019-05',dtype='datetime64[D]')print A # 31.How to compute ((A+B)*(-A/2)) in place (without copy)?A = np.ones(3)*1B = np.ones(3)*2C = np.ones(3)*3print np.add(A,B,out=B)print np.divide(A,2,out=A)print np.negative(A,out=A)print np.multiply(A,B,out=A)# 32.Extract the integer part of a random array using 5 different methods# uniform():从均匀分布绘制样本# floor():逐元素地返回输入的底# ceil():元素方式返回输入的上限# astype():数组的复制，强制转换为指定的类型# trunc():按元素方式返回输入的截断值Z = np.random.uniform(0,10,10)print Z print (Z - Z%1)print (np.floor(Z))print (np.ceil(Z) - 1)print (Z.astype(int))print (np.trunc(Z))# 33.Create a 5x5 matrix with row values ranging from 0 to 4A = np.zeros((5,5))A += np.arange(5)print A # 34.onsider a generator function that generates 10 integers and use it to build an array# zeros():返回给定形状和类型的新数组，用零填充# numpy.fromiter():从可迭代对象创建新的1维数组def generate(): for x in range(10): yield xZ = np.fromiter(generate(),dtype=float,count=-1)print Z # 35.Create a vector of size 10 with values ranging from 0 to 1, both excluded# linsapce():在指定的间隔内返回均匀间隔的数字Z = np.linspace(0,1,11,endpoint=False)[1:]print(Z) # 36.Create a random vector of size 10 and sort itZ = np.random.random(10)Z.sort()print Z # 37.How to sum a small array faster than np.sum? # add():按元素添加参数Z = np.arange(10)print np.add.reduce(Z)# 38.Consider two random array A and B, check if they are equal# allclose():如果两个数组在元素级别在容差内相等，则返回True# array_equal():如果两个数组具有相同的形状和元素，则为True，否则为FalseA = np.random.randint(0,2,5)B = np.random.randint(0,2,5)equal = np.allclose(A,B)print (equal)equal = np.array_equal(A,B)print (equal)C = np.random.randint(0,10,10)D = np.random.randint(0,10,10)print C,D print np.allclose(C,D)print np.array_equal(C,D)# 39.Make an array immutable (read-only)# writeable:确保返回的数组可以写入Z = np.zeros(10)Z.flags.writeable = False# 40.Consider a random 10x2 matrix representing cartesian coordinates, convert them to polar coordinates A = np.random.random((10,2))X,Y = A[:,0],A[:,1]R = np.sqrt(X**2+Y**2)T = np.arctan2(Y,X)print (R)print (T)# 41.Create random vector of size 10 and replace the maximum value by 0# argmax():返回沿轴的最大值的索引Z = np.random.random(10)Z[Z.argmax()] = 0print (Z)# 42.Create a structured array with x and y coordinates covering the [0,1]x[0,1] area# meshgrid():从坐标向量返回坐标矩阵Z = np.zeros((5,5), [('x',float),('y',float)])Z['x'], Z['y'] = np.meshgrid(np.linspace(0,1,5), np.linspace(0,1,5))print(Z)# 43.Given two arrays, X and Y, construct the Cauchy matrix C (Cij =1/(xi - yj))# outer():计算两个向量的外积# numpy.linalg.det():计算数组的行列式X = np.arange(8)Y = X + 0.5C = 1.0 / np.subtract.outer(X,Y)print (np.linalg.det(C))# 44.Print the minimum and maximum representable value for each numpy scalar typefor dtype in [np.int8, np.int32, np.int64]: print (np.iinfo(dtype).min) print (np.iinfo(dtype).max)for dtype in [np.float32, np.float64]: print (np.finfo(dtype).min) print (np.finfo(dtype).max) print (np.finfo(dtype).eps)# 45.Print the minimum and maximum representable value for each numpy scalar typenp.set_printoptions(threshold=np.nan)Z = np.zeros((8,8))print Z # 46.How to find the closest value (to a given scalar) in a vector? A = np.arange(100)v = np.random.uniform(0,100)index = (np.abs(A - v)).argmin()print (A[index])# 47.创建一个表示位置(x,y)和颜色(r,g,b)的结构化数组Z = np.zeros(10, [ ('position', [ ('x', float, 1), ('y', float, 1)]), ('color', [ ('r', float, 1), ('g', float, 1), ('b', float, 1)])])print (Z)# 48.对一个表示坐标形状为(100,2)的随机向量，找到点与点的距离Z = np.random.random((10,2))X,Y = np.atleast_2d(Z[:,0], Z[:,1])D = np.sqrt((X-X.T)**2 + (Y-Y.T)**2)print (D)# 49.如何将32位的浮点数(float)转换为对应的整数(integer)?A = np.arange(10,dtype=np.int32)A = A.astype(np.float32, copy=False)print A# 50.对于numpy数组，enumerate的等价操作是什么？# enumerate:多维索引迭代器;返回迭代器产生数组坐标和值的对Z = np.arange(9).reshape(3,3)print Zfor index,value in np.ndenumerate(Z): print (index, value)for index in np.ndindex(Z.shape): print (index, Z[index])# 51.生成一个通用的二维Gaussian-like数组X, Y = np.meshgrid(np.linspace(-1,1,10), np.linspace(-1,1,10))D = np.sqrt(X*X+Y*Y)sigma, mu = 1.0, 0.0G = np.exp(-( (D-mu)**2 / ( 2.0 * sigma**2 ) ) )print (G)# 52.对一个二维数组，如何在其内部随机放置p个元素?# numpy.random.choice():从给定的1-D数组生成随机样本# numpy.put():用给定值替换数组的指定元素n = 10p = 4A = np.zeros((n,n))np.put(A, np.random.choice(range(n*n), p, replace=False), 1)print A # 53.减去一个矩阵中的每一行的平均值X = np.random.rand(5,10)Y = X - X.mean(axis=1,keepdims=True)print Y Y = X - X.mean(axis=1).reshape(-1,1)print Y# 54.如何通过第n列对一个数组进行排序?Z = np.random.randint(0,10,(3,3))print Z print (Z[Z[:,1].argsort()])# 55.如何检查一个二维数组是否有空列？Z = np.random.randint(0,3,(3,10))print ((~Z.any(axis=0).any()))# 56.如何用迭代器(iterator)计算两个分别具有形状(1,3)和(3,1)的数组?A = np.arange(3).reshape(3,1)B = np.arange(3).reshape(1,3)it = np.nditer([A,B,None])for x,y,z in it: z[...] = x + yprint (it.operands[2])# 57.创建一个具有name属性的数组类class NamedArray(np.ndarray): def __new__(cls, array, name="no name"): obj = np.asarray(array).view(cls) obj.name = name return obj def __array__finalize__(self, obj): if obj is None: return self.info = getattr(obj, 'name', "no name")Z = NamedArray(np.arange(10), "range_10")print (Z.name)# 58.考虑一个给定的向量，如何对由第二个向量索引的每个元素加1(小心重复的索引)?Z = np.ones(10)I = np.random.randint(0,len(Z),20)Z += np.bincount(I, minlength=len(Z))print(Z)# 59.根据索引列表(I)，如何将向量(X)的元素累加到数组(F)?# numpy.bincount():计算非负整数数组中每个值的出现次数X = [1,2,3,4,5,6]I = [1,3,5,7,9,4]F = np.bincount(I,X)print F # 60.考虑一个(dtype=ubyte) 的 (w,h,3)图像，计算其唯一颜色的数量w,h = 16,16I = np.random.randint(0,2,(h,w,3)).astype(np.ubyte)F = I[...,0]*(256*256) + I[...,1]*256 +I[...,2]n = len(np.unique(F))print n# 61.考虑一个四维数组，如何一次性计算出最后两个轴(axis)的和？# randint():将随机整数从低（包括）返回到高（不包含）.A = np.random.randint(0,10,(3,4,3,4))print A sum = A.sum(axis=(-2,-1))print sum# 62.考虑一个一维向量D，如何使用相同大小的向量S来计算D子集的均值？D = np.random.uniform(0,1,100)S = np.random.randint(0,10,100)D_sums = np.bincount(S)D_counts = np.bincount(S)D_means = D_sums / D_countsprint (D_means)print (pd.Series(D).groupby(S).mean())# 63.如何获得点积 dot prodcut的对角线?# np.sumA = np.random.uniform(0,1,(5,5))B = np.random.uniform(0,1,(5,5))print np.sum(A*B.T, axis=1)# np.diagprint np.diag(np.dot(A,B))# np.einsum()print np.einsum("ij,ji-&gt;i", A, B)# 64.考虑一个向量[1,2,3,4,5],如何建立一个新的向量，在这个新向量中每个值之间有3个连续的零？Z = np.array([1,2,3,4,5])nz = 3Z0 = np.zeros(len(Z) + (len(Z)-1)*(nz))Z0[::nz+1] = Z print Z0# 65.考虑一个维度(5,5,3)的数组，如何将其与一个(5,5)的数组相乘？A = np.arange(25).reshape(5,5)A[0,1] = A[1,0]print A # 66.考虑一个可以描述10个三角形的triplets，找到可以分割全部三角形的line segmentfaces = np.random.randint(0,100,(10,3))F = np.roll(faces.repeat(2,axis=1),-1,axis=1)F = np.sort(F,axis=1)G = F.view( dtype=[('p0',F.dtype),('p1',F.dtype)] )G = np.unique(G)print G # 67.给定一个二进制的数组C，如何产生一个数组A满足np.bincount(A)==C# np.repeatC = np.bincount([1,1,2,3,4,4,6])A = np.repeat(np.arange(len(C)), C)print A # 68.如何通过滑动窗口计算一个数组的平均数?# np.cumsumdef moving_average(a,n=3): ret = np.cumsum(a, dtype=float) ret[n:] = ret[n:] - ret[:-n] return ret[n - 1:] / nZ = np.arange(20)print (moving_average(Z, n=3))# 69.Consider a one-dimensional array Z, build a two-dimensional array whose first row is (Z[0],Z[1],Z[2]) and each subsequent row is shifted by 1 (last row should be (Z[-3],Z[-2],Z[-1]) # from.numpy.libfrom numpy.lib import stride_tricksdef rolling(a, window): shape = (a.size - window + 1, window) strides = (a.itemsize, a.itemsize) return stride_tricks.as_strided(a, shape=shape, strides=strides)Z = rolling(np.arange(10),3)print Z # 70.如何对布尔值取反，或者原位(in-place)改变浮点数的符号(sign)？# np.logical_not, np.negativeZ = np.random.randint(0,2,100)print np.logical_not(Z,out=Z)Z = np.random.uniform(-1.0,1.0,100)print np.negative(Z, out=Z)# 71.考虑两组点集P0和P1去描述一组线(二维)和一个点p,如何计算点p到每一条线 i (P0[i],P1[i])的距离？def distance(P0,P1,p): T = P1 - P0 L = (T**2).sum(axis=1) U = -((P0[:,0]-p[...,0])*T[:,0] + (P0[:,1]-p[...,1])*T[:,1]) / L U = U.reshape(len(U),1) D = P0 + U*T - p return np.sqrt((D**2).sum(axis=1))P0 = np.random.uniform(-10,10,(10,2))P1 = np.random.uniform(-10,10,(10,2))p = np.random.uniform(-10,10,(1,2))print (distance(P0,P1,p))# 72.考虑两组点集P0和P1去描述一组线(二维)和一组点集P，如何计算每一个点 j(P[j]) 到每一条线 i (P0[i],P1[i])的距离？P0 = np.random.uniform(-10,10,(10,2))P1 = np.random.uniform(-10,10,(10,2))p = np.random.uniform(-10, 10, (10,2))print (np.array([distance(P0,P1,p_i) for p_i in p]))# 73.虑一个数组Z = [1,2,3,4,5,6,7,8,9,10,11,12,13,14],如何生成一个数组R = [[1,2,3,4], [2,3,4,5], [3,4,5,6], ...,[11,12,13,14]]?# stride_tricks.as_stridedZ = np.arange(1,15,dtype=np.uint32)R = stride_tricks.as_strided(Z,(11,4),(4,4))print R# 74.计算一个矩阵的秩# np.linalg.svdZ = np.random.uniform(0,1,(10,10))U, S, V = np.linalg.svd(Z)rank = np.sum(S &gt; 1e-10)print rank # 75.如何找到一个数组中出现频率最高的值？# np.bincount, argmaxZ = np.random.randint(0,10,50)print (np.bincount(Z).argmax())# 76.从一个10x10的矩阵中提取出连续的3x3区块# stride_tricks.as_stridedZ = np.random.randint(0,5,(10,10))n = 3i = 1 + (Z.shape[0]-3)j = 1 + (Z.shape[1]-3)C = stride_tricks.as_strided(Z,shape=(i,j,n,n),strides=Z.strides + Z.strides)print C # 77.创建一个满足 Z[i,j] == Z[j,i]的子类# class class Symetric(np.ndarray): def __setitem__(self, index, value): i,j = index super(Symetric, self).__setitem__((i,j), value) super(Symetric, self).__setitem__((j,i), value)def symetric(Z): return np.asarray(Z + Z.T - np.diag(Z.diagonal())).view(Symetric)S = symetric(np.random.randint(0,10,(5,5)))S[2,3] = 42print (S)# 78.考虑p个 nxn 矩阵和一组形状为(n,1)的向量，如何直接计算p个矩阵的乘积(n,1)？# np.tensordotp,n = 10,20M = np.ones((p,n,n))V = np.ones((p,n,1))S = np.tensordot(M,V,axes=[[0,2],[0,1]])print S # 79.对于一个16x16的数组，如何得到一个区域(block-sum)的和(区域大小为4x4)?# np.add.reduceatZ = np.ones((16,16))k = 4S = np.add.reduceat(np.add.reduceat(Z, np.arange(0, Z.shape[0], k), axis=0), np.arange(0, Z.shape[1], k), axis=1)print (S)# 80.如何利用numpy数组实现Game of Lifedef iterate(Z): # Count neighbours N = (Z[0:-2,0:-2] + Z[0:-2,1:-1] + Z[0:-2,2:] + Z[1:-1,0:-2] + Z[1:-1,2:] + Z[2: ,0:-2] + Z[2: ,1:-1] + Z[2: ,2:]) # Apply rules birth = (N==3) &amp; (Z[1:-1,1:-1]==0) survive = ((N==2) | (N==3)) &amp; (Z[1:-1,1:-1]==1) Z[...] = 0 Z[1:-1,1:-1][birth | survive] = 1 return ZZ = np.random.randint(0,2,(50,50))for i in range(100): Z = iterate(Z)print (Z)# 81.如何找到一个数组的第n个最大值?# np.argsort# numpy.random.shuffle():通过随机播放其内容来修改序列Z = np.arange(10000)np.random.shuffle(Z)n = 5print (Z[np.argsort(Z)[-n:]])# 82.给定任意个数向量，创建笛卡尔积(每一个元素的每一种组合)(# np.indicesdef cartesian(arrays): arrays = [np.asarray(a) for a in arrays] shape = (len(x) for x in arrays) ix = np.indices(shape, dtype=int) ix = ix.reshape(len(arrays), -1).T for n, arr in enumerate(arrays): ix[:, n] = arrays[n][ix[:, n]] return ixprint (cartesian(([1, 2, 3], [4, 5], [6, 7])))# 82.如何从一个正常数组创建记录数组(record array)?np.core.records.fromarraysZ = np.array([("Hello", 2.5, 3), ("World", 3.6, 2)])R = np.core.records.fromarrays(Z.T, names='col1, col2, col3', formats = 'S8, f8, i8')print (R)# 83.考虑一个大向量Z, 用三种不同的方法计算它的立方# np.power:第一个数组元素从第二个数组提升到权力，逐元素x = np.random.rand()print np.power(x,3)# 84. 考虑一个10x3的矩阵，分解出有不全相同值的行 (如 [2,2,3]) Z = np.random.randint(0,5,(10,3))print Z E = np.all(Z[:,1:] == Z[:,:-1], axis=1)U = Z[~E]print (U)U = Z[Z.max(axis=1) != Z.min(axis=1),:]print (U)# 85.将一个整数向量转换为matrix binary的表现形式print (np.unpackbits(I[:, np.newaxis], axis=1))# 86.给定一个二维数组，如何提取出唯一的(unique)行?# np.ascontiguousarrayZ = np.random.randint(0,2,(6,3))T = np.ascontiguousarray(Z).view(np.dtype((np.void, Z.dtype.itemsize * Z.shape[1])))_, idx = np.unique(T, return_index=True)uZ = Z[idx]print (uZ)# 87.考虑两个向量A和B，写出用einsum等式对应的inner, outer, sum, mul函数# np.einsumA = np.random.uniform(0,1,10)B = np.random.uniform(0,1,10)print ('sum')print (np.einsum('i-&gt;', A))# np.sum(A)print ('A * B')print (np.einsum('i,i-&gt;i', A, B)) # A * Bprint ('inner')print (np.einsum('i,i', A, B)) # np.inner(A, B)print ('outer')print (np.einsum('i,j-&gt;ij', A, B)) # np.outer(A, B)# 88. 考虑一个由两个向量描述的路径(X,Y)，如何用等距样例(equidistant samples)对其进行采样(sample)?# Considering a path described by two vectors (X,Y), how to sample it using equidistant samplesnp.cumsum, np.interp)phi = np.arange(0, 10*np.pi, 0.1)a = 1x = a*phi*np.cos(phi)y = a*phi*np.sin(phi)dr = (np.diff(x)**2 + np.diff(y)**2)**.5 # segment lengthsr = np.zeros_like(x)r[1:] = np.cumsum(dr) # integrate pathr_int = np.linspace(0, r.max(), 200) # regular spaced pathx_int = np.interp(r_int, r, x) # integrate pathy_int = np.interp(r_int, r, y)# 89.对于一个一维数组X，计算它boostrapped之后的95%置信区间的平均# np.percentileX = np.random.randn(100) # random 1D arrayN = 1000 # number of bootstrap samplesidx = np.random.randint(0, X.size, (N, X.size))means = X[idx].mean(axis=1)confint = np.percentile(means, [2.5, 97.5])print (confint) Reference这100道练习，带你玩转Numpy 100 numpy exercises]]></content>
      <categories>
        <category>Computer Science</category>
        <category>Data Mining and Analysis</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[numpy入门实战教程（进阶篇）]]></title>
    <url>%2F2018%2F01%2F29%2Fnumpy%E5%85%A5%E9%97%A8%E5%AE%9E%E6%88%98%E6%95%99%E7%A8%8B%EF%BC%88%E8%BF%9B%E9%98%B6%E7%AF%87%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Abstract：numpy入门实战教程进阶篇，附代码。 1.广播法则（Broadcast Rule）广播法则能使通用函数有意义地处理不具有相同形状的输入。 第一法则：若所有的输入数组维度不都相同，一个“1”将被重复地添加在维度较小的数组上直至所有数据都拥有相同的维度 第二法则：确定长度为1的数组沿着特殊方向表现地好像它有沿着那个方向最大形状的大小。对数组来说，沿着那个维度的数组元素的值理应相同。 应用广播法则后，所有数组的大小必须匹配。 2.花哨的索引和索引技巧numpy比普通的python序列提供更多的索引功能。数组的索引功能：索引整数、切片、被整数数组和布尔数组索引。 2.1 通过数组索引（Indexed by Array）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# indexed via arraya = arange(12)**2print ai = array([1,1,3,8,5]) print iprint a[i] # # the elements of a at the positions ij = array([[3,4],[9,7]])print a[j]# 当被索引数组a是多维的时，每一个唯一的索引数列指向a的第一维5。# 以下示例通过将图片标签用调色版转换成色彩图像展示了这种行为palette = array([ [0,0,0], [255,0,0], [0,255,0], [0,0,255], [255,0,255], [255,255,255]])image = array([[0,1,2,0], [0,3,4,0]])print palette[image]# 我们也可以给出不不止一维的索引，每一维的索引数组必须有相同的形状a = arange(12).reshape(3,4)print a i = array([ [0,1], [1,2]])j = array([[2,1],[3,3]])print a[i,j]print a[i,2]print a[:,j]print a[i,:]l = [i,j]print a[l] # 与 a[i,j] 相等# 搜索时间序列最大值6time = linspace(20, 145, 5) # time scaledata = sin(arange(20)).reshape(5,4) # 4 time-dependent seriesprint timeprint dataind = data.argmax(axis=0) # index of the maxima for each seriesprint ind time_max = time[ind]data_max = data[ind, range(data.shape[1])]print time_maxprint data_maxprint all(data_max == data.max(axis=0)) # Truea = arange(5)print a # 当一个索引列表包含重复时，赋值被多次完成，保留最后的值a = arange(5)a[[0,0,2]] = [1,2,3]print a 2.2 通过布尔数组索引（Indexed by Boolean Function）当我们使用整数数组索引数组时，我们提供一个索引列表去选择。通过布尔数组索引法使我们可以显式地选择数组中我们想要和不想要的元素。 我们能想到的使用布尔数组的索引最自然方式是使用和源数组一样形状的布尔数组。 123456789101112131415161718# indexed via boolean arraya = arange(12).reshape(3,4)b = a &gt; 4print b print a[b]a[b] = 0 # All elements of 'a' higher than 4 become 0(取反)print a # 通过布尔来索引的方法更近似于整数索引；对数组的每个维度我们给一个一维布尔数组来选择我们想要的切片a = arange(12).reshape(3,4)b1 = array([False,True,True])b2 = array([True,False,True,False])print a[b1,:]print a[b1]print a[:,b2]print a[b1,b2]# 注意一维数组的长度必须和你想要切片的维度或轴的长度一致，# 在之前的例子中，b1是一个秩为1长度为三的数组(a的行数)，b2(长度为4)与a的第二秩(列)相一致 2.3 ix()函数ix()函数可为了获得多元组的结果而用来结合不同向量。如，若你想要用所有向量a、b和c元素组成的 三元组来计算a+b*c： 12345678910111213141516171819202122232425262728# ix_() functiona = array([2,3,4,5])b = array([8,5,4])c = array([5,4,6,8,3])ax,bx,cx = ix_(a,b,c)print ax,bx,cxprint ax.shape,bx.shape,cx.shaperesult = ax+bx*cxprint resultprint result[3,2,4]result1 = ax*bx+cxprint result1print result1[3,2,4]print result[3,2,4]print a[3] + b[2] + c[4]# second methoddef ufunc_reduce(ufct, *vectors): vs = ix_(*vectors) r = ufct.identity for v in vs: r = ufct(r,v) return r print ufunc_reduce(add,a,b,c)# 这个reduce与ufunc.reduce(比如说add.reduce)相比的优势在于它利用了广播法则，# 避免了创建一个输出大小乘以向量个数的参数数组。 2.4 字符串索引（Indexed by String）参见Structured arrays 3.线性代数（Linear Algebra）3.1 简单数组运算12345678910111213141516# coding:utf-8from numpy import *from numpy.linalg import *# Linear Algebraa = array([[1.0,2.0],[3.0,4.0]])print (a)print a.transpose()print inv(a) # 计算矩阵的（乘法）逆u = eye(2) # unit 2X2 matrix; "eye" represents "I",指元数组print u # j = array([[0.0, -1.0],[1.0, 0.0]])# dot(i,j) # matrix product; dot():两个数组的点积print trace(u) # trace():沿数组的对角线返回总和y = array([[5.],[7.]])print solve(a, y) # solve():求解线性矩阵方程或线性标量方程组 3.2 矩阵类12345678910111213141516# coding:utf-8from numpy import *from numpy.linalg import *# matrixA = matrix('1.0 2.0; 3.0 4.0')print Aprint type(A)print A.T # transposeX = matrix('5.0 7.0')Y = X.Tprint Ymatrix([[5.],[7.]])print (A * Y)print (A.I) # inverseprint solve(A,Y) 3.3 索引：比较矩阵和二维数组注意numpy中数组和矩阵有重要区别。numpy提供了2个基本对象：（其他对象都是建构在它们之上的） 一个N维数组对象 一个通用函数对象 ​ 矩阵：继承自numpy数组对象的二维数组对象。对数组和矩阵，索引都必须包含合适的一个或多个这些组合：整数标量、省略号、整数列表；布尔值，整数，布尔值构成的元组，和一个一维整数或布尔值数组。矩阵可被用作矩阵的索引，但通常需要数组、列表或其他形式来完成这个任务。 索引从0开始 使用矩阵的行和列表示一个二维数组或矩阵。沿0轴的方向被穿过的称作行，沿1轴的方向被穿过的是列。 ​ 123456789101112131415161718192021222324252627282930313233343536373839404142# coding:utf-8from numpy import *from numpy.linalg import *# index:compare matrix with 2d arrayA = arange(12)print A A.shape = (3,4)M = mat(A.copy()) # mat():将输入解释为矩阵print (type(A)," ",type(M)) print (A)print (M)print (A[:])print (A[:].shape)print (M[:])print (M[:].shape)print (A[:,1])print (A[:,1].shape)# 注意最后两个结果的不同。对二维数组使用一个冒号产生一个一维数组，然而矩阵产生了一个二维矩阵。# 例如，一个M[2,:]切片产生了一个形状为(1,4)的矩阵，相比之下，一个数组的切片总是产生一个最低可能维度11的数组。# 例如，如果C是一个三维数组，C[...,1]产生一个二维的数组而C[1,:,1]产生一个一维数组。# 从这时开始，如果相应的矩阵切片结果是相同的话，我们将只展示数组切片的结果。# 假如我们想要一个数组的第一列和第三列，一种方法是使用列表切片：print A[:,[1,3]]# 稍微复杂点的方法是使用 take() 方法 method:print A[:,].take([1,3],axis=1) # take():从轴沿一个数组中取元素# 如果我们想跳过第一行，我们可以这样：print A[1:,].take([1,3],axis=1)print A[ix_((1,2),(1,3))]# 保留第一行大于1的列。一种方法是创建布尔索引：print A[0,:] &gt; 1print A[:,A[0,:]] &gt; 1print M[0,:] &gt; 1print M[:,M.A[0,:]&gt;1]# 如果我们想要在矩阵两个方向有条件地切片，我们必须稍微调整策略，代之以：print A[A[:,0] &gt; 2, A[0,:] &gt; 1]print M[M.A[:,0]&gt;2,M.A[0,:]&gt;1]print A[ix_(A[:,0] &gt; 2, A[0,:] &gt; 1)]print M[ix_(M.A[:,0] &gt; 2, M.A[0,:] &gt; 1)] 4.技巧和Tipps4.1 “自动”改变形状更改数组的维度，你可以省略一个尺寸，它将被自动推导出来。 12345# change the shape automaticallya = arange(30)a.shape = 2,-1,3 # -1 means "whatever is needed"print a.shapeprint a 4.2 向量组合（Vector Stacking）我们如何用两个相同尺寸的行向量列表构建一个二维数组？在 MATLAB 中这非常简单：如果 x 和 y 是两个相同长度的向量，你仅仅需要做 m=[x;y]。在 Numpy 中这个过程通过函数 column_stack 、dstack、hstack 和 vstack 来完成，取决于你想要在那个维度上组合。例如： 123456789101112# vector stackingx = arange(0,10,2)print xy = arange(5)print ym = vstack([x,y]) # 按行顺序堆叠数组print mxy = hstack([x,y]) # 按列顺序组合数组print xy 4.3 直方图（histogram）Numpy中histogram函数应用到一个数组返回一对变量：直方图数组和箱式向量。注意：matplotlib也有一个用来建立直方图的函数(叫作hist,正如matlab中一样)与Numpy中的不同。主要的差别是pylab.hist自动绘制直方图，而numpy.histogram仅仅产生数据。 12345678910111213# histogram# Build a vector of 10000 normal deviates with variance 0.5^2 and mean 2mu, sigma = 2, 0.5v = numpy.random.normal(mu,sigma,10000)# Plot a normalized histogram with 50 binspylab.hist(v, bins=50, normed=1) pylab.title('Matplotlib Version')# matplotlib version (plot)pylab.show()# Compute the histogram with numpy and then plot it(n, bins) = numpy.histogram(v, bins=50, normed=True) # NumPy version (no plot)pylab.plot(.5*(bins[1:]+bins[:-1]), n)pylab.title('Numpy Version')pylab.show() 学习资料numpy v1.11手册 numpy参考手册 numpy快速入门教程 numpy通用索引：所有函数、类、术语 numpy全目录 kesci: Numpy快速上手指南 —- 进阶篇 Reference【Python数据分析】Numpy的详细教程 Scipy: Quickstart tutorial]]></content>
      <categories>
        <category>Computer Science</category>
        <category>Data Mining and Analysis</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[numpy入门实战教程（基础篇）]]></title>
    <url>%2F2018%2F01%2F29%2Fnumpy%E5%85%A5%E9%97%A8%E5%AE%9E%E6%88%98%E6%95%99%E7%A8%8B%EF%BC%88%E5%9F%BA%E7%A1%80%E7%AF%87%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Abstract：numpy入门实战教程，附代码。 1.概览 numpy的主要对象是同种元素的多维数组。这是一个所有元素都是同一种类型、通过同一个正整数元组索引的元素表格（通常元素是数字） 维度（dimensions）：轴 轴的个数：秩（rank） 例如，在3D空间一个点的坐标[1, 2, 3]是一个秩为1的数组，因为它只有一个轴。那个轴长度为3.又例如，在以下例子中，数组的秩为2(它有两个维度).第一个维度长度为2,第二个维度长度为3. [[ 1., 0., 0.], [ 0., 1., 2.]] 数组类：ndarray（数组），注意numpy.array和标准Python库类array.array并不相同，后者只处理一维数组和提供少量功能。更多重要ndarray对象属性有： ndarray.ndim ——数组轴（秩）的个数 ndarray.shape ——数组的维度；是一个指示数组在每个维度上大小的整数元组。例如一个n排m列的矩阵，它的shape属性将是(2,3),这个元组的长度显然是秩，即维度或者ndim属性 ndarry.size——数组元素的总个数 ndarray.dtype ——用来描述数组中元素类型的对象，可以通过创造或指定dtype使用标准Python类型。另外Numpy提供它自己的数据类型。 ndarray.itemsize ——数组中每个元素的字节大小。例如，一个元素类型为float64的数组itemsiz属性值为8(=64/8),又如，一个元素类型为complex32的数组item属性为4(=32/8). ndarray.data——包含实际数组元素的缓冲区，通常我们不需要使用这个属性，因为我们总是通过索引来使用数组中的元素 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# input1from numpy import*a = arrange(15).reshape(3,5)print a# output1array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14]])# input2print a.shape# output2(3,5)# in3print a.ndim# out32# in4print a.dtype.name# out4'int64'# in5print a.itemsize# out58# in6print a.size# in7print tyep(a)# out7numpy.ndarray# in8b = array([6, 7, 8])b # out8array([6,7,8])# in9print type(b)# outnumpy.ndarray ​ 2.创建数组 使用array函数从常规的python列表和元组创造数组，所创建的数组类型由原序列中的元素类型推导而来。 ​ 12345678910111213141516# create arrayb = array([(1,2,3,4),(5,6,7,8)],dtype=complex) # 默认创建的数组类型(dtype)都是float64print bzeros((3,4))print zerosones((2,3,4),dtype=int16)empty((2,3))print emptyprint arange(10,30,5)print arange(0,2,0.3)# 当arange使用浮点数参数时，由于有限的浮点数精度，通常无法预测获得的元素个数。# 因此，最好使用函数linspace去接收我们想要的元素个数来代替用range来指定步长 ​ 3.打印数组 当打印数组时，numpy会以类似嵌套列表的形式显示，呈以下布局 最后的轴从左到右打印 次后的轴从顶向下打印 剩下的轴从顶向下打印，每个切片通过一个空行与下一个隔开 一维数组被打印成行，二维数组组成矩阵，三维数组组成矩阵列表 ​ 12345678910# print arraya = arange(6) # 1d arrayprint (a)b = arange(12).reshape(4,3) # 2d arrayprint (b)c = arange(24).reshape(2,3,4) # 3d arrayprint (c)print (arange(10000))print (arange(10000).reshape(100,100))set_printoptions(threshold='nan') # 设置printoptions参数更改打印选项 ​ 4.基本运算 数组的算术运算是按元素的。新的数组被创建并且被结果填充。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# basic operationa = array([20,30,40,50])b = arange(4)print a,bc = a+bprint cprint b**2 # b_2print 10*sin(a)print 100*cos(b)print 90*tan(c)print a&lt;35 #judge# NumPy中的乘法运算符*指示按元素计算，矩阵乘法可以使用dot函数或创建矩阵对象实现A = array([[1,2],[2,3],[3,4]]) #matrixprint A B = array([[10,20],[20,30],[30,40]])print Bprint A * B# 有些操作符像+=和*=被用来更改已存在数组而不创建一个新的数组a = ones((2,3),dtype=int)b = random.random((2,3))a *= 3print a b += aprint b # 当运算的是不同类型的数组时，结果数组和更普遍和精确的已知，这种行为叫做 upcasta = ones(3, dtype=int32)b = linspace(0,pi,3)print b.dtype.namec = a+bprint c d = exp(c*1j)print d print d.dtype.name # # 许多非数组运算，如计算数组所有元素之和，被作为ndarray类的方法实现a = random.random((2,3))print a print a.sum()print a.mean()print a.max()print a.min()b = arange(12).reshape(3,4)print b print b.sum(axis=0) # in columnsprint b.min(axis=1) # in rowsprint b.cumsum(axis=1) # cumulative sum along each row ​ 5.通用函数ufunc5.1 函数 numpy提供常见的数学函数如sin,cos,exp，其统称为通用函数ufunc，在numpy里这些函数作用按数组的元素运算，产生一个数组作为输出。 ​ 其他函数：all, alltrue, any, apply along axis, argmax, argmin, argsort, average, bincount, ceil, clip, conj, conjugate, corrcoef, cov, cross, cumprod, cumsum, diff, dot, floor, inner, inv, lexsort, max, maximum, mean, median, min, minimum, nonzero, outer, prod, re, round, sometrue, sort, std, sum, trace, transpose, var, vdot, vectorize, where ​ 1234567891011# ufuncB = arange(3)print B print exp(B)print sqrt(B)C = array([2.,-1.,4.])print add(B,C)# **n : 表示n次方运算a = arange(10)**3print a ​ 5.2 索引、切片和迭代 一维数组可被索引、切片和迭代，如同列表和其他python序列。 ​ b[i]中括号中的表达式被当作i和一系列:，来代表剩下的轴。NumPy也允许你使用“点”像b[i,…]。 点(…)代表许多产生一个完整的索引元组必要的分号。如果x是秩为5的数组(即它有5个轴)，那么: x[1,2,…] 等同于 x[1,2,:,:,:] x[…,3] 等同于 x[:,:,:,:,3] x[4,…,5,:] 等同于 x[4,:,:,5,:] 123456&gt; # c = array( [ [[ 0, 1, 2], # a 3D array (two stacked 2D arrays) ... [ 10, 12, 13]]... &gt; # [[100,101,102], ... [110,112,113]] ] ) &gt; # c.shape (2, 2, 3) &gt; # c[1,...] # same as c[1,:,:] or c[1] array([[100, 101, 102], [110, 112, 113]]) &gt; # c[...,2] # same as c[:,:,2] array([[ 2, 13], [102, 113]])&gt; 1234567891011121314151617181920212223242526272829303132333435363738394041424344# ufuncB = arange(3)print B print exp(B)print sqrt(B)C = array([2.,-1.,4.])print add(B,C)# 索引，切片和迭代print B[2] # output the third element of B print B[0:2]a[:6:2] = -1000print aprint a[::-1] # reversied a# 多维数组可以每个轴有一个索引。这些索引由一个逗号分割的元组给出def f(x,y): return 10*x+yb = fromfunction(f,(5,4),dtype=int)print bc = fromfunction(f,(3,3),dtype=float)print cprint b[2,3]print b[1,1]print b[0:5,1]print b[0:4,0]print b[:,1]print b[:,2]print b[1:3,2]# 当少于轴数的索引被提供时，确失的索引被认为是整个切片print b[-1]# 迭代多维数组是就第一个轴而言的for row in b: print (row)for column in b: print (column)# 如果一个人想对每个数组中元素进行运算，我们可以使用flat属性，该属性是数组元素的一个迭代器for element in b.flat: print (element,end=",") ​ 6.形状操作6.1 更改数组的形状一个数组的形状由其每个轴上的元素个数给出。 ​ 123456789101112131415# shape operationa = floor(10*random.random((3,4)))print aprint a.shapeprint a.ravel() # flatten the arraya.shape = (6,2)print a.transpose() # 矩阵转置transposea.shape = (2,6)print a.transpose()# reshape函数改变参数形状并返回它，而resize函数改变数组自身print aa.resize((2,6))print a# 如果在改变形状操作中一个维度被给做-1，其维度将自动被计算 ​ 6.2 组合(stack)不同的数组几种不同方法可沿不同轴将数组堆叠在一起。 ​ 1234567891011121314151617181920# stack different arraysa = floor(10*random.random((2,2)))print a b = floor(10*random.random((2,2)))print b print vstack((a,b)) # 将a和b纵向组合在一起print hstack((a,b)) # 将a和b横向组合在一起# 函数column_stack以列将一维数组合成二维数组，它等同与vstack对一维数组print column_stack((a,b)) # # With 2D arraysa = array([4.,2.])b = array([2.,8.])print a[:,newaxis] # This allows to have a 2D columns vectorprint column_stack((a[:,newaxis],b[:,newaxis]))print vstack((a[:,newaxis],b[:,newaxis])) #The behavior of vstack is different# row_stack函数，另一方面，将一维数组以行组合成二维数组# 对那些维度比二维更高的数组，hstack沿着第二个轴组合，vstack沿着第一个轴组合,concatenate允许可选参数给出组合时沿着的轴# 在复杂情况下，r_[]和c_[]对创建沿着一个方向组合的数很有用，它们允许范围符号(“:”):print r_[1:4,0,4]# 当使用数组作为参数时，r_和c_的默认行为和vstack和hstack很像，但是允许可选的参数给出组合所沿着的轴的代号。# 函数:hstack, vstack, column_stack, row_stack, concatenate, c_, r_ ​ 6.3 将一个数组分割（split）成几个小数组使用hsplit你能将数组沿着它的水平轴分割，或者指定返回相同形状数组的个数，或者指定在哪些列后发生分割: 1234567# split an array into several piecesa = floor(10*random.random((2,12)))print aprint hsplit(a,3) # split into 3print hsplit(a,4)print hsplit(a,(3,4)) # split a after the third and the fourth column# vsplit沿着纵向的轴分割，array split允许指定沿哪个轴分割 ​ 6.4 复制和视图 当运算和处理数据时，它们的数据有时被拷贝到新的数组有时不是。这有三种情况： 1.完全不拷贝：简单的赋值不拷贝数据对象或其数据 2.视图view和浅复制：不同的数组对象分享同一个数据，视图方法创造一个新的数组对象指向同一数据 3.深复制：完全赋值数据及其数据 ​ 12345678910111213141516171819202122232425262728293031323334 copy and view# 1.totally not copied a = arange(12)b = aprint b is a b.shape = 3,4 # change the shape of aprint a.shape# Python 传递不定对象作为参考，所以函数调用不拷贝数组def f(x): print (id(x))print f(a)# 2.view and shadow copy# 不同的数组对象分享同一个数据。视图方法创造一个新的数组对象指向同一数据,数据其实是存在源数组的c = a.view()print c is aprint c.base is a # c is a vie of the data owned by aprint c.flags.owndata # judge,c has no datac.shape = 2,6print a.shape # chage c's shape but a's shape goesnbt changec[0,4] = 1234print a # a's data changes# 切片数组返回它的一个视图：s = a[:,1:3] # spaces added for clarity; could also be written "s = a[:,1:3]"s[:] = 10 # # s[:] is a view of s. Note the difference between s=10 and s[:]=10print a# 3.deep copyd = a.copy() # a new array object with new data is createdd is aprint d is aprint d.base is a # d doesn't share anything with ad[0,0] = 9999print a ​ 7.函数和方法method总览这是个numpy函数和方法分类排列目录，这些名字链接到numpy示例，可看到函数是起作用的。 ​ 1234567891011121314151617181920212223# 1.创建数组arrange, array, copy, empty_like, eye, fromfile, fromfunction, identity, linspace, logspace, mgrid, ogrid, ones, ones_like, r, zeros, zeros_like# 2.转化 astype, atleast ld, atleast 2d, atleast 3d, mat# 3.操作array split, column stack, concatenate, diagonal, dsplit, dstack, hsplit, hstack, item, newsxis,ravel, repeat, reshape, resize, squeeze,swapaxes, take,transpose# 4.询问all, any, nonzero, where# 5.排序argmax, argmin, argsort, max, min, ptp, searchsorted, sort# 6.运算choose, compress, cumprod, cumsum, inner, fill, imag, prod, putmask, real, sum# 7.基本统计cov, mean, std, var# 8.基本线性代数cross, dot, outer, svd, vdot Reference Scipy：Quickstart tutorial 【Python数据分析】Numpy的详细教程 知乎专栏：Data Science with R&amp;Python kesci: Numpy快速上手指南 —- 入门篇]]></content>
      <categories>
        <category>Computer Science</category>
        <category>Data Mining and Analysis</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[panda数据分析小练习附代码]]></title>
    <url>%2F2018%2F01%2F26%2Fpanda%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B0%8F%E7%BB%83%E4%B9%A0%E9%99%84%E4%BB%A3%E7%A0%81%2F</url>
    <content type="text"><![CDATA[Abstract：11个panda数据分析小练习，附实现代码的GitHub仓库地址。 panda数据分析小练习来自guipsamora/pandas_exercises 代码实现实现代码地址：ScarlettYellow/pandas_excercises]]></content>
      <categories>
        <category>Computer Science</category>
        <category>Data Mining and Analysis</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas基础命令总结笔记]]></title>
    <url>%2F2018%2F01%2F26%2Fpandas%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Abstract: pandas基础命令总结笔记。 1.简介pandas和numpy是用Python做数据分析最基础且最核心的库 2.缩写解释 &amp; 库的导入df —- 任意的pandas DataFrame(数据框)对象s —- 任意的pandas Series(数组)对象 12import pandas as pd # 导入pandas库并简写为pdimport numpy as np # 导入numpy库并简写为np 3.数据的导入12345678pd.read_csv(filename) # 导入csv格式文件中的数据pd.read_table(filename) # 导入有分隔符的文本 (如TSV) 中的数据pd.read_excel(filename) # 导入Excel格式文件中的数据pd.read_sql(query, connection_object) # 导入SQL数据表/数据库中的数据pd.read_json(json_string) # 导入JSON格式的字符，URL地址或者文件中的数据pd.read_html(url) # 导入经过解析的URL地址中包含的数据框 (DataFrame) 数据pd.read_clipboard() # 导入系统粘贴板里面的数据pd.DataFrame(dict) # 导入Python字典 (dict) 里面的数据，其中key是数据框的表头，value是数据框的内容。 4.数据的导出1234df.to_csv(filename) # 将数据框 (DataFrame)中的数据导入csv格式的文件中df.to_excel(filename) # 将数据框 (DataFrame)中的数据导入Excel格式的文件中df.to_sql(table_name,connection_object) # 将数据框 (DataFrame)中的数据导入SQL数据表/数据库中df.to_json(filename) # 将数据框 (DataFrame)中的数据导入JSON格式的文件中 5.创建测试对象1pd.DataFrame(np.random.rand(10,5)) # 创建一个5列10行的由随机浮点数组成的数据框 DataFrame 123456//# 从一个可迭代的对象 my_list 中创建一个数据组my_list = [&apos;Kesci&apos;,100,&apos;欢迎来到科赛网&apos;]pd.Series(my_list) df.index = pd.date_range(&apos;2017/1/1&apos;, period = df.shape[0]) # 添加一个日期索引 index 12345import pandas as pdimport numpy as npdf= DataFrame(np.random.rand(10,10))df.index = pd.date_range(&apos;2018/1/1&apos;,period=df.shape[0])df 6.数据的查看与检查12345678910111213141516171819df.head(n) # 查看数据框的前n行df.shape #查看数据框的行数与列数df.tail(3) #查看数据框的最后n行df.info() #查看数据框的索引、数据类型和内存信息df.describe() # 对于数据类型为数值型的列，查询其描述性统计的内容s.value_counts(dropna=False) #查询每个独特数据值出现次数统计df.dtypes #查看每列的数据类型//示例df = pd.DataFrame(np.random.rand(10,5))df.head(3) # 查看数据框的前3行df.apply(pd.Series.value_counts) #查询数据框中每个列的独特数据值出现次数统计df = pd.DataFrame(np.random.rand(10,5))df.tail(4) #查看数据框的最后4行df.shape #查看数据框的行数与列数s = pd.Series([1,2,3,4,np.nan,5,5,6,7,8])s.value_counts(dropna=False) 1234# 将字符型转为浮点数dollarizer = lambda x: float(x[1:-1])chipo.item_price = chipo.item_price.apply(dollarizer) # change item_price into floatprint chipo.item_price 7.数据的选取1234df[col] #以数组series形式返回选取的列df = pd.DataFrame(np.random.rand(5,5),columns=list(&apos;ABCDE&apos;))df[&apos;C&apos;] 1234df[[col1,col2]] # 以新的数据框(DataFrame)的形式返回选取的列df = pd.DataFrame(np.random.rand(5,5),columns=list(&apos;ABCDE&apos;))df[[&apos;B&apos;,&apos;E&apos;]] 1234s.iloc[0] #按照位置选取s = pd.Series(np.array([&apos;I&apos;,&apos;Love&apos;,&apos;Data&apos;]))s.iloc[0] 1234s.loc[&apos;index_one&apos;] # 按照索引选取s = pd.Series(np.array([&apos;I&apos;,&apos;Love&apos;,&apos;Data&apos;]))s.loc[1] 1234df.iloc[0,:] # 选取第一行df = pd.DataFrame(np.random.rand(5,5),columns=list(&apos;ABCDE&apos;))df.iloc[0,:] 1234df.iloc[0,0] # 选取第一行的第一个元素df = pd.DataFrame(np.random.rand(10,5),columns=list(&apos;ABCDE&apos;))df.iloc[0,0] 1euro.iloc[: , :-3] # Select all columns except the last 3 12# .loc is another way to slice, using the labels of the columns and indexesprint euro12.loc[euro12.Team.isin(['England', 'Italy', 'Russia']), ['Team','Shooting Accuracy']] # Present only the Shooting Accuracy from England, Italy and Russia 8.数据的清洗1del crime['Total'] 1234567df.columns = [&apos;a&apos;,&apos;b&apos;] # 重命名数据框的列名称df = pd.DataFrame(&#123;&apos;A&apos;:np.array([1,np.nan,2,3,6,np.nan]), &apos;B&apos;:np.array([np.nan,4,np.nan,5,9,np.nan]), &apos;C&apos;:&apos;foo&apos;&#125;)df.columns = [&apos;a&apos;,&apos;b&apos;,&apos;c&apos;]df 123456pd.isnull() # 检查数据中空值出现的情况，并返回一个由布尔值(True,Fale)组成的列df = pd.DataFrame(&#123;&apos;A&apos;:np.array([1,np.nan,2,3,6,np.nan]), &apos;B&apos;:np.array([np.nan,4,np.nan,5,9,np.nan]), &apos;C&apos;:&apos;foo&apos;&#125;)pd.isnull(df) 123456pd.notnull() # 检查数据中非空值出现的情况，并返回一个由布尔值(True,False)组成的列df = pd.DataFrame(&#123;'A':np.array([1,np.nan,2,3,6,np.nan]), 'B':np.array([np.nan,4,np.nan,5,9,np.nan]), 'C':'foo'&#125;)pd.notnull(df) 123456df.dropna() # 移除数据框 DataFrame 中包含空值的行df = pd.DataFrame(&#123;'A':np.array([1,np.nan,2,3,6,np.nan]), 'B':np.array([np.nan,4,np.nan,5,9,np.nan]), 'C':'foo'&#125;)df.dropna() 123456df.dropna(axis=1) # 移除数据框 DataFrame 中包含空值的列df = pd.DataFrame(&#123;'A':np.array([1,np.nan,2,3,6,np.nan]), 'B':np.array([np.nan,4,np.nan,5,9,np.nan]), 'C':'foo'&#125;)df.dropna(axis=1) 1234567df.dropna(axis=1,thresh=n) # 移除数据框df中空值个数不超过n的行df = pd.DataFrame(&#123;'A':np.array([1,np.nan,2,3,6,np.nan]), 'B':np.array([np.nan,4,np.nan,5,9,np.nan]), 'C':'foo'&#125;)test = df.dropna(axis=1,thresh=1)test 123456df.fillna(x) # 将数据框 DataFrame 中的所有空值替换为 xdf = pd.DataFrame(&#123;'A':np.array([1,np.nan,2,3,6,np.nan]), 'B':np.array([np.nan,4,np.nan,5,9,np.nan]), 'C':'foo'&#125;)df.fillna('Test') 1234s.fillna(s.mean()) #将所有空值替换为平均值s = pd.Series([1,3,5,np.nan,7,9,9])s.fillna(s.mean()) 1234s.astype(float) # 将数组(Series)的格式转化为浮点数s = pd.Series([1,2,np.nan,2,3])s.astype(float) 12s = pd.Series([1,3,5,np.nan,7,9,9]) #将数组(Series)中的所有1替换为'one's.replace(1,'one') 12s = pd.Series([1,3,5,np.nan,7,9,9])s.replace([1,3],['one','three']) # 将数组(Series)中所有的1替换为'one', 所有的3替换为'three' 1234df.rename(columns=lambda x: x + 2) # 将全体列重命名df = pd.DataFrame(np.random.rand(4,4))df.rename(columns=lambda x: x+ 2) 1234df.rename(columns=&#123;'old_name': 'new_ name'&#125;) # 将选择的列重命名df = pd.DataFrame(np.random.rand(10,5),columns=list('ABCDE'))df.rename(columns=&#123;'A':'newA','C':'newC'&#125;) 1234df.set_index('column_one') # 改变索引df = pd.DataFrame(np.random.rand(10,5),columns=list('ABCDE'))df.set_index('B') 1234df.rename(index = lambda x: x+ 1) # 改变全体索引df = pd.DataFrame(np.random.rand(10,5))df.rename(index = lambda x: x+ 1) 9.数据的过滤(filter),排序(sort)和分组(groupby)123# Reset the index so it begins with 0 againiris = iris.resnet_index(drop = True)print iris.head() 1apple.sort_index(ascending = True).head() 1euro12[euro12.Team.str.startswith('G')] #Select the teams that start with G 1234df[df[col] &gt; 0.5] # 选取数据框df中对应行的数值大于0.5的全部列df = pd.DataFrame(np.random.rand(10,5),columns=list('ABCDE'))df[df['A'] &gt; 0.5] 1234df[(df[col] &gt; 0.5) &amp; (df[col] &lt; 0.7)] # 选取数据框df中对应行的数值大于0.5，并且小于0.7的全部列df = pd.DataFrame(np.random.rand(10,5),columns=list('ABCDE'))df[(df['C'] &gt; 0.5) &amp; (df['D'] &lt; 0.7)] 1234df.sort_values(col1) # 按照数据框的列col1升序(ascending)的方式对数据框df做排序df = pd.DataFrame(np.random.rand(10,5),columns=list('ABCDE'))df.sort_values('E') 1234df.sort_values(col2,ascending=False) # 按照数据框的列col2降序(descending)的方式对数据框df做排序df = pd.DataFrame(np.random.rand(10,5),columns=list('ABCDE'))df.sort_values('A',ascending=False) 1234df.sort_values([col1,col2],ascending=[True,False]) # 按照数据框的列col1升序，col2降序的方式对数据框df做排序df = pd.DataFrame(np.random.rand(10,5),columns=list('ABCDE'))df.sort_values(['A','E'],ascending=[True,False]) 12345678df.groupby(col) # 按照某列对数据框df做分组df = pd.DataFrame(&#123;'A':np.array(['foo','foo','foo','foo','bar','bar']), 'B':np.array(['one','one','two','two','three','three']), 'C':np.array(['small','medium','large','large','small','small']), 'D':np.array([1,2,2,3,3,5])&#125;)df.groupby('A').count() 12345678df.groupby([col1,col2]) # 按照列col1和col2对数据框df做分组df = pd.DataFrame(&#123;'A':np.array(['foo','foo','foo','foo','bar','bar']), 'B':np.array(['one','one','two','two','three','three']), 'C':np.array(['small','medium','large','large','small','small']), 'D':np.array([1,2,2,3,3,5])&#125;)df.groupby(['B','C']).sum() 1234567df.groupby(col1)[col2].mean() # 按照列col1对数据框df做分组处理后，返回对应的col2的平均值df = pd.DataFrame(&#123;'A':np.array(['foo','foo','foo','foo','bar','bar']), 'B':np.array(['one','one','two','two','three','three']), 'C':np.array(['small','medium','large','large','small','small']), 'D':np.array([1,2,2,3,3,5])&#125;)df.groupby('B')['D'].mean() 1234567df.groupby(col1)[col2].mean() # 按照列col1对数据框df做分组处理后，返回对应的col2的平均值df = pd.DataFrame(&#123;'A':np.array(['foo','foo','foo','foo','bar','bar']), 'B':np.array(['one','one','two','two','three','three']), 'C':np.array(['small','medium','large','large','small','small']), 'D':np.array([1,2,2,3,3,5])&#125;)df.groupby('B')['D'].mean() 123//计算每一单的平均金额order_grouped = chipo.groupby(by=[&apos;order_id&apos;]).sum() print order_grouped.mean()[&apos;item_price&apos;] #What is the average amount per order? 123456789df.pivot_table(index=col1,values=[col2,col3],aggfunc=mean) # 做透视表，索引为col1,针对的数值列为col2和col3，分组函数为平均值df = pd.DataFrame(&#123;&apos;A&apos;:np.array([&apos;foo&apos;,&apos;foo&apos;,&apos;foo&apos;,&apos;foo&apos;,&apos;bar&apos;,&apos;bar&apos;]), &apos;B&apos;:np.array([&apos;one&apos;,&apos;one&apos;,&apos;two&apos;,&apos;two&apos;,&apos;three&apos;,&apos;three&apos;]), &apos;C&apos;:np.array([&apos;small&apos;,&apos;medium&apos;,&apos;large&apos;,&apos;large&apos;,&apos;small&apos;,&apos;small&apos;]), &apos;D&apos;:np.array([1,2,2,3,3,5])&#125;)df.pivot_table(df,index=[&apos;A&apos;,&apos;B&apos;], columns=[&apos;C&apos;],aggfunc=np.sum) 1234567df.groupby(col1).agg(np.mean) # 以col1分组，分组函数为平均值df = pd.DataFrame(&#123;'A':np.array(['foo','foo','foo','foo','bar','bar']), 'B':np.array(['one','one','two','two','three','three']), 'C':np.array(['small','medium','large','large','small','small']), 'D':np.array([1,2,2,3,3,5])&#125;)df.groupby('A').agg(np.mean) 1234df.apply(np.mean) # 对数据框df的每一列求平均值df = pd.DataFrame(np.random.rand(10,5),columns=list('ABCDE'))df.apply(np.mean) 1234df.apply(np.max,axis=1) # 对数据框df的每一行求最大值df = pd.DataFrame(np.random.rand(10,5),columns=list('ABCDE'))df.apply(np.max,axis=1) 10.数据的连接(join)与组合(combine)1234567891011121314df1.append(df2) # 在数据框df2的末尾添加数据框df1，其中df1和df2的列数应该相等df1 = pd.DataFrame(&#123;'A': ['A0', 'A1', 'A2', 'A3'], 'B': ['B0', 'B1', 'B2', 'B3'], 'C': ['C0', 'C1', 'C2', 'C3'], 'D': ['D0', 'D1', 'D2', 'D3']&#125;, index=[0, 1, 2, 3])df2 = pd.DataFrame(&#123;'A': ['A4', 'A5', 'A6', 'A7'], 'B': ['B4', 'B5', 'B6', 'B7'], 'C': ['C4', 'C5', 'C6', 'C7'], 'D': ['D4', 'D5', 'D6', 'D7']&#125;, index=[4, 5, 6, 7])df1.append(df2) 12345678910111213df1.join(df2,on=col1,how='inner') # 对数据框df1和df2做内连接，其中连接的列为col1df1 = pd.DataFrame(&#123;'A': ['A0', 'A1', 'A2', 'A3'], 'B': ['B0', 'B1', 'B2', 'B3'], 'key': ['K0', 'K1', 'K0', 'K1']&#125;) df2 = pd.DataFrame(&#123;'C': ['C0', 'C1'], 'D': ['D0', 'D1']&#125;, index=['K0', 'K1']) df1.join(df2, on='key') 123pd.merge(data1, data2, on='subject_id', how='inner') #Merge only the data that has the same 'subject_id' on both data1 and data2pd.merge(data1, data2, on='subject_id', how='outer') # Merge all values in data1 and data2, with matching records from both sides where available 11.数据的统计123456789df.describe() # 得到数据框df每一列的描述性统计df.mean()df.describe(include = "all") #统计所有的列df = pd.DataFrame(np.random.rand(10,5),columns=list('ABCDE'))df.describe()df = pd.DataFrame(np.random.rand(10,5),columns=list('ABCDE'))df.mean() 1234df.corr() # 得到数据框df中每一列与其他列的相关系数df = pd.DataFrame(np.random.rand(10,5),columns=list('ABCDE'))df.corr() 1234df.count() # 得到数据框df中每一列的非空值个数df = pd.DataFrame(np.random.rand(10,5),columns=list('ABCDE'))df.count() 1234df.max() # 得到数据框df中每一列的最大值df = pd.DataFrame(np.random.rand(10,5),columns=list('ABCDE'))df.max() 1234df.min() # 得到数据框df中每一列的最小值df = pd.DataFrame(np.random.rand(10,5),columns=list('ABCDE'))df.min() 1234df.median() # 得到数据框df中每一列的中位数df = pd.DataFrame(np.random.rand(10,5),columns=list('ABCDE'))df.median() 1234df.std() # 得到数据框df中每一列的标准差df = pd.DataFrame(np.random.rand(10,5),columns=list('ABCDE'))df.std() ReferencePandas Cheat Sheet — Python for Data Science 十分钟学会pandas]]></content>
      <categories>
        <category>Computer Science</category>
        <category>Data Mining and Analysis</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python关键代码总结]]></title>
    <url>%2F2018%2F01%2F26%2FPython%E5%85%B3%E9%94%AE%E4%BB%A3%E7%A0%81%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[Abstract: Python关键代码，主要关于类与对象、封装。 1.类与对象 Python 是一种面向对象的程序语言，因此它也有类（Class）与对象（Object）这两个概念。在了解 Python 面向对象编程的案例前，我们需要先熟悉面向对象编程的一些基本概念： 类 (Class)：用来描述具有相同的属性和方法的对象的集合。它定义了该集合中每个对象所共有的属性和方法。对象是类的实例。 类变量：类变量在整个实例化的对象中是公用的。类变量定义在类中且在函数体之外。类变量通常不作为实例变量使用。 数据成员：类变量或者实例变量用于处理类及其实例对象的相关的数据。 方法重写：如果从父类继承的方法不能满足子类的需求，可以对其进行改写，这个过程叫方法的覆盖（override），也称为方法的重写。 实例变量：定义在方法中的变量，只作用于当前实例的类。 继承：即一个派生类（derived class）继承基类（base class）的字段和方法。继承也允许把一个派生类的对象作为一个基类对象对待。例如，一个「狗」类的对象派生自「动物」类，这是模拟”是一个（is-a）”关系（狗是一种动物）。 实例化：创建一个类的实例，类的具体对象。 方法：类中定义的函数。 对象：通过类定义的数据结构实例。对象包括两个数据成员（类变量和实例变量）和方法。 12345class Vehicle: passcar = Vehicle()print (car) //如car是Vehicle的一个实例/对象 12345678class Vehicle: def __init__(self, number_of_wheels, type_of_tank, seating_capacity, maximum_velocity): self.number_of_wheels = number_of_wheels self.type_of_tank = type_of_tank self.seating_capacity = seating_capacity self.maximum_velocity = maximum_velocity tesla_model_s = Vehicle(4, &apos;electric&apos;, 5, 250) 我们将发送信息到对象的过程称为方法，即对象的行为： 123456789101112class Vehicle: def __init__(self, number_of_wheels, type_of_tank, seating_capacity, maximum_velocity): self.number_of_wheels = number_of_wheels self.type_of_tank = type_of_tank self.seating_capacity = seating_capacity self.maximum_velocity = maximum_velocity def number_of_wheels(self): return self.number_of_wheels def set_number_of_wheels(self, number): self.number_of_wheels = number 1234567891011121314class Vehicle: def __init__(self, number_of_wheels, type_of_tank, seating_capacity, maximum_velocity): self.number_of_wheels = number_of_wheels self.type_of_tank = type_of_tank self.seating_capacity = seating_capacity self.maximum_velocity = maximum_velocity @property def number_of_wheels(self): return self.number_of_wheels @number_of_wheels.setter def number_of_wheels(self, number): self.number_of_wheels = number 以上语句实现了两个方法，number_of_wheels 和 set_number_of_wheels。我们可以称为 getter &amp; setter，因为第一个方法获取了属性值，而第二个方法将为该属性设置一个新的值。在类的内部，使用 def 关键字可以为类定义一个方法，与一般函数定义不同，类方法必须包含参数 self，且为第一个参数。 在 Python 中，我们能使用 @property (decorators) 定义 getter &amp; setter. 123print(tesla_model_s.number_of_wheels) # 4tesla_model_s.number_of_wheels = 2 # setting number of wheels to 2print(tesla_model_s.number_of_wheels) # 2 使用这些方法作为属性. 2.封装：隐藏信息封装是一种限制直接访问目标属性和方法的机制，但同时它又有利于对数据（对象的方法）进行操作。 封装是一种将抽象性函数接口的实现细节部分包装、隐藏起来的方法。同时，它也是一种防止外界调用端，去访问对象内部实现细节的手段，这个手段是由编程语言本身来提供的。 对象所有的内部表征对于外部来说都是隐藏的，只有对象能直接与内部数据交互。首先，我们需要理解公开（public）和私有（non-public）实例变量和方法。 2.1 公开实例变量初始化公开实例变量： 123456789class Person: def __init__(self, first_name): self.first_name = first_name tk = Person(&apos;TK&apos;)print(tk.first_name)class Person: first_name = &apos;TK&apos; 给tk对象赋值，以及二次赋值 12345678//给tk对象赋值，以及二次赋值class Person: def __init__(self,first_name): self.first_name = first_name tk = Person(&apos;TK&apos;)tk.first_name = &apos;kaios&apos;print(tk.first_name) 2.2 私有实例变量和公开实例变量一样，我们可以使用 constructor 方法或在类的内部声明而定义一个私有实例变量。语法上的不同在于私有实例变量在变量名前面加一个下划线： 123456789class Person: def __init__(self, first_name, email): self.first_name = first_name self._email = email//上述定义的 email 变量就是私有变量。tk = Person(&apos;kaio&apos;,&apos;tk@email.com&apos;)print(tk._email)//输出私有变量值 我们可以访问并且更新它，私有变量仅是一个约定，即他们需要被视为 API 非公开的部分。所以我们可以使用方法在类的定义中完成操作，例如使用两种方法展示私有实例的值与更新实例的值： 12345678910class Person: def __init__(self, first_name, email): self.first_name = first_name self._email = email def update_email(self, new_email): self._email = new_email def email(self): return self._email 先初始化 Person 类并赋值，然后通过定义的方法访问并打印私有变量的值。如我们直接赋值给私有变量新的值，那么打印出来还是原有的值，我们只能通过在类里面定义的方法进行操作而更新私有变量。 123456tk = Person(&apos;TK&apos;, &apos;tk@mail.com&apos;)print(tk.email()) # =&gt; tk@mail.comtk._email = &apos;new_tk@mail.com&apos;print(tk.email()) # =&gt; tk@mail.comtk.update_email(&apos;new_tk@mail.com&apos;)print(tk.email()) # =&gt; new_tk@mail.com 2.3 公开方法对于公开方法（public methods），我们同样能在类以外的地方调用，以下定义了一个类与方法： 12345678910class Person: def __init__(self, first_name, age): self.first_name = first_name self._age = age def show_age(self): return self._age tk = Person(&apos;TK&apos;, 25)print(tk.show_age()) 2.4 私有方法私有方法应该要看作 API 的私有部分。如下我们声明了私有方法_get_age和公开方法 show_age。show_age 方法可以在类的外部调用，而_get_age 只能在类内部使用。 12345678910111213class Person: def __init__(self, first_name, age): self.first_name = first_name self._age = age def show_age(self): return self._get_age() def _get_age(self): return self._agetk = Person(&apos;TK&apos;, 25)print(tk.show_age()) # =&gt; 25 2.5 封装小结通过程序封装，我们确保了对象的内部表征对外是隐藏的。而面向对象的编程带来的主要好处之一是代码的重用，实现这种重用的方法之一是通过继承机制。继承完全可以理解成类之间的类型和子类型关系。 若我们有一辆车，且知道车轮数、座位容量和最大时速，那么一辆电动车类就继承常规汽车类中的相同属性. 123456789101112131415161718192021222324class Car: def __init__(self, number_of_wheels, seating_capacity, maximum_velocity): self.number_of_wheels = number_of_wheels self.seating_capacity = seating_capacity self.maximum_velocity = maximum_velocity//更新类中的一个对象：my_car = Car(4, 5, 250)print(my_car.number_of_wheels)print(my_car.seating_capacity)print(my_car.maximum_velocity) //初始化对象后，Python 可以将父类（parent class）作为参数应用到子类（child class）中。因此电动车类可以从汽车类继承对象。class ElectricCar(Car): def __init__(self, number_of_wheels, seating_capacity, maximum_velocity): Car.__init__(self, number_of_wheels, seating_capacity, maximum_velocity) //我们不需要实现其他方法，因为电动汽车类已经从汽车类继承了对象：my_electric_car = ElectricCar(4, 5, 250)print(my_electric_car.number_of_wheels) # =&gt; 4print(my_electric_car.seating_capacity) # =&gt; 5print(my_electric_car.maximum_velocity) # =&gt; 250 3.Reference科赛 x 机器之心 | 从零上手Python关键代码]]></content>
      <categories>
        <category>Computer Science</category>
        <category>Data Mining and Analysis</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo Error|Deployer not found:git]]></title>
    <url>%2F2018%2F01%2F23%2FHexo-Error-Deployer-not-found-git%2F</url>
    <content type="text"><![CDATA[Abstract：使用hexo时遇到了这个Error，本文是解决方法 ~ Error：执行 hexo deploy 后,出现 error deployer not found:git 的错误处理 原因：hexo 更新到3.0之后，deploy的type 的github需要改成git 解决方法：输入代码 1npm install hexo-deployer-git --save]]></content>
      <categories>
        <category>Computer Science</category>
        <category>Coding</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[研究设计|明星企业家微博发布策略对个人形象评价的影响]]></title>
    <url>%2F2018%2F01%2F21%2F%E7%A0%94%E7%A9%B6%E8%AE%BE%E8%AE%A1-%E6%98%8E%E6%98%9F%E4%BC%81%E4%B8%9A%E5%AE%B6%E5%BE%AE%E5%8D%9A%E5%8F%91%E5%B8%83%E7%AD%96%E7%95%A5%E5%AF%B9%E4%B8%AA%E4%BA%BA%E5%BD%A2%E8%B1%A1%E8%AF%84%E4%BB%B7%E7%9A%84%E5%BD%B1%E5%93%8D%2F</url>
    <content type="text"><![CDATA[Abstract：传播学研究方法课的大作业——一份研究设计（不包含研究实施和研究结果），研究主题为：明星企业家微博策略对个人形象评价的影响。 摘要：本研究基于戈夫曼的戏剧理论，通过数据挖掘与分析法、实验法、内容分析法、调查法，重点研究明星企业家微博发布时间节点和微博内容的语言风格对其形象评价的交互影响，进而探究明星企业家微博发布策略对其个人形象评价的影响。本研究希望通过对明星企业家微博发布策略的研究，为企业家如何利用微博舆论场合理发声，塑造自身良好的形象，进而推动企业品牌的发展提供一定的借鉴意义。 关键词：明星企业家 微博发布策略 形象评价 印象管理 一、引言大多数成功的企业背后，都有一个出色而传奇的企业家，他们对于企业品牌的维持和发展发挥着不可替代的作用。随着社会化媒体的流行和发展，企业家作为企业的重要象征符号，对消费者的态度和行为产生着越来越大的影响，他们的个人形象在商业竞争中发挥着越来越明显的作用（何志毅，2005）。明星企业家作为企业家中特殊的群体，在微博和公众视野中更为活跃。微博由于其具有的环境开放、信息传播速度快速、信息辐射极广的优势，近两年的发展势头仍较为强劲，在我国自媒体平台中占有无可替代的重要地位，所以微博对于企业家个人形象的建构起着至关重要的作用。然而当前，对于企业家微博及其形象的相关研究还不尽完善，有关企业家微博发布策略对其形象评价的影响的相关领域研究还是亟待完善的。 本研究基于戈夫曼的戏剧理论，通过数据挖掘与分析法、实验法、内容分析法、调查法重点研究了明星企业家微博发布时间节点和微博内容的语言风格对其形象评价的交互影响，进而探究明星企业家微博发布策略对个人形象评价的影响。即具体探究企业家微博怎样的语言风格更有利于给受众形成积极的印象？什么样的时间节点的微博更有利于给受众留下积极的影响？时间节点和语言风格两者是如何对明星企业家形象产生作用的？对于这些问题的研究，在理论上成为新媒体营销沟通理论的有益补充；在实践上希望通过明星企业家群体微博发布成功或是失败的例子带给社会众多企业家一些有益的启示。 二、文献综述1.研究概念与范畴的界定1.1明星企业家1.1.1企业家 对于“明星企业家”的研究，首先应该明确“企业家”的概念，“企业家”（entrepreneur）一词来源于法文，其原意是指“冒险事业的经营者或组织者”，在现代经济学与企业理论中，多将企业家分为两类，一类是企业所有者企业家，作为所有者他们仍从事企业的经营管理工作；另一类是受雇于所有者的职业企业家。在本研究中，我们所指称的企业家只指第一种类型，而把第二种类型称作职业经理人。 1.1.2明星企业家的概念化定义 那么何谓“明星企业家”？在当下的语境下，狭义的明星多指具有高知名度的影视音乐领域的表演者，而广义的“明星”则泛指各领域类具有高度知名度的人。因此“明星企业家”既指“有高知名度的大中型企业的所有者经营者”。 1.1.3明星企业家的操作化定义 结合本研究的研究方向，我们将“明星企业家”操作化定义为，在中国大陆地区具有高财富净值的企业实际控制人（《财富》杂志“2017·中国500强企业排行榜”中上榜企业的实际控制者，或深圳胡润研究院发布的《36计•胡润百富榜2017》中的上榜企业家），且受到国内各大媒体的较多关注（国内专业媒体对其在过去一年内有3篇以上报道，以“企业家品牌+企业家姓名”作为关键词在百度指数中搜索，选取指数高的企业家），在过去半年内持续使用微博且微博粉丝数在百万以上的的企业家，我们将之定义为“明星企业家”。 1.2微博发布策略1.2.1微博的定义与特点 微博（Weibo），即微型博客（MicroBlog）的简称，也即是博客的一种，是一种通过关注机制分享简短实时信息的广播式的社交网络平台。微博最早起始于2006的美国Twitter，是一个基于用户关系信息分享、传播以及获取的平台。用户可以通过WEB、WAP等各种客户端组建个人社区，以简短的文字更新信息，并实现即时分享。微博的关注机制分为单向、双向两种。微博在中国一般指新浪微博，本次研究中的研究对象也是指新浪微博。 微博由于自身具有的受众广、成本低、传播迅速的特点，为企业家个人形象及企业品牌的塑造提供了便利的渠道，微博合理使用可以扩大企业家的影响力和提升其个人形象。比尔盖茨到中国举办慈善晚宴前亦开通新浪微博，与Twitter账户同步，短短10小时就有愈10万粉丝报到。可以这么说，对于企业家而言，微博是能够让企业家的个人形象魅力、企业的产品与服务，形成巨大影响并深刻触达用户的重要网络平台。所以研究明星企业家的微博发布策略是很有现实意义的。 1.2.2微博发布策略 主要包括微博发布的内容、语言风格、时间节点及频次三个主要方面， 对于不同的微博发布内容、语言风格和时间节点频次，受众的反应和态度有着很大的差别。本文主要通过语言风格和时间节点两个维度划分明星企业家的微博，进而探究其对于企业家形象的影响。这里的语言风格是指一个人通过发布微博的行为表现出来语言的格调；时间节点是指一个人发布微博所选取的时间点。 1.3企业家形象1.3.1个人形象 个人形象是一个人的外表或容貌以及内在品质的外部反映。 1.3.2企业家形象 企业家形象是指个体或公众对企业家的总体感知。企业家形象维度包括:能力、诚信、可靠、魅力等。企业家个人形象与所在企业及产品有高关联度，企业家是企业品牌的代表和象征（Cravens, Oliver&amp; Ramanmoorti,2003）。企业家通过合理的微博发布策略，不仅可以打造自己的个人品牌，提升个人形象，同时还可以提升企业及产品的品牌形象。而且在此之前已经有相关研究发现:比起企业微博，企业家微博对人们的影响力更大。 1.3.3明星企业家形象 明星企业家由于自身的高财富值、高知名度和在微博上的较高活跃度，更容易被人们熟知并形成对他们从外表到内在的形象总体感知。 2.研究对象的现状综述（研究背景）2.1微博现状据CNNIC发布的第33次《中国互联网络发展状况统计报告》，2013年微博的用户规模和使用均有下降；而从2014年下半年起至今，微博焕发第二春，用户规模和活跃度都有所提升。据2017年第40次《中国互联网络发展状况统计报告》，微博的用户规模已达到27,143万，半年增长率达到7.1%。 2.2当前企业家微博发布对于个人形象评价的影响情况随着微博的流行，越来越多的企业家将微博作为企业家个人品牌和企业品牌塑造的重要工具。企业家通过写微博进行自己展示和呈现,试图建立与消费者之间的联结和关系,以期影响消费者对企业家形象以及品牌形象的认知。然而，当前许多企业家微博的发布仍存在许多问题。有的企业家通过微博这个平台频繁与网友进行交流塑造了良好的个人形象，同时推动了企业的发展，甚至在微博上形成了自己专属的一批死忠粉。如：小米的雷军。而有的企业家并不擅长运营微博，凭借主观想法随意发布微博信息，不但没有达到预期效果，还可能导致自身形象受损，甚至对企业品牌产生了不良的影响。这些企业家们微博发布的语言风格和时间节点迥异，显然这些因素会影响到企业家的个人品牌形象构建，所以本文将重点探究明星企业家微博的语言风格和时间节点对企业家形象的影响以及两者是如何交互产生作用的。 2.3当前明星企业家微博发布对个人形象评价的影响情况明星企业家频繁出现在公众的视野，通过微博平台发布消息，与网友进行互动，并迅速引发社会话题的发酵。大多数明星企业家通过较为合理的微博发布策略，与网友之间建立了较为和谐的社会互动关系，同时也有的明星企业家由于发布的微博受到了公众的质疑。2014年，联想总裁杨元庆进驻微博，进驻以来,以其自己的风格与微博用户进行互动交流,迅速获得众多粉丝，2016年博文月阅读量超过1000万以上，其与联想众官微、高管等账号之间互动交流形成的社交矩阵，不仅塑造了杨元庆真诚、包容的形象，也对于联想品牌形象有很大的提升作用。 3.研究对象现状的动因自从2015年起，之前仿佛被用户抛弃的微博逐渐复苏，微博用户活跃程度提升。通过文献梳理发现，近两年对于相关话题的研究还几近空白，之前对于企业家微博发布策略对其形象的影响的相关研究无论是从微博发布策略的角度还是企业家形象视角都不够全面和深入。我们希望通过对明星企业家微博发布策略的研究，为企业家如何利用微博舆论场合理发声，塑造自身良好的形象，进而推动企业品牌的发展提供一定的借鉴意义。 4.关于类似话题研究的综述4.1有关明星企业家的文献综述当前学术界相关话题的研究对象一般都是企业家微博，而对于明星企业家这个定义还没有进行研究。 4.2有关微博发布策略的文献综述对于企业家微博发布策略的研究，学者对于企业家微博发布策略的研究主要是通过对企业家微博内容的划分。在微博情景下, Kwon&amp;Sung(2011)把微博信息分为五类:产品相关、企业相关、资源、网址及品牌名提及；谢庆红等(2011)则把企业家微博信息分为与企业相关的微博，即企业家在微博上主要提及与工作相关的内容，以及与个人相关的内容，这部分主要包括企业家个人兴趣爱好、家庭朋友、生活感悟等内容；魏川(基于微博的企业家个人品牌策论研究 2011)通过专业性和互动性两个维度将基于微博的企业家微博策略进行分类；黄静（企业家微博信息对于其形象评价的影响机制研究2014）则把企业家微博划分为做人和做事两个方面；周飞、沙振权(企业微博信息质量对粉丝品牌态度的影响机理研究 2015)将企业家微博信息质量界定为消费者对企业家在微博平台上发布信息的真实性、有用性、趣味性等方面的评价,并分为准确性、适当性和可读性三个维度测量。可见，学者对于这个话题的研究主要是对企业家微博内容的多维度划分，而且基本集中在2011年—2013年微博在中国刚刚兴起并迅速发展的阶段。而伴随着微信朋友圈对于微博的冲击和微博自身的问题，微博热在中国渐渐消减，业界学者把注意力大多转移到其他新兴媒体的研究，对于企业家微博发布策略这个问题的研究几近空白。自2015年，微博复苏迹象越来越突出，但近两年来对于这方面的研究仍然很是缺乏。由于之前的相关研究已经对企业家微博的内容划分和研究较为深入，以下我们对于企业家微博发布策略的研究主要集中在语言风格和时间节点两个维度。 从研究方法上看，此前的对企业家微博信息和发布策略的研究基本上是采用传统的人工分析数据的方法（如人工对信息进行编码，多个独立裁判对信息进行分类），这些方法具有时间和⼈⼒成本高，数据规模有限，分析结果带有主观性等缺点。而本次研究设计的研究，我们采用计算机领域前沿的数据挖掘技术对大规模数据进行分析，克服了这一研究局限。 4.3有关企业家微博的影响机制的文献综述在此之前的大多数研究论文把研究焦点放在了企业家微博对于企业产生的影响，只有少部分重点研究了企业家微博对于其个人形象的影响。如：何嫱.企业家微博对企业品牌形象的影响及作用机制。外国关于“明星企业家Twitter发布策略对个人形象建构的影响”的相关研究还比较少。题材比较相似的一篇是Caroly M.Brown:在《6 Twitter Tips for Entrepreneurs》中对企业家提出了六点使用推特的建议，但研究形式和内容比较简单。其他文章要么把研究对象放在了企业身上，要么研究如何利用Twitter进行媒介数据分析。 4.4有关个人形象的文献综述对于个人形象的研究国内外已经非常充分了。在国内，宋新宇（2002）认为个人品牌就是个人在工作中显示出的个人价值，它就像企业品牌、产品品牌一样拥有知名度、美誉度和忠诚度。周玉波（2003）认为个人品牌是一种名称，有公认的无形价值，有公众肯定倾向的客观依据，代表该人具有优秀的综合素质或者精湛的专业技能，标志着身份，体现着实力。 在国外，麦克纳利（2003）认为个人品牌是在建立某种关系时，他人根据体验所持有的一种印象或情感。希勒（2003）认为个人品牌是你在受众中的首要印象。蒙托亚和梵得荷（2004）认为个人品牌是他人对你的能力、作用以及价值的评价和看法。国内外这些关于个人形象的研究为我们进行明星企业家微博策略对个人形象评价的影响的研究提供了理论基础。 三、研究假设与研究框架1.研究内容本研究将以使用微博的知名企业家为案例，选取分属于不同的行业，如电商、科技、房地产、家电等，考虑研究对象的年龄跨度和自身特点，选取具有一定的代表性，较为全面的N位“明星企业家”，保证其研究价值与指导意义。从选取企业家的个人微博使用策略出发，基于戈夫曼的戏剧理论，从印象管理的角度，运用数据挖掘与分析法、实验法、内容分析法、调查法，探究其微博发布时间节点和微博内容的语言风格，分析其对于个人形象评价的作用与影响，延伸到对他们所在企业的影响，试图论证明星企业家的微博使用对其个人形象评价有着关键性的影响，并由此，给企业家群体的微博使用策略提供几点尝试性的参考。 2.理论基础戏剧理论：戏剧理论是用表演和比喻来说明日常生活中人的互动的理论。它的基本观点是：社会是一个舞台，全体社会成员是在这个舞台上扮演不同角色的演员。他们都在社会互动中“表演”自己，塑造自己的形象并更好地达到自己的目的。根据戏剧理论的观点，在企业家的微博使用过程中，以个人运用符号的能力为出发点，重视符号在微博互动过程中的作用。在基于微博平台的互动中，表演者要如何管理自己的行动，使其在观众面前呈现出他期望中的角色，即“印象管理”或“自我呈现”，它是戏剧理论的重要特征。在处理别人对自己的印象时，即在进行印象管理时，人们常常会努力运用各种符号，如道具，来布置自己的“舞台”，以便达到“印象管理”的目的，即通过自己努力地表演使自己以符合人们期待的方式出现在人们面前，并且试图与他人保持一定的社会距离以保持他的形象。 3.研究假设企业家形象在企业品牌形象塑造方面扮演着重要角色，能够显著提升企业的品牌形象和绩效评价（ Gaines-Ross，2000；何志毅、王广富，2005）。它是个体或公众对企业家的总体感知，受企业家言语行为的影响（ Benoit and Brinson， 1999）。微博为网络时代人们获取和传播信息的重要渠道，具有即时性、 辐射范围广、 速度快的特点（ Kaplan and Haenlein，2011）。企业家通过微博平台发布的信息成为公众评价的重要线索，影响其在公众心目中的形象（谢庆红等， 2013）。 然而，企业家的微博信息在不同情境下（如企业家发布微博的时间）可能产生不同的影响。 基于此做出以下假设： H1：明星企业家的微博发布时间节点和微博内容的语言风格对其形象评价有交互影响： H1a：发布StyleT(j)语言风格的微博对明星企业家的形象评价有积极影响。 H1b：发布Tnode(j)时间节点下发布微博对明星企业家的形象评价有积极影响。 H1c：在Tnode(i)时间节点下，StyleT(j)语言风格对明星企业家的形象评价有积极影响。 4.研究框架 四、研究⼀： 基于数据挖掘技术研究明星企业家微博发布时间节点与语言风格对其形象评价的交互影响1.样本来源为验证假设 H1、 H1a 、 H1b和H1c， 我们在新浪微博网站中利用数据挖掘技术采集企业家微博的客观数据，采用面板数据分析方法进行假设检验。 1.1 样本选择样本说明:选取明星企业家，以 [财富值高 + 知名度高+ 微博活跃企业家] 作为样本的选择标准。 样本选取方法: 层级筛选法（R-M-N），下面是具体说明： （1）样本初步选取(财富值):在《财富》杂志“2017·中国500强企业排行榜”或深圳胡润研究院发布的《36计·胡润百富榜2017》中搜集上榜企业家。 （2）样本第二次筛选(知名度):在第一次样本选取出的企业家中选取出知名度高的企业家。 以关键词为“企业家品牌+企业家姓名”作为关键词在百度指数中搜索，将指数⾼的企业家界定为明星企业家。 （3）样本第三次筛选(微博活跃企业家):在上一次筛选出的样本中选取出N个为微博活跃用户的明星企业家。 微博活跃企业家：在过去半年内持续使用微博且微博粉丝数在百万以上的企业家。 样本基本情况如表1所示：表1 样本基本情况 注：第一行是表头信息，第二行是示例。 1.2 数据采集本实验采⽤数据挖掘技术对选取的明星企业家的微博进行数据爬取，以采集研究所需样本。 1.2.1 数据抓取 采⽤python爬虫技术，通过新浪微博API (Application Programming Interface) 接口对选取的N个明星企业家的微博进行数据爬取，限定时间段为2012年1⽉1⽇——2017年12月31日。爬取数据包含四组信息，分别为：明星企业家微博个⼈信息、限定时间段内每条微博具体信息、每条微博的用户评论信息、每条微博的转发信息。 1.2.2 数据集说明 下面是对所有所需爬取数据的信息的具体说明。 爬取数据以csv格式存放。每一条数据都有独一无二的 id 字段，相关数据之间通过外键关联在一起，以保持数据之间的关联结构。 表2 明星企业家微博数据分类及条数 注：上表统计使用数据挖掘技术爬取的每一个被选取的明星企业家的微博数据分类及条数。明星企业家（Star Entrepreneur）简称SE，N个明星企业家样本分别为SE1, SE2, … , SEn. 表3 明星企业家微博信息表 字段 说明 id 用户id url 用户主页url name 用户昵称 gender 用户性别，1~男，0~女，-1~未知 description 用户简介 verified_reason 认证信息，为空代表没有认证 follow_count 关注人数 follower_count 粉丝数 status_count 发表的微博数 表4 明星企业家微博内容信息表 字段 说明 id 微博 id url 微博 url content 微博内容 time 微博发布时间，用 UTC 时间戳表示 author 发表这篇微博的用户的 id comments_count 微博评论数 reposts_count 微博转发 likes_count 微博点赞数 表5 明星企业家微博评论信息表 字段 说明 id 评论 id content 评论内容 user 发表评论的用户的 id time 评论发表时间，用 UTC 时间戳表示 like 点赞数 target_type 评论对象的类型，一种是 “weibo_post”，表示针对原微博的评论，一种是 “weibo_comment”，表示回复某一个评论的评论 target_id 如果 target_type 是 “weibo_post”，则表示微博 id，如果 target_type 是 “weibo_comment”，则表示评论 id 表6 明星企业家微博转发信息表 字段 说明 id 转发 id content 转发时发表的内容 like 点赞数 user 转发微博的用户的 id time 转发时间，用 UTC 时间戳表示 origin_post 原微博的 id 2.模型设定与变量定义2.1 模型设定我们将模型设定为: ImageE{FanN(2x),ForwardN(1x),CommentN(1x),SentimentJ(4x)} = α + β1 StyleT+ β2 Tnode + β3 StyleT Tnode + β4 Cont* 2.2 变量定义 表7 变量定义表 本研究模型中定义了1个因变量，为ImageE，包含4个指标；2个变量，分别为StyleT和Tnode。以下为对各变量及指标的详细解释： ImageE： 表示用户对明星企业家形象的评价，明星企业家个人形象的衡量标准如下，包含有四个指标。为了更好地量化明星企业家的个人形象(ImageE)，我们依据4个指标的重要性差异为它们设定不同的权重：ImageE{FanN(2x),ForwardN(1x),CommentN(1x),SentimentJ(4x)}, 其中FanN、ForwardN、CommentN、SentimentJ四个指标的权重分别为2x, 1x, 1x, 4x, 加权结果以百分制表示。四个指标的具体定义如下： （1）FanN：表示单条微博发表后48h的时间区间内，明星企业家微博粉丝的变化量。在微博背景下，粉丝数量反映了其影响⼒、受欢迎程度以及受众的总体态度(Chaetal.，2010)，与营销的影响⼒效果正相关(金永生等，2011)。本文通过微博开放平台的粉丝变化趋势API获取到粉丝数量的历史变化趋势的数据，来衡量特定时段⽤户对企业家的评价和态度。粉丝增长量越大，则说明该时段内用户对企业家的态度和评价越好。 （2）ForwardN：表示单条微博的用户转发量。单条微博的转发量反映该微博引起的关注度⼤⼩。 （3）CommentN：表示单条微博的用户评论量。单条微博的评论量也反映该微博引起的关注度⼤⼩。 （4）SentimentJ：表示单条微博的用户转发和评论内容的情感极性值，分为正面(= 1),负面(= -1),中性(= 0)。情感极性包括正面、负面、中性三种；通过分析单条微博的用户转发和评论内容的情感极性，可获知用户对该条微博的情感态度。我们通过腾讯AI开放平台的情感分析API接⼝对每条微博的用户转发和评论内容的情感极性进行分析，分析结果表示为：正面情感为数值1，负面情感为数值-1，中性情感为数值0。 StyleT： 表示明星企业家微博的语言风格类型，共划分为W个语言风格, 分别为StyleT(1), StyleT(2),…,StyleT(w)。我们采用深度学习技术，基于大规模数据训练，使⽤LSTM深度神经网络模型对微博样本进行自动化分类，并采用使用测试集检验模型的准确率。LSTM神经网络模型如图1所示；信息分类编码表见附录(人工编码样例)。 图1 LSTM(⻓短期记忆神经⽹络)模型 LSTM(⻓短期记忆神经⽹络)是RNN网络(递归神经网络)的拓展，具有时序建模的作用，是目前最适合做语言建模的神经⽹络。LSTM通过输入门，遗忘门，输出门保持和更新细胞状态，可以判断哪些信息是有用的，哪些是没用的，并把有用的信息在LSTM中保存。 文本是人类文化的产物，即人是唯一准确的判别标准，机器分类难免有所偏差，且LSTM模型现在还无法达到很高的准确率。故我们在实验二中使用人工测试实验法对研究假设进行进一步验证。 Tnode： 表示明星企业家微博的发布时间节点，共划分P个不同的时间节点，分别为Tnode(1), Tnode(2),…,Tnode(p)。划分方法采用简单统计方法，按P个时间节点将所有微博样本划分为P类。 Cont： 表示我们设置的控制变量。在已有文献的基础上(金永生等，2011;Chaetal .，2010)，我们主要考虑以下影响指标FanN(微博粉丝数变化量)的因素:明星企业家每个时段的微博信息总量(Total)，⽤该时段发布的信息总条⽬数来衡量;期初的粉丝数量(PreE)，用各时段期初的粉丝数量来衡量。此外，我们还在相应的检验中控制企业家所在⾏业(Ind)和微博使用的时间(Time，用企业家⾃开始使用微博截至样本期间的时间衡量)因素。 3.样本数据分析针对获取到的样本数据，我们通过描述性和诊断性统计来进行数据分析，以验证研究假设。 3．1描述性统计表8 因变量及其指标的描述性统计特征 表8分别统计在总体样本数据中，因变量ImageE的4个指标的4项统计指标：Min(最⼩值)，Max(最⼤值)，Average(平均值)和σ(标准差)。 3．2诊断型统计表9 明星企业家微博语言风格对其形象评价的影响 表9分别统计在总体样本数据中，⾃变量StyleT的每⼀个值所对应的ImageE及其四指标值的值，以此分析哪种语言风格的微博能积极地塑造明星企业家的个⼈形象。 表10 明星企业家微博发布时间节点对其形象评价的影响 表10分别统计在总体样本数据中，自变量Tnode的每一个值所对应的ImageE及其四指标值的值，以此分析发布于什么时间节点的微博能积极地塑造明星企业家的个⼈形象。 表11 明星企业家微博发布时间节点和语言风格对其形象评价的交互影响 表11是展示自变量StyleT与Tnode对ImageE的交互影响的正交关系矩阵。分别统计在总体样本数据中，当⾃变量StyleT取值StyleT(i)且⾃变量Tnode取值Tnode(i)时，所对应的每⼀个ImageE(i)的值。以此分析发布于什么时间节点的哪种语言风格的微博能积极地塑造明星企业家的个⼈形象。 3．3 回归模型检验采用多元回归⽅法，将⾃变量、调节变量、⾃变量和调节变量的交互项、 控制变量逐步放⼊多元回归分析模型。为了检验交互项，我们对所有的连续变量进⾏均值中⼼化处理，以降低多重共线性的问题(Aiken &amp; West，1991)。若经过处理后的变量样本均值为零，则说明样本分布与处理前相同。若多 重共线性诊断结果显示所有的VIF值均⼩于5，则说明多重共线性问题并不严重，未对假设检验结果构成影响。 五、研究二：实验法进⼀步验证明星企业家微博发布时间节点与语言风格对其形象评价的交互影响研究二的目的在于通过实验法验证企业家发布微博的时间节点与微博信息的语言风格对其形象评价的交互影响，即验证假设H1、H1c，排除可能的混淆因素，提高研究结论的内部效度。 1．预实验:微博样本选取1．1 样本预选取从新浪、腾讯、搜狐、⽹易等微博门户⽹站上，用数据挖掘的方法挖掘 2017 年 1 月~4 月期间实名制企业家的微博信息，经过讨论后从中保留约Pre-N 条微博信息样本(15&lt;pre-N&lt;40)。 信息样本的要求： 1.每种微博信息组合数量约为 10条，即共需收集10wp 条初样本。2.不同类型的信息的字符⻓度基本⼀致。3.脚本内容完全参照新浪微博的格式设计，包括字体、颜色、排版等 3．2样本再选取将这些微博信息进行初次简单的修改:删去企业家的名字和职位、企业名称、帖⼦的转发数、发布时间等, 交给 5wp 位随机选择的普通消费者。⾸先询问被试，是否使用过微博。没有使用过微博的被试不再进⾏下一步的调查。 然后要求他们根据⾃⼰的认知对不同时间节点的这些微博信息进行语言风格分类，得出 fin-N 条微博样本。并根据分类将这些样本设置在不同实验组中，组成企业明星微博认知调查测试卷。 2．被试选取随机抽取200名在华中科技⼤学就读的本科学生作为被试，排除不经常使用微博的学生，将男⼥比例控制为 1：1。为保证实验的严谨性，在被试的选取过程中，排除本院系内成员，避免被试对实验有提前了解。 3．实验方法3．1实验设计方法选取本次实验选取正交实验设计⽅法，正交试验设计(Orthogonal experimental design)是研究多因素多⽔平的又⼀种设计⽅法，它是根据 正交性从全⾯试验中挑选出部分有代表性的点进⾏试验，这些有代表性的点具备了“均匀分散，⻬整可⽐”的特点。在本次研究中，实验设计中的两个因变量时间节点、语言风格，且均有两种或者两种以上水平方向上的因素(如语言风格水平方向上的因素可划分为⼝语化和非口语化两种)。此次研究为探究多因素多⽔平对因变量的影响，宜采用正交实验设计。变量为问卷中不同的微博信息，因变量为被试对企业家的好感度、被试对企业的好感度。 3．2研究方法设计使⽤正交实验设计的⽅法，得到正交矩阵矩阵(表12)，设置 wp 个实验组，将两个⾃变量的⽔平因素分别安排到正交表⾏与列，组成交互矩阵，得出 wp 个组合，每个组合形成⼀个实验组 Aij，使得每一变量的水平分量都有与另⼀变量⽔平分量碰撞的机会。再设置⼀个对照组 A00，不含因变量，与实验组形成对⽐。在以往对企业家形象的测量中，企业家的诚实、可靠性在所有维度中较为突出。(Milleretal.，1986)，且由于诚实、可靠、支持等维度都有着较强的联系与交叉性，于是我们将这些评价维度结合为⼀个维度”好感度”，将评价指标单⼀化，既避免可以多维度之间的交叉影响，⼜可以⽅便研究数据的分析。“好感度”评估分为对企业家的好感度 S1 和对企业的好感度 S2。 表12 注:实验中的量表均采用李克特7分量表。 4．实验材料与过程4．1实验材料企业明星微博认知调查测试卷 4．2实验过程(1)实验组 在问卷中，不同实验组给被试者提供一个的虚拟明星企业家的相同企业家 的简介材料，如 A先生是一家手机软件公司的CEO，微博粉丝500万，具有⾼知名度。然后在不同的实验组中为被试者提供改企业家在Tnode (i)节点下发表的 Style(j)风格的微博信息，即让不同实验组的被试接触到不同的信息刺激。 在阅读完测试卷中的企业家的简介及微博后，要求被试对该企业家的好感 度打分，将好感度划分为 1，2，3，4，5，6，7 个梯度，分数越高代表对企业家的好感度越高，反之，分数越低表明对企业家的好感度越低。对照组中只提供该企业家的简介，不提供微博信息，在被试浏览完企业家简介后，给企业家的好感度打分。每组对企业家的好感度终值取组内所有被试的平均值。 为了进⼀步探究企业家微博对企业品牌形象的影响，增加一个对企业形象的好感度的维度。即在对企业家好感度打分后，再对企业的好感度进行打分，仍然将好感度划分为 1，2，3，4，5，6，7 个梯度，每组终值为所有被试的平均值。 (2)对照组在对照组中向被试提供与实验组相同的企业家简介，但不提供微博信息，在阅读简介后直接让被试对企业家和企业的好感度进行评分。 表13评分表 4．3操控检验我们提出两个问项测试测试 Tnode(i)styleT(j)信息的认可程度: 我认为以上微博信息展示了企业家 A先生的语言风格侧重的是 styleT (x)，发布的时间节点为 Tnode(y)，对于同一实验组，如果对于被试选择的分类与实验设置的分类有明显差异，即可对微博信息进行重新的分类调整。为了排除被试由于阅读过部分微博信息而产生熟悉感对结果造成影响，我们让被试对微博信息的熟悉感以及可信度进行评价。实验最后，为了保证实验的严谨性，拍出被试此前有接触过需再次测量被试参与实验的卷入度和人口统计学信息，并询问被试是否了解本次实验 的目的、是否相信所有材料的描述等。 5．数据的处理和分析在实验中完成数据收集后，对每组数据取均值填⼊下表，进行极差分析。 表14 极差分析法说明: （1）在试验范围内，各列对试验的影响从大到小的排队。某列的极差最大，表示该列的数值在试验范围内变化时，使试验指标数值 的变化最大。所以各列对试验指标的影响从⼤到⼩的排队，就是各列极差 D 的数值从⼤到小的排队。（2）试验指标随各因素的变化趋势。（3）使试验指标最好的适宜的操作条件(适宜的因素水平搭配)。（4）对所得结论和进一步研究方向的讨论。 六、研究意义与研究局限1.研究意义1.1 理论意义（1）丰富和发展有关企业家的社交网络印象管理的理论 本文将探究明星企业家这个群体的微博发布策略，及其基于微博的形象建构策略，从戈夫曼的戏剧理论出发，分析基于微博的印象管理理论，扩展戏剧理论，印象管理的应用范畴，特别是在微博这样一个开放式的，有一定的公共话语性的社交平台上，进行印象管理的新特点。 （2）拓宽企业家微博发布研究范畴 本文在探究明星企业家微博发布策略之时，除了探究其微博发布的内容特点之外，还延展到了企业家微博发布的时间节点选择，语言风格运用等层面，使得对企业家微博使用的研究，从内容分析，拓展到了策略分析，发布内容、语言风格、时间节点，三者共同构成了企业家的微博发布策略，从策略着眼探究该现象，可以拓展研究的广度与深度，为相关研究提供一个新的视角。 1.2 模型意义（1）构建了基于微博数据的明星企业家形象评价的衡量标准 此前对企业家形象评价的衡量标准局限在较单⼀的维度，如粉丝量变化，这实际上无法全面客观地反映⽤户对企业家的形象评价。而在本研究设计的研究⼀的模型中，设定ImageE为因变量，表示用户对明星企业家的形象评价，并设定4个多维度指标（FanN,ForwardN,CommentN,SentimentJ）作为其衡量标准，如此则构建起了较为全面的基于微博数据的明星企业家形象评价的衡量标准，以提高研究的科学性和准确性。 （2）拓宽明星企业家微博发布策略对其形象评价影响的理论边界 本研究在企业家微博发布策略中选取微博发布时间节点与语⾔⻛格这两个维度，研究其对明星企业家形象评价的交互影响，得出“在Tnode(i)时间节点下，明星企业家使用StyleT(j)语言⻛格写微博可积极地影响用户对其形象的评价”的结论，可拓宽明星企业家微博发布策略对其形象评价影响的理论边界。 1.3现实意义微博作为当下社交媒体的重要组成部分，具有公开性强，范围广，互动性强等特点，已经成为了公共舆论的重要阵地。基于微博的公众人物形象已经成为了人物媒体形象的重要组成部分，本研究将聚焦明星企业家这个群体的微博发布策略，探究其特点，特别是优点与不足，联系其基于微博的个人形象，为企业家群体的微博发布使用，提供一些可能的参考，在微博这一平台上做好个人的形象建构管理。 对于一些企业而言，企业的创立者、管理者的形象，与企业品牌的形象关联极为紧密，就如一个硬币的两面，特别是对于明星企业家而言，就如受众会把马云和阿里巴巴划上等号，明星企业家几乎就是企业品牌的个人化再现。因此个人形象管理同企业的品牌形象管理同样重要，两者不可偏废。本研究聚焦于明星企业家的个人形象，目的在于从个人的形象管理出发，为如何在微博这一平台上塑造良好的个人形象提出几点行之有效的建议。 1.4 操作意义（1）替代人工搜集数据的方法，使用数据挖掘技术进行数据采集 此前的对企业家微博信息的研究基本上是采用传统的人工搜集数据的⽅法（如问卷法，焦点访谈法，二手数据采集法），这些方法具有的共同缺点是：时间和精力消耗大，而搜集的样本数量有限，样本时间区间有限，且样本数据不可避免地带有⼀定的主观性。而本研究设计的研究一采用数据挖掘技术爬取明星企业家微博历史数据，以作为研究所需的样本。通过数据挖掘获取的数据具有强客观性，样本数量⼤且维度全面，有助于提高研究分析的内部和外部效度。 （2）替代人工进行内容分析和分类编码的方法，使用深度学习技术进行数据分析，包括微博内容情感极性分析和语言风格分类 此前的对企业家微博信息的研究基本上是采用传统的人工分析数据的方法（如人工对信息进行编码，多个独立裁判对信息进行分类），这些方法具有的共同缺点是：消耗大量时间和⼈⼒，只能对⼩规模数据进⾏处理，且数据分析的结果不可避免地带有⼀定的主观性。而本研究设计的研究一采用计算机领域前沿的深度学习技术对大规模数据进⾏分析，主要使用LSTM深度学习模型对微博内容的语言风格进行分类预测，以及使用腾讯AI开放平台的情感分析API接口对所有微博样本的用户转发和评论内容的情感极性进⾏分析。这种分析方法相比传统人工分析具有更高的分析效率、更大规模数据处理能力和更高的准确性，有助于提高研究分析的内部和外部效度。 深度学习（deep learning）是机器学习的分支，是机器学习中⼀种基于对数据进行表征学习的算法, 试图使⽤包含复杂结构或由多重⾮线性变换构成的多个处理层对数据进⾏高层抽象。 （3）双实验强化对研究假设的验证 本研究设计了2个研究实验，分别为研究⼀与研究二。研究⼀使⽤数据挖掘与分析技术进行自动化数据采集和分析，而文本是⼈类⽂化的产物（即⼈是唯⼀准确的判别标准），机器对数据进⾏分类的结果难免有所偏差。故我们设置实验二，使用人工测试实验法对研究假设进行进⼀步验证。双重实验可强化对研究假设H1、H1a、H1b、H1c的验证，以提高研究结果的内外部效度。 2.研究局限与展望本研究的局限性与未来的研究方向主要展现在以下4个方面： （1）研究范围上，本研究设计基于中国微博用户，验证明星企业家微博发布策略对其形象评价的影响。那么这个研究结果是否具有对于全世界的普适性意义呢？未来可收集国外（如Twitter）的样本数据进行跨文化的实证检验。 （2）研究对象上，对于明星企业家的微博发布策略，本研究设计仅选取2个维度：微博发布时间节点、微博内容的语言风格。未来可扩展到对更多维度微博发布策略的研究。 （3）本研究设计基于微博数据选取4个重要指标来衡量明星企业家的形象评价（因变量），而实际上其影响因⼦还有很多，未来的研究可进⼀步探索更多维度的调节因素，例如企业所处⽣命周期、危机事件情景等等，以提⾼对明星企业家形象评价衡量的科学性和准确性。 （4）设定因变量ImageE的衡量指标时，我们本想将微博内容语义挖掘囊括进来（即通过自然语言处理技术对微博内容进行语义分析），但考虑到目前语义分析技术尚不成熟，可能会影响整体准确率，故未引⼊这一指标。未来可尝试引⼊微博语义分析这一指标，以完善对明星企业家形象评价的衡量标准。 七、研究实施时间安排 八、伦理道德说明关于“明星企业家的微博发布策略及其微博形象的建构”的研究，必要的伦理道德必须做一下说明：首先，本研究不追求涵盖所有研究对象，仅就选取的案例进行分析研究，其结果具有一定的普遍性，但不能完全代表全体；其次，本研究将基于研究对象的微博发布策略进行分析探究，不涉及研究对象其他无关方面的信息，对于研究信息的获取全部基于公开资料，不涉及研究对象的隐私；再次，本研究将公开承认无法完全避免研究者的个人价值观以及文化观，因此研究得出的结论将或多或少带有个人研究特质，并不是唯一的、所有人的研究结果；最后，在本研究开始之前，我们将基于对研究对象完全客观中立的态度进行，不涉及其他平台上对于研究对象的任何评价，排除个人因素对于研究过程的影响。 九、参考文献[1]黄静.企业家微博信息对于其形象评价的影响机制研究［J ].管理世界，2014，（9）[2]魏川.基于微博的个人品牌策略研究［D].武汉：华中科技大学，2011[3]周飞、沙振权.企业家微博信息质量对粉丝品牌态度的影响机理研究［J].北京工商大学学报(社会科学版),2015,30（5）[4]何嫱.企业家微博对企业品牌形象的影响及作用机制［D].成都，西南财经大学，2013[5]朱丽娅.企业家微博信息对其形象评价的影响研究［D ].武汉，武汉大学，2014[6]黄静.企业家微博写什么［J ].商业经济与管理，2014（2）[7]莫可道.如何打造企业家领导人博客［J ].销售与市场:管理版，2009[8]李秀杰.微博在企业形象管理中的应用及效应分析［J].企业技术开发月刊，2011[9]潘明.现代企业领导人的魅力价值对企业的影响［J ].办公室业务，2015[10]蔡晓燕.微博舆论场中企业家意见领袖的形象［J ].湖北函授大学学报，2014（6）[11]付晓萌、郭佳佳.新浪微博八年兴衰史[12]微博2017年用户研究报道[13]2016微博企业白皮书[14]微博开放平台 http://open.weibo.com/wiki/首页[15]Azkara;Tania Arriaga;Croasdell;David.NASF in Twitter: An Entrepreneur’s Community of Practice Using Electronic Networks of Practice[16]Caroly M.Brown:.6 Twitter Tips for Entrepreneurs[17]Alexandra loanid, Cezar Scarlat.Factors Influencing Social Networks Use For Business: Twitter and YouTube Analysis 附录微博信息编码表（示例） 1、社会重大事件：引起公众广泛关注的事件；社会非重大事件：未引起公众广泛关注或公众关注性较低的事件；划分依据：被百度指数收录的事件为社会重大事件。2、此内容编码表仅为一个示例，实际对时间节点和语言风格的分类需按研究设计中的操作步骤、基于对采集的海量样本数据的分析才能得出。]]></content>
      <categories>
        <category>Communication Science</category>
        <category>Medium Study</category>
      </categories>
      <tags>
        <tag>Media Study</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[企业文化与传播建设方案|风陵渡文化科技]]></title>
    <url>%2F2018%2F01%2F18%2F%E4%BC%81%E4%B8%9A%E6%96%87%E5%8C%96%E4%B8%8E%E4%BC%A0%E6%92%AD%E5%BB%BA%E8%AE%BE%E6%96%B9%E6%A1%88-%E9%A3%8E%E9%99%B5%E6%B8%A1%E6%96%87%E5%8C%96%E7%A7%91%E6%8A%80%2F</url>
    <content type="text"><![CDATA[Abstract：组织传播原理的大作业 ~ 构建一个企业并为其设计整套的企业文化与建设方案，以及传播方案。 一、企业简介“风陵渡文化传媒科技股份有限公司”（以下简称“风陵渡”）是一家以文化创意与文化内容生产为主要业务的国际文化传媒科技企业。公司立足于中国武侠文化，发掘武侠文化的内涵与创意，致力于打造中华武侠大IP，使中国的武侠文化在世界文化产业格局中占据重要地位，推动中国文化的国际话语力量。 二、创立背景当今的全球流行文化产业格局里，美国有漫威、DC打造的超级英雄，有好莱坞打造的一系列电影，在世界范围内都有着巨大的影响力与号召力；日本发达的漫画产业，使其成为二次元文化的重要阵地，有着大批拥趸。中国有着深厚的传统文化底蕴，但在世界范围内的流行文化话语体系里，却频频失语，缺乏一张有力的文化IP。 武侠文化根植于传统历史，又符合现代人的喜好，以金古梁三位武侠大师为代表，他们的武侠小说深受全球华人的喜爱，但随着三位的逐渐淡出，武侠文化渐渐也开始被淡化。风陵渡集团渴望将武侠文化打造成为中华文化新名片，成为世界文化产业格局里重要的的一极。 三、企业业务核心业务 具体业务 翻拍改编重制经典武侠影视作品，创作拍摄新派武侠影视片。 再版重制武侠文化相关书籍，改编创作武侠文化的小说、漫画等，扶持武侠文化相关的网络文学。 策划举办武侠主题活动，如展览，交流会，演艺活动等，宣传推广武侠文化。 4 发展武侠文化主题旅游项目。 投资发展武侠文化游戏，利用AR、VR技术模拟武侠故事场景，提升用户体验。 建立武侠世界的主题馆、情景体验馆，利用AR、VR技术重现书中的武侠世界。 生产中国武侠文化周边产品，如：玩偶、cosplay服装、卡贴、日常摆件等。 在网络上利用新媒体技术和平台对武侠文化进行宣传，通过H5动画、微信公众号小程序、APP等创新表现形式。…… 四、企业组织运行架构1.公司组织架构图：直线—职能型与矩形结构相结合 2.信息传播框架图 五、企业文化1.企业文化内容1.企业使命：讲好中华武侠故事，打造中华文化大IP，成就顶级文化企业 2.企业愿景：做全世界内容最优质最有吸引力的文化工匠 3.经营哲学：人才驱动创新，创新驱动内容，内容驱动发展 4.企业发展核心：人才 内容 5.企业价值追求： 铸剑大师：十年磨一剑，潜心铸造最优质的内容。 武学宗师：云淡风轻，不问一时胜败，只求百年传承。 武痴：醉心文化，一招一式皆是心血所凝。 6.企业精神： 关键词：热爱 使命 文化工匠：热爱文化，热爱武侠，热爱创意，热爱创新，热爱中华。 文化宗师：侠之大者，为国为民，为中华重塑武侠，向世界展示武侠。 7.企业员工文化： 关键词：兄弟，师徒，同门 8.企业选人用人育人： 选人用人：有务实的态度，有对文化的热爱，有创新的意识，有文化传承的使命 育人：定制式培养，个性化发展，在学习中成长，在实战中成熟 9.企业信条： 10.经营原则：稳健经营，坚守文化，务实创新，注重传承，做百年文化工坊。 2.企业文化建设方略1.阅读，每个月必须至少读一本书，建设企业图书馆；记录，善于记录生活与创意；实干，在实战中操练，让每一个员工都深度参与业务实战中；自由开放，充分根据每个员工自己的特点，为其创造良好的发展环境。 2.育人机制： A.新员工入职前会有公司的灵魂人物为大家演讲，欢迎各位新鲜的血液 B.师徒制：师傅带徒弟，徒弟之间还有兄弟情、同门情，组成工作小组 C.定期武侠小课堂分享会 D.国外交流项目：漫威、迪士尼、皮克斯等公司参观交流学习 E.举办武侠主题的公司年会 3.培训内容： A.管理培训 B.内容培训 C.技术培训 D.营销培训 4.人才激励机制： 岗位体系：内容序列、技术序列、市场序列、管理序列 绩效考核：制定两个标准：剑术（业绩考核）、侠义（行为考核）；以两个季度为周期评选 5.奖惩机制： 根据工作年龄进行定期发放纪念品（私人订制）：刚进公司时是一双拳头的胸牌（象征着赤手空拳）一年是小刀的手链；三年是天魔琴；五年是孔雀翎；十年是倚天剑&amp;屠龙刀；十五年是圣火令 根据绩效考核评选出：文化工匠（年度最勤奋奖）、年度最有创意奖、年度科技奖、武学宗师（年度最具贡献奖） 六、社交网络传播策略框架设计1.四个核心 2．社会化传播解决方案]]></content>
      <categories>
        <category>Communication Science</category>
        <category>Medium Study</category>
      </categories>
      <tags>
        <tag>Medium Study</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[企业社交网络传播策略框架设计]]></title>
    <url>%2F2018%2F01%2F17%2F%E4%BC%81%E4%B8%9A%E7%A4%BE%E4%BA%A4%E7%BD%91%E7%BB%9C%E4%BC%A0%E6%92%AD%E7%AD%96%E7%95%A5%E6%A1%86%E6%9E%B6%E8%AE%BE%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[Abstract：如何为企业设计社交网络传播策略框架？ 社交网络传播策略的概念社交网络营销的核心是关系营销。社交的重点在于建立新关系，巩固老关系。 社交平台：免费流量 社会化媒体格局： 社会化媒体营销:利用社会化网络，在线社区，博客，百科或者其他互联网协作平台媒体来进行营销，公共关系和客户服务维护开拓的一种方式。 社交网络传播策略框架设计目的 品牌推广 传递企业价值 四个核心 1.社会化解决方案：整合营销，融合/跨界营销 2.令人惊喜的体验：注重用户体验 3.市场细分：利用大数据分析系统，精准定位目标群体（垂直、细分） 4.科技：泛娱乐化科技营销，创新科技传播形式; 大数据采集与分析系统；DSP/DMP/RTB技术； 社会化解决方案 1.1 social listening and analystics（社会化媒体聆听） 在社交媒体上（微博、微信等平台），针对一些关键词，用一些检测工具，投放这些关键词到框架中，进行监听，并把监听的数据转化成报告，制定社会化媒体营销的策略，再优化检测到的营销效果，不断优化。 1.1.1 culture trend（文化趋势）: 研究当下文化流行趋势、目标群体文化，如研究 90 后的消费习惯、文化、关注话题等； 1.1.2 insight（洞察）：通过社交媒体大数据进行消费者洞察，如某类人群喜欢什么样的平台，媒体的行为习惯是什么样的，在做什么事情，以及频次。这些数据分析结果可指导制定营销策略，如媒介投放时间、传播形式和方式等。 1.1.3 brand（品牌）：分析品牌在全网的SOV；分析品牌相关关键词/相关热点话题背后的营销点。 1.1.4 competitors（竞争对手）：分析竞争对手在社交媒体上的宣传举措，寻找借鉴点。 可以做一个雷达图，从不同的维度和广度评价竞争对手，就一目了然知道我们跟竞争对手之间的差距，以及我们能够借鉴的东西是什么。 1.2 social content what: social insight（消费者洞察）+ social story（内容是否有故事性，易于传播） + social experience（令人惊喜的体验） + social action（用户行为预测）。 how——contend seeding: 在第三方平台上，比如别人的博客，BBS 等，直接伪装成用户在上面时进行内容植入，或者说互动。 where &amp; who：1.自媒体，如微信公众号，微博账号等。2.KOL ：有放大的效果，通过意见领袖的社会化影响力放大传播效果。 1.3 social CRM（社会化客户关系管理） 1.3.1 Customer Profile：消费者数据采集与分析系统 1.3.2 企业智能系统：沟通企业与消费者的通路 1.4 social media marketing - 1.4.1 Social Marketing Compaign：如话题营销 - 1.4.2 Optimization &amp; Measurement：制定传播效果的衡量标准]]></content>
      <categories>
        <category>Communication Science</category>
        <category>Medium Study</category>
      </categories>
      <tags>
        <tag>Medium Study</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[品牌传播概论复习笔记]]></title>
    <url>%2F2018%2F01%2F15%2F%E5%93%81%E7%89%8C%E4%BC%A0%E6%92%AD%E6%A6%82%E8%AE%BA%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Abstract：品牌传播概论期末复习笔记 ~ 一、简答题（5个，每题10分）1.简述品牌的定义及其特征什么是品牌品牌是给拥有者带来溢价、产生增值的一种无形资产。是一个人对一个产品、服务或公司的直观感受。 从学理概念界定：强调品牌是名字、名词、符号或设计中的一种或总和。其目的是使自己的产品或服务有别于其他竞争者。 从公众态度界定：强调品牌是一种偏见，建立品牌的目的就是要形成对于对手的一种不公平，品牌的价值来自于顾客的肯定。公众对于组织及其产品认识的总和。 从与消费者关系界定：强调品牌就是产品和消费者的关系，若不能与消费者结成亲密关系，产品就从根本上丧失被称为品牌的资格。企业需要建立好这种关系，以使得自己的品牌在市场上长盛不衰。 从价值角度界定：强调品牌的价值，突出品牌资产、品牌承诺等方面。建立好品牌对企业来说具有决定性的意义。 从顾客认知过程来看，往往是从品牌的利益、属性体验到品牌的功能定位，之后才意识到品牌在用户、文化、个性上的独特，最后才能领悟到品牌的核心价值。从品牌塑造过程来看，则应该以其作出的价值承诺为核心，建立品牌文化，树立品牌个性，定位目标市场，从这几个方面出发设计品牌的属性和提供的利益。 品牌特征1、品牌本身不具有独立的物质实体，是无形的；品牌有物质载体，品牌通过一系列物质载体来表现自己。 2、品牌是企业的无形资产品牌成为资产重组的旗帜，是公司品牌形成的重要标志。 3、品牌是企业市场竞争的工具在产品功能、结构等因素趋于一致的时代，赢得市场的关键在于谁的品牌过硬。 4、品牌具有一定的风险性和不确定性受到市场周期、竞争对手、突发事件的影响。 2.简述品牌传播的主体。一、企业品牌 企业是品牌传播主体，如何让自己的品牌脱颖而出，获得市场认可与实现品牌价值，是当下每个企业最为关心的问题。因此，品牌传播成为每个企业无以回避的必然选择。 建立品牌卖点的差异化功效的差异化销售服务的差异化 二、非盈利组织品牌 一类是群众团体组织如专业学术团体、消费者协会、个体经济协会、工会、宗教协会、校友会、同乡会等。 一类是事业性组织 如：学校、医院、图书馆、新闻媒体、出版社、文艺团体、科研院所、体育机构等。 其最鲜明的特点则为有着鲜明的主体目的性、行为一体性；由此，组织则需通过其主体性的品牌传播行为与环境取得和谐，以实现自身的主体性目标。 三、个人品牌 是个人拥有的独特的、鲜明的、确定的、易被感知的外在形象或内在修养，以及对目标影响群体形成的整体性、长期性的重大影响力的集合体。可转化为商业价值的一种社会注意力资源。 它包括个人专长、思想观念、世界观、价值观等一系列的内容。 无形价值 影响力：知名度、行业威望传播力：美誉度、声誉拓展力：超越空间、行业持久力：超越时间、持续成为社会焦点 有形价值 商业价值品牌代言人出场费薪酬 四、国家品牌传统的外交正在消失，政治家们必须好好学习品牌资产管理。 要为自己的国家找到合适的品牌定位，参与到竞争激烈的市场营销之中，保证消费者满意，而最为重要的，是使顾客建立对品牌的忠诚度。 国家，简单地说，将变成国家 National Brand 国家在国民（尤其是外国人）心目中的形象 要素:国名、国旗、国歌 3.简述品牌资产的定义及其价值。品牌资产的定义品牌资产（Brand Equity）也称品牌权益，是指只有品牌才能产生的市场效益，或者说，产品在有品牌时与无品牌时的市场效益之差。 David A∙Acker: 品牌资产是这样一种资产，它能够为企业和顾客提供超越产品和服务本身的利益之外的价值；同时品牌资产又是与某一特定的品牌紧密联系的；如果说品牌文字、图形作改变，附属于品牌之上的财产将会部分或全部丧失。 品牌资产的价值1.为消费者提供价值 通过品牌名称、品牌标志物的认知作用，有助于消费者加工整理、存储有关产品及品牌信息。 增强消费者的购买信心，缩短购买商品的决策过程。 2.为企业提供价值 培养消费者的品牌忠诚 品牌体现的质量，能促使该品牌产品以溢价销售 品牌资产为品牌扩张提供了有利的条件 品牌资产提供了对竞争者来说是进入目标市场的一种障碍的竞争优势 4.简述联合品牌战略的定义及其作用。一、联合品牌传播战略的定义 两个或两个以上品牌从各自利益出发、寻求同一愿景、开展品牌传播的一种战略思维和方式。 联合品牌的注意要点： 品牌间价值相当各有优势注意品牌间的定位差异和目标取向竞争品牌是否可以参与互动 二、联合品牌传播战略的作用 1.可降低营销成本 彼此可以利用对方的资金、通路、人员等既定的要素，将对方的品牌信息传达给消费者，以减少营销传播成本。 2.扩大品牌可接触范围 品牌为了扩大其知名度或达到提示消费者的目的，需要有较大的暴露范围和较高的暴露频次。 3.可提升或巩固品牌形象 一个成长期的品牌与强势品牌的互动，可使消费者对这一品牌产生与对强势品牌相近似的认同感，从而提升品牌的形象。或者两个定位相当的品牌的互动传播，则可使形象得到巩固。 5.简述品牌定位的定义及其六要素。定义品牌定位就是为企业的品牌在市场上树立一个明确的、有别于竞争对手的、符合消费者需求的形象，其目的是在消费者心中占领一个有利的地位。 品牌定位的本质：即差异化。 包含了2个层面的内容： （1）目标顾客差异化：根据顾客需求的差别将市场细分，并从中选出某细分市场作为企业的目标市场。（2）顾客价值差异化：即与竞争者相较，企业能够为目标顾客提供差别化的利益。 这种差别化的利益可以表现在3个方面： ①功能利益。②情感利益：“金六福酒”诉求于“中国人的福气酒”。③自我表达利益：拥有“奔驰轿车”的男士可以表达自己的成功与高贵。 六要素**消费者的需求：了解；功能性，情绪性需求 目标消费者群 竞争范畴：设定 利益点：产品利益，消费者利益，自我表征利益，感性利益 原因(支持点)：产品本身，产品外部的支持点 品牌个性**：差异化 一、了解消费者需求功能性需求“我要更浓烈口味的咖啡”“我喜欢正宗口味的咖啡”情绪性需求“我认为喝咖啡时间就是社交时间”“我想寻求的是喝咖啡时的放松感” 二、放眼于正确的目标消费者（1）地区/行销网络行为人口统计地理人口 （2）心理利益/情境消费者需求 三、设定竞争范畴 四、利益点层次推演 产品利益（Product Benefit → Advantage): 产品可以做到什么 消费者利益（Customer Benefit → Functional Benefit):因为产品的作用/功能而带给消费者的实质好处 感性利益（Emotional Benefit):使用该品牌产品带给消费者的感觉或信仰 自我表征的利益（Expressive Benefit):使用该品牌产品让消费者认为自己属于某类型的人（或让别人认为自己是属于某类型的人） 五、支持点的检视图 支持点的考虑方向: 产品本身： 设计，制程， 配方/内容物，来源 产品外部：权威证实 / 背书，检查报告，使用者证明 其它 六、品牌个性 品牌个性代表一个品牌的性情、气质、精神当各品牌的产品功能和利益点不相上下时，它可造成品牌差异化，给消费者多一个理由选择你的品牌 6.简述品牌传播的主要模式。一、信息邂逅的广告模式 希望目标消费者能高概率地接触本广告信息、或希望所发表的广告信息能高概率地引发媒体接触者关注。任一消费者对于具体广告信息的接触则只能是“信息邂逅”的浪漫一遇。 消费者与广告信息邂逅的两个层面：一是广告信息的无意识接受。二是无意识过程中意识突然唤醒后的接受。 二、搜索满足的品牌传播模式 即消费者受众出于消费信息的需要，不再只是被动的、且主要依凭无意识接受来获得广告信息，而是主动进行搜索，且在搜索中不断比较、求证各类品牌信息，以满足消费决策最基本信息的需求。 实现消费者的“搜索满足”，其途径主要有如下两种方式： （一）全面、客观、互联的数据库平台行业数据、品牌数据、产品数据、消费数据等构成数据库平台；直面消费者的终端形式为：电脑、家庭数字电视、手机。 （二）即时、具体、人性化的互动平台是由品牌网站或虚拟商店的咨询员、行业网站值班专家、商场网上导购员、网络社区专业领袖、有消费经验的热心人、数字电视广告频道主持人等操作，能即时、有针对性回答消费者的具体咨询，且程序互动、充满人性化的服务平台。 7.简述广告定义及其在品牌传播中的作用。一、广告的定义指品牌所有者以付费方式，委托广告经营部门通过传播媒介，以策划为主体，以创意为中心，对目标受众所进行的以品牌名称、品牌标志、品牌定位、品牌个性等为主要内容的宣传活动。 广告在品牌传播中的作用 1、广告可直接提升品牌知名度广告使消费者知道品牌及其所代表的产品，对它具有熟悉感。 2、广告可建立正面的品质认知度①品牌品质是消费者极为看重的②某种程度上，广告的品质可以反映品牌的品质。③品牌延伸时，广告可帮助消费者将原有的品牌品质印象转嫁到新的产品上。 3、广告为品牌联想提供了空间品牌联想，指消费者想到某一个品牌的时候联想到的所有内容。 4、建立品牌忠诚度品牌忠诚度是人们在购买决策中多次表现出来的对某个品牌有偏好。广告对品牌忠诚形成的作用模式：认知-使用-态度-强化-信任-强化-忠诚。 5、树立品牌个性大卫.奥格威认为，市场上的广告95%在创作时是缺乏长远打算、仓促推出的。年复一年，始终没有为产品树立具体的形象。 8.简述品牌形象代言人的定义及其作用。一、品牌形象代言人的定义 是指企业在一定的时期内，以契约的形式指定一个或几个能够代表企业品牌或产品形象并展示、宣传企业品牌或产品形象的人或物。 其职能包括各种媒介宣传，传播品牌信息，扩大品牌知名度、认知度等，参与公关及促销，与受众近距离的信息沟通，并促成购买行为的发生，建树品牌美誉与忠诚。 二、品牌形象代言人的作用 （1）传播产品的具体功能通过品牌形象代言人的语言、动作来直接说明产品的具体功能特征、能给消费者带来的利益。 （2）通过展现形象本身，让观众产生对品牌形象的联想将形象定位于喜爱自己的特定消费群体，强化品牌效应，引导消费时尚。 （3）通过展现“品牌形象代言人”的鲜明个性让观众联想品牌的独特个性 （4）形成品牌识别品牌形象代言人是一个一个直观的、很容易辨认的“联想物”，是区别于其他品牌的重要标志。 二、案例分析题（2个，共30分）2.1 分析题一背景资料：宝洁（Procter &amp; Gamble，简称P&amp;G）总部位于美国俄亥俄州辛辛那堤市。全球员工近110,000人。它同时是财富500强中第十大最受赞誉的公司。宝洁公司是中国最大的日用消费品公司，飘柔、舒肤佳、玉兰油、帮宝适、汰渍及吉列等品牌在各自的产品领域内都处于领先的市场地位。 问题：1.宝洁公司采取了何种品牌传播战略？该品牌传播战略的优劣是什么？ 差异化：一品多牌占领细分市场（提高市场占有率），多品牌出击遏制竞争对手（从销售渠道上减少竞争对手进攻的可能）。 定义：企业对于各种不同的产品分别赋予不同的品牌名称和标识，并运用传播工具对这些品牌分别进行传播。 存在合理性：差异化品牌传播战略符合产业发展的规律。产业发展的过程就是市场不断扩大、不断被细分的过程。产业存在不同细分市场的事实为多品牌战略奠定了基础。 划分标准：差异化品牌传播之间品牌区分的类型，以价格和档次来划分。常见的分类标准有：功能、风格特色、目标人群。宝洁是典型的按功能划分产品。如飘柔-柔顺、潘婷-滋养、海飞丝-去屑、沙宣-专业护理、伊卡璐-芳香等。 实施特点：一是不同的品牌针对不同的目标市场。飘柔强调使“头发更飘、更柔”；潘婷则突出“拥有健康，当然亮泽”；海飞丝则是“头屑去无踪，秀发更出众”。二是品牌的经营具有相对的独立性。在宝洁内部，飘柔、潘婷和海飞丝分属于不同的品牌经理管辖，他们之间相互独立、相互竞争。 优势：1、多占货架面积2、给低品牌忠诚者提供更多的选择3、降低企业风险4、鼓励内部合理竞争、激扬士气5、各品牌具有独特个性和利益点，能吸引不同的消费者**，满足不同消费者的需要，从而使各个品牌都在消费者心目中留下深刻的印象，从而获得自己应有的市场份额。 劣势：1.分散传播资源增加品牌传播的费用开支，许多品牌同时面市，需要有足够的后备资源才可能将这些品牌各个激活。2.品牌结构冗杂对公司和顾客来说品牌结构更复杂，以至于无力有效地反应。3.可能破坏企业整体生态**品牌不可能个个完美，在整体的品牌结构中，某个或某些品牌属于不良成分，不但无端耗费企业的传播资源，而且可能破坏企业的整体生态。 2.请使用“利益阶梯图”分析舒服佳品牌？ 舒肤佳（safeguard）：宝洁；”爱心护全家，尽在舒肤佳 1.消费群体 人口统计：以家庭为单位为主，家庭中以家庭主妇为购买者； 使用行为：日用品，每天清洁皮肤 需求：每天清洁双手但又不贵 包装按传统方式，以纸盒为主题，颜色简单朴素，以蓝白红为主色（蓝色象征安全），对家庭妇女很有吸引力。 2.产品利益 ：“健康、杀菌、护肤” 其中以“除菌”为轴心概念，诉求“有效除菌护全家”；护肤概念中主推“滋润”。 3.消费者利益 清洁皮肤：清除泥土污垢、皮肤分泌物、排泄物、化学物质或细菌 除菌，抵御致病细菌威胁 营养呵护皮肤 4.感性利益 关心全家人尤其是小孩子的健康 爱心妈妈、呵护全家 舒肤佳广告中亲情是不变的主题，主角大多是普通家庭的妇女。竭力向消费者传达：舒肤佳能帮助妈妈保护家庭健康，帮助小孩茁壮成长。品牌标志：盾牌（被保护的感觉） 5.自我表征利益：除菌专家 消费者常识教育：“除菌意识”自我价值宣称——除菌专家形象在产品上市之初，舒肤佳就将自己的诉求重点放在“除菌”上，以“中华医学会推荐”、“实验证明”等方式论证人体在踢球、挤车、玩游戏等众多场合很容易被细菌感染。显然，这是舒肤佳在对消费者进行常识教育，力求做大除菌香皂概念。然后，舒肤佳不失时机地宣称自己所含的抗菌活性成分—迪保肤，不但能够有效去除皮肤表面暂留的微生物，还能有效抑制细菌的再生。就这样，通过说教式的广告表现、平易近人的广告人物诱导，舒肤佳成功地在消费者的心目中树立起了“除菌专家”的品牌形象 2.2 分析题二提起白酒，人们往往会联想到悠久的历史、高大上的包装，这些形象长期以来占据了中国白酒市场，直到江小白的出现。小酒包装，包装外面的句子很青春，但不非主流，热爱生活，但不热血。“我是江小白，生活很简单”。简单的小白瓶，一段走心的文案，这种简简单单的包装设计却在燃爆了整个营销领域，也使得江小白从一个名不见经传的小品牌，一下跃变为一个红遍全国的酒类黑马，并牢牢抓住了80后、90后年轻人的心，将白酒行业普遍认为年轻人不喝白酒的“共识”成功打破了。 问题：1.简述江小白的品牌形象。2.评价江小白的品牌口号。3.结合其瓶身包装，谈谈江小白品牌传播的成功之处。 定义：品牌形象是消费者对传播过程中所接收到的所有关于品牌的信息进行个人选择与加工后存留与头脑中的关于该品牌的印象和联想的总和。 品牌传播形象符号系统： 1.品牌名称：江小白（品牌名称是品牌中能够读出声音的部分，是品牌传播的核心要素，是品牌显著特征的浓缩）2.品牌标识：（指品牌构成中由字体、图像或字体、图像、象征物融为一体组合成的视觉识别部分。它是品牌要素的重要组成部分）3.品牌口号：“我是江小白，生活很简单”（用来传递有关品牌的描述性或说服性信息的短语）4.品牌定位：45度小瓶白酒。它是一款富含时尚青春气息，符 合80，90后年轻人口味的颠覆性白酒产品。5.品牌形象特点：屌丝型，文艺心，追求简单，年轻化 品牌口号：“我是江小白，生活很简单” 1.不喊做大做强的发展口号，因为那不是批判卓越的唯一标准2.体现其文艺青春的感觉、有态度的个性表达，以文艺屌丝范传递正能量（江小白代表着青春而简单的个性，甚至有些自嘲的屌丝文化情结，有着强烈个性表达、爱憎分明的文艺青年情结。他们希望“我是江小白，生活很简单”的生活理念成为每一个人生活的态度，成为8090后自己的表达。）3.品牌IP，将卡通形象与白酒产品紧密结合，自带传播能力，越来越多人愿意借江小白来抒发和表达自己，对这个复杂世界而言，或许人人都是江小白4.坚守“简单包装，精致佳酿”的反奢侈主义产品理念，坚持“简单纯粹，特立独行”的品牌精神，以持续打造“我是江小白”的品牌IP与用户进行互动沟通，推动中国传统美酒品牌的时尚化国际化5.情绪饮料：有态度，有个性，有情绪 不说历史， 用创新创造新的历史。 颠覆传统，表达鲜活的当代人文 。 回归简单，用心 酿造简单的美酒。 品牌包装：个性化 1.可见性：亮颜色（蓝绿红），青春的包装图案，增强可见性2.信息性：产品信息3.感情诉求：激起预期的感情4.工作性能：包装保护产品，是否环境卫生，是否使用简单，便于携带5.文案：语录瓶，表达瓶（扫码进入H5，写一句话，得到定制瓶；既是消费者也是生产者）6.好产品是营销的前提；品质领先；内在本质的差异化；产品沟通力；将瓶身和外包装作为超级自媒体（强互动性，用户为王，自带流量）；发生情感链接（审判抚慰情绪，品牌链接情感）；注重社会化营销；发挥想象力，在满足功能的前提下尽情浪漫 “青春不是一段时光，而是一群人”“吃着火锅唱着歌，喝着小白划着拳，我是文艺小青年” 三、项目策划题（20分）背景资料： 华中科技大学是国家教育部直属的全国重点大学，由原华中理工大学、同济医科大学、武汉城市建设学院于2000年5月26日合并成立，是首批列入国家“211工程”重点建设和国家“985工程”建设高校之一。学校秉承“育人为本、创新是魂、责任以行”的办学理念，坚持“一流教学、一流本科”的建设目标，采取多种举措，深化教育教学改革，全面推进素质教育，构建和完善充满活力的创新人才培养体系。几十年来，已为国家培养了近20万名高级人才。 作为华中科技大学的一名学子，请你为母校设计一份品牌传播策划案。 （这个策划案，大家可以充分发散思路，不一定非要很官方、很正式的那种概念，可以从某个点、某个特殊纪念物、某个著名人物、某个著名景点、甚至是结合某个市场品牌来进行推广,不要给自己限定框架，充分发挥想象） 四、品牌策划案要求品牌策划案要求： 1、品牌背景(品牌性质；需解决问题；执行地域等)2、品牌传播调研(市场分析、产品分析、消费者分析)3、品牌传播策划(品牌定位、品牌精神、品牌形象、传播内容、传播策略)4、品牌传播执行(传播发布计划；控制与管理等)5、品牌传播评估（效果综述；受众反应；市场反应；此部分可简要叙述即可） 发掘亮点，突出重点，罗列要点，简明扼要，细节方面不需要展开论述。 五、知识点延伸品牌传播是品牌所有者通过各种传播手段持续地与目标受众交流，以最优化地增加品牌资产的过程。 1.品牌传播战略品牌知名度品牌知名度是指某品牌被公众知晓、了解的程度，它表明品牌为多少或多大比例的消费者所知晓，反映的是顾客关系的广度。品牌知名度用品牌再认率和品牌回忆率来衡量。 如何提高品牌知名度？ 1.可视性：让人第一眼就能感知到产品把某一色调打造成自己的专属符号，往往更容易让消费者看到你、记住你。 2.反差感：差异化产品属性产品不仅要可视，更要与同类型产品建立反差感。 3.抱大腿：与大IP进行绑定把产品与人们生活中更关心、更熟悉和更热门的信息绑定在一起。 4.故事性：让产品成为故事的一部分故事是非常具有传播性的，因为人人都喜欢讲故事。 5.表达自我：让产品帮助用户表达形象和价值观如果你的产品可以帮助用户展示自身的想法，地位，形象和价值观等，人们就很乐意去亲近它。 6.极限属性：打造“最系列”或“首款系列”；所谓的“极限属性”，其实就是指你的产品有哪些“最XX的属性”。 品牌认知度所谓品牌认知，是指消费者感知到的某一品牌产品质量而形成的印象，是长期形成的品牌资产之一。 如何提高品质认知度？技术优势、产品领导、优秀服务 品牌联想度品牌联想是指透过品牌而产生的所有联想，是独特的销售利益点传播和品牌定位沟通的结果 。 品牌联想是消费者在看到某一品牌时所勾起的所有印象、联想和意义的总和。比如产品特点、使用场合、品牌个性、品牌形象等。 品牌联想的策略 （1）创造品牌故事。（2）为品牌设计灵魂人物。（3）借助有名望的消费者。（4）迎合消费者心理。 品牌联想产生的价值●帮助处理信息——引发个人传播●差异化—与竞争者明确区隔●提供购买的理由●品牌延伸的依据 集聚品牌传播战略一、集聚品牌传播战略的定义 企业设定一个单一品牌，将企业所有的资源都归属到这个品牌之下，并整合运用品牌传播工具围绕这一品牌进行传播的战略。 企业品牌延伸的结果，即企业以某一产品或服务为载体创出品牌后，再将既有的品牌延伸开发、生产或收购、兼并的其他产品上。 一牌一品：指一个品牌下只有一种产品的品牌战略，实施一牌一品战略的最大好处，是有利于树立产品的专业化形象。 一牌多品：即一个品牌下有多种产品的品牌战略。 二、集聚品牌传播战略的优劣势 优势： 1、有助于整合企业优势资源，优化资源配置。2、有助于节省品牌设计、传播的费用。3、有助于借助晕轮效应，利于新产品的推出。 劣势： 1、可能出现“株连效应”，破坏整体品牌形象。2、可能会模糊品牌的核心价值和定位，造成传播的紊乱，从而稀释品牌资产。 集聚品牌传播战略应遵循的基本原则 集中品牌下的各种产品必须要有密切的相关性 应有相同的服务系统，即不同类的产品可以在同一渠道一并进行推广 应在技术上密切相关 在质量档次上相当 差异化品牌传播战略一、差异化品牌传播战略的定义 企业对于各种不同的产品分别赋予不同的品牌名称和标识，并运用传播工具对这些品牌分别进行传播。 差异化品牌传播战略符合产业发展的规律。 产业发展的过程就是市场不断扩大、不断被细分的过程。 产业存在不同细分市场的事实为多品牌战略奠定了基础。 二、差异化品牌传播之间品牌区分的类型 1.以价格和档次来划分 如欧莱雅集团旗下的化妆品品牌，有高端的兰蔻、碧欧泉，中端的薇姿、理肤泉，低端的羽西、美宝莲、小护士、卡尼尔。 高端品牌的意义为特定人群提供高品质产品，并且通过品牌的外化形成区分；低档品牌的意义在于提供最高性能价格比的选择。 2.按功能划分 如宝洁的飘柔-柔顺、潘婷-滋养、海飞丝-去屑、沙宣-专业护理、伊卡璐-芳香等。 3.以风格特色划分 SMH公司旗下的20多个腕表名牌，OMEGA的名人气质，RADO的先进工业设计，LONGLINES的浪漫气质和优雅风范等等，以满足不同消费者的需求。 4.以目标人群划分 中国联通的七大品牌：世界风-中高端客户，为客户提供非标准化的后付费产品。如意通-面向大众市场，定位是方便、实惠、亲切。新时空-面向集团用户与行业用户等。 三、差异化品牌传播的优势 采用原因 1、多占货架面积2、给低品牌忠诚者提供更多的选择3、降低企业风险4、鼓励内部合理竞争、激扬士气5、各品牌具有不同的个性和利益点，能吸引不同的消费者 实施特点 一是不同的品牌针对不同的目标市场。飘柔强调使“头发更飘、更柔”；潘婷则突出“拥有健康，当然亮泽”；海飞丝则是“头屑去无踪，秀发更出众”。 二是品牌的经营具有相对的独立性。在宝洁内部，飘柔、潘婷和海飞丝分属于不同的品牌经理管辖，他们之间相互独立、相互竞争。 四、差异化品牌传播战略的劣势 1.分散传播资源 增加品牌传播的费用开支，许多品牌同时面市，需要有足够的后备资源才可能将这些品牌各个激活。 2.品牌结构冗杂 对公司和顾客来说品牌结构更复杂，以至于无力有效地反应。 3.可能破坏企业整体生态 品牌不可能个个完美，在整体的品牌结构中，某个或某些品牌属于不良成分，不但无端耗费企业的传播资源，而且可能破坏企业的整体生态。 差异化品牌传播战略应遵循的基本原则 ： 不同的品牌针对不同的目标市场。 品牌的经营具有相对的独立性。 银弹品牌传播战略一、银弹品牌传播战略的定义 企业对其生产或经营的各种产品在使用同一个主品牌的同时，再根据各种产品的不同性能和特点设定不同的名称和标识，作为改变或支持主品牌形象的工具。 银弹品牌是附在主品牌之下，且二者关系密切，一般同时出现。 银弹品牌是品牌结构中一个有力的工具，它可以通过增加与顾客有关的联想物起到驱动作用。 银弹品牌还可以扩展主品牌，使它在本不适合的领域进行竞争。而且银弹品牌可以说明新的受托行为是新颖的、有新闻价值的。 二、银弹品牌传播战略的使用特点 1.处于从属地位广告主宣传的重心是主品牌，副品牌处于从属地位 2.直观、形象地表达产品优点和个性形象（“松下—画王”彩电的副品牌“画王”传神地表达了画面逼真自然、色彩真鲜艳 ） 3.副品牌较主品牌内涵丰富，适用面窄。（“小厨娘”用于电饭煲等厨房用品十分贴切，能产生很强的市场促销力，但用于电动刮胡刀、电脑则会力不从心 ） 4.副品牌口语化、通俗化，能起到生动形象地表达产品特点的作用，而且传播快捷广泛，易于较快地打响副品牌 （“海尔—帅王子”、“TCL—巡洋舰”等均具有这一特点 ） 三、银弹品牌与主品牌之间的关系 描述性的银弹品牌传播 ：银弹品牌名称、标识只起到区分产品差异的作用，对主品牌并不具驱动作用。 主品牌驱动银弹品牌传播 ：当主品牌是基本驱动者时，银弹品牌出现了另一个变体。如DELL的DIMENSION。 主品牌和银弹品牌起共同驱动作用 ：SONY的WALKMAN。 联合品牌传播战略一、联合品牌传播战略的定义 两个或两个以上品牌从各自利益出发、寻求同一愿景、开展品牌传播的一种战略思维和方式。 联合品牌的注意要点： 品牌间价值相当 各自有优势 注意品牌间的定位差异和目标取向 竞争品牌是否可以参与互动 二、联合品牌传播的策略表现 1.联合广告可口可乐与腾讯 2.联合促销买一种品牌的产品可获赠另一品牌的产品，或者同时购买两种品牌产品可以获得优惠。 3.交叉POP（街头广告）顾客可以再某一品牌的通路终端上获得另一品牌的宣传信息。 4.联合服务ATM银联 5.联合PR新品记者招待会 从消费者角度出发，品牌间通过互动营销方式互动传播，往往使得消费者可以以更小的成本或更小的努力获得其需求的满足。这些人性化的措施使得品牌更富亲和力，对其品牌形象的提升是大有裨益的。 三、联合品牌传播战略的作用 1.可降低营销成本 彼此可以利用对方的资金、通路、人员等既定的要素，将对方的品牌信息传达给消费者，以减少营销传播成本。 2.扩大品牌可接触范围 品牌为了扩大其知名度或达到提示消费者的目的，需要有较大的暴露范围和较高的暴露频次。 3.可提升或巩固品牌形象 一个成长期的品牌与强势品牌的互动，可使消费者对这一品牌产生与对强势品牌相近似的认同感，从而提升品牌的形象。或者两个定位相当的品牌的互动传播，则可使形象得到巩固。 2.品牌形象一、品牌形象的定义及发展品牌形象是消费者对传播过程中所接收到的所有关于品牌的信息进行个人选择与加工后存留与头脑中的关于该品牌的印象和联想的总和。 这个定义有两个要点： 品牌形象是从传播过程的接收者角度出发的概念，是有关于人们如何看待这个品牌的概念； 品牌形象塑造的主要手段是传播。 品牌形象VS企业形象 企业形象：1、一种宏观意义上的概念，囊括了公司所有的产品、技术、品牌以及其他公司资产；2、可能涵盖好几个品牌形象，也可能是所有公司品牌形象的高度概括与综合。 品牌形象：1、辐射的范围相对狭小，就产品而言，多指某一产品或产品线；2、具体到某一品牌形象。 品牌传播形象符号系统 一、品牌名称品牌名称是品牌中能够读出声音的部分，是品牌传播的核心要素，是品牌显著特征的浓缩。一个好的品牌名称本身就是一句最简短、最直接的广告语，能够迅速而有效地表达品牌的中心内涵和关键联想。 品牌命名的法则 1、品牌命名要易读、易记。（1）要短而精。（2）可以选择一些高熟悉性的词。（3）可以选择一些高意义性的词。是指在单位时间内，有一个词语联想到其他词语的数量。（4）可以选择一些高意向性的词。容易并快速唤起人们心里图像的状况。 2、品牌命名要有特色。是指品牌命名要具有显著性的特征。 3、品牌命名要有想象力。（1）强化产品的属性和利益。（2）赋予品牌一定的情感和自我表现价值。（3）赋予品牌名称一定的文化内涵。（4）暗示产品类别。（5）暗示产品质量。（6）体现产品和行业的特点。（7）与目标消费者群的审美倾向相适应。 4、品牌命名要有亲近感。是指品牌命名要有人情味、亲和力，能够给顾客产生赏心悦目的感觉和亲近的体验，以拉近品牌与消费者的距离。 5、品牌命名要考虑到品牌名称的延伸功能。 6、品牌命名要考虑到品牌国际化的需要。能够适应多种语言，尤其是英语。 二、品牌标识是指品牌构成中由字体、图像或字体、图像、象征物融为一体组合成的视觉识别部分。它是品牌要素的重要组成部分。 品牌标识的分类 字体型：由某种文字的字母、字词或数字经过特殊的书写形式而构成的品牌标识类型。 图像型：由某种经过构思和设计的图像而组成的品牌标识类型。 综合型：由字母、字词或数字与图像、象征物融为一体的品牌标识类型。 品牌标识的设计方法 1、品牌标识与品牌名字相结合。（1）品牌标识完全由品牌名字构成。如诺基亚。（2）品牌标识与品牌名字意思明显一致。如苹果。（3）品牌标识图像含蓄地表达品牌名称的内涵。如迪斯尼。（4）品牌标识由品牌名称的个别字母组成。如KFC。 2、力求简单明了。（1）品牌标识设计构造要尽量简约、均衡。（2）尽量与品牌名称相一致。（3）尽量选择人们熟悉的人、事、物、景等作为品牌标识的设计元素，增强人们对标识的熟悉度和亲近感。 3、力求突出特色。（1）选择适当的字体。（2）选择恰当的颜色。（3）使用独特的形状。（4）设计具有联想力的象征物。 三、品牌口号用来传递有关品牌的描述性或说服性信息的短语。就是通常所指的品牌广告语。 品牌口号作为一种语言符号，它具有解释性功能，起到了传达品牌形象的沟通作用，弥补了符号商标在内涵沟通上的劣势。在加上品牌广告语具有一定的稳定性与整合性的特点，使得其渗透力强、影响力大、传播面宽、辐射面广、从而使得品牌达到理想的传播效果。 品牌口号的设计方法 1、利用品牌名称设计广告语提高公众品牌意识，树立品牌形象。如“选品质，选雀巢”。 2、将品牌与产品放在同一广告语中强化品牌定位，指明产品特殊之处。如“好空调、格力造”。 3、围绕消费者的心理需要进行设计让广告语具有煽情的效果。如“哇哈哈—我的眼里只有你”。 4、围绕品牌的核心价值设计广告语如耐克的核心价值为“专一敬业，超越自我”，因此，其多年来的广告语为“想做就做”。 四、品牌包装一是指设计生产容器或包裹物的系列活动过程； 二是指容器或包裹物本身。 品牌包装的设计 1、形式与内容要表里如一，具体鲜明2、要充分展示商品。这主要采取两种方式，一是用形象逼真的彩色照片表现，真实地再现商品。如巧克力、方便面等，逼真的彩色照片将食品的色、味、型表现得令人馋涎欲滴。二是直接展示商品本身。全透明包装，开天窗包装在食品、纺织品、轻工产品中是非常流行的。3、要有文字说明。4、要强调商品形象色。5、要注意功效设计。 品牌包装的评估：VIEW模式 可见性 信息性 感情诉求 工作性能 3.品牌传播的手段广告广告的诉求策略 理性诉求：是一种采用理性说服方法的广告形式。这种广告说理性强，有理论、有材料，虚实结合，有深度，能够全面地论证企业的优势或产品的特点。 早期雕牌广告： “只买对的，不买贵的” 感性诉求：主要诉诸于消费者的感性思维，“以情动人”，使消费者在感动之余认同该产品。 代言人诉求：借用名人效应提高品牌知名度。 理性诉求的表现思路： 明确传递信息，以信息本身和具有逻辑性的说服加强诉求对象的认知，引导诉求对象进行分析判断。理性诉求的力量主要来自具体的信息、明晰的条理、严密的说理。 理性诉求可以做正面说服，传达产品、服务的优势和购买产品、接受服务的利益，也可以做负面表现，说明或者展现不购买的影响或危险。 感性诉求广告主要通过调动目标受众的情感、打开受众的心扉、煽动受众的情绪，以达到广告传播的目的。 销售传播一、销售传播的背景 销售传播发展的原因： 内因：品牌自身具备使用销售推广工具的条件等。 外因：竞争对手频繁使用销售推广工具；由于经济的疲软和消费者的成熟；广告传播效果相对下降等。 销售传播在短期内能产生较好的销售反应。但对于品牌形象而言，大量使用销售推广会降低品牌忠诚度，增加顾客对价格的敏感，淡化品牌的质量概念，促使企业偏重短期行为和效益。对小品牌来说，销售传播会带来较大好处。 销售传播的类别 免费样品 折价赠券 包退包换 现金退回 多买多送 使用者奖励 赠品 抽奖或竞赛活动 使用示范 以塑造品牌形象为主的销售推广，才能真正起到既促进销售、又提升品牌的作用。 对负责品牌塑造的团队来说，从办公桌抬起头，用顾客的角度实际看看外面的世界是很重要的。购物已经变成人们娱乐活动的一环，品牌塑造的下一个大变革在于顾客经验：在每一次与顾客接触的机会中，建立起顾客忠诚度及终身关系。 口碑传播一、口碑传播的定义 指人与人之间直接口口相传的一种信息沟通交流活动。这种交流主要是通过语言来完成，但也可以通过非语言的方式来进行，如动作、手势、表情等。 在品牌传播的手段中，口碑传播最容易为消费者接受。口碑传播的优势在于它同消费者直接对话和交流，双方有问有答，在接触中满足其消费心理，让其买得放心，用得舒心，形成良好的口碑。 口碑传播能够减少宣传成本，增大潜在客户的成交概率，增强客户的信赖感，提升产品的复购率，能够提升企业的形象，强化用户对品牌的忠诚度。 二、口碑传播的用户驱动力 1、产品驱动 产品或服务本身非常好，使得用户非常愿意将其分享给周边的朋友；产品在体验、模式、服务、性价比等其中一方面，打磨得明显优于同行。 2.精神驱动 用户并不是本身实际需求被解决，而是产品的灵魂人物的精神激发了用户。 比如你听到的「这手机真有情怀！」、「我们都欠他一张电影票！」，这两句话非常熟悉了吧，这种用户的推荐就来自于满足用户精神上满足的驱动。 3、利益驱动 因为产品本身设计的推荐机制，通过分享推荐好友，可以获得一定的利益。 三、如何做口碑传播 1.通过权威媒体、名人进行“背书”，为你的企业、品牌、产品奠定基础。 2.通过搜索营销的优化，使有利于品牌的信息出现在首页，增强消费者信心 3.通过软文（新闻），以第三方的角度诠释品牌，增强权威性 4.通过“话题事件”的形式进行病毒式传播 品牌叙事一、品牌叙事的定义 狭义的品牌叙事，指品牌相关宣传资料提供给目标受众的品牌背景文化、品牌价值理念及产品利益诉求等方面的内容； 广义的品牌叙事，指通过品牌的相关宣传介绍资料、媒体发布的广告和新闻公关活动以及品牌与相关社会文化现象相融合的文化传播活动中透射出来的品牌内涵，它是品牌背景文化、价值理念以及产品利益诉求点的形象化生动化的体现，又叫做品牌故事。 二、品牌叙事的表现形式 以品牌创始人为叙述主题 在世界经济进入品牌经济时代的100多年来，涌现出许多不仅在品牌经营决策上有远见卓识，而且在个人人格魅力上具有深远影响的传奇式人物，他们的品牌就是依托这些传奇式人物而进入人们的心扉，给人启迪，催人奋进，并因对这些传奇人物的偏爱，爱屋及乌，对其所创立和拥有的品牌产品情有独钟。如香奈儿、玫凯琳、李宁。 以虚拟人物为叙述主题 通过虚拟人物或神话传说来阐述品牌叙事，以此扩展丰富奇妙的想象空间，带给目标受众美的愉悦和心灵共鸣，是品牌创造者们屡试不爽的神秘武器。此类品牌叙事的好处在于品牌伸展空间大，而且给人以神秘感，容易引起人们的好奇与注意。 以地域环境为叙述主题 在美容化妆品界，产品原料产地以及使用水质、制作工艺的不同，带来迥然不同的使用效果和品位感受，是众多美容消费者深信不疑的消费心理。许多品牌经营者抓住这一消费心理，通过对优美的自然风貌、地理环境、自然水源以及人文景观的描述，带给目标受众纯净自然、品质卓越的温馨感受。如雅漾活泉水。 以产品功能为叙述主题 以产品功能特点为叙述主题的品牌叙事，旨在重点凸显品牌独特的产品优势，以引起目标受众的兴趣和注意，达到广为传播、带动市场销售的目的。如以要点为销售渠道的薇姿，通过着意对起源于法国中部的千年历史小城VICHY的温泉水，能增强皮肤天然防御功能方面的渲染，凸显薇姿产品的独特功效和神秘色彩。 品牌形象代言人一、品牌形象代言人的定义 是指企业在一定的时期内，以契约的形式指定一个或几个能够代表企业品牌或产品形象并展示、宣传企业品牌或产品形象的人或物。其职能包括各种媒介宣传，传播品牌信息，扩大品牌知名度、认知度等，参与公关及促销，与受众近距离的信息沟通，并促成购买行为的发生，建树品牌美誉与忠诚。 二、品牌形象代言人的作用 （1）传播产品的具体功能通过品牌形象代言人的语言、动作来直接说明产品的具体功能特征、能给消费者带来的利益。 （2）通过展现形象本身，让观众产生对品牌形象的联想将形象定位于喜爱自己的特定消费群体，强化品牌效应，引导消费时尚。 （3）通过展现“品牌形象代言人”的鲜明个性让观众联想品牌的独特个性 （4）形成品牌识别品牌形象代言人是一个一个直观的、很容易辨认的“联想物”，是区别于其他品牌的重要标志。 三、选择品牌形象代言人的策略 1、为名而不唯名 有所名，用其名：形象代言人最好要有所名，而且要比企业现时的名牌更有名。其次，代言人最好要有较高的知名度和美誉度。 无所名，扬其名：一些有创造性的企业往往另辟蹊径，启用新人，创造名人。“创造名人”比“依仗名人”更具挑战性，因而也更具魅力。如，可爱的儿童和宠物。 成功与否，与名企关系不大并非所有的名人对所有的产品或品牌都是适合的，形象代言人运用的成功与否，关键还要看与产品和品牌是否相适合。 2、性相近、才能情相投 形象代言人的个性要与企业的品牌个性相吻合 只有形象代言人的个性与企业名牌个性相吻合时，才能给目标市场消费者留下鲜明而深刻的印象，才能使企业形象达到更有效的整合，发挥出更大的名人效应。 形象代言人要得到目标消费群的普遍认同 “顾客就是上帝”，企业在选择形象代言人时，必须针对企业不同目标市场的消费者状况，根据目标市场的营销环境进行恰当的选择。形象代言人的个性要能够融于企业的文化内涵 真正能够将企业形象代言人和企业整合起来的就是两者的文化。形象代言人理应加强文化知识的学习和修养，否则，就会给人以肤浅的感觉，同时也会给企业带来许多负面的影响。 3、适合的，才是最好的 形象代言，贵在以身作则 企业靠的是信誉，作为企业形象代言人，靠的也是信誉，如果禁不住企业优厚待遇的诱惑，盲目代言，为企业做了虚假广告，欺骗了消费者，其代言人的含金量也会因此大大折扣。 互为代言，才能双赢 一方面，企业有企业一套选择形象代言人的标准；另一方面，代言人也应考虑到自己是否能够担当得起代言企业的神圣使命。 真正的代言人是消费者——口碑 企业在实施代言人战略时要树立起全员代言的观念，除了企业家，还有企业的模范员工、众多的用户代表都可以成为企业的代言人。 明星代言，选的是时机 找明星是为了借力。找处于上升趋势的明显，既可节约广告费用，又可借助代言人影响力的飙升赢得比预期更多的品牌收益。]]></content>
      <categories>
        <category>Communication Science</category>
        <category>Advertising and Marketing</category>
      </categories>
      <tags>
        <tag>Communication Science</tag>
        <tag>Advertising and Marketing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[新闻学原理复习笔记]]></title>
    <url>%2F2018%2F01%2F14%2F%E6%96%B0%E9%97%BB%E5%AD%A6%E5%8E%9F%E7%90%86%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Abstract: 新闻学原理期末复习笔记 ~ Chapter1 新闻学概论新闻：研究新闻现象和新闻活动规律的科学。 普利策：“倘若一个国家是一条航行在大海上的船，新闻记者就是船头的了望者，他要在一望无际的海面上观察一切，审视海上的不测风云和浅滩暗礁，及时发 出警报。” 新闻从业者：良心与勇气，光荣与梦想。 1.1 新闻理论基本知识 什么是新闻 新闻价值 新闻真实 新闻客观性 新闻自由 新闻管理体制 新闻对社会的意义 报道精彩的新闻 新闻职业伦理规范 1.2 课程设置 新闻学基础理论：新闻、媒介与传媒；新闻与舆论、宣传；新闻价值；新闻真实；新闻客观性 新闻业理论：新闻史；新闻体制；新闻选择和框架；新闻专业主义与新闻伦理 数据新闻：媒介融合；精确新闻与数据新闻；算法新闻；智能硬件与新闻业 1.3 中国新闻学面临的挑战 西方新闻学思想与中国国情的张力(政治经济结构) 传统新闻学与传媒技术数字化、受众接收习惯之间的冲突(技术) 学术界与业界之间缺乏沟通与互动(教育制度) 新闻业的市场化对专业主义的侵蚀 Chapter2 新闻、媒介与新闻业2.1 什么是新闻新闻事实（信息）——客观存在 新闻作品：主观反映客观的结果 2.2 新闻、报道、传播的区别 报道：通过报纸、杂志、广播、电视或其他形式把新闻告诉受众；是可查证的事实的客观叙述 传播：是意义的分享，社会信息的传递 信息：（香农）信息是用来消除收信人的某种不定性的东西，作用是消除观察者在相应认识上的不定性 事实：事物的实际变动、出现、发生和发展过程中的各种情况；特征有：客观性，具体性，不变性，独立性，联系性 新闻： 内涵：是经报道的新近事实的信息，是新近发生或变动的事实，是对社会公众普遍关注的新近发生或正在发生的事实的报道； 作用：传递信息、公共知识、制度化的竞争性商品； 根本属性：事实性，新鲜性，公众性（广泛关注） 其他性质：机构性，指称性，意识形态倾向 2.3 传统媒体环境下的新闻类别• 以新闻内容分类 :政法新闻、经济新闻、文教卫新闻(包括文 艺)、体育新闻、社会新闻等。• 以新闻的时间性分类 :突发性新闻，延缓性新闻 。• 以新闻发生地分类 :国际新闻、国内新闻、地方新闻 。• 以新闻与读者的关系分类:硬新闻、软新闻。• 以新闻媒介分类:报纸、电视、网络。 2.4 传媒、媒介与新闻业• 传媒:信息传播的载体、手段、途径、体制的统称，如大众传媒。• 媒介:强调不同的传播技术属性。如电子媒介、数字媒介、纸媒。 媒介融合。• 媒体:突出传播活动的主体性和体制性，常是新闻事业的代名词， 如中央级媒体，外国媒体等。 新闻业：政治属性、经济属性、文化属性、科技属性 我国新闻业的构成 • 主管部门• 新闻组织与机构• 新闻从业人员• 新闻受众• 渠道与内容 我国新闻媒体的分类 • 报纸:党报、都市报、行业报•杂志• 电视:四级办电视• 新媒体:网站、社交媒体、自媒体 • 通讯社:新华社、中国新闻社 媒介的社会决定论 • 数字鸿沟• 信息(技术)赋权• 媒介再现• 媒介与阶级、性别和种族• 传播政治经济学 Chapter3 新闻与宣传【重点】新闻：新闻是新近发生的事实的报道，它的基本职能是告知人们 所需要的信息。 将新闻与宣传视均为一种传播活动(新闻= 新闻传播/报道活动) 3.1 宣传 运用各种有意义的符号传播一定的观念以影响人们的思想，引导人们的行动的一种社会行为，它的基本职能是传播一种观念(理论、方针、政策、伦理道德、立场态度)。 宣传是以重要的符号包括消息、谣言、报道、图片和其他种种社会传播方式来控制 意见的做法。(拉斯韦尔) 宣传是有目的地传播某种事理以影响他人意识和行为的一种社会活动 宣传的主要环节 宣传的表现：广告，公关，业配新闻，政治宣传 3.2 新闻与宣传的辨析 根本区别在于受众本位与传者本位的区别 最基本区别：新闻传播信息，宣传传播观念 出发点：新闻——受众需要，宣传——宣传者自身需要(宣传者本位和受众本位) 归宿点：新闻——一般无主观意图，宣传——让受众按宣传意图行事(影响受众、受众自己分析判断) 传播方式：新闻——不可以重复，宣传——可以重复 传播内容：新闻——以事实为主，宣传——以观点为主 传播的要求不同(新闻要求真实、全面、客观、公正、及时、准确; 宣传要求定性的准确，观点和材料的统一;观点要求正确、鲜明，材料要求真实、典型) 新闻重信息，宣传重形式 ；新闻重新异，宣传重复； 新闻重事实，宣传重观点； 新闻重实效，宣传重时机 ；新闻重沟通，宣传重操纵；新闻重平衡，宣传重倾斜 3.3 宣传的技巧 辱骂法（给对手贴标签） 光辉泛化法（光辉泛化法:将某事物与好字眼联系在一起，使我们不经证实而接受或赞同这一事物） 转移法（指将某一令人尊敬事物的权威、认可和威信转移到另一事 物上），生活方式营销（将生活方式转移到产品中） 证词法:让某个令人尊敬或使人讨厌的人就某个给定的观点或项目或产品或人物说好话或坏话 平民法:是指某讲话者以他和他的观点来自人民为由，让受众相信他们。 洗牌作弊法:是指通过对事实或谬误、例证或干扰物，以及合理陈述或不 合理陈述的选择和使用，以期对某观点、项目、人物或产品做尽可能好或 尽可能坏的说明。(单方面信息，只有争议中一方的意见被表达出来)对 毛的评价 3.4 新闻生产中的五种过滤器赫尔曼和乔姆斯基的“宣传模式” 1、大众传媒的结构、所有权和利润取向;2、媒体的主要广告收入来源;3、记者对政府、企业和军方新闻来源的依赖;4、对媒体节目做出的反面回应(批评、诉讼、政令、请愿、游行 示威、威胁和惩罚、广告商撤回赞助)5、使用或反对某种意识形态，用来框架媒体的报道 3.5 宣传效果 强效果论：魔弹论，皮下注射论 有限效果论：抵制与心理过滤 适度效果论：议程设置假说，教养理论，沉默的螺旋 3.6 11种新闻中夹杂宣传意图1、用单向和静态的方式表现人物和事件，使受众形成定向思维 2、把观点包装成事实 3、有选择的使用引语，通过表面客观的手段达到主观的目的 4、使用情感动词和副词直接或间接引语呈现否定或肯定的态度 5、在信息方面有所选择，使用一些事实而不使用另外一些事实 6、不顾受众的知情权对某个新闻事件完全不报道或漏掉新闻事件的某些事实 7、采用不同的称号，如一个新闻事件中的“游击队员”，可能在其他 地方就变成了“自由斗士”。 8、用笼统的词语进行概述，比如使用“许多人”或“大多数人”等词 语。(一小撮不明真相的群众) 9、根据要塑造的形象，选择性的使用不同的语言、照片或音像资料。 10、以偏概全，用个体代表整体。 11、借口无法查对，对事实不再进行追踪，这种方法经常用在报道结尾。 Chapter4 舆论【不考】4.1 舆论 公众关于现实社会以及社会中的各种现象、问题所表达的信 念、态度、意见和情绪表现的总和，具有相对的一致性、强烈程度 和持续性，对社会发展及有关事态的进程产生影响。其中混杂着理智和非理智的成份。 舆论监督 广义:对社会一切不良现象的监督。 狭义:通过公共论坛所抒发的舆论力量对国家机关及其工作人员滥用职权等不当行为的监督与制约。 主体:普通公民和新闻媒体。 对象:现实社会以及各种现象、问题。一般是各级政府机构和政 府官员。 载体:公共论坛。 舆论属性与本质 公共事务与公共意识 社会中的优势意见(典型、准确、权威、引导、扩散) 舆论中的社会知觉(集群心理、社会心态) 什么不是舆论：新闻，传媒，意识形态，谣言 舆论的三种存在形态 潜舆论：问题的潜伏期 包括两种类型，一是没有公开表达的信念，二是知觉到而又不易确切捕捉到的公众情绪。 有时潜舆论比显舆论更能确切地说明舆论的真正倾向，因为显舆论的发表会受到各 种其他因素的规范，而各种社会规范很难直接干预情绪型潜舆论的表达，在较少 约束的情况下，无形中使得它反而拥有了几分纯真。 显舆论：对外界刺激的认知、意向和情感的综合反应 在一定范围内相当数量的公众，以各种公开的形式表达的对舆论客体的态度， 它或是由外界刺激直接引起，或者是由情绪型舆论经过一段时间的酝酿，转化而来。 表现：为适应具体环境而形成的显舆论；显舆论中的自我表现成份；显舆论中的自我防卫成份。 行为舆论：主要以行为的方式来表达的舆论 通常还会夹杂着评议和文字的意见表达，严格说是一种综合型舆论，在行为中既有情绪的表达，也有公开的言语。 与其他社会行为的主要区别，在于行为目的是为了观念的传播或情绪的发泄。 发生原因：社会结构的诱发；社会内部发生“结构紧张”；某些一般化了的信念得到发展和普及；契机要素的引发；对行为参与者的动员，其中舆论领袖的作用甚为重要。 类型（根据集合行为理论）：旨在实现价值志向的行为舆论(环境保护、捐助灾民)；旨在实现规范志向行为舆论(学习雷锋、扫黄打非)；敌意暴露行为；恐慌和疯狂行为；各种相对短时的围观。 4.2 舆论的要素公众是舆论的主体现实社会以及各种现象、问题是舆论的客体自身是信念、态度、意见和情绪表现的总和舆论的数量，一致性原则舆论的强烈程度舆论的持续性(存在时间)舆论的功能表现:影响舆论客体舆论的质量:理智与非理智 4.3 舆论主体：公众舆论公众群体具有两个方面的特点(两个标志): 一、由相近或相同的认知而关联二、具有社会参与的自主性 三、当前存在舆论监督的主体缺失问题 4.4 舆论客体：现实社会以及各种社会现象、问题现象、问题一、与公共事务相关二、引起广泛关注三、不同程度地具有争议性 4.5 舆论的存在形式(舆论自身):态度、意见和情绪表现的总和态度的表现形式是多样的，以言语形式的表达，构成显舆论;以情绪形式的表达， 构成潜舆论;以规模行为来表达，构成行为舆论(如游行示威、群众集会等)。 舆论所表达出来的意见，不是个别人的意见，也不是少部分人的意见，而是相当多的社会成员所表达出来的意见的集合，或者综合。 它不是一致性的意见，而是在特定问题上多种意见的状态的描述。作为舆论的意见，它既包含一个人的认知因素，也包含着他的情感和理性评价因素，同时，还包含着意志行为，即准备采取某种行为的一种意志、一种状态。 4.6 舆论的强烈程度判断舆论的影响力，不仅有数量做基础，还要考虑该舆论的诉求是否鲜明、是 否集中、是否强劲有力。考察舆论的强度，也是一个反映舆论状态的重要指标。 舆论强度，它更大程度反映了这种意见质量如何，反映了某一种社会意见影响 决策的冲击力量。 舆论的强烈程度有两种表现方式，一种是用行为舆论来表达，通常行为舆论比 言语舆论的强烈程度大些。另一种除了部分通过言语表达外，相当程度上表现 为没有用言语表达的内在态度，其强烈程度需要通过舆论调查来测量其量级。 4.7 舆论的功能表现:影响舆论客体舆论存在的综合表现，是能够以自在的方式，直接地或间接地、明显地或隐蔽 地影响着舆论客体。 如果说一种舆论在它存在的范围内没有产生对客体的任何影响，那么这种舆论 便谈不上是舆论，而是一种一般的无足轻重的议论。 人们所以能够感觉到周围存在着各种相近的或相对立的舆论，就是由于各种舆 论在相互交织中时时影响着舆论客体，促使客体朝着主导性舆论的方向发展或 转变。所以这种影响表现为各种舆论相互作用的过程。 4.8 舆论的七个必要要素舆论的七个必要要素: 舆论的主体、舆论的客体、舆论本身、舆论的数量、舆 论的强烈程度、舆论的持续性和舆论的功能表现。这七个要素缺一不可，少了 一个即无法构成舆论。 任何一种意见形式，只要同时具备以上七个要至少，便可视为一种舆论。 任何一种舆论都可能存在外界对它的质量评判(正确与否、理智与否、方向正 确与否等等)，但是这个要至少不妨碍确认舆论本身的存在，即使判定某种舆 论是不正确的 4.9 舆论的形成社会变动、较大事件的发生刺激意见的出现 意见在社会群体的互动中趋同1、意见的互动与趋同2、舆论领袖的作用 权力组织及其领导人、大众传播媒介促成所希望的舆论(电信诈骗) 文化与道德传统对舆论形成的制约 4.10 大众传播媒介与舆论大众传播媒介与舆论的互动： 一、反映舆论:大众传播媒介是舆论的重要载体二、制造舆论和影响舆论:大众传播媒介营造了相应的舆论 环境1、议程设置:大众传播媒介对舆论的形成和发展方向的影响2、典型报道、热点引导与大规模政策总动员:大众传播媒 介对舆论的引领 大众媒介对各种舆论形态的引导： 麦克利德认为大众媒介在舆论形成过程中扮演了三种角色:1、渠道或联系者，即大众媒介是信息的流通渠道;2、变动的代言人，即大众媒介是舆论的代表者;3、大众媒体是舆论的认识方法的提供者。 4.11 舆情舆情：指社会舆论的总体特征 政治舆情：政府是舆情产品的最大需求方 商业舆情： 一是产品形象调研，了解用户对产品的使用感受; 二是品牌监测，对网上的负面信息和投诉进行处理; 三是竞争分析，了解自己和竞争对手的优势劣势各在哪里;  四是危机公关，及时处理重大失误和疏漏 舆情行业存在的问题 生产过程不透明 产品质量没有统一的标准 产品的运用范围没有限定，监控行为需要规范 人员的职业化程度不够 Chapter5 新闻价值5.1 新闻选择标准新闻价值标准决定新闻事实值得不值得报道，属于新闻选择的业务标准。 新闻政策标准决定新闻事实允许不允许报道，属于新闻选择的政治标准。 新闻选择的“业务标准”，是指根据新闻报道的业务要求规范所确定的选择标准。它所代表和反映的是 新闻事实信息内在的客观传播价值。 新闻选择的“政治标准”，是指根据政治因素的制约和政治环境的需要所确定的选择标准。它所代表和反 映的是新闻事实信息附带的主观宣传价值。 新闻价值标准是一个“入口”标准，凡是不符合新闻价值的内容和事实，都不允许进入新闻制作和传播过 程之中。 新闻政策标准是一个“出口”标准，凡是不符合新闻政策的内容和事实，都不允许流入媒体，进入新闻传 播过程之中。 5.2 新闻价值价值(新闻价值)是一个体现主客体关系的概念；新闻价值的客体是新闻事实，主体是新闻受众。 新闻价值 定义：事实所包含的足以构成新闻的种种特殊素质的总和 广义：新闻对社会的意义和作用 来源：接受者不知；接收者对事实有兴趣、关心或需要； 新闻价值的产生前提是传播者、接收者双方相互认可 主观说：新闻价值就是选择新闻的标准。 传受合一：新闻价值是新近事实或相应作品所含新闻构成要素的总和。它是传播者选择事实 和接受者选择新闻的客观标准。 满足需要：新闻价值是指新闻与收受者、社会之间的需求关系，表现为对收受 者和社会需求的满足。 功能说：所谓新闻价值，是指新近发生的事实在传播过程中 所履行的能满足人们知晓、认识、教育、审美等诸种需要的功能。 新闻价值的作用 帮助记者和新闻机构进行判断 具有一定的社会外部性(影响社会进程) 对新闻产业有推动作用 新闻价值的实质 新闻价值是对新闻的本质及其特性的量化把握，衡量事实在多大程度 上是新闻;其构成要素是新闻的本质及其特性的具体化和数量化， 可以成为衡量新闻事实的标准和尺度 新闻价值与新闻的价值 新闻的价值是指新闻作品通过传播满足社会其他需求的程度，实质上是新闻作品的社会功能的量化把握(项德生、郑保卫) 新闻的价值即新闻的传播价值，是指“新闻传播之后所实现的各种 价值效应之和。(童兵) 新闻的价值可能完全包含新闻价值，也可能不完全包含甚至毫无关 联 新闻的价值中包括宣传价值、政治价值、经济价值、道德价值、文 化价值等 实际操作中，选择新闻的标准多样化(新闻、技术、法律、道德、 政策、纪律、宣传、经济、文化等)，但新闻价值是最核心的标准 5.3 受众与新闻价值一、人们只保留、记忆对他们有意义的那一部分信息 ; 选择性接触、选择性注意、选择性记忆 二、人们本能的关注新鲜的、警惕性的信息 三、人们关注新闻的三方面价值驱动 1、人们对异常事物的关注(本能)2、人对相关利益的关注(利益驱动)3、人对自身偏好的关注(兴趣) 5.4 新闻价值的构成要素新闻价值构成要素是新闻事实对主体产生新闻价值效 应的客观依据，即事实具备什么样的属性(关系属性) 才能成为新闻事实。 1990年《中国大百科全书·新闻出版卷》:时新性、接近性、显著性、 重要性、趣味性。 不变要素包括真实性和新鲜性两种。它们是新闻事实信息是否具有新闻价值的决定性条件，也是新闻能否产生和存在的决定性条件。 可变要素包括重要性、接近性、显著性、趣味性。它们的存在决定着事实信息新闻价值的含量大小，是新闻事实信息 体现其价值的重要条件。 当代新闻价值的变迁：时效性 、重要性 、接近性 5.5 新闻的重要性（公共性）：新闻对公共利益的影响(1)事实影响人的多少;(2)事实对人和社会影响时间的长短;(3)事实影响空间范围的大小;(4)事实影响人们实际利益的程度。 5.6 十要素说(陈力丹) 事实发生的概率越小 事实或状态的不确定性越大，减少不确定性的事实或信息 事实的发生与受众的利益越相关 事实的影响力越大，影响面越广，越能立即产生影响力 事实与接受者的心理距离越近(兴趣、生活地域、性别、年龄、教育程度和专业、 经济收入、民族或种族或宗族的心理距离) 著名人物、著名地点 新闻事实中蕴含的冲突性 新闻事实的故事性(情感、原型) 新闻价值的具备在于满足了人们的求知欲： 不知道的刚发生的事实(一般是偶然的、突发的) 不知道的最新的变动(常规发生的事实中发生了新的变化) 不知道的最新发现(事实可能发生在过去，但刚才发现) 不知道的最新发表的观点(被揭示的隐秘观点，更可能引 起关注) 不知道的最新知识 5.7 新闻价值选择的标准受众需要 市场取向 记者经验积累 社会控制的需要 5.8 新闻业的价值监视环境 连接社会 传承文化 提供娱乐 Chapter6 新闻真实【重点】新闻真实、新闻素养、后真相时代 6.1 假新闻样本分析 假新闻的首发媒体 作者 失实程度 选题类型 故事框架类型 消息来源 纠错媒体 纠错时间差 假新闻生产主体的认错、纠错情况 特点 假新闻出现了新的炮制者——利益机构 传统媒体仍然是假新闻的主要生产者，但新媒体越来越成为传播渠道，假新闻借助社交网络传播更加迅速，假新闻呈现媒介融合的特征 新闻价值仍然是假新闻传播范围的决定因素 社会新闻(软新闻)仍然是假新闻的主角 新媒体上产生了辟谣平台，网民自发辟谣 造假方法 无中生有(记者捏造) 道听途说(未经核实) 移花接木(张冠李戴) 颠倒黑白 添枝加叶(细节失实、数据失实) 失实原因 党政机关夸大业绩，利用媒体给自己戴数字大红花 商业利益冲击正常采编活动 进入市场经济之后，新闻传播体制没有同步改革，致使新闻生产缺乏章法 ，人员管理混乱 有一些记者仅凭道听途说写新闻，缺少核实 有一些记者自己杜撰新闻 片面追求轰动效应 传媒把明显的恶搞作品当做“新闻”公开发表和传播 一些传媒工作者缺少基本的常识和科学知识 记者缺少专业主义素养 原因分析 事实核对查证不力 记者专业素养不够 营销策划行为盛行 6.2 新闻真实重要命题： 新闻真实是事实的真实传媒呈现世界的真实程度是有限的新闻真实是一个过程是一种有机真实，不是本质真实 几组概念： 现象真实和本质真实事实真实(概括)和总体真实微观真实和宏观真实党性原则(政治原则)和真实性原则(业务原则) 新闻真实的要求：新闻事实的真实 1、构成新闻的基本要素要完全真实 2、新闻中引用的各种材料要真实可靠 3、能表现整体上本质的真实 4、对人、单位、事件的评价要客观 5、不能脱离新闻来源随意发挥 6、新闻报道的语言必须准确 新闻真实所受到的客观条件的制约 1、新闻只能选取很少的事实加以报道，因而媒体呈 现世界的真实程度是有限的 2、新闻工作面临的基本矛盾:具体事件的纷繁复杂 与新闻报道不可避免的简约，因此具体新闻的真实， 只能表现为一个认识过程 3、新闻是否真实还取决于接受者的认同 4、选择事实时的文化背景、现实政治经济体制的影 响，使得真实性难以完全确认 新闻失实的原因 权势的选择标准使得传媒很难完全真实的反映现实 传播符号表达意思的有限造成对事实的误读 文学想象造成具体的新闻失实 体制性失实，即由政治、经济价值观引发的失实 采访写作编辑过程中造成的失实 新闻策划造成传媒假事件泛滥 传媒对科学的误读 故意制造、传播虚假事实 新闻采访、写作与编辑过程中造成的失实 采访不深入造成的失实 编辑过程的差误出现失实 编辑环节之间缺乏沟通造成的失实 编辑合适程序不对造成的失实，核实必须向当事人和旁观证人核实，不能 向作者核实 写作中作者想当然造成的失实。 新闻策划造成传媒假事件泛滥 消息来源与报道者重合; 传媒假事件隐藏着传媒自身的公关需求，或记者的单纯职业主义动机。 方式上由传媒或记者导演事实。 事件媒介化。媒体对自己导演的事件进行报道。 暧昧的真实。由媒介策划的新闻事件变成社会事实。 体制性的失实 故意制造、传播虚假事实或虚张声势地夸大事实。 套话、套路写作所造成的不真实 有意不报道公认的具有新闻价值的事实 互联网的影响 信源的增多使得虚假信息被传播的机会增大 也使得真实的声音得到传播，形成多元的真实 意见过剩而事实不足 在信息爆炸的时代更需要专业的从业者进行调查和甄别  公民新闻和深度报道互相补充 新闻工作的基本原则 新闻工作必须首先对真实负责。 新闻工作必须通过核实进行约束。 不得随意添加不存在的东西。 谨慎使用匿名信源，尽可能使你的方法和动机透明 公开。(据悉、前日，对信息作出清晰详细的说明) 不得误导信源 原创性与谦虚 不要贸然做假设 建立失实后的更正与答辩机制 6.3 新闻事实核查事实核查 塔奇曼:新闻需要的往往不仅仅是信息，而且是真实无误的事实。我所说 的事实，是指通过专业的而且是可靠的方式收集到的相关信息。信息的内 容、获得的方式，以及二者间的关系都能够被查证。验证事实既是一种政 治需要，也是一种职业技能 核查的必要性 该言论基于可验证的事实 该言论会对公众产生误导性的影响 该言论应具有足够的重要性 该言论看起来容易广泛流传 该言论会让普通人想要知道真假 (偷肾传说、恶意传播艾滋病) 如何进行事实核查 来自其他方面的消息能否证实事情的存在 是否有文献根据 提供信息的人有没有不当的动机，其动机是否经得起检验 对二手资料要保持怀疑 对有争议和受到攻击的观点的报道，是否公平 对不同意见的当事人是否给予回应，要尊重持不同观点的人的反诘权利 如何避免成为核查对象: 新闻是个技术活 是否对引语进行了复核?以确保其准确并且不会断章取义? 是否核对过网址、电话号码和罕见的姓名? 在报道中第一次引用人名时是否列出了姓名的全称? 是否检查过年龄、住址、职务，以确保其准确无误? 报道中的时间是否包括星期和日期? 如何避免成为核查对象:准确性清单，细节是魔鬼 报道的导语是否得到了充分支持 能帮助受众理解新闻的背景资料是否完整 新闻中的所有利害相关者是否都得到确认，是否联系过各方代表并给予发 言机会 新闻是否偏向某一方或做了难以察觉的价值判断?有些人是否会格外喜欢 这篇报道? 是否在新闻中对每条信息的出处进行了标注和查询，以保证其正确无误? 这些事实是否足以支持新闻的前提假设?有争议的事实是否得到了多个信 源的支持? 6.4 新闻素养媒体的社会角色 1.媒体帮助民众证明哪些事实是真实和可信的。 2.新闻工作者适当扮演“释义者”的角色。 3.新闻工作者继续发挥作为公共调查者的功能。 4.媒体帮助民众见证一切，付出专门的努力采编 一般人采访不到的新闻。 5.媒体是向民众传授获取新的知晓方法的中介。 6.媒体成为聪明的网络信息的聚合者。 7.由新闻工作者创建便于民众交流的公共论坛。 8.媒体成为监督权力者、公民记者学习的榜样。 六项生活在现在这个世界上而必须具备的“新闻素养” 1.我碰到的是什么新闻内容? 2.我得到的信息是完整的吗?假如不完整，缺少了什么? 3.信源是谁/什么?我为什么要相信他们? 4.提供了什么证据?是怎样检验或核实的? 5.其他可能性解释或理解是什么? 6.我有必要知道这些信息吗? Chapter7 新闻生产【重点】7.1 导入重点： 哪些因素影响了新闻生产? 市场驱动的新闻业 自媒体时代的新闻业 记者和信源是什么样的关系? 公共关系如何影响新闻业? 媒体的分类 纸媒、广播、电视、网络(媒介属性) 专业媒体、自媒体(生产方式) 党媒、市场化媒体、社会媒体(运营方式) 新闻生产方式的变迁 PGC UGC AAC（算法生产内容） 传统媒体的新闻生产理解新闻生产 政治经济学观点。新闻加工的结果与国家结构、经济结构以及新闻 机构自身的经济基础联系起来。 社会学。新闻记者在工作中的努力是如何受制于组织和行业的要求 的。 文化的观点。广泛的社会文化传统和符号系统对记者的约束力。 新闻生产的三个环节 发现具有潜在新闻价值的议题和事件 在已发现的事件和议题中进行筛选和选择，使之成为报纸和新闻节目的主要内容。(信息的重要程度和情感的吸引力) 报道新闻(客观平衡公正) 不同类型媒体的新闻生产 主流媒体/党媒 专业性媒体/市场化媒体 网络媒体(草根媒体、公民媒体) 智能媒体时代 个性化新闻推荐：用户画像 机器人写作 临场化新闻 VRAR 传感器新闻（物体生产内容） 分布式新闻（去中心化，协同式生产） 智能媒体新闻生产的伦理问题 今日头条的版权问题 Facebook的算法推荐遭遇质疑假新闻 无人机/谷歌地球对私人领域的窥探 6.3 新闻信源概念 信源：记者向之寻求信息的人，通常是与社会核心机构有联系的官员或专家。 普通信源 公共关系信源：一些组织机构为了达到宣传目的专门为媒体提供 消息或新闻稿 记者与消息来源的共生关系 记者需要消息来源为其提供信息和观点。 而消息来源则需要新闻业来树立良好形象或向社会推销观点和产品。 消息来源对新闻报道来说至关重要 新闻业如何处理与消息来源的关系决定了新闻报道的质量。 塔奇曼(2008)的“新闻网”理论指出记者都是通过一定的社会网络来获取新闻线索，并且在一定框架下报道新闻。 消息来源越丰富，记者的职业能力越强。记者通过增加更多的受访 者和引语作为相互支持的事实，新闻记者能够拉开自身与新闻事件 之间的距离，而让这些消息来源来说出记者想说的话。记者可以隐 藏自己的观点，通过选择性的呈现消息来源的观点来表达自己与新 闻机构的看法。 信源与话语权 信源提供有力于自身的新闻事实。 媒体偏向于权威的信源，尤其是权威的男性。 意识形态占主流的信源，呈现一种主导性的认知框架。 信源偏向于社会精英(政府、专家)，而忽视了草根 使用匿名信源的伦理规范 为什么信源不透露姓名身份? 1、匿名信源的使用必须是为报道事关公共利益的重大新闻事件。 2、在使用匿名信源时，必须尽量给出关于信源的背景信息，不允许使用“有消 息称”、或“要求匿名的受访者称”这样简单的说法，而必须加以进一步限定 和说明。 3、核实信源的可信度(是否与此事有利益关系) 4、告知至少一个编辑，知情编辑必须和记者一样对匿名信源真实身份完全保密:知情编辑不得将其透露给其他记者或未经授权的编辑。 杜绝滥用匿名信源(假新闻、人身攻击) 信息的甄别与核实 失实新闻(信源真实，新闻失实) 假新闻(信源失实或伪造信源) 策划性新闻(信源与报道者重合) 客观主义报道(信源的平衡) 总结 记者和信源之间的关系是精心协商的关系，每一方都希望能 实现他们的目标，维持他们的机构地位和社会地位。 “归根到底，新闻不是记者想什么，而是其信源说什么，而 且经由新闻机构、新闻流程和惯例为中介，摒弃了很多记者 个人化的偏爱”。 新闻业自身的独立性决定了消息来源是否能够操纵新闻报道， 这种独立性包括所有权、人事权、编辑权和财政的独立。 6.5 新闻与公关公共关系：一般指一个社会组织用传播手段使自己与 相关公众之间形成双向交流，使双方达到 相互了解和相互适应的管理活动。公关是关于消息来源的学科。 新闻学与公关学的关系 反对新闻学与公关相融合，坚持新闻专业主义 倡导新闻学与公关学融合，树立大学科的传播理念 公关为何能影响新闻业:信源的新闻把关功能 新闻与信源是一种“双重守门人之间的复杂的共生关系”，因此新 闻生产过程中的把关是消息来源和新闻媒体之间的互动的结果。 消息来源是“第一级守门”，媒体人员是“第二级守门”，而初步 完成的稿件给消息来源进行检查是“第三级守门”。 霍尔的首要定义人理论指出，从某种程度上消息来源已经成为新闻 事实的“第一定义者”。 把关人理论：卢因认为，在研究群体传播时，信息的流动是在一些含有“门区”的渠道里进行的，在这些渠道中，存在着一些把关人，只有符合群体规范或把关人价值标准的信息才能进入传播渠道 公关中的传媒场域 新闻报道是各种利益集团(消息来源)运用公关手段来影 响媒体，争夺权益的权力场域。这些利益集团中包括支配 性利益集团和挑战者利益集团。 占据既得利益的支配性利益集团。政府、企业。 资源相对贫乏甚至是处于边缘性地位的挑战者集团。农民 工、艾滋病患者 媒体关系 可控媒体 不可控媒体 关系评估 相互控制度、信任度、关系满意度、关系承诺度 公关与媒体的关系类型 对立模式 合作模式 同化模式 交换模式 公关控制新闻的策略 控制新闻渠道：包括评估记者，建立记者黑名单;对记者进行公关;对媒体进行收买或公关; 制造和策划新闻 信息补贴 公关“弱影响说”与“内部途径模式” 公关消息来源对新闻报道的影响不是直接的，而更多的依赖于公关 人员和记者的私人关系 如果记者能够在新闻把关中占主导地位，那就是所谓的传媒中心论， 那么信源就只是新闻业的附庸，而新闻专业主义就能成为新闻筛选的原则。 弱影响说强调了记者和新闻机构的主观能动性，认为它们能够成为 新闻把关者，能够使用一定的新闻框架去呈现新闻。 公关“强影响说”与“外部途径模式” 公关材料对于记者来说有着“绝对的重要性”。 新闻稿新闻:简单的依赖特定新闻稿进行报道。 如果记者在新闻把关中受到众多结构性因素的影响，只能身不由己 随波逐流，就是所谓的结构决定论。 强影响说强调了政治、经济等外在社会结构对新闻业的影响。信源 占优势地位。 假公关与假新闻 社会化媒体时代的公关 软文写作，企业商战，抹黑对手 以往有明确的公关对象，现 在需要面向整个社会 案例:3Q大战、蒙牛伊利 公关战 小结 公关人员与媒体人员之间的复杂共生关系 冲突论、非敌对论、消极度削弱理论 舞伴关系 双重守门人关系 Chapter8 新闻客观性【重点】重点: 什么是新闻客观性? 什么原因造成了新闻的不客观? 8.1 新闻把关(传播者塑造新闻事实)把关人理论 传播者不可避免地会站在自己的立场和视角上，对信息进行筛选 和过滤，这种对信息进行筛选和过滤的传播行为就叫做把关(即 守门)，凡有这种传播行为的人就叫做把关人(守门人)。 卢因：在群体传播过程中存在着一些把关人，只有符合群 体规范或把关人价值标准的信息内容才能进入传播的管道 怀特：新闻媒介的报道活 动不是“有闻必录”，而是对众多的新闻素材进行取舍选择和加 工的过程。在这个过程中，传播媒介形成一道关口，通过这个关 口传达给受众的新闻或信息只是少数。 怀特的“把关”模式的不足在于没有意识到把关是一种组织行为， 而认为它主要是新闻编辑基于个人主观判断的取舍选择活动，此 外这个模式没有说明新闻把关的标准 新闻把关 头版新闻的决定是一个社会过程，在这个社会过程中，他们既要满足社 会对于他们的期望，也要承担他们对所刊登新闻的记者们的责任。 编辑们要遵循一套共同的规范，要为报纸的多个赞助方负责任，常常涉 及到利益的权衡与妥协。 选题会和编辑会的功能。 从新闻生产的角度来说，新闻是一种社会过程的产物，在此过程中，媒 介人员自己决定什么有新闻价值，什么人物重要，什么视角应该包含在 报道之中。具有一定主观能动性。但新闻生产同样受到政治、经济和机 构惯例与行规的决定性影响，在相关框架内进行发挥。 影响把关的要素 新闻信息的客观属性 专业标准和市场标准（新闻价值和新闻要素） 媒介组织的立场和方针 把关人实质 大众媒介的新闻报道与信息传播并不具有纯粹的“客观中立性”， 而是根据传媒的立场、方针和价值标准而进行的取舍选择和加工活动。 新闻和信息的选择尽管受到媒体的经营目标、受众需求以及社会 文化等多种因素的制约，但是与媒介方针和利益一致或相符的内容更 容易优先入选、优先得到传播 媒介的“把关”是一个多环节、有组织的过程，其中虽有记者、 编辑个人的活动，但是“把关”的结果在总体上是传媒组织的立场和 方针的体现 商业化对新闻把关的影响 党媒的新闻把关原则 市场媒体的新闻把关原则(标题党现象) 新闻把关应该遵循怎样的原则? 新媒体时代的新闻把关 传统媒体的新闻把关 新媒体的新闻把关 专业人士在新媒体的新闻把关中起到什么作用? 8.2 影响新闻生产的诸多因素新闻场域 是各种要素关系的结构体系 是相对自主的社会空间 是具有策略性和竞争性倾向的系统(各种力量的博弈) 其边界是一些动态的界限 新闻场域的影响 新闻网络必要的组织规范，以及这些规定引领下的思维逻 辑，将无可避免地以一贯的方向形塑社会的图像 新闻是新闻机构的产物，而非世界上发生的事 新闻场域中的权力关系 媒介从业人员生产自己的产品，但是他们并不能独立生产新闻产品。他 们进行生产的环境并非自己所选，而是由历史所直接确立、给定的(马 克思)。 经济力量决定了目标和决策制定的环境，而参与者还必须要考虑到受众 和节目，以努力创造出“合适”的产品。 政府通过多种方式管理媒介。但是法律条文的通过是一回事，执行管理 是另外一回事。此外，国家的管理政策通常需要解读，这就使得媒介组 织可以用他们自己的方式来解读这些规则，以适应它们自己的需要。 政府垄断大量战略资源信息，同时也通过直接的审查制度来进行管理。 媒介组织通过自我管理和审查的办法来规避风险。 新闻生产中的各种影响因素 环境（文化，法律，政治，技术） 投资者 母公司 媒介企业 新闻部门 新闻来源 广告商 新闻消费者 传媒发展的障碍和影响因素 物理障碍(地理空间) 文化障碍(风俗、禁忌、价值观、特殊的行为方式、语言) 经济障碍(文化倾销) 政府障碍(恫吓、诽谤、税务稽查、许可证制、新闻审查、查封、 截发、限制出行、逮捕、死亡威胁、缺乏保护针对记者的暴力事件) 传媒障碍 (记者、编辑训练不足、缺乏客观性、贪婪、公信力丧失、 自我审查) 技术障碍 新闻场域中的政治权力 法律控制 控制消息来源 新闻检查制度 威胁恐吓 宣传纪律(以上为刚性权力)  新闻发言人制度 背景吹风会 新闻公关 感情投资(新闻口) 8.3 新闻框架（传播者与接受者共同塑造新闻真实）框架： 高夫曼：框架是指一种存在于人们头脑中的认知结构 或认知取向，人们通过这种结构，从一套框架转到另一套框架来建构 社会真实。他认为框架一方面是源自过去的经验, 另一方面受到社会文 化意识的影响 。 甘姆森进一步把框架分为两个层次，一类指界限,也就包含了取舍的意 思, 代表了取材的范围;另一类是架构——人们以此来解释外在世界。 传播者构建框架 新闻中两个最持久的价值是社会秩序和国家领导权(甘斯)  媒介内容反映制作者的意图。 媒介内容是受众喜好的体现。 媒介内容是社会的总体体现。 媒介内容对受众的影响。 媒介内容是一种自我解释的文本。 种族、阶级、性别在媒介内容中的体现。 受众的诠释性架构 我们感知世界的示意图，它依赖于人们的社会经验、 阅历以及个人将事件定性和分类的能力。人们为了给 时间赋予意义，而从价值体系、原则、象征符号和活 动的画面的总和出发，构建的一大堆词义和再现是这 种架构方式的基础。 新闻构建中的权力位置及其与消息源的关系决定了架 构方式。 媒介内容与认知框架 文化霸权和意识形态的塑造:社会中的统治阶级使他们的世界观被社 会的其他成员接受并当做普遍的观念。霸权的作用过程是隐蔽的，它 在常识的层次上运行，并且塑造常识。使得我们认为理所当然的东西 不必争论，也没有争论的空间。 霸权是关于世界的观念，是可以修正和反对的。统治者通过定义关于 社会的假定来维持其权力，寻求稳定与合理性，并且将潜在的反对力 量融入基本的意识形态，进行收编。 编码解码理论 霍尔认为媒介不是简单的反映了世界，而是重新表现了世界。 媒介定义了真实。它不仅是对已经存在的意义的传达，也是一 种积极劳动，是使事件有意义。 媒介重塑了基本事件和价值，而这些事件和价值构成了霸权的 基础。 8.4 新闻客观性（公正、平衡和真实）客观性 报纸必须独立于纷争与党派之外，它的任务是成为大众教育者以及公众 的论坛 保护天赋人权和公众利益的先发制人的声明，是新闻客观性结构的永久 基石 写出事实让观众得出自己的结论，这才是更重要的 内涵 诚实 超脱、平衡对待事物、公正 不抱成见 不牵扯个人利益 只相信事实，努力将价值观与事实分开 客观性的标准与形式 新闻客观性并不是指媒介是客观的，而是指那个外在的世界可以客 观地报道。 写作层面的客观性报道和职业层面的客观性规范。前者是一种报道 的呈现方式，后者是专业的理念、守则。 在电报发明之后，记者们需要把“最关键的实施浓缩成为简短的电 讯稿”，将事实的要素按照新闻价值从大到小的顺序进行排列，形 成了经典的新闻写作形式“倒金字塔”结构。 作为一种策略仪式:使用引号表示出特定概念;对观点追根溯源;以恰 当的顺序和格式构建信息;表达出双方或所有主要方在政治问题上的观 点，以及遵循通行的有关体面和良好品位的标准 从朴素的事实观到吸纳解释性报道:把直接报道当做客观性的实践是过 时的，是浅显、幼稚又狭隘的，是误导。解释性报道使客观性的形式更 全面、更深刻 作为体制的新闻客观性 真实性:把观点、解释和评论与事实明确分开，报道中药引用有名 有姓的资料来源，避免模棱两可和累赘冗长; 准确性:使报道忠于现实，或忠于其他可靠的对现实的说法，尤其 在具体的事实上; 完整性:叙述的完整，提供题解新闻所需最起码的相关信息。记者 应尽量讲出与报道相关的事实的真相。 记者应持一种超然、中立、公平和独立的态度，避免党派性、带个 人偏见、隐秘的动机和受外部利益的不恰当的影响。在再现一桩有 争议的事件的各方面时，客观的新闻描述是公正、无私和平衡的。 客观性的一般操作要求 将事实和意见分开 用中性词表述事实 为事实涉及的各方提供应答机会，给受众提供全面的信息 在写作方面:倒金字塔结构，5W1H(what when who where why how)  以第三人称来报道、强调可以查证的事实、不采取立场 至少表达新闻故事的两面 新闻不客观的四种类型 体现了新闻工作者的个人偏见 反映了媒介企业、其母公司或大型投资者的利益 反映了外部社会精英的自身利益(统治阶级、主流意 见) 人人都会犯的错误(随机错误) 影响新闻客观的因素 记者在报道事实之前，其认识问题的方式和思维习惯已经存在了 不少事实本身体现了一定的立场观点 记者对事实的感觉和知觉具有相对性 每个受众对客观的感知不一样 客观性的功能和缺点 客观性维护了记者的社会形象，增强了他们参与公共事务的合法性。 这是他们获得独立地位的根本所在。记者凭借有道德的、客观的运用 自身所拥有的特殊技能，建立自己职业工作者的地位。 客观性也限制了记者的主观能动性，限制了记者的政治权利与观点表 达。使得记者的脑力劳动发生了异化。 新闻业作为公众的代言人，它可以独立于政党，但不能独立于社会。 新闻业承载了公众的知情权需求，新闻媒体的权力与公众权利不矛盾。 Chapter9 新闻专业主义【不考】9.1 新闻专业主义 一种独立于任何权威之外的新闻从业 理念，带有一定的理想主义色彩和强烈反权威精神。它 要求记者以客观、真实、准确的态度去报道事实，挖掘 事实的真相，把事实的原生态展现在读者面前。它最突 出的特点，是相信可以从非党派、非团体的立场客观地 报道新闻事实。新闻专业主义的目标是服务于全体人民， 而不是某一利益团体。 社会公器、独立于利益集团 传媒具有社会公器的职能，新闻工作必须服务于 公众利益，而不仅仅限于服务政治或经济利益集 团。 新闻从业者是社会的观察者、事实的报道者，不 是某一利益集团的宣传员。 社会责任感与公共精神 西方新闻专业主义的核心原则 传媒具有社会公器的职能，新闻工作必须服务于公众利益，而不仅限于服 务政治或经济利益集团; 新闻从业者是社会观察者、事实的报道者，而不是某一利益集团的宣传员， 或政治、经济冲突的参与者或鼓动者; 新闻从业者是资讯流通的“把关人”，采纳的基本准则是中产阶级为主体 的主流社会价值观念; 以实证科学的理性标准评判事实的真假观念，服从于事实这一最高权威， 而不是臣服于任何政治权力或经济势力; 受制于建立在上述原则上的专业规范，接受专业组织的自律，而不接受在 此之外的任何权力或权威的控制。 新闻的十大基本原则(新闻专业主义的内涵) 新闻工作者首先要对真实负责 首先要忠于公民 实质是用核实进行约束 必须独立于报道对象 必须成为独立的权力监督者 新闻媒体必须成为公众评论和妥协的论坛和广场 必须让重大事件有趣而且与受众息息相关 应该使新闻全面均衡 有责任按良心行事 公民对新闻也享有权利和承担义务 新闻专业主义的特点 客观性 真实性 独立性 自由性 新闻专业主义的具体操作 价值判断:如何选择事实，选择哪些事实，尤 其是对头版头条的选择上。 新闻叙事:新闻专业主义主要体现在新闻报道 是如何呈现事实的。对弱势群体的报道、对官 员和政府的报道、新闻评论。 信息披露 舆论监督 新闻是个技术活 核心信源：核心信息被忽略与扭曲（媒介审判） 如何寻找核心信源:暗访、直接约访、资料的搜集、整合与分析 保护记者:保留核心的人证与物证应对诉讼 新闻专业主义的作用 一旦冠以“专业人士”的名称，美国新闻工作者社会 地位陡增，职业形象焕然一新，在人们的心目中成为 穿着体面、出入高层，进行社会信息鉴别的把关人、 对社会舆论具有判断作用的仲裁者。 按照新闻专业主义的理论，报业是一种自治的体系， 它必须对政府、对政党、对政客采取一种独立的和批 判的态度，否则便不可能保持它在公众中树立的“保 护者”形象，便不可能拥有公众的信任。而良好形象 和高度信任，是媒介赢得市场的重要资本之一。 新闻专业主义的本质 一种社会控制模式 包括伦理准则、专业奖励体系、专业教育和职业培训 一种营销手段 自我控制与自我信仰 一种意识形态 局限性 第一，记者界不愿指责违反专业标准的同行，职 业协会经常会形成封闭式的团体。 第二，专业标准可能是过度抽象和模糊，难以被 贯彻执行。 第三，政治控制和商业束缚 第四，媒介从业者对自己的工作拥有较少的独立控制权。 第五，违反专业标准很少有立即可见的后果。 9.2 新媒体环境下的新闻专业主义 液态新闻业——弥散的新闻 记者的新的角色定位，新媒体环境下对新闻专业主义带来了什么样的机遇和挑战? 自媒体的职业化 网络主播的职业化 今日头条还需要人工编辑吗? Chapter10 新闻自由与新闻体制【重点】10.1 媒介的四种理论 媒介的威权主义理论 媒介的自由之上理论 媒介的社会责任理论 媒介的苏联共产主义理论 西方新闻自由思想演变的基本脉络：集权主义——自由主义——社会责任 10.2 集权主义理论 背景：中世纪教会和政 府为了镇压自由思想的传播，乃规定了集权主义 的报业制度即集权主义(国家统治、极权主义) 理论 对个人的看法：个人的活动范围和能力有限 对国家的看法：国家是人类充分发展的基本条件 对人与国家关系的看法:个人必须依靠国家，才 能实现他的目的 对知识与真理的性质:知识来自最有智慧的人， 应该成为社会一切成员的绝对的准则 对待报刊的态度和作法： 报刊的政治底线:只要报刊不直接批评当前政 治领袖及其措施就可以。 往往让私营报刊和官方刊物并存。 对私营报刊和通讯工具进行有效的限制和控制。 特许制度，印刷品检查制度等等。 集权主义理论国家控制新闻传播的措施有: 出版特许制，征收知识税（向报刊征收），津贴制度 集权主义理论的思想溯源及表现形态：国家必须掌握在智者手中，国家权力不能分化（主张者：苏格拉底，柏拉图，亚里士多德，西塞罗，蒙田，尼采，纳粹，马基雅维利，霍布斯，黑格尔，墨索里尼和希特勒） 集权主义报刊理论哲学思想的主要观点: 个人离开社会便无意义，不完整，并毫无价值; 社会和国家是集体意志力表现的实体，社会和国家的价值至上; 知识的多寡，因人的聪明才智有所不同，而真理 则是绝对惟一的，真理常由心智高人一等、位居 统治阶级的人所有。 具体做法 政府拥有媒介，包括广播、电视、报纸、互联网、电影。开办 媒介的许可证制度，如刊号、牌照。 对媒介内容的审读和检查。媒体刊发的材料必须首先得到官方 的许可或审批。相关禁令的颁布，哪些内容可以报道，哪些内 容不能触及。 对媒介报道的事后追惩制度。对违反公论或者现行法律的行为 提起诉讼，禁止媒体对当权者或政府制度进行批判。 推行特种营业税制度，通过限制出版物的利润额来控制媒体。 10.3 自由主义理论渊源：古希腊 传媒的功能是参与公众教育，防止政府背离最初的目标， 政府必须经得起报纸的批评。 自由主义报刊理论形成于资产阶级反封建运 动过程中，18世纪末、19世纪初，西方各主要资 本主义国家基本上都以法律确认的形式，使自由 主义报刊成为一种制度，成为资本主义制度的一 个组成部分。 基本的理论假设 1、对人的看法:人是理智的动物，人本身就是目的。 2、对国家的看法:国家虽然是必要的，但是本身 不是目的，国家并不比个人更重要。 3、对知识和真理的看法:观点的自由市场理论; 自我修正过程。 自由主义报刊理论的基本原则:（该理论的主旨是为了确立、维护和发展新闻自由 ） 报刊不受政府的干涉(与政府关系成为核心问题) 报刊拥有对政府的监督权 “意见的自由市场”和“自我修正”理论 对事实的信念 阐释 言论出版自由是人的天赋权利的一部分，人类有足够的理性 分辨事物的正误善恶，政府不能剥夺人们接近和了解他人观 点的权利。 政府应该给公民享受不受新闻审查的自由，给予这些社会成 员自由思考和行动的权利。正确和错误的观点都应该有表达 的机会 传媒的根本目标是通过提供各种事实和观点作为判断的基础， 来揭露真相，协助解决政治和社会问题。 传媒要想在民主社会中发挥正确的功能，必须不 受政府控制; 相反，传媒拥有对政府的监督权，要防止政府超 越自己的权力边界。 在“观点的自由市场”中，各种观点自由竞争， 最后在不同的观点的“自我修正过程”中产生出 真理。 新闻自由是公民实现其他自由权利的基石。 新闻自由本身不是目的，而是实现自由社会这一目的的手段”。(菲利克斯. 法兰克福) “最好的政府及最好的法律能够为大多数人创造 最大的幸福，而新闻自由和公开讨论的自由式有 利于建设善的政府的。” 自由主义报刊理论的哲学基础 理性原则 自然法则（古典经济学） 权力制衡 自由主义报刊理论的现实困惑: 政党报刊的黑暗时期、大众化报纸诞生后黄色泛滥 资本取代行政(政府)控制了报刊(核心) 煽情新闻泛滥 自由竟争被垄断取代，垄断扼杀意见自由市场 垄断对社会的危害;谁来监督媒介? 10.4 社会责任理论基本的理论假设 1、自由是伴随着义务的，所以享有我们政府特权地位 的报刊，就对社会承担当代社会的某种主要职能。 2、对人性的看法。人并不天生地希望寻求真理和服从 真理，人虽然能够运用他的理性，但是人们厌倦这 样作。因此，“观点的自由市场”理论和“自我纠 正法则”失去了前提条件。 对报刊与政府关系看法 如果报刊不足以保证社会 从媒介获取它所需要的益处，政府就应当出面。 例如，出台法律禁止报刊的造谣诽谤。 但是，政府只有在特别需要的时候才予以干 涉，而且还要谨慎从事。 社会责任论的积极影响: 为公众评价西方的新闻媒介建立了一个价值体系，成 为人们对大众传播媒介进行批评派的武器，对传媒造 成巨大社会舆论压力。 西方各国新闻媒介先后都以社会责任论建构新闻道 德自律。 由于自律以及来自各方面的压力，使新闻媒介煽情新 闻在一定程度上得到遏止。 在一定程度上影响了司法机构的判案标准。(注意保 护公民的隐私权等权利) 成为新闻从业人员培训和新闻教育的重要内容。 社会责任论受到批评的几个方面: 主张新闻事业应承担的社会责任没有必要的条件， 容易为一些政府和团体干预与侵犯新闻自由制造 口实 主张政府直接创办新闻传媒以平衡私人传媒，可 能影响报道的公正和意见公开，危害市场化经营。 没有消除传媒商业化的弊病 社会责任论的内在矛盾困惑: 向人的道德、良心呼吁，实际上是对人的理性呼 吁，与对理性的怀疑自相矛盾。 向政府发出呼吁，要求政府有限制地管束新闻媒 介，与保护新闻自由的基本原则违背。 强调了集体目标，与资本主义中心——个人主义 矛盾。 在对“责任”、“对谁负责”、“怎样负责”等 的界定上争议很大 10.4 传媒的苏联共产主义理论马克思主义理论基础 1、社会变革的辩证法。革命是统治权力的更迭而没有任何深刻的社会变革。 2、物质决定论。任何社会占统治地位的观念和 制度必然是经济上占统治地位的阶级所持有的观 念和态度。 3、共产主义的目标是无阶级无国家的社会。国 家是阶级统治的工具，必将消亡。 哲学基础 唯物主义辩证法(动力) 政治经济学的决定论(动因) 科学社会主义(目标) 该理论对媒介的看法 媒介只是一种工具：打字机；扩音器；喉舌 媒介的最高目的是帮助实现国家的统一 媒介的职责:鼓动者、宣传者、组织者 与报刊的典型集权主义的比较 苏联的报刊制度是一个集权主义的制度，而且是历史上受最严密控制的制度之一。 苏联的集权主义与典型的集权主义又有许多区别。 与报刊的自由主义、社会责任理论的比较 区别一:所依据的哲学不同。苏联制度依据的是马克思主义哲学，后者依据的是启蒙运动的唯理主义、自然权利哲学。 区别二:对于人的概念不同。苏联共产主义认为人是可以训练的，人本身并 不重要，人要服从普罗米修斯式的领导人。后者 认为人是有理解力的，有辨别力的。 区别三:对于国家的概念不同。苏联共产主义认为国家是个人的“照管者”，后者认为管辖的最少的政府是最好的政府。 区别四:关于真理的看法不同。苏联共产主义认为真理是“给定”的，后 者认为真理是通过论辩获得的。 区别五:对于控制的看法不同。苏联共产主义认为应该通过所有权、党籍、指示、 检查、审查、批评和强迫等方法加强控制，后者 主张最少程度的控制。 新闻理论的变化 1、苏联解体之后，社会责任理论被归于自由主义理论之中，形成社会—自由 至上主义与社会—集权主义理论。 2、发展理论的提出。大众传播的所有手段，必须由中央政府统一调动，在国 家建设的重大任务中发挥作用:扫盲、扶贫、培养政治意识，以及为经济发 展提供帮助等等。 3、民主-参与理论 4、民主社会主义理论:媒体的公共所有制和社会所有制，而不是国有制和私人所有 5、三个乐章理论:市场经济世界、马克思主义经济模式、进步中世界(第三 世界国家) 三种主要的传媒理论 市场经济国家:自由主义的 （媒介不受外界干涉，媒介为公众的知情 权服务，媒介报道要公正客观） 共产主义国家:共产主义的 （媒介改造和培养人们的阶级和文化意识， 媒介满足人们的目标需要，媒介有目标地报道经历的现 实） 发展中国家:发展主义的（媒介是一个统一的而非多元的力量，媒介 是有益于社会变革的工具，媒介旨在作用于国家与社会 之间双向交流的工具） 新闻目的 市场:追求真理、对社会负责、不进行政治或文化宣传、 不偏不倚的服务于大众、支持资本主义教义、履行监督 政府的职责 共产:寻找真理、对社会负责、教育公众，组成政治和 文化同盟、要求以支持正确教义的方式服务于公众、塑 造观念和行为 发展:提供真理、对社会负责、进行文化、政治教育、 与政府合作，追求有益目标的改革，以此服务于公众， 维护和平(和谐)的工具 对媒介自由的看法 市场:媒介自由意味着记者不受任何外界的控制，自由的媒 介不是权力的奴仆，也不受权力的操纵 共产:媒介自由意味着所有人的意见都可以发表，而并非只 是发表富有和权力阶层的意见，要求自由媒介去反抗合法社 会的压迫，要求国家制定新闻政策以保证自由媒介采取正确 的形式 发展:媒介自由意味着记者良心的自由，国家的生存比媒介 的自由更重要，需国家制定新闻政策为媒介自由提供法律保 障 小结 在所有的报业体系中，新闻媒介都是那些实施政治、经济权力者的代言人。因 此，报纸、广播、杂志、电视并非独立的行为者，尽管它们存在着实践独立权 力的潜力。 新闻媒介的内容总是反映投资者的兴趣。 所有的报业体系都赞成社会责任的教义，宣称自己可以满足人民的需要和兴趣，并声明它们愿意提供让人民参与的渠道。 三种报业模式中的每一种模式都认为其他模式是异端。 10.5 新闻自由在中国的引入和演变自由的实质是人的活动与外界关系的问题。 新闻自由 分类：传播自由，收受自由 实现：新闻自由的实现需要一定的社会条件(实现的程 度和范围与社会提供的条件相适应) 123456经济基础时实现新闻自由的物质保证 民主政治是实现新闻自由的政治前提 法制社会是新闻自由实现的基本保障 文化发展是实现新闻自由的精神支撑 技术进步是新闻自由实现的有力杠杆 公民的主体素质与新闻自由的实现密切相关 新闻自由实现的标志 最高、最基本标志:新闻活动者是自主的活 动者，在优良的法律、道德规范之外，不受任何力量的限制。 两个指标: 新闻传媒要有相对的独立性(经济、政治) 新闻传媒要成为“社会公器” 10.6 资产阶级新闻自由资产阶级新闻自由的实质：建立在私有制经济基础上，主要是私人所有；受到垄断资产阶级的全面控制(本质上是金钱和资本的自由)；是有限度的自由而非绝对自由 无产阶级对新闻自由的理解 新闻自由建立在剥夺敌对阶级新闻自由的基础 上 有领导的自由而非无政府的自由，是民主和集 中的统一，自由和纪律的统一 Chapter11 中国新闻体制改革【重点】当下新媒体面临的一切制度问题:  网络媒体记者证 严格管制互联网电视 网络实名制 网评员制度 净网运动 互联网群组管理条例 都无法脱离既有的新闻体制 新媒体在一定程度上有抗争空间，但莫不在新闻体制的框架下操作 重点 了解我国的新闻体制 了解我国传媒体制改革的过程 11.1 中国的新闻体制新闻事业体制的含义 新闻体制：国家(社会或政党)管理新闻事 业的新闻制度、新闻事业构成形式和规范模式，它包 括新闻单位行政隶属关系、所有制形式、内部结构、 组织体系和干部制度等等” 包括宏观和微观两个层面: 宏观层面指新闻单位所有制形式、国家(或社会)新闻制度、新闻法规、政党新闻政策等，从宏观上调节新闻单位与政党、政府、社会的关系; 微观层面指新闻单位的内部结构、组织人事制度、分配制度、经营管理等多方面的内容。 新闻事业体制是新闻制度的一个核心组成部分 制度较多地被定义为一种社会约束或规则，包括正式约束和非正式约束 非正式约束主要包括价值信念、伦理规范、道德观念、 风俗习性、意识形态等因素。在非正式约束中，意识形 态处于核心地位。 正式约束是指人们有意识创造的一系列政策法则。包括 政治规则、经济规则和契约，从宪法到成文法和不成文 法，到特殊的细则，最后到个别契约。 新闻事业体制指的是正式约束 制度的最核心作用是增强人们的预期，使复杂的 人际交往过程变得更易理解和更可预见， 从而 有效地对社会进行调控 制度应当具备普适性、稳定性和开放性的特征 新中国传媒制度的渊源 1.马克思主义经典作家的新闻思想 ： 耳目和喉舌论(马克思、列宁) 党报的地位与使命(马、恩、列) 新闻事业的党性原则(列宁) 2.抗战根据地和解放区的新闻实践 3.苏联的新闻体制模式(高度封闭的集权体系) 中国新闻事业体制变革的历史轨迹 党报体制阶段 新闻专制体制阶段：党报体制的异化(否定党 对新闻事业的领导;阶级斗争工具;人治;新闻事业规模萎缩) 转型时期：“事业单位，企业化管理”(多种报纸并存;运作方式多样——计划型、计划+市场型;市场型;集团化发展;内部领导体制改革) 新闻媒体的管理模式 董事会管理模式(私营媒体) 社会化管理模式(公共媒体、另类媒体) 行政化管理模式(政党和国营媒体) 我国的党管媒体原则 党报体制的正式规则 传媒的政治地位（政府部门） 传媒的所有制形式（国有） 传媒的财经制度(统收统支，不需要进行成 本核算，也不需要上交利润和税金) 党报体制的实施系统 1、领导关系(党对新闻事业的绝对领导) 2、组织结构(中宣部是最高领导机关、国家新 闻出版广播电视电影总局) 3、传媒布局(按照行政区划，科层制、条块分 割) 当今中国新闻事业体制的主要特点 新闻事业结构特点:条块分离(分割) 坚持社会主义生产资料公有制 党管新闻——方向、干部、业务和结构管理 坚持社会主义方向和为社会主义服务、为人民服务 的基本方针，当党和人民的喉舌 事业单位，企业化管理 11.2 中国传媒制度改革关键词：党管媒体，传媒产业化，事业单位，企业化管理，制播分离，广电集团化，三网融合，媒介融合 建国以来对我国新闻媒介属性认识的变化 第一阶段：上层建筑（单一属性） 第二阶段：上层建筑/信息产业(双重属性) 123456781978年 新闻事业的重新定位，事业单位企业化管理，双轨制启动在媒介的管理形式上，表现为“事业单位，企业化管理”或“事业性质，企业化经营”。1979年新广告元年传媒财经制度的改革，对媒体实行财政“断奶”，恢复媒体的广告功能和经营功能。1984年 新闻立法新闻传播法制建设的展开，《新闻法》仍然空缺 第三阶段：产业化与集团化 1234561992年 将新闻出版列为第三产业，划出党政机关行列1996年至今 集团化运作，资本运营，上市融资、合作或合资合营媒体产业化经营开始，集团化建设，媒介进入资本市场。传媒所有制形式新探索。股份制经营、委托制经营、合伙制经营、合作制 经营。1999年 网台分离，电视与广播、有线和无线合并，停止四级办台。1999年 全国报刊的行政职能和出版职能分开 第四阶段：2009至今（市场经济的全面渗透） 社交媒体的崛起 纸媒的衰落 媒体生态环境剧变 无以复加的市场化 集团化运作的影响 成立传媒集团能够打破“条块分割”的现状么? 集团化之后对“事业单位，企业化管理”的布局 有何影响? 文化体制改革 2001年 入世 2003年，文化体制改革的正式启动。 广电系统三分开与三分离。管办分开、政事分开、政企分开。 广电集团内部部分频率与部分资产实施事业产业分离、所有权与经营权分 离、制播分离。 2010年，三网融合 2011年，事业单位转型 2014年 媒介融合 媒体的分化 主流媒体 vs 市场化媒体 传统媒体 vs 新媒体 三种逻辑的博弈 新闻逻辑 宣传逻辑 商业逻辑 当前我国新闻体制改革的敏感点和难点: 如何清晰地界定和有效地贯彻“党管 媒体”原则 Chapter12 数据新闻和精确新闻【不考】12.1 精确新闻精确报道方法 确定报道选题 进行社会调查 统计分析数据 新闻写作和发布 原则 第一，要尽量做到价值中立，这是保证研究的科学性的前 提。 第二，记者应以定量研究专业知识为基础，严格遵循定量研究的操作规范，最大程度地减小误差。 所以，采写精确新闻的记者，需要记者当务之急是通过对定量研究的系统学习，掌握定量研究的专业知识，并在实践中小心谨慎，从而保证新闻数据的客观精确。 一般来说主要是解读科研与调查机构已经发布的调查报告， 进行二手数据分析。 最小抽取样本与多源求证结合。 特点 貌似客观、精确、真实，在新闻报道中大量运用 相比普通的报道方式能较普遍的反映民意 报道更有深度和说服力 费时费力，需要在社会调查的基础上进行解读 并不适宜报道所有新闻类型 可能导致读者感觉枯燥难懂 精确新闻可能出现的问题 非概论抽样得出调查结果(量化设计不够科学) 正确数据得出错误的结论 商业利益驱动下的软性广告 12.2 数据新闻定义:基于数据科学的知识和技术，通过或结合数据分析 对新闻时事所做的报道。 数据新闻是数据科学与新闻学的结合，其底线应同时遵循数据科学伦理和新闻伦理。 数据分析可以为我们呈现“故事的轮廓”(Sarah Cohen 语)，或提供“新的视角” (David McCandless语)。 制作数据新闻的步骤 确定选题 发现和获取数据 整理和清洗数据 分析数据(发现规律) 可视化呈现(选择合适的图形、丰富图形的内涵、用代码 呈现图形) 数据新闻是未来 数据驱动公关 12.3 信息可视化信息图中一般含有哪些信息 统计数据、过程 想法:概念、理论、意识形态 年表:历史、事件顺序、时间线、计划表 地理:方位、地区性指标 分解:材料、成分、列表、结构 层级:组织结构、需求评估 关系:外部、内部、人员、产品/服务 个性化:品牌的人性化、组织文化 补充议程设置： 该理论认为大众传播往往不能决定人们对某一事件或意见的具体看法，但可以通过提供给信息和安排相关的议题来有效地左右人们关注哪些事实和意见及他们谈论的先后顺序。大众传播可能无法决定人们怎么想，却可以影响人们想什么。]]></content>
      <categories>
        <category>Communication Science</category>
        <category>Medium Study</category>
      </categories>
      <tags>
        <tag>Medium Study</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[广播电视概论知识点总结]]></title>
    <url>%2F2018%2F01%2F14%2F%E5%B9%BF%E6%92%AD%E7%94%B5%E8%A7%86%E6%A6%82%E8%AE%BA%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[Abstract: 广播电视概论的课程笔记。主要从广播电视本体、传受主体和产制实务等方面了解了广播电视的传播特性与规律、广播电视节目类型与产制流程、广播电视传受关系、广播电视管理体制与职业要求等。 00 导论数字时代广电新发展 唱衰和看好：互联网、数字媒体的冲击； 仍是最大众化的媒体：仍是首要且重要的信息来源； 变革寻求新图景：融媒体；国家政策；三微(微博，微视频，微信)一端； 在媒介“互联 +”到“智能+”浪潮的推动下， 播电视加快互 联互通、融和 体化的步伐，凭借视听媒介先天优势和传统优势深厚积淀，在资讯视频化、视频资讯化的 再次乘 破 浪。 视频发展：发挥传统广播电视的先天优势；可视化运用：不断开掘交互性、故事性的可视化手段 电视成为数字移动及多媒体终端; 传播 式及互联 化、传播内容互动化、 用户体验智能化 第1章：广播电视的性质与作用1.2 广播电视的物质属性一、广播电视的物质基础❖ 广播电视是运用现代电子技术传播声像符号信息的大众媒介。❖ 广播电视传播技术及其媒介物是广播电视传 播的物质基础。 二、广播电视的物质属性 物质属性，即由广播电视物质基础决定的性质。有学者也称之为自然属性。 广播电视的物质属性是即时形象性。 即时性是广播电视传播技术发展着的时效特性。 形象性是视听符号构筑的整体形象性。 即时形象性是广播电视信息传播时效性与形象感的统一。 三、广电的比较优势广播电视的物质属性决定其传播优劣势。❖ 传播时效:及时性、实时性❖ 传播的空间:无远弗届，“在场”传播 在场传播，是一种社会性传递信息的行为，是个人之间、集体之间以及个人与集体之间交换、传递新闻、事实、意见的信息过程。广电在场传播：传播者调用多种采制手段和传播符号营造一种时空同步的物理场，尽可能给出有声有色的信息，让受众在丰富的信息场中获得身置其中的心理感受 ❖ 传播符号:视听兼俱，整体性形象❖ 传播接收:家庭接收，个人接收;“随时、随 地、随身 ” 四、传播劣势线性媒介，传播的易逝性。❖ 视听符号传播带来的信息的浅表性。❖ 接收的随意性❖ (数字技术和全媒体运用，使得传统广播电 视可以扬长补短) 1.2 广播电视的社会属性一、广播电视的阶级性❖ 广播电视机构和广播电视事业具有阶级性， 直接或间接地为一定的政党、阶级、或利益 集团服务。❖ 在我国，广播电视和其他媒体都具有党性， 坚持党管媒体，坚持媒体的“喉舌”性质。 二、广播电视的大众性❖ 广播电视受众面广❖ 传播内容老少咸宜，雅俗共赏 ❖ 传播方式贴近生活，融入生活 1.3 广播电视的作用 一、信息传播❖ 传播信息是广播电视的基本职能，受众接触广播电视的 主要目的是获取信息。❖ “新闻立台”，新闻节目是广播电视的主干。(除专业频道 外)❖ 二、环境监测❖ 三、文化传承❖ 四、怡情悦性 五、广播电视的负面影响❖ 用辩证眼光看待广播电视的正面作用与负面影响。❖ 信息过载造成的“信息焦虑”❖ 过度形象化、在场性传播造成媒介奇观，纵 容“窥视欲” 简单理解文化传承功能，迎合低俗文化❖ 煽情主义泛滥，满足于情感的渲泄，而少了 理性的与行动的参与。 第2章 广播电视的发展2.1 基本概念❖ 一、广播的广义与狭义❖ 1、广义的广播，包括声音广播和电视广播。❖ 2、狭义的广播，仅指声音广播 1、广播的划分:无线广播/有线广播;调频广 播/调幅广播;模拟广播/数字广播;地面广播/ 卫星广播;等等。❖ 2、电视的划分:无线电视/有线电视;甚高频 (VHF)/超高频(UHF);模拟电视/数字电 视;地面电视/卫星电视;等等 传输网络和制式：1、 传输网络: 包括发射网、传送网和接收装置传送网主要有微波中继传送、导线(光缆、电缆)传 送、卫星传送2、 制式 广播的制式:调频和调幅 电视制式:NTSC、PAL、SECAM 2.2 广电技术发展一、广播的发展❖ 经历三个时期:❖ 1、初创时期:19世纪后期到20世纪初期。电磁波的理论 及实验;无线电波传送声音的实验;首座广播电台。❖ 2、发展时期:二战前后到上个世纪50年代末、60年代 初。❖ 3、繁荣时期:上个世纪60年代末直到现在。技术不断发 展，出现立体声广播、数字广播等;出现窄播趋势。 二、电视的发展1、 电视的诞生: 保罗.尼普库的机械扫描法(盘) 被称作“电视之父”的贝尔德 首座电视台:BBC 2、 电视发展的主要阶段: 黑白电视阶段彩色电视阶段:1954年，美国正式播出彩色电视节 目。卫星电视和有线电视阶段:1962年，美国首次利用 “电星一号”通讯卫星传播电视节目，至此，开启了电 视太空传播时代。 3、 电视发展的趋势:❖ 高清晰度电视:1989年，日本成为世界上第一个每天播出 高清晰度电视节目的国家。❖ 立体电视❖ 文字电视❖ 交互式电视❖ 网络电视❖ 手机电视 2.3 广播电视事业的发展❖ 一、美、英、日等国的广播电视事业❖ 1、美国广播电视事业❖ KDKA电台，1920年❖ “美国之音”，1942年❖ 四大广播网❖ NBC，1926年❖ CBS，1927年❖ ABC，1943年 ❖福克斯广播网 CNN的发展:❖ 1980年创办❖ 新闻报道的理念:独家新闻;同步报道;题 材广泛❖ 《新闻教室》❖ MSNBC 2、英国广播电视事业❖ (1)广播业❖ 英国广播系统由BBC广播网(5个全国广播电 台)、38个地方广播电台和200多个独立地方 广播电台、3个独立全国广播电台组成。英国 的国际广播由BBC经营，设有BBC世界广播 电台 (2)电视业❖ 公共电视:BBC1、BBC2，C4(被称作新型公共广播模式)❖ 商营电视ITV，C5❖ BSkyB ，1991年成立 ❖ Ofcom，电信监管局 日本 ❖ (1)NHK，1925年3月电台广播;1953年2 月电视广播。公营体制。❖ (2)日本民间广播协会联盟:包括153家民 营商业广播公司。私营体制。 二、我国广播电视事业❖ (一)我国大陆广播事业 ❖ 1、 旧中国的广播事业❖ 境内第一家电台:1923年，奥斯邦在上海开 办的“大陆报——中国无线电公司广播电台” 国人自办的第一家电台:1926年，哈尔滨无线广播 电台❖ 国人自办的第一家私营电台:1927年，上海新新公 司广播电台❖ 国人自办的第一家公营电台:交通部天津广播无线电 台❖ 国民党政府的电台:中央广播电台，1928年 新中国的广播事业 中国共产党领导的第一座广播电台:延安新 华广播电台，1940年12月30日。这一日成为 中国人民广播创建纪念日。❖ 新中国成立后，1949年12月5日，将北京新华 广播电台定名为中央人民广播电台❖ 数字广播:1996年开始数字化广播进程。 大陆电视事业❖ 1、发端:1958年5月1日试播，同年9月2日正式开 播，北京电视台。❖ 2、起步:1973年10月1日正式播出彩色电视节目。❖ 3、发展:改革开放后，1978年5月1日，北京电视 台改名为中央电视台;1986年，创建中国教育电视 台。1985年，我国卫星电视事业开始。❖ 现有中央、省级副省级卫视频道55个。 （三)港澳台的广播电视事业 (1)香港的广播电视事业 广播:第一个电台开办于1928年，“香港广播电台”。目前主要的电台有:香港电台、商业电台和新城电 台 (2) 电视:起步于1957年。目前主要的电视机构:亚视(ATV)、无线或港视(TVB)、卫星电视、 香港有线电视台、华娱(CETV)、 、传讯电视网、 凤凰卫视中文台(5个频道)互动电视:1998年3月正式推出全球首创的互动电 视。 2、澳门:❖ 广播电台，1933年，到1980年才正式命名:“澳门广播电台”❖ 澳门电视台，1984年❖ 澳门卫星电视台、澳门有线电视台 3、台湾的广播电视事业❖ 主要的广播机构:“中央广播电台”、公营电 台、民营电台、军用电台 电视业:❖ 第一座电视台:国立教育电视广播电台于1963年12月正式播出。❖ 主要的电视机构:台视(TTV)、中视 (CTV)、华视(CTS)、全民电视公司 (FTV)、公共电视台(PTS) 第3章 广播电视的传播符号第一节 广播电视符号的划分一、符号的含义 1、定义:信息的载体2、构成:能指+所指 3、表意:意指过程。它是一个双向的 动态的传播、分享经验与文化的过 程。 第二节 广播电视符号的构成❖ 1、广播符号:有声语言、音乐、音响❖ 2、电视符号:画面、声音、屏幕文 字、其他符号形式(如:图画、动画、 漫画、地图、图表、电子特技画面等) ❖ 一、声音的表意 ❖ 1、有声语言:❖ 包括解说词、现场声音语言、同期声。 ❖ 表意特点:❖ 准确、不受现场时空的限制❖ 交待基本事实❖ 补充画面不足与不能❖ 联结画面 ❖ 2、音乐:表情功能、联结作用❖ 3、音响:传达空间特征、渲染情绪、 塑造声音形象(与音乐一道) 二、画面的表意❖ 1、影响画面表意的因素: ❖ 构图❖ 景别:全、中、近、特❖ 角度与高度:平、仰、俯/正、侧、背❖ 运动方式:推、拉、摇、移、跟等 2.画面表意的特点：❖ 形象性:具体的人、景、物❖ 纪实性:只能记录镜头前的人物、事件❖ 运动性:镜头内景物的运动;镜头的运 动等❖ 多义性:含义模糊或多重意义 三、文字的表意❖ 1、文字的运用(画内文字、后期加工上的文字)❖ 2、表意特点:❖ 准确，清楚，可以由表及里，跨越时空。介绍、解 释、补充❖ 争抢时效(单独使用)❖ 结构作用 第三节 广播电视符号的整合传播一、树立符号系统观❖ 声画符号构成有机系统。❖ 不同类型的符号相互匹配，扬长避短。❖ 音乐、音响、画面是非语言符号，擅长表达情 感、塑造形象。❖ 有声语言、文字则是语言符号，可以表达抽象 的思想、情感。 二、追求多层次的传播效果❖ 不同符号组合获得多层次的含义。❖ 直接意指，主要运用声画合一获得。❖ 声画合一:声音和画面同时指向一个具体的形象的组 合形式。分为画内声画合一和画外声画合一。❖ 具有声画同步出现，视听统一，高保真特点。❖ 间接意指，主要声画对位获得。❖ 声画对位:❖ 指声音和画面围绕同一个内容中心，在各自独 立表现的基础上，又有机地结合起来的表现形 式。❖ 声音与画面具有一定的“信息差”，调动观众的参 与。 三、掌握视听语言的语法与技法 ❖ 声音语言:❖ 物理性:音调、音量、音色❖ 心理性:基于声音物理特性引发人们感知的 心理因素❖ 表情性:基于声音物理特性引发的情感色彩 ❖ 画面语言:构图、景别、角度或高度，镜头 运动方式，镜头的组接方式等。❖ 长镜头:以写实性见长，多用于新闻报道或 纪录片摄制;❖ 蒙太奇组接:能够创造时空和节奏，多用于 表现性风格的题材创作。 第4章 广播电视的节目构成第一节 广播电视节目的划分1、节目定义 具有一定内涵，有相应名称，在一定时间和空间 条件下传播的，由声音和图像等要素组成的电子 符号系统。 (这是从符号要素界定其系统，也可以从体裁或类 型方面界定节目。) 2.节目类型【四分法】1)新闻性节目:《新闻联播》、《焦点访 谈》、《纪事》等(2)社教性节目:《百家讲坛》、《探索·发现》等(3)服务性节目:《生活》、《为您服务》、《天气 预报》等(4)综艺性节目:《非常6+1》、《欢乐总动员》、 《春节晚会》等 第二节 广播电视新闻性节目1、广播电视新闻性节目要素 体裁:即样式。2、广播新闻体裁(1)体裁的划分 按是否运用音响的标准，分为又播体裁和录 音体裁。 广播新闻体裁 口播体裁指单纯运用有声语言来表现内容的新闻体裁，包括广播消息、广播通讯、广播特写、广播评论等。 录音体裁指运用的有声语言之外，还要运用实况音响素材来表现内容的广播新闻，包括录音报道、广播讲话等。 (2)广播新闻主要体裁例举 广播新闻(口播新闻) 录音报道:指采制新闻现场音响对新闻事实进行简明扼要的报道。 现场报道:记者在新闻事件的现场进行解释、访问和录音的新闻广播形式。具有迅速及时、现场感强的特点。 实况广播(现场直播):与新闻事件发生、发展以至结束 同步直接播出的新闻报道形式。具有传播时间上的同步性、 报道事件的相对完整性。 3、电视新闻体裁❖ (1)体裁的划分❖ 消息类:又播新闻、 图片新闻、图像新闻、字幕新 闻、现场报道、访谈新闻、连续报道与系列报道❖ 专题类:专题新闻与专题报道、调查性报道、电视专 访❖ 言论类:电视评论、电视论坛、编前编后、即兴点评， 等等。 (2)体裁例举❖ 电视现场报道。它是最能发挥电视传播优势的报道方式。❖ 电视新闻调查就某件新闻事件、某个重大社会问题、 社会现象作深入调查的节目形式。《新闻调查》❖ 电视专访:《高端访问》❖ 电视评论:《焦点访谈》 4、广播电视新闻性节目的发展1、早期新闻性节目:信息量小、时效性弱、形式单一 北京电视台1960年节目表显示:图片报道(18:55，5分钟)、电视新 闻/国际新闻(19:30，10分钟)、简明新闻(21，40;5分钟)中央电视台1984年节目表显示:午间新闻(17:00，5分钟，同周一至 六)、午间新闻(18:00，周日，15分钟)、新闻联播(19:00， 30分钟)、观察与思考/电视论坛(周一，两周一次;20:00，15分 钟)、专题报道(周三、四，20:10，15分钟) 央视1993年第一套节目表显示:上午:新闻(7 点、8点、10点，各20分钟)、午间新闻(12 点，25分钟);下午:14点、16点、18点新 闻5分钟或10分钟;晚上:19点新闻联播，21 点新闻10分钟、22点新闻30分钟、零点新闻 2.新闻性节目的发展：信息量大，时效性强，形式丰富 ❖ 央视上个世纪90年代开始的一系列新闻改革:新闻 滚动播出;新闻直播年;名牌新闻栏目;加大深度 报道和组合式报道;新闻频道;参与国际新闻竞争; 等等❖ 东方卫视的“新闻立台”策略❖ 栏目化、杂志化❖ 播新闻、说新闻、演新闻❖ 民生新闻、公共新闻 第三节 广播电视教育性节目❖ 1、特性受教育者的广泛性施教育者的权威性教育内容的多样性教学方式的形象性 2、作用提高思想理论水平普及科技文化知识补充正规学校教育 类型❖ 社会性教育节目❖ 知识性教育节目、对象性教育节目、竞赛性教 育节目❖ 教学性教育节目❖ 综合教学、专业教育、应用 第四节 广播电视服务性节目❖ 1、服务性节目的定义❖ 指那些实用性强、作通信息、作咨询、当 参谋、反映群众呼声等方式，直接为社会 各界解决各种实际问题，为受众排忧解难， 对受众的心理和生活需求产生直接影响作 用的广播电视节目。 2、特性:实用性、能直接产生影响作用. 3、类型:❖ 独立形态的服务性节目: 单项性服务节目，如《天气预报》 综合性服务节目，如《为您服务》❖ 非独立形态的服务性节目:指其他类型节目中含有的 服务性要素 第五节 广播电视文艺性节目❖ 1、文艺性节目的定义❖ 或是以音像形式传播其他文艺样式的作品、 或是以音像手段塑造艺术形象来反映社会生 活的广播电视节目。❖ 2、特性:兼融性、渗透性、综合性、连续 性 2、类型:❖ 直播节目类型:❖ 音乐、戏曲、舞蹈、文学、电视小品、文艺专栏、综合性文艺节目❖ 加工节目类型:❖ 电影录音剪辑、MTV、LTV 六、广播电视节目的创新❖ 立足媒介特性，张扬形象传播优势❖ 借助新媒介技术，追求更深度的参与互动❖ 面对竞争压力，求得个性化生存❖ 适应受众需求，打造多元化的节目类型 第5章 广播电视节目的采制：广播电视采制的含义、新闻类节目体裁新闻类节目体裁新闻体裁的分类，历来不统一。尤其因电子传播手段广泛运用于新闻报道，因新闻业务改革的深入和记者创新意识的加强等原因，新闻体裁的分类更是多种多样。从新闻报道的现状来看，传统的新闻体裁有的仍保留下来，至今仍适用；有的已被淘汰不用，有的是新出现的样式。一般而言，常见新闻体裁（这里仅就新闻报道体裁而言）大致有以下几类： 1.消息 消息是对新近发生的有社会意义的事实进行简明扼要、迅速及时的报道的一种新闻体裁。它篇幅短小，特别讲求时效。消息又有报刊文字消息、广播消息（口播和录音新闻）、电视消息（口播和声像新闻）。从不同的角度分，消息有多种不同的类型。2.通讯通讯是一种运用多种表达方式，具体、生动、及时地报道具有新闻价值的人物、事件、情况和问题的新闻文体。它和消息均是主要的新闻报道形式，是记者的常规武器。通讯也分报刊文字通讯,广播新闻专题，电视新闻专题等。从表现形式和内容来看，通讯也可分成多种类型。3.特写新闻特写是一种“再现”新闻事件、人物或场景的形象化报道。它强调视觉印象，以描绘为主要手法，往往截取事件发展进程中的某个片断、细节或画面，绘声绘色，给人以特写镜头般的印象。新闻特写有报刊文字特写、广播特写和电视特写。4.深度报道关于深度报道，一般认为它不是一种独立的体裁，而只是一种报道方式，它是完整反映重要新闻事件和社会问题，追踪其来龙去脉，揭示其实质意义的一种高层次的报道方式。各种新闻体裁都可作深度报道，几种体裁的溶合往往更适合于作深度报道。 第一节 广播电视节目的采制流程一、采制流程含义1、采制:即采访与制作的统称。相当于广义上的制作，指包 括节目内容选取、采录、编辑、合成等环节在内的整个 生产流程。2、采制流程:❖ 多种划分。这里分作前期、后期。❖ 随着数字技术、卫星技术、光纤技术的运用，前后期界 限越来越模糊，形成“制/播合一”流程。 一般性制作流程:❖ 前期:策划、选题、脚本(撰稿)、录制❖ 后期:画面编辑、画面编辑、文字编辑、音乐音响合成， 特技运用、节目包装等 新闻类节目采制流程：❖ 前期:选题、采访(前期采访)、拍摄(现 场采访，也叫实地拍摄;现场报道)❖ 后期:画面编辑、文字稿(解说词)编辑、 音乐、音响(包括同期声、实况音响)的合 成、播出。 二、采制的重要地位 1、采制手段和方式构成了节目传播形态。比如，ENG 电子新闻采集系统，有了它才有了电视现场报道。2、采制手段与方式影响着节目信息承载的数量与质 量。比如，微波中继采访，提高了信号的稳定性， 使得电话连线这种双向互动式采访成为可能。3、采制过程与节目本身融为一体。现场直播或者纪实 性节目都会有意地保留节目采制过程的细节、氛围 或突发状况，使之具有原生态。 二、采制的重要地位1、采制手段和方式构成了节目传播形态。比如，ENG 电子新闻采集系统，有了它才有了电视现场报道。2、采制手段与方式影响着节目信息承载的数量与质 量。比如，微波中继采访，提高了信号的稳定性， 使得电话连线这种双向互动式采访成为可能。3、采制过程与节目本身融为一体。现场直播或者纪实 性节目都会有意地保留节目采制过程的细节、氛围 或突发状况，使之具有原生态。 三、采制理念❖ 采制既是技术手段，也是传播理念。“媒介即 信息”用到这里可以说采制即节目。❖ 1、时空观:实时在场❖ 2、真实观:记录、再现、虚拟❖ 3、节目观:策划、制作 第二节 广播电视新闻类节目的采制❖ 基于媒介特性要求的整体性思维❖ 新闻价值与形象价值❖ 采录保持现场感、真实感 一、广播电视新闻采录的特点❖ 1、采录的含义:采访与摄像/录音的统称。❖ 2、采录的重要性:❖ 获得新闻事实的素材;❖ 形成报道的基础;❖ 直接成为报道的表现形态，如无剪辑拍摄、现 场直播。 ❖ 3、采录的特点❖ 采访手段的多样性❖ 采、编、播一体化❖ 记录现场的原生态❖ 建立采访者、被采访者和受众间的交流感 二、广播电视新闻采录的要求❖ 1、 精心选材:❖ 衡量选题的新闻价值与形象价值❖ 两把尺子:❖ 一是新闻性:重大、影响，相关。国外有人称三“I”。❖ 二是可视性或可听性，它是广播电视媒介个性优势所 在，决定着题材的可操作性。 ❖ 选题的来源:❖ 相关文件、其它媒体(同行)、新闻线人 (报料)、跑线记者常规性、突发性题材、 部门提供线索、社会提供、企业、事业 单位提供 ❖ 2、 访前准备充分:❖ 采录设备检查与准备❖ 文字及其他相关材料准备 ❖ 3、 现场采录:❖ 保持过程性:音响与镜头段落记录过程❖ 传达现场感与真实感:❖ 注意镜头运动与构图要有空间感 ❖ 捕捉细节 ❖ 记者采访准备访问提纲(有形或无形)导入性交谈问题间应有一定的逻辑性问题可开可合，因人而宜善于发现访问对象谈话中的问题 记者(出镜/出声)的特殊要求❖ 口齿清楚，完成现场评述❖ 举止装扮得体，与环境相适宜，丰富非语 言信息❖ 把握报道思路，善于场面调度，结构报道 三、广播新闻采录的特殊要求❖ 1、精心采录现场音响❖ 音响的作用:❖ 音响的采录: 辅助性的音响:显明新闻发生的空间时间特征;显明 新闻事件的某些特征;显明新闻人物的某些特征等。 主体性的音响:叙述事实、观点(人物访谈)❖ 音响采录的原则:真实、典型、清晰、精当 四、电视新闻采录的特殊要求1、电视新闻摄像的原则真实性原则 现场采访，严守现场时间和空间。对于时过境迁事与人物因采用现实的事与人为载体，载负起所述事与人。 用画面说话原则画面是一种语言，有它自己的词汇，修辞规律。画面是思考的结果。重视同期声原则 2、电视新闻的拍摄内容❖ 介绍性镜头:交待新闻发生的基本要素❖ 中心性镜头:交待新闻事实的主体❖ 插入镜头:用于场景转换、节奏调整 3、电视新闻摄像的拍摄规则注意遵守轴线规律注意突出主体画面注意画面的连惯性，如M句子注意画面节奏，包括画面长度、镜头运动和主体运动、画面色彩影调注意起幅和落幅 第三节 广播电视非新闻类节目的制作❖ 几中主要的制作方式❖ 制作的任务❖ 制作人员的工作职责❖ 节目合成 一、电视制作方式❖ 是否有后期划分: ❖ 先录后播❖ 无剪辑摄制❖ 实况转播❖ 实况录像❖ 现场直播 ❖ 依据制作空间分为:❖ 演播室制作❖ 现场制作❖ “演播室+现场”制作 二、制作的任务❖ 1、前期准备: 文字准备:策划、选题、脚本(撰稿)等 物资器材准备❖ 2、中期采录:采访、录音、摄像等❖ 3、后期合成:画面、声音、文字等的编辑组 合 三、制作人员的工作职责❖ 各类制作人员:❖ 策划、制作与编导人员❖ 撰稿人员:负责文字脚本❖ 技术人员:负责摄像、录音、照明等 ❖ 媒介人员:主持人、播音员 (一)策划❖ 1、策划:综合运用多学科、多专业知识的创造性的思维活动。❖ 2、策划五要素:策划者、策划依据、策划方法或手段、策划对象、策划效果测定和评估❖ 3、策划的内容:创作策划——好点子制作策划——节目风格形式传播策划——定位和效果经营策划——效益分析实施策划——制作部分的优化组合 （二）制片人❖ 1、概念:从电影生产中借用过来，指广播电视节目生 产的主持者、投资者或其代理人。❖ 2、职责:——主持节目的整体设计——参与节目的制作与管理——做好节目的发行与播出——经费预算、审核与管理 （三）编导（编辑、导演和导播）❖ 1、任务:指导节目制作;编排审查播出节目;组织领导通联工作❖ 2、编辑——广播节目编辑:驾驭广播语言、编写串联词、 不同节目类型的不同编辑要求——电视节目编辑:驾驭图像编辑技巧、写好报道 词(解说词) 四、合成❖ 1、含义:将编辑或挑选好的语言、音响、音 乐与图像(文字)等节目要素，按一定的规 律，有机地结合成一个整体，成为表现主题 思想可供播出的节目成品。❖ 2、特点:❖ 时间性、技术性、协调性、综合性 ❖ 法则❖ ——主次律❖ ——互易律❖ ——分立对位律❖ ——听觉的相对完整律 ❖ ——淡出淡入律 第6章 广播电视传者的素养 第一节 广播电视传者的地位与作用 ❖ 一、地位❖ 在传受双方关系与传受过程中显示的传者的 地位。❖ 主导性的、强势传者地位。“魔弹论”，传者中 心。❖ 平等的、适度媒介权力。 ❖ 二、作用❖ 1、内容采集❖ 2、内容的加工❖ 3、内容的传送与反馈 ❖ 第二节 广播电视传者的素养❖ 一、理论素养❖ 政治理论水平、综合学科知识、专业理论知识 ❖ 二、专业技能❖ 笔头、又头、镜头的表达能力。❖ 三、职业道德❖ 敬业精神、坚持“双为”服务原则 ❖ 第三节 广播电视传者——主持人❖ 一、主持人界说❖ 在广播电视传播中策划或制作、串联把 握一个栏目节目，以其个性方式 直接与 受众进行平等交流的媒介人物。 ❖ 二、分类❖ 采编播合一的主持人❖ 采编播合作的主持人(分离)❖ 客串主持人 ❖ 三、主持人作用❖ 1、节目的参与者:参与节目采制❖ 2、节目个性的体现者:使之形成有机整体❖ 3、传受互动的实践者:情感与信息交流互 动 ❖ 四、主持人个性风格❖ 相对独立稳定的外形及内部特征。❖ 1、新闻节目主持人❖ 2、综艺节目主持人 第7章 广播电视受众的需求第一节 广播电视受众的地位与作用❖ 一、广播电视受众的地位❖ 受众主体地位❖ 理论上讲:节目经由传者与受众的共同生产 完成。❖ 实践上看:受众即是市场，是媒介赖以生存 发展之本。 二、广播电视受众的作用❖ 1、参与传播:受众直接参与或间接参与节 目制作和播出过程。❖ 2、反馈信息:受众接收节目的反响，受众借 助电话、短信、网络、书信等方式，表达对 媒介内容的评价、建议。 一、广播电视受众的特点1、受众分布的广泛性和地域性2、受众构成的多样性和复杂性3、受众接收行为的主动性和随意性 随着大众媒介素养的提高，借助方便快捷的通讯工具，受众的媒介接触显示更鲜明的主动性和个性化特点。 第二节 广播电视受众的构成❖ 1、目标受众❖ 2、基本受众❖ 3、参照受众❖ 4、潜在受众❖ 在实际的媒介接触中，受众与媒介和传者的关系 会发生变化。切中受众心理与需求的传播则有利 于受众朝向理想受众发展。 第三节 广播电视受众的需求一、广播电视受众的心理1、受众的心理特点 好奇、求益、娱乐、逆反、认同等2、影响受众心理的主要因素 社会文化、个人兴趣爱好、自我期待等 二、广播电视受众的需求❖ 1、使用与满足(使用与满足理论)上个世纪四十、七十年代实证研究成果:使用 与满足的形式:感情释放(或替代性的感情经 验);愿望的想像;有用的建议。❖ 2、受众使用媒介和获得的满足与媒介的功能、 媒介的特性、受众使用媒介的社会原因等有关。 第四节 广播电视受众调查一、广播电视受众调查的内容❖ 收视/听率调查。视听率指某一时间、某一地区收 看/听节目的户数(人数)除以同一地区拥有电视 机/收音机的总户数(人数)的百分比。❖ 受众群的构成。❖ 受众对节目的评价，如节目偏好、认知、满意度等。 二、广播电视受众调查的方法❖ 观察法、访谈法、问卷调查法、量表法等。 三、广播电视受众调查的应用❖ 节目制作、频道规划和广告经营等方面。 第8章：广播电视体制的改革第一节 广播电视体制界说一、广播电视体制的含义1、国家管理广播电视业的制度、政策， 广播电视业的构成形式和规范模式，它 包括广播电视机构的所有制形式、隶属 关系、组织形态、内部结构和人事制度 等等。2、广播电视体制包括多个层面:❖ 宏观:指国家制订的有关广播电视的法规政策等;❖ 中观:指广播电视机构的设置、经营模式等;❖ 微观:指广播电视节目采制、人员管理等方面 的内容。 二、主要的广电体制类型多种划分方式;❖ 二分法:商业型和非商业型(公共型)。❖ 三分法:国有型、公共型和私有型。❖ 四分法:国有国营、公有公营、私有商营 和混合型。 第二节 我国社会主义的广电体制一、我国广播电视体制的历史回顾❖ 1、第一阶段:建国初期——十一届三中全会前(1949-1977)❖ “条块结合，以条为主”，中央和省(包括自 治区、直辖市)两级管理的体制。 ❖ 第二阶段:十一届三中全会后——十四大会议召开前(1978- 1991)❖ 广播电视在“四级办”方针指导下空前繁荣❖ “广播电视二重性”理论讨论❖ 第三阶段:十四大会议召开——加入世界贸易组织(1992~ 2001)❖ 在社会主义市场经济转型过程中开始广播电视产业化、集团 化模式 ❖ 第四阶段:2002年至今❖ 入世后，广播电视节目经营开始逐步开放， 加大与世界传媒业的接轨，事业和产业分开 运营成为一大发展趋势。 二、我国广电体制的基本特征❖ 1、所有权和经营权可合可分的多样性公有体制❖ 2、强调政治属性，坚持党管原则❖ 3、一定的市场化取向 第三节 广播电视体制的改革趋向一、世界主要国家广播电视体制改革动向1、私有商营化❖ 私有化浪潮始于上个世纪80年代，到90年 到更趋明显。原来实行国有体制和公有体 制的国家纷纷将广播电视私有化。2、管制放松化 各国对广播电视事业的管制趋于理性化，对原来过于严格的管理体制进行调整。3、体制多元化理顺意识形态和经济效益的关系，政府和 广播电视行业的关系，管理体制的多元化 是世界广播电视体制调整改革的一大趋势。 4、媒介集团全球化❖ 随着经济全球化的浪潮，这些通过 兼并、联合组成的媒介集团已越过 国界，它们的联合模式、内部的管 理体制等也随之扩散到世界各国。 二、我国近年来广播电视体制改革的态势1、广播电视体制改革的总体目标 建立起适应社会主义市场经济发展要求的、有中国特色的广播电视体制。2、广播电视体制改革态势❖ 宏观层面:建立新形势下党委领导的，在政府宏观 调控下市场化有序运行的宏观管理体制。包括:转 变政府职能，确立发展战略和提供法制环境。❖ 中观层面:集团化是广播电视产业化的必由 之路。❖ 微观层面:建立制播分营体制;建立现代人 力资源管理机制等]]></content>
      <categories>
        <category>Communication Science</category>
        <category>Medium Study</category>
      </categories>
      <tags>
        <tag>Medium Study</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最强大脑节目社会评议数据分析]]></title>
    <url>%2F2018%2F01%2F07%2F%E6%9C%80%E5%BC%BA%E5%A4%A7%E8%84%91%E8%8A%82%E7%9B%AE%E7%A4%BE%E4%BC%9A%E8%AF%84%E8%AE%AE%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[Abstract：最强大脑节目社会评议数据分析。广电概论的大作业 ~ 1 电视收视率与网站播放量概况1.1 电视收视情况 电视收视率（Rtg%）是指某一时段内收看某电视频道（或某电视节目）的人数（或家户数）占电视观众总人数（或家户数）的百分比。作为“注意力经济”时代的重要量化指标，它是深入分析电视收视市场的科学基础，是节目制作、编排及调整的重要参考，是节目评估的主要指标，是制定与评估媒介计划、提高广告投放效益的有力工具[1]。 上表是最强大脑第四季的收视情况表 [2]. 由表可知：第四季从第1期到第13期的CSM52城市网收视率排名一直高居前三名，其中第7、9、10、12期为同时段收视率第一；整体收视成上升趋势，观众收视热度不减反增；其中为可见此节目的受欢迎程度之高。 1.2 视频网站播放量 视频网站 总播放数 总评论数 总评分 优酷 198, 737, 396 61，796 9.1 乐视 9404.9万 1.4万 9.4 搜狐 61, 350, 553 \ 7.2 上表数据来源于优酷、乐视、搜狐三大视频网站 [3][4][5]。其中优酷的播放量最大，接近20亿次，可见最强大脑在视频网站上的热度也很高。 2 媒体热度分析 下面的媒体数据分析均选取2017.1.03—2017.4.10即最强大脑第四季的完整播出时间区间（为更好展示变化趋势，故选取前后延展了3天的时间区间）。 2.1 微博热度趋势[6] 由上图可见，最强大脑的微博热度很高，在每期播出的前后几天热度达到峰值，可见每期都在微博上引起了不小的热议和关注。其中第5期的热度最高，原因是“水哥”王昱珩退赛引发很大争议。 2.2 今日头条热度趋势 [7] 热度指数：根据今日头条热度指数模型，将用户的阅读、分享、评论等行为的数量加权求和得出相应事件、文章或关键词的热度值。以天为单位制成趋势图，表现热度随时间的变化情况。 图1：今日头条热度趋势 图2：热度峰值分析 图1是最强大脑的今日头条热度趋势图，图2是引发各峰值的文章。可见头条热度呈波动趋势，其中出现5个高峰。引起①-⑤峰值的话题分别是：水哥输给百度AI，水哥退赛，节目组取消魔方国际赛，黑幕争议，脑王争霸赛剧透。由此可见，核心人物退赛、节目黑幕、赛程变动或剧透、人机大战等事件容易引起网民大量关注，易制造话题性、提高节目曝光度，这也不失为节目的一种话题营销手段。同时可见第四季播出期间引发了一些负面舆论争议，对节目形象或有损伤。 3 网民画像分析 [7] 5.3 数据来源于今日头条。 3.1 性别渗透率 渗透率:表示用户对特定事件关注度的比例.性别渗透率 = 某性别用户对关键词的关注度／该性别用户总关注度. 由上图可知，头条上关注“最强大脑” 的用户中女性渗透率高于男性。可见女性网民对最强大脑的关注度更高。 3.2 年龄渗透率 性别渗透率 = 某年龄段用户对关键词的关注度／该年龄段用户总关注度. 图1：年龄渗透率（头条用户） 由上图可知，各年龄层的头条用户均对最强大脑有一定比例的关注度，可见其赢得了多年龄层用户的喜爱。其中24-30岁的头条用户对“最强大脑”的关注度最高，31-40岁次之，18-23岁再次。可见青壮年网民对最强大脑的关注度高于中老年网民。 图2：最强大脑用户年龄结构对比（电视观众与微博） 区别于纯娱乐类综艺节目，最强大脑将科学与娱乐有机结合，丰富精美道具的使用，世界级超强脑力选手比拼，各国战队的较量等令节目紧张而富有趣味性，赢得了多年龄层观众的喜爱。其电视观众年龄层分布较均匀，各年龄层差距不明显。其中70后及以上观众占比超过6成(62.02%)，中老年人群成为该节目的主力观众，这与节目的科学与智力性有很大关系。而微博端关注该节目的用户仍以90后、00后为主，比例达63.36%，可见微博依然是年轻人的主场。[10] 3.3 地域渗透率 地域渗透率 = 某省份或城市用户对包含特定关键词文章的关注度/该省份或城市产生的总关注度. 图1：关注“最强大脑”网民的省份分布（注：颜色由深到浅表示网民渗透率由高到低） 由上图可见，在全国所有省份中，广东的地域渗透率（19237）最高，Top5的还有：江苏、北京、山东、浙江。互联网发达、网民数量众多可能是广东的渗透率最高的原因之一；最强大脑节目由江苏卫视打造，这应该是江苏地域渗透率次高的原因。 图2：关注“最强大脑”网民的城市分布 由上图可见，在全国所有城市中，北京的地域渗透率（14695）最高，Top5的还有：上海、深圳、广州、武汉。 TOP5省份与城市均为经济较为发达的地区，可见经济发达地区的头条用户对最强大脑的关注度相对较高，这可能与此节目的科学益智性要求受众有一定的受教育程度有关。 3.4 网民兴趣分布 上图是关注“最强大脑”网民的兴趣统计。由图可知，头条上关注“最强大脑” 的用户的兴趣中“育儿”占比最大(0.68%)，其余Top5为：时尚(0.50%)，科技(0.46%)，娱乐(0.40%)，游戏(0.37%)。 4 社会评议与典型意见挖掘4.1 豆瓣评分[8]图1：四季评分对比 第1季评分 第2季评分 第3季评分 第4季评分 8.0 8.1 7.8 6.1 图2：第四季评分 豆瓣评分 参评人数 5星比例 4星比例 3星比例 2星比例 1星比例 6.1 2883 12.2% 26.0% 33.0% 14.3% 14.5% 由图1可见，网民对最强大脑第四季的评价远低于前三季。但第四季的收视率仍然高居同时段Top3，可见豆瓣评分与其综艺热度并没有必然联系。 图2显示，第四季的三星及以下评分占比高达61.8%，可见网民对其负面评价偏多。 4.2 网民情感极性分析 [11] 上图来源于清博大数据，网民对第四季的情感倾向中负面情感占比高达39.79%，正面情感只占18.39%，可见第四季具有很大的舆论争议性。 这可能与其爆出的“节目组黑幕”有关。可见这档旨在“让科学流行起来”的脑力竞技真人秀，显然遭遇了前所未有的信任危机。 第四季增加了许多“撕逼大战”的情节，赛前叫板对手、赛后炮轰节目成固定套路，为这场基于脑力的PK平添了不少戾气，这种故意设置的“撕来撕去的口水乱战”可能是导致其负面评价多的原因之一。 4.3 今日头条评论分析图1：头条评论关键词 [7] 上图显示的是今日头条上与“最强大脑”相关的评论中出现频率最高的10个关键词。可见对于最强大脑，头条用户最感兴趣的是：项目难度（记忆难度，盲拧魔方），节目组黑幕（“钻空子”，“不配”，“鲍云”），选手真实实力（“余不配”）等话题。 图2：头条用户热门评论 上图是今日头条上与“最强大脑”相关的最热门评论（图右的数值表示点赞数），热门评论可反映出网民对哪些观点最赞同，且它们都评论于2篇以“退赛事件”为主题的文章。由此可见，网民对水哥退赛的支持和崇拜、对最强大脑节目组黑幕的批判。 5 热点舆论事件挖掘——“王昱珩退赛/节目组黑幕” [9] 在第四季播出期间，王昱珩退赛/节目组黑幕引起了最强烈的舆论争议，下面我们对这个热点事件进行深入挖掘。我们以事件的关键人物——王昱珩发表的一条微博（与“最强大脑”相关的最热门微博）为例，分析此条微博的传播、转评背后反映出的问题。 5.1 微博详情 @闲人王昱珩 的微博共收获转发数15,120次（其中有效转发12,671次）、 评论数45,380条，点赞数135,534个，覆盖人次14,021,904人。体现出此条微博的覆盖范围之广，以及网民对王昱珩退赛的支持。 5.2 转发评论趋势 该微博于03-03 21:46发布后，于03-04 00:00（发布后2个多小时）达到转发、评论高峰，转发峰值5280条、评论峰值12871条，此后微博传播速度逐渐降低。由此可见，该微博传播速率之快，一发布即迅速引起了网民的大量关注。 5.3 转评表情分析 转发评论的表情以爱心、哭泣、无奈、赞许为主，表达了网民对王昱珩的宽慰和赞许、对其不继续参赛的失落难过，对节目组黑幕的讽刺无奈。可见舆论主要站在王昱珩这边。 6 总结总体来说，最强大脑第四季收视率稳居高位，无论是电视平台、网络视频平台还是媒体平台上的热度都很高。电视观看用户年龄段分布均匀，以中老年为主力观众；而网络媒体平台的关注用户则以青壮年为主。性别上，女性普遍比男性更关注最强大脑；地域上，广东、北京、上海、深圳、江苏等经济发达地区用户对其关注度更高。 尽管节目热度很高，但从其豆瓣评分反映出网民对其评价较前三季偏低，且负面评价占比较大。主要原因是播出期间爆出的“王昱珩退赛, 节目组黑幕”的负面新闻，这在媒体平台上引起了广泛的热议，舆论一边倒偏向王昱珩，而最强大脑节目组遭到了网民的“声讨”，负面评价很多。 我们认为：最强大脑作为一档科学益智类节目，可以为了节目效果添加一定的娱乐属性，但过度综艺化、导演“黑幕”或虚假做作的口水乱战等情节有违此节目的科学精神，故其招致了网民的大量负面舆论评价。这种情节不应再次发生，希望最强大脑第五季能秉持科学精神，回归纯粹。 7 Reference[1] CSM.电视收视率 http://www.csm.com.cn/cpfw/ds/ [2] 百度百科.最强大脑第四季 https://baike.baidu.com/item/%E6%9C%80%E5%BC%BA%E5%A4%A7%E8%84%91%E7%AC%AC%E5%9B%9B%E5%AD%A3#6 [3] 优酷.最强大脑第四季 https://list.youku.com/show/id_zc0193db8cdc411e6acec.html [4] 乐视综艺.最强大脑第四季 http://www.le.com/zongyi/10033500.html [5] 搜狐视频.最强大脑第四季 http://tv.sohu.com/s2016/jswszqdndsj/ [6] 微指数.最强大脑 http://data.weibo.com/index [7] 今日头条 https://www.toutiao.com/ [8] 豆瓣电影.最强大脑第四季 https://movie.douban.com/subject/26950044/ [9] 新浪微博.闲人王昱珩 https://weibo.com/5400369364/Ey7Ojl9Lb?from=page_1005055400369364_profile&amp;wvr=6&amp;mod=weibotime&amp;type=comment [10] 微博.季播综艺节目时移收视与微博指数浅析 https://weibo.com/ttarticle/p/show?id=2309403990982931697724 [11] 界面.一季度综艺排行榜看节目品牌正负舆情 https://m.jiemian.com/article/1153555_yidian.html]]></content>
      <categories>
        <category>Communication Science</category>
        <category>Medium Study</category>
      </categories>
      <tags>
        <tag>Medium Study</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库系统概论复习笔记]]></title>
    <url>%2F2018%2F01%2F05%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%AE%BA%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Abstract: 数据库系统概论期末复习笔记 ~ Part1 数据库概述关系模型的构成：关系数据结构、关系操作集合、关系完整性约束。 1.1 数据库发展1.数据库发展过程： 2.三个时期的比较： 1.2 基本概念1.数据模型三个要求： 真实模拟现实社会 容易理解 便于在计算机上实现 两类： 概念模型：= 信息模型；面向用户，按用户的观点来对数据和信息建模，主要用于数据库设计 逻辑模型/物理模型：对数据最底层的抽象，描述数据在系统内部的表示方式和存取方式，面向计算机系统；包括层次模型，网状模型，关系模型，面向对象模型，对象关系模型 三要素： 数据结构 数据操作 数据的完整性约束条件 常用数据模型： 层次 网状 关系 面向对象 对象关系 半结构化 2.四个基本概念数据Data 数据库Database：长期储存在计算机内、有组织的、可共享的大量数据的集合。 人员数据视图: 数据库结构图： 基本特征： 数据按一定数据模型组织、描述和储存 可为各种用户共享 冗余度较小 数据独立性较高 易扩展 数据库管理系统DBMS： 位于用户与操作系统之间的一层数据管理软件.是基础软件,是一个大型复杂的软件系统 用途：科学地组织和存储数据、高效地获取和维护数据 数据控制功能： 数据的安全性（Security）保护:保护数据,以防止不合法的使用造成的数据的泄密和破坏. 数据的完整性（Integrity）检查:将数据控制在有效范围内,或保证数据之间满足一定关系 并发（Concurrency）控制:对多用户的并发操作加以控制和协调,防止相互干扰而得到错误的结果 数据库恢复（Recovery）:将数据库从错误状态恢复到某一已知的正确状态 主要功能：数据定义，数据操纵（增删查改），数据控制（数据的完整性、安全性定义与检查数据库的并发控制与故障恢复），数据服务（拷贝、转储、重组、性能检测、分析……） 数据库系统DBS： 在计算机系统中引入数据库后的系统构成 组成：数据库，数据库管理系统，应用系统，数据库管理员 特点：数据结构化；数据的共享性高，冗余度低,易扩充；数据独立性高；数据由DBMS统一管理和控制 3.数据独立性（两个独立性） 物理独立性：指用户的应用程序与存储在磁盘上的数据库中数据是相互独立的.当数据的物理存储改变了,应用程序不用改变 逻辑独立性：指用户的应用程序与数据库的逻辑结构是相互独立的.数据的逻辑结构改变了,用户程序也可以不变 物理独立性与逻辑独立性保证了数据独立性也是由DBMS的二级映像功能来保证的 4.信息世界基本概念实体：客观存在并可相互区别的事物。可以是具体的人、事、物或抽象的概念。 属性：实体所具有的某一特性称为属性.一个实体可以由若干个属性来刻画。 码：唯一标识实体的属性集。 域(Domain) 属性的取值范围称为该属性的域。 实体型(Entity Type) ：用实体名及其属性名集合来抽象和刻画同类实体称为实体型。 实体集(Entity Set) 同一类型实体的集合称为实体集 联系(Relationship)现实世界中事物内部以及事物之间的联系在信息世界中反映为实体内部的联系和实体之间的联系。 实体内部的联系通常是指组成实体的各属性之间的联系 实体之间的联系通常是指不同实体集之间的联系 A.两个实体型： 一对一联系（1：1）：如果对于实体集A中的每一个实体,实体集B中至多有一个（也可以没有）实体与之联系,反之亦然,则称实体集A与实体集B具有一对一联系,记为1:1。 一对多联系（1：n）：一个班级中有若干名学生,每个学生只在一个班级中学习 多对多联系（m：n）：课程与学生之间的联系：一门课程同时有若干个学生选修,一个学生可以同时选修多门课程 B.两个以上实体型： 5.数据库管理员的具体职责（DBA） 决定数据库中的信息内容和结构 决定数据库的存储结构和存取策略 定义数据的安全性要求和完整性约束条件 监控数据库的使用和运行 周期性转储数据库 数据文件 日志文件 系统故障恢复 介质故障恢复 监视审计文件 数据库的改进和重组：性能监控和调优定期对数据库进行重组织,以提高系统的性能需求增加和改变时,数据库须需要重构造 1.3 关系数据库理论1. 关系模式（五元组） 格式：R（U，D，DOM，F） R：关系 U：属性 D：域 DOM：属性到域的映射 属性组U上的一组数据依赖F(如函数依赖(Functional Dependency,FD),多值依赖(Multivalued Dependency,MVD)) 2. 相关概念A.函数依赖： R(U)是属性集U上的关系模式.X,Y是U的子集.若R(U)的任意一个可能的关系r,r中不可能存在两个元组在X上的属性住相等,而在Y上的属性值不等,则X函数确定Y或Y依赖于X,记做X→Y,若X→Y,Y→X,记做X←→Y(非主属性中某属性值唯一) 非平凡函数依赖:X→Y,但X不包含Y,则称X→Y非平凡函数依赖(例子 (Sno,Cno)→Grade ) 平凡函数依赖:X→Y,且X包含Y则称X→Y平凡函数依赖(例子 (Sno,Cno)→Cno ) 完全函数依赖:X→Y且对X的任何一个真子集X’,都有X’→Y不成立,则称Y对X完全函数依赖(例子 (Sno,Cno)→Grade ) 部分函数依赖:X→Y,但Y不完全依赖于X,则称Y对X部分函数依赖(例子 (Sno,Cno)→Sdept [由Sno便可推出Sdept] ) 传递函数依赖:在R(U)中,如果X→Y,(X不包含Y),Y→X不成立,Y→Z,Z不属于Y,则称Z对X传递函数依赖,记做X→Z(例子 Sno→Sdept, Sdept→Mname成立,所以Sno→Mname) B.多值依赖： 定义: 形式1:设R(U)是属性集U上的一个关系模式.X,Y,Z是U的子集,并且Z=U-X-Y,感谢模式R(U)中多值依赖X→→Y成立,当且仅当对R(U),的任一关系r,给定的一对(x,z)值,有一组Y的值,这组值仅仅决定于x值而与z值无关 形式2:在R(U)的任一关系r中,如果存在元组t,s使得t[X]=s[X],那么必然存在元组w,v属于r,(w,v可以与s,t相同),使得w[X]=v[X]=t[X],而w[Z]=v[Z]=t[Z],v[Y]=s[Y],v[Z]=tZ则Y多值依赖与X记做X→→Y,这里X,Y是U的子集,Z=U-X-Y 平凡多值依赖: 若X→→Y,而Z=φ,即Z为空,则称X→→Y为平凡多值依赖 多值依赖性质: 多值依赖具有对称性.即若X→→Y,则X→→Z其中Z=U-X-Y 多值依赖具有传递性,即若X→→Y,X→→Z,则X→→Z - Y,X→→Y - Z 函数依赖可以看做多值依赖的特殊情况,即若X→Y则X→→Y.这是因为当X→Y时,对X的每一个值x,Y有一个确定的值y与之对应,所以X→→Y 若X→→Y,X→→Z,则X→→YZ 若X→→Y,X→→Z,则X→→Y∩Z 多值依赖与函数依赖的区别: 多值依赖的有效性与属性集的范围有关 若X→→Y在U上成立则在W(U包含W, W包含XY)上一定成立,反之则不然,即X→→Y在W(U包含W)上成立,在U上并不一定成立,这是因为多只依赖的定义中不仅涉及属性组X和Y,而且涉及U中的其余属性Z 一般得在R(U)上若有X→→Y在W(U包含W)上成立,则称X→→Y为R(U)的嵌入型多值依赖 但是在关系模式R(U)中函数依赖X→Y的有效性仅决定于X,Y这两个属性集的值.只要在R(U)的任何一个关系r中,元组在X和Y上的值满足函数依赖的定义,则函数依赖X→Y在任何属性集W(U包含W, W包含XY)上成立 若函数依赖X→Y在R(U)上成立,则对任何Y的子集Y’具有X→Y’成立,而多值依赖X→→Y若在R(U)上成立,却不能保证对于任何一个Y’ 即Y的子集的X→→Y’都成立 码： 候选码，主码：设K为R中的属性组合,若K完全依赖于U则K为R的候选码(Candidate key),若候选码多于一个,则选定一个为主码(Primary key)。【唯一标识实体的属性或属性集为候选码，可以有多个；主码只能有一个；主码一定是候选码，候选码不一定是主码】 主属性，非主属性，全码：包含在任何一个候选码中的属性,称为主属性(Primary attribute).不包含在任何码中的属性称为非主属性(Nonprime attribute)或非码属性(Non-key attribute).最简单的情况,单个属性是码.最极端的情况,整个属性组都是码,称为全码(All-key) 3. 模式存在问题 数据库冗余太大 更新异常 插入异常 删除异常 4.范式（规范化）第一范式1NF：每一个分类必须是一个不可分的数据项,则属于第一范式规范。 2NF：若R属于1NF, 且存在非主属性完全函数依赖于码。 3NF：R属于2NF，且非主属性既不部分依赖于码，也不传递依赖于码。 BCNF：属于3NF，且多有属性都不部分依赖或传递依赖于码，所有决定属性集都包含码。 4NF：所有非平凡的多值依赖都是函数依赖。 5NF：连接依赖均由候选码所蕴含。 范式规范化过程： 1.4 关系模型1.关系模型从用户角度看，关系模型中数据的逻辑结构时一张二维表，由行和列组成。 2.相关概念 元组Tuple：表中的一行即为一个元组（包含不同属性集的值） 关系Relation：对应一张表；要求关系的每个分量都是不可分的数据项，不允许表中表 属性Attribute：表中的一列为一个属性 主码key：表中的某个属性组，可唯一确定一个元组 域domain：属性的取值范围 分量：元组中的一个属性值 关系模式：对关系的描述；关系名（属性1，属性2，…） 3.完整性约束 作用：【约束】保证数据库中数据的正确性和相容性 包括： 域完整性约束：保证数据库字段取值的合理性；包括检查（CHECK）、默认值（DEFAULT）、不为空（NOT NULL）、外键（FOREIGN KEY）等约束。 实体完整性(Entity integrity): 指关系的主关键字[主码]不能重复也不能取“空值“ 参照完整性: 定义建立关系之间联系的主关键字与外部关键字引用的约束条件 用户定义的完整性(user defined integrity)：实体完整性和参照完整性适用于任何关系型数据库系统，它主要是针对关系的主关键字和外部关键字取值必须有效而做出的约束；用户定义的完整性是根据应用环境的要求和实际的需要，对某一具体应用所涉及的数据提出约束性条件，这一约束机制一般不应由应用程序提供，而应有由关系模型提供定义并检验，用户定义完整性主要包括字段有效性约束和记录有效性。 4.关系模型的优缺点优点： 建立在严格的数学概念的基础上 概念单一: 实体和各类联系都用关系来表示；对数据的检索结果也是关系 关系模型的存取路径对用户透明: 具有更高的数据独立性,更好的安全保密性；简化了程序员的工作和数据库开发建立的工作 缺点： 存取路径对用户透明导致查询效率往往不如非关系数据模型 为提高性能,必须对用户的查询请求进行优化增加了开发DBMS的难度 5.系统结构解释从数据库管理系统角度看数据库系统通常采用三级模式结构,是数据库系统内部的系统结构。 从数据库最终用户角度看(数据库系统外部的体系结构),数据库系统的结构分为: 单用户结构主从式结构 分布式结构 客户／服务器 浏览器／应用服务器／数据库服务器多层结构等 6.实例instance 模式的一个具体值 反映数据库某一时刻的状态 同一个模式可以有很多实例 实例随数据库中的数据的更新而变动 7.模式Schema（又称：逻辑模式）解释： 模式：数据库中全体数据的逻辑结构和特征的描述 反映：数据的逻辑结构及其联系 所有用户的公共数据视图，综合所有用户的需求 说明：一个数据库只有一个模式 地位：数据库系统模式结构的中间层 特点： 与数据的物理存储细节和硬件环境无关 与具体的应用程序、开发工具个高级程序设计语言无关 定义： 数据的逻辑结构（数据项的名称、类型、取值范围等） 数据之间的联系 数据有关的安全性、完整性要求 8.外模式(External Schema)说明： 数据库用户（包括应用程序员和终端用户）使用的局部数据的逻辑结构和特征的描述 数据库用户的数据视图，是与某一应用有关的数据的逻辑表示 地位：介于模式与内模式之间 模式与外模式的关系：一对多 外模式是模式的子集 一个数据库可以有多个外模式，反映不同用户的应用需求、看待数据的方式、对数据保密的要求 对模式中同一数据，在外模式中的结构、类型、长度、保密级别都可以不同 外模式与应用的关系：一对多 同一外模式也可以为某一用户的多个应用系统所使用 但一个应用程序只能使用一个外模式 用途: 保证数据库安全性的一个有力措施 每个用户只能看见和访问所对应的外模式中的数据 9.内模式（internal schema，又称存储模式）说明： 是数据物理结构和存储方式的描述 一个数据库只有一个内模式 是数据在数据库内部的表示方式： 记录的存储方式：顺序存储，hash存储等 索引的组织方式 数据是否压缩存储 数据是否加密 数据存储记录结果的规定 10.三级模式结构（数据库系统的系统结构）组成：内模式，模式，外模式 同一个模式可以有任意多个外模式 每一个外模式,数据库系统都有一个外模式／模式映象,定义外模式与模式之间的对应关系 11.二级映象作用：二级映像在DBMS内部实现这三个抽象层次的联系和转换。 说明：映象定义通常包含在各自外模式的描述中 外模式/模式映像： 作用：保证数据的逻辑独立性 说明： 外模式描述的是数据的局部逻辑结构 模式描述的是数据的全局逻辑结构 当模式改变时,数据库管理员修改有关的外模式／模式映象,使外模式保持不变，应用程序是依据数据的外模式编写的,从而应用程序不必修改,保证了数据与程序的逻辑独立性,简称数据的逻辑独立性 模式／内模式映像: 作用:保证数据的物理独立性 说明: 当数据库的存储结构改变了（例如选用了另一种存储结构）,数据库管理员修改模式／内模式映象,使模式保持不变，应用程序不受影响.保证了数据与程序的物理独立性,简称数据的物理独立性 12 触发器触发器又叫做事件-条件-动作（event-condition-action）规则。当特定的系统事件发生时，对规则的条件进行检查，如果条件成立则执行规则中的动作，否则不执行该动作。规则中的动作体可以很复杂，通常是一段SQL存储过程。 触发器类型 行级触发器（FOR EACH ROW） 语句级触发器（FOR EACH STATEMENT） 激活触发器 触发器的执行，是由触发事件激活的，并由数据库服务器自动执行一个数据表上可能定义了多个触发器，遵循如下的执行顺序: （1） 执行该表上的BEFORE触发器;（2） 激活触发器的SQL语句;（3） 执行该表上的AFTER触发器。 1.5 SQL概述1.SQL的特点综合统一： 集数据定义语言（DDL）,数据操纵语言（DML）,数据控制语言（DCL）功能于一体 可以独立完成数据库生命周期中的全部活动： 定义关系模式,插入数据,建立数据库； 对数据库中的数据进行查询和更新； 数据库重构和维护 数据库安全性、完整性控制等 用户数据库投入运行后,可根据需要随时逐步修改模式,不影响数据的运行 数据操作符统一 高度非过程化： 非关系数据模型的数据操纵语言“面向过程”,必须指定存取路径 SQL只要提出“做什么”,无须了解存取路径 存取路径的选择以及SQL的操作过程由系统自动完成 面向集合的操作方式： 非关系数据模型采用面向记录的操作方式,操作对象是一条记录 SQL采用集合操作方式: 操作对象、查找结果可以是元组的集合；一次插入、删除、更新操作的对象可以是元组的集合 以同一种语法结构提供多种使用方式： SQL是独立的语言:能够独立地用于联机交互的使用方式 SQL又是嵌入式语言:SQL能够嵌入到高级语言（例如C,C++,Java）程序中,供程序员设计程序时使用 语言简洁，易学易用：SQL功能极强,完成核心功能只用了9个动词 2.SQL的数据定义功能 3.三个基本概念1.基本表： 本身独立存在的表SQL中一个关系就对应一个基本表一个(或多个)基本表对应一个存储文件一个表可以带若干索引 2.存储文件: 逻辑结构组成了关系数据库的内模式 物理结构是任意的,对用户透明 3.视图: 从一个或几个基本表导出的表 数据库中只存放视图的定义而不存放视图对应的数据 视图是一个虚表 用户可以在视图上再定义视图 4.SQL支持关系数据库三级模式结构 5.标识符定义：由用户定义的可识别的字符序列；标识某个实体的一个符号 规则: 第一个字符必须是字母或下划线（_）或@或#； 后续字符可以是：字母、数字、_、#、$、@等； 注意: 不能使用SQL中的关键字和运算符,不允许嵌入空格或其他特殊字符 Part2 数据库设计2.1 总体设计过程1.数据库设计步骤 2.设计描述 3.数据库设计的特点 2.2 需求分析1.分析和表达用户需求首先把任何一个系统都抽象为： 分解处理功能和数据: 分解处理功能: 将处理功能的具体内容分解为若干子功能 分解数据: 处理功能逐步分解同时,逐级分解所用数据,形成若干层次的数据流图 表达方法: 处理逻辑：用判定表或判定树来描述 数据：用数据字典来描述 将分析结果再次提交给用户,征得用户的认可 2.任务通过调查,收集与分析数据,获得用户对数据要求: 信息要求: 指用户需要从数据库中获得信息的内容与性质,再由信息要求导出数据要求 处理要求: 指用户要完成什么处理功能,对初一响应时间有什么要求,处理方式是批处理还是联机处理 安全性与完整性要求 3.需求分析过程 4.数据流图符号说明： 例子： 数据字典： 与数据流图的区别 数据流图 — 表达了数据和处理的关系 数据字典 — 则是系统中各类数据描述的集合 2.3 概念结构设计1.四类方法 1.自顶向下: 即首先定义全局概念结构的框架,然后逐步细化 2.自底向上：即首先定义个局部应用的概念结构,然后将他们集合起来,得到全局概念 3.逐步扩展：首先定义最重要的核心概念结构,然后向外扩充,以滚球的方法逐步生成其他概念结构,直至总体概念结构 4.混合策略：即将自顶向下和自底向上相结合,用自顶向下策略设计一个全局概念结构框架,以它为骨架集成由底向上策略中设计的个局部概念结构 3.三种抽象1.分类classification： 定义某一类概念作为现实世界中一组对象的类型 抽象了对象值和型之间的“is member of”的语义 2.聚集aggregation： 定义某一类型的组成成分 抽象了对象内部类型和成分之间“is part of”的语义 复杂的聚集,某一类型的成分仍是一个聚集 3.E-R图： 任务: 将各局部应用涉及的数据分别从数据字典中抽取出来 参照数据流图,标定各局部应用中的实体、实体的属性、标识实体的码 确定实体之间的联系及其类型（1:1,1:n,m:n） 两条准则: 属性不能再具有需要描述的性质.即属性必须是不可分的数据项,不能再由另一些属性组成 属性不能与其他实体具有联系.联系只发生在实体之间 2.4 逻辑结构设计1.E-R图与关系模型转换 转换内容: 将实体、实体的属性和实体之间的联系转换为关系模式 转换原则: 一个实体转换为一个关系模式；实体的属性即为关系的属性；实体的码即为关系的码 2.E-R图实体型间的联系有以下不同情况 一个1:1联系可以转换为一个独立的关系模式,也可以与任意一端对应的关系模式合并: 转换为一个独立的关系模式；与某一端实体对应的关系模式合并 一个1:n联系可以转换为一个独立的关系模式,也可以与n端对应的关系模式合并: 转换为一个独立的关系模式；与n端对应的关系模式合并 一个m:n联系转换为一个关系模式：三个或三个以上实体间的一个多元联系转换为一个关系模式 具有相同码的关系模式可合并: 目的：减少系统中的关系个数 合并方法： 将其中一个关系模式的全部属性加入到另一个关系模式中,然后去掉其中的同义属性（可能同名也可能不同名）,并适当调整属性的次序 3.优化数据模型方法 确定数据依赖 对于各个关系模式之间的数据依赖进行极小化处理,消除冗余的联系. 确定各关系模式分别属于第几范式. 分析对于应用环境这些模式是否合适,确定是否要对它们进行合并或分解. 对关系模式进行必要的分解或合并 4.设计用户子模式 使用更符合用户习惯的别名 针对不同级别的用户定义不同的外模式,以满足系统对安全性的要求. 简化用户对系统的使用 5.任务 将概念结构转化为具体的数据模型 逻辑结构设计的步骤 将概念结构转化为一般的关系、网状、层次模型 将转化来的关系、网状、层次模型向特定DBMS支持下的数据模型转换 对数据模型进行优化 设计用户子模式 6.逻辑结构设计时3个步骤 2.5 数据库物理设计1.步骤 确定数据库的物理结构,在关系数据库中主要指存取方法和存储结构 对物理结构进行评价,评价的重点是时间和空间效率 2.索引存取 选择索引存取方法的一般规则: 如果一个(一组)属性经常在查询条件中出现,则考虑在这个(这组)属性上建立索引(组合索引) 如果一个属性经常作为最大值和最小值等聚集函数的参数,则考虑在这个属性上建立索引 如果一个(或一组)属性经常在连接操作的连接条件中出现,则考虑在这个(或这组)属性上建立索引 关系上定义的索引数过多会带来较多的额外开销: 维护索引的开销 查找索引的开销 3.聚簇用途: 大大提高按聚簇码进行查询的效率 节省存储空间 局限性: 聚簇只能提高某些特定应用的性能 建立与维护聚簇的开销相当大 适用范围: 既适用于单个关系独立聚簇,也适用于多个关系组合聚簇 当通过聚簇码进行访问或连接是该关系的主要应用,与聚簇码无关的其他访问很少或者是次要的时,可以使用聚簇 2.6 数据库实施2.7 数据库运行和维护Part3 数据查询、数据更新、触发器3.1 数据查询0.select1.SELECT column_name,column_nameFROM table_name; 选取特定列 2.SELECT * FROM table_name; 选取所有列 3.distinct关键词：返回唯一不同值（筛掉重复值，只列出不同的值） SELECT DISTINCT column_name,column_nameFROM table_name; 1.聚集函数COUNT(列个数或元组个数) SUM(列值总和) AVG(列平均值) + ([DISTINCT|ALL] &lt;列名&gt;) MAX(列中最大值) MIN(列中最小值) VARIANCE(列的标准方差) STDDEV(列的标准差) 2.where语句where where &lt;属性列名&gt; [not] between A and B where &lt;属性列名&gt;[not] in … (SELECT语句)/(&lt;值1&gt;)[,&lt;值2&gt;]……..) where &lt;属性列名&gt; [NOT]LIKE &lt;匹配串&gt; where &lt;属性列名&gt; IS [NOT] NULL [NOT] EXISTS (SELECT语句) &lt;条件表达式&gt; AND / OR &lt;条件表达式&gt; [ AND / OR &lt;条件表达式&gt;]…….. 3.ORDER BY 子句:ORDER BY 关键字用于对结果集按照一个列或者多个列进行排序。ORDER BY 关键字默认按照升序对记录进行排序。如果需要按照降序对记录进行排序，您可以使用 DESC 关键字。 说明: 升序：ASC;(缺省值) 降序：DESC; 当排序列含空值时: ASC：排序列为空值的元组最后显示 DESC：排序列为空值的元组最先显示 4.GROUP BY 子句细化聚集函数的作用对象 5.(not) exists带有 EXISTS 谓词的子查询不返回任何数据,只产生逻辑真值 “true” 或逻辑假值 “false” eg.查询没有选修1号课程的学生姓名. SELECT SnameFROM StudentWHERE NOT EXISTS(SELECT *FROM SCWHERE Sno = Student.Sno AND Cno=’1’); 6.连接查询同时涉及多表的查询。 格式: [&lt;表名1&gt;.]&lt;列名1&gt; &lt;比较运算符&gt; [&lt;表名2&gt;.]&lt;列名2&gt; [&lt;表名1&gt;.]&lt;列名1&gt; BETWEEN [&lt;表名2&gt;.]&lt;列名2&gt; AND [&lt;表名2&gt;.]&lt;列名3&gt; 等值连接： =eg.查询每个学生及其选修课程情况： select student., SC.from student, SCwhere student.sno = SC.sno 自然连接说明:在等值连接中把目标列中重复的属性列去掉 自身连接说明:一个表与其自己进行连接 要求:需要给表起别名以示区别,由于所有属性名都是同名属性,因此必须使用别名前缀 例子:查询每一门课的间接先修课（即先修课的先修课） SELECT FIRST.Cno,SECOND.CpnoFROM Course FIRST,Course SECONDWHERE FIRST.Cpno = SECOND.Cno; 复合条件连接说明:WHERE子 句中含多个连接条件 例子:查询选修2号课程且成绩在90分以上的所有学生的学号和姓名 SELECT Student.Sno, SnameFROM Student, SCWHERE Student.Sno = SC.Sno/ 连接谓词/AND SC.Cno=’2’AND SC.Grade &gt; 90;/ 其他限定条件 / 嵌套查询概述: 一个 SELECT-FROM-WHERE 语句称为一个查询块,将一个查询块嵌套在另一个查询块的 WHERE 子句或 HAVING 短语的条件中的查询称为嵌套查询 上层查询快称为外层查询或父查询,下层查询块称为内层查询或子查询 1.不相关子查询 要求: 子查询的查询条件不依赖于父查询 实现: 由里向外逐层处理.即每个子查询在上一级查询处理之前求解,子查询的结果用于建立其父查询的查找条件 eg.查询选修了2号课程的所有学生姓名 select snamefrom studentwhere sno in (select snofrom SCwhere cno = ‘2’); 2.相关子查询: 要求:子查询的查询条件依赖于父查询 实现: 首先取外层查询中表的第一个元组,根据它与内层查询相关的属性值处理内层查询,若WHERE子句返回值为真,则取此元组放入结果表,然后再取外层表的下一个元组,重复这一过程,直至外层表全部检查完为止 例子:找出每个学生超过他选修课程平均成绩的课程号 SELECT Sno, CnoFROM SC xWHERE Grade &gt;=(SELECT AVG(Grade)FROM SC yWHERE y.Sno=x.Sno); 3.2 数据更新数据插入1.插入元组 insert into &lt;表名&gt; [(&lt;属性列1&gt;[,&lt;属性列2 &gt;]…)]VALUES (&lt;常量1&gt; [,&lt;常量2&gt;] … ) eg.将一个新学生元组（学号：200215128;姓名：陈冬;性别：男;所在系：IS;年龄：18岁）插入到Student表中 insert into student (‘xxxx’,’xxxx’,’xxx’,18) 2.插入子查询结果: INSERT INTO &lt;表名&gt; [(&lt;属性列1&gt;[,&lt;属性列2 &gt;]…)]子查询 eg. insert into Dept_age(Sdept, Avg_age)select Sdept, AVG(Sage)from Studentgroup by Sdept; 数据更改UPDATE &lt;表名&gt; SET &lt;列名&gt;=&lt;表达式&gt;[,&lt;列名&gt;=&lt;表达式&gt;]… [WHERE &lt;条件&gt;]; eg.将学生 200215121 的年龄改为22岁(修改某一个元组的值) UPDATE StudentSET Sage=22WHERE Sno=’ 200215121 ‘; 将所有学生的年龄增加1岁(修改多个元组的值) UPDATE StudentSET Sage= Sage+1; 将计算机科学系全体学生的成绩置零(修改多个元组的值) UPDATE SCSET Grade=0WHERE ‘CS’= (SELETE SdeptFROM StudentWHERE Student.Sno = SC.Sno); 数据删除DELETEFROM &lt;表名&gt;[WHERE &lt;条件&gt;]; eg.删除学号为 200215128 的学生记录(删除某一个元组的值) DELETEFROM StudentWHERE Sno=’200215128’; 删除所有的学生选课记录(删除多个元组的值) DELETEFROM SC; 删除计算机科学系所有学生的选课记录(带子查询的删除语句) DELETEFROM SCWHERE ‘CS’= (SELECT SdeptFROM StudentWHERE Student.Sno=SC.Sno); 3.3基本表，表完整性基本表1.定义基本表 create table &lt;表名&gt;（&lt;列名&gt; &lt;数据类型&gt;[ &lt;列级完整性约束条件&gt; ][,&lt;列名&gt; &lt;数据类型&gt;[ &lt;列级完整性约束条件&gt;] ] …[,&lt;表级完整性约束条件&gt; ] ）; 约束类型： Primary key 定义主键,保证主键列无重复值 实体完整性Unique 保证该列无重复值 实体完整性Foreign key 定义外键,保证数据表间数据的一致性 参照完整性Check 定义表中某些列的数据范围 自定义完整性Default 为列的数据提供默认值 自定义完整性 数据基本类型: 数据类型 含义 CHAR(n) 长度为n的定长字符串VARCHAR(n) 最大长度为n的变长字符串,实际存储有效长度INT 长整数（也可以写作 INTEGER）SMALLINT 短整数NUMERIC(p,d) 定点数,由p位数字（不包括符号、小数点）组成,小数后面有d位数字REAL 取决于机器精度的浮点数Double Precision 取决于机器精度的双精度浮点数FLOAT(n) 浮点数,精度至少为n位数字DATE 日期,包含年、月、日,格式为 YYYY-MM-DDTIME 时间,包含一日的时、分、秒,格式为 HH:MM:SS eg.建立一个学生选课表 CREATE TABLE SC(Sno CHAR(9),Cno CHAR(4),Grade SMALLINT,PRIMARY KEY (Sno,Cno),/ 主码由两个属性构成,必须作为表级完整性进行定义/FOREIGN KEY (Sno) REFERENCES Student(Sno),/ 表级完整性约束条件,Sno是外码,被参照表是Student /FOREIGN KEY (Cno) REFERENCES Course(Cno)/ 表级完整性约束条件, Cno是外码,被参照表是Course/); 2.修改基本表 ALTER TABLE &lt;表名&gt;[ ADD &lt;新列名&gt; &lt;数据类型&gt; [ 完整性约束 ] ][ DROP &lt;完整性约束名&gt; ][ ALTER COLUMN&lt;列名&gt; &lt;数据类型&gt; ][ADD [COLUMN&lt;约束名&gt; ] &lt;约束定义&gt; ]; eg.向Student表增加“入学时间”列,其数据类型为日期型 . ALTER TABLE StudentADD S_entrance DATE 3.删除基本表 DROP TABLE &lt;表名&gt;［RESTRICT| CASCADE］;(缺省情况是 RESTRICT) eg.删除Student表 DROP TABLE Student CASCADE ; 实体完整性定义为列级约束条件: CREATE TABLE Student(Sno CHAR(9) PRIMARY KEY,Sname CHAR(20) NOT NULL,Ssex CHAR(2) ,Sage SMALLINT,Sdept CHAR(20)); 定义为表级约束条件: CREATE TABLE Student(Sno CHAR(9),Sname CHAR(20) NOT NULL,Ssex CHAR(2) ,Sage SMALLINT,Sdept CHAR(20),PRIMARY KEY (Sno)); 参照完整性在 CREATE TABLE 中用 FOREIGN KEY 短语定义哪些列为外码用REFERENCES短语指明这些外码参照哪些表的主码 CREATE TABLE SC(Sno CHAR(9) NOT NULL,Cno CHAR(4) NOT NULL,Grade SMALLINT,PRIMARY KEY (Sno, Cno), /在表级定义实体完整性/FOREIGN KEY (Sno) REFERENCES Student(Sno),/在表级定义参照完整性/FOREIGN KEY (Cno) REFERENCES Course(Cno)/在表级定义参照完整性/); 3.4 视图特点: 是从一个或几个基本表（或视图）导出的表 只存放视图的定义,不存放视图对应的数据 基表中的数据发生变化,从视图中查询出的数据也随之改变 一个窗口,透过它可以看到数据库中自己感兴趣的数据及其变化 特殊用途: 定义基本表是,为了减少数据库中的沉余数据,表中只存放基本数据,由基本数据经过各种计算派生出的数据一般是不存储的,但由于视图中数据并不是实际存储,所以定义视图时可以根据应用的需要,设置一些派生属性列.这些派生属性由于在基本表中并不实际存在也称它们为虚拟列.带虚拟列的视图也称为带表达式的视图 作用: 视图能够简化用户的操作 视图使用户能以多种角度看待同一数据 视图对重构数据库提供了一定程度的逻辑独立性 视图能够对机密数据提供安全保护 适当的利用视图可以更清晰的表达查询 基于视图的操作: 查询 删除 受限更新 定义基于该视图的新视图 建立视图create view&lt;视图名&gt; [(&lt;列名&gt; [,&lt;列名&gt;]…)]AS &lt;子查询&gt;[WITH CHECK OPTION]; eg.建立信息系学生的视图,并要求进行修改和插入操作时仍需保证该视图只有信息系的学生 CREATE VIEW IS_StudentASSELECT Sno,Sname,SageFROM StudentWHERE Sdept= ‘IS’WITH CHECK OPTION; 基于视图的视图 (建立信息系选修了1号课程且成绩在90分以上的学生的视图) CREATE VIEW IS_S2ASSELECT Sno,Sname,GradeFROM IS_S1WHERE Grade&gt;=90; 删除视图DROP VIEW &lt;视图名&gt;[CASCADE]; 该语句从数据字典中删除指定的视图定义,如果该视图上还导出了其他视图,使用CASCADE级联删除语句,把该视图和由它导出的所有视图一起删除 删除基表时,由该基表导出的所有视图定义都必须显式地使用DROP VIEW语句删除视图查询: 视图查询更新视图限制: 一些视图是不可更新的,因为对这些视图的更新不能唯一地有意义地转换成对相应基本表的更新 允许对行列子集视图进行更新 对其他类型视图的更新不同系统有不同限制 若视图是由两个以上基本表导出的,则此视图不允许更新 若视图的字段来自字段表达式或常数,则不允许对此视图执行INSERT和UPDATE操作,但是允许执行DELETE操作, 若视图的字段来自聚集函数,则此视图不允许更新 若视图定义中含有GROUP BY子句,则此视图不允许更新 若视图定义中含有DISTINCT,则此视图不允许更新 Part4 数据库安全性1.概述数据库的安全性是指保护数据库以防止不合法的使用所造成的数据泄漏、更改或破坏。 数据库的不安全因素 非授权用户对数据库的恶意存取和破坏 数据库中重要或敏感的数据被泄露 安全环境的脆弱性 2.数据库安全性措施 数据库安全性控制的常用方法 用户标识和鉴定 存取控制 视图 审计 密码存储 3.用户身份鉴别用户标识：用户名，用户标识号 鉴别方法： 静态口令鉴别 动态口令 生物特征 智能卡 4.存取控制存取控制机制的组成： 定义用户权限 合法权限检查 常用存取控制方法： 自主存取控制：通过SQL的GRANT 和 REVOKE语句实现 强制存取控制 自主存取控制 GRANT 1234567891011121314GRANT &lt;权限&gt;[,&lt;权限&gt;]... ON &lt;对象类型&gt; &lt;对象名&gt;,&lt;对象类型&gt; &lt;对象名&gt;,...TO &lt;用户&gt;[,&lt;用户&gt;]...[WITH GRANT OPTION];GRANT SELECT /*把查询Student表的权限授给用户U1*/ON TABLE StudentTO U1;GRANT ALL PRIVILEGES /*把对Student表和Course表的全部操作权限授予所有用户*/ON TABLE Student,CourseTO PUBLIC;GRANT UPDATE(Sno),SELECT /*把查询Student表和修改学生学号的权限授给用户U4*/ON TABLE StudentTO U4; 2.REVOKE 123REVOKE &lt;权限&gt;[,&lt;权限&gt;]... ON &lt;对象类型&gt; &lt;对象名&gt;,&lt;对象类型&gt; &lt;对象名&gt;,...FROM &lt;用户&gt;[,&lt;用户&gt;]...; 强制存取控制自主：可能存在数据的无意泄露 强制：全部实体分为主体和客体两大类。 DAC+MAC安全性检查： 先进行自主存取控制检查，通过自主存取控制检查的数据对象再由系统进行强制存取控制检查，只有通过强制存取控制检查的数据对象方可存取。 5.视图机制把要保密的数据对无权存取这些数据的用户隐藏起来，对数据提供一定程度的安全保护 间接实现了支持存取谓词的用户权限定义 6.审计什么是审计 审计日志（Audit Log） 将用户对数据库的所有操作记录在上面 DBA利用审计日志，找出非法存取数据的人、时间和内容 C2以上安全级别的DBMS必须具有审计功能 AUDIT语句和NOAUDIT语句 AUDIT语句：设置审计功能 NOAUDIT语句：取消审计功能 1234AUDIT ALTER,UPDATE /*对修改SC表结构或修改SC表数据的操作进行审计*/ON SC;NOAUDIT ALTER,UPDATEON SC; 审计一般可以分为用户级审计和系统级审计 用户级审计 任何用户可设置的审计 主要是用户针对自己创建的数据库表和视图进行审计 系统级审计 只能由数据库管理员设置 监测成功或失败的登录要求、监测授权和收回操作以及其他数据库级权限下的操作 7.数据加密防止数据库中数据在存储和传输中失密的有效手段 根据一定的算法将原始数据—明文（Plain text）变换为不可直接识别的格式­—密文（Cipher text） 加密方法： 存储加密 传输加密 8.其他安全性保护推理控制 隐蔽信道 数据隐私保护 Part5 数据库恢复技术5.1 事务的基本概念事务是一系列的数据库操作，是数据库应用程序的基本逻辑单元。事务处理技术主要包括数据库恢复技术和并发控制技术。 事务是用户定义的一个数据库操作序列，这些操作要么全做，要么全不做，是一个不可分割的工作单位。 事务和程序比较 两个概念 在关系数据库中，一个事务可以是一条SQL语句、一组SQL语句或整个程序。 一个程序通常包含多个事务 显式定义： commit提交：提交事务的所有操作，将事务中所有对数据库的更新写回到磁盘上的物理数据库中去 ROLLBACK表示回滚，系统将事务中对数据库的所有已完成的操作全部撤销，回滚到事务开始时的状态 事务的ACID特性： 原子性（Atomicity）事务是数据库的逻辑工作单位，事务中包括的诸操作要么都做，要么都不做。 一致性（Consistency）事务执行的结果必须是使数据库从一个一致性状态变到另一个一致性状态。 隔离性（Isolation）一个事务的执行不能被其他事务干扰。 持续性（Durability ）一个事务一旦提交，它对数据库中数据的改变就应该是永久性的。 5.2 数据库恢复概述1.故障故障： 系统故障：软硬件故障 人为故障：操作失误，恶意破坏 故障种类： 事务内部故障：解决——撤销事务 系统故障：不破坏数据库；内存缓冲区的信息全部丢失；解决： 事务未提交：UNDO所有未完成事务（强行撤销） 事务已提交：REDO所有已提交事务（重做） 介质故障：外存故障；解决——装入故障前某时刻的数据副本/重做所有成功事务 病毒 2.恢复的实现技术恢复操作的基本原理：冗余 利用存储在系统其它地方的冗余数据来重建数据库中已被破坏或不正确的那部分数据 建立冗余数据最常用的技术是数据转储和登记日志文件。 数据转储转储是指DBA将整个数据库复制到磁带或另一个磁盘上保存起来的过程，备用的数据称为后备副本或后援副本。 静态转储：在系统中无运行事务时进行的转储操作 动态转储：转储操作与用户事务并发进行 登记日志文件日志文件格式： 以记录为单位的日志 以数据块为单位的日志 Referencekzangv-SQL笔记SQL学习笔记之基础操作Lewis’s blog-数据库系统概论]]></content>
      <categories>
        <category>Computer Science</category>
        <category>Database System</category>
      </categories>
      <tags>
        <tag>Database System</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机组成原理复习笔记2]]></title>
    <url>%2F2018%2F01%2F05%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B02%2F</url>
    <content type="text"><![CDATA[Abstract: 计算机组成原理期末复习知识点总结 ~ 1.基础概念硬件和软件等效原理： 任何可以利用软件实现的工作可以利用硬件来实现，反之，任何可以通过硬件来实现的事件也同样可以利用软件来实现。此原理说明，可以用不同的选择来实现相同的计算机功能 如对于微波炉的控制系统，一个简单的嵌入式系统会比一个复杂的计算机程序性能好的多。 计算机体系结构：硬件系统 + 指令集体系结构（ISA） ISA: 在机器上运行的所有软件和执行这些软件的硬件之间的协定接口，实现人机对话。 系统时钟： 系统时钟在每秒钟内发射的脉冲数目是时钟的频率，单位赫兹。 每条计算机指令的执行需要若干个固定的时钟周期，大多数指令需要的时间都多于一个时钟周期。 一个微处理器每秒钟实际执行的指令数目与微处理器的系统时钟的速度成正比。 摩尔定律：硅芯片的密度每18个月翻一番（—— Intel公司奠基人Gordon Moore）。 计算机分层组织结构：假设计算机按不同层次结构建造，每个层级具有某项特定功能并有一个特定的假想机器与之对应 第0层：数字逻辑电路——电子线路、逻辑门等 第1层：控制系统——微代码或硬导线连接 第2层：机器——指令集体系结构ISA 第3层：系统软件——操作系统、库代码 第4层：汇编语言——汇编语言代码 第5层：高级语言——C++，JAVA等 第6层：用户——执行的程序 取指-译码-执行周期：控制单元从存储器取指令——PC程序计数器决定指令所在位置——对指令译码以变成ALU可理解的语言——从存储器中取出执行指令所需的各种操作数数据并放入CPU的寄存器——ALU执行指令并将执行结果放入寄存器或存储器。 ALU：算术逻辑单元，中央处理器的执行单元。 系统总线模型： 数据总线：将数据从主存传输到CPU的寄存器 地址总线：保持数据总线正在访问的数据地址 控制总线：传输各种必要的控制信号，以指定信息传输发生的方式 2.数据表示方法位：1 bit，一个二进制数 字节：1 byte，2个或多个相邻字节构成 浮点运算： IEEE754：单精度标准；8位指数，23位有效数，偏移量127，当指数为255时表示正负无穷大或NaN 双精度标准采用11位指数和52位有效数字，偏移量1023，指数为2047时表示NaN 无论单双精度都有两种表示0的方法，当指数部分和有效数都为0时，无论正负符号位都是0。 3.错误检测与校正CRC循环冗余码校验 可以决定在一大块或者一长串信息字中是否出现一个错误，要检测的数据字块的规模越大，要求的校验和就越大，并且需要对求校验和的方法提供某种适当的保护，求校验和的方法以及CRC方法都是一种系统性的误差检测（systematic error detection）方案，即将错误校验位夹在原始信息数据位的后面 CRC采用模2算术，假设信息字节为I = 1001011（可以是任意大小的字节），发送器和接收器都对某个任意的二进制位组合模式达成协议，如P=1011（如该位组合模式的开始和结束位都是1则效果最好），记P的位数为n = 4，将I左移n-1位，并用新的I作为被除数，P作为除数进行模2除法，得到余数r，将余数r加到移位后的I上，组成要发送的信息M。在本例中，r = 100，I + r = M = 1001011100。 模2运算： 模2加法 模2减法 模2乘法：只有1x1 = 1，其他都=0 模2除法：0÷1＝0 ,1÷1＝1. 与算术除法类似，但每一位除的结果不影响其它位，即不向上一位借位，所以实际上就是异或。一直做到余数的位数小于除数时，该余数就是最终余数。 算术左移：末位补0；算术右移：首位补1 逻辑左移：末位补0；逻辑右移：首位补0 海明码 可校正错误 采用奇偶校验 普通的奇偶校验只能检错（且无法检测双位出错）无法纠错。通常使用在随机错误最可能发生的情形（假定每一位出错的几率都是固定的，与其他位的出错没有关联）。 海明编码的奇偶校验位（冗余位）根据信息字本身的位数决定。最后形成的编码字由m位信息字本身和r位校验位组成，满足m + r + 1 &lt;= 2^r。两个编码字之间不同的位的位置数目称为两个编码字的海明距离，对于一种编码方法中任意一对编码的最小海明距离（minimum Hamming distance），用D（min）表示。海明编码可以检测出D（min）-1个单位错误，能够纠正[（D（min）-1）/2]个错误。因此如果要纠正k个错误，最小海明距离必须大于2k+1。 创建海明编码的方法：首先根据公式确定编码所需的校验位数目r，算出编码字的位长度n = m + r，从右向左从1开始编号。位数是2的指数幂的位设置为奇偶校验位，其他位为数据位。对于各个编码位置，第b位编码由满足b = b1 + b2 + … + bj的奇偶校验位b1，b2，…，bj检测。 例如：对ASCII字符K编码，K为01001011，m=8，r=4。从1开始从右向左编号，第1，2，4，8位为奇偶校验位，1 = 1，2 = 2，3 = 2 + 1，4 = 4，5 = 4 + 1，6 = 4 + 2，……，10 = 8 + 2，11 = 8 + 2 + 1，12 = 8 + 4。因为第1、3、5、7、9、11位的求和表达式中含有1，所以第一位（最低位）的奇偶校验位将检测这几个位置的奇偶特性，同理，第2位对2、3、6、7、10、11位作用，第4位对4、5、6、7、12位作用，第8位对8、9、10、11、12作用。分别对各自对应的位数奇偶校验，产生编码字010011010110。假如在传递编码字的过程中发生了一个错误，如发生在第9位，则接收到的为010111010110。接受端可以发现第1位、第8位的奇偶校验位出错，而第2位、第4位没有出错。因此可以推测出出错的码位是1 + 8 = 9位。取反即可。海明编码在出错率非常低的正常情况非常有效，但如果发生成块的错误（相邻数据位），则无效。 4.布尔代数通用门电路： 与非门（可构建所有数字电路）和或非门 布尔表达式： 变量与算符组合 算符：AND，OR，NOT 德摩根律：~（x · y） = ~x + ~y 组合逻辑电路：可以用来构建包含基本布尔算符，输入输出的数字电路，组合逻辑的输出完全取决于给定的输入值。 数字集成电路： 特点：体积小，可靠性高，功耗低，集成度高 数字电路实现的逻辑功能都是以集成电路（IC）形式体现的 目前广泛采用CMOS电路和TTL电路两种类型； CMOS已成为主导技术并有可能取代TTL。二者相比，前者功耗小，集成度高，后者速度快，但集成度不如CMOS。 CMOS系列：金属氧化物半导体晶体管作为开关元件的门电路叫MOS电路 集成电路使用特性：负载能力；延迟特性；功耗特性；未使用的输入端引脚要接到一个固定的逻辑电平 5.计算机微观体系结构CPU 取指-译码-执行指令 = 数据通路 + 控制单元 数据通路datapath：由存储单元（寄存器）和算术逻辑单元（ALU）组成的网络，组件通过总线连接，利用时钟控制时间 控制单元control unit：负责对各种操作进行排序并保证各种正确的数据适时地出现在所需的地方。 寄存器register：位于CPU内；存储二进制数据； 通用大小：16bit，32bit，64bit 寄存器的编址与存储器不同，每一存储器字都有一个唯一的二进制地址，这些地址从0开始编码，而仅存次则由CPU内部控制单元进行编址。 ALU：在程序执行过程中执行逻辑和算术原酸 2个数据输入，1个数据输出 操作会影响状态寄存器（SR）的某些数据位的数值 控制器发出的信号控制ALU执行运算 控制单元：监视所有指令的执行和各种信息的传送过程 功能：从内存取指——对指令译码——确保数据适时出现在正确的地方，负责通知ALU应该使用哪个寄存器，执行哪些中断服务程序；使用PC寄存器来寻找下一条指令的位置，使用SR存放某些特殊操作的状态 总线Bus 一组导电线路的组合，一个共享公用的数据通道将系统内的各个子系统连接到一起，由多条线路构成，允许多位数据并行传递 在任何时刻只能有一个设备（如寄存器、ALU、内存或其他某个设备）使用总线 总线的速度受到总线长度和共享总线的设备数目的影响。 典型计算机总线包含数据总线、地址总线、控制总线和电源线：数据总线（data bus）传递必须在计算机不同位置之间移动的实际信息；控制总线（control line）提示哪个设备允许使用总线和使用的目的，也用来传递有关总线请求、中断和时钟同步信号的响应；地址总线（address line）指出数据读写位置。 各种信息的传递都发生在一个总线周期（bus cycle）内，总线周期是完成总线信息传送所需的时间脉冲间的时间间隔。- 同步（synchronous）总线由时钟控制，各种事件只有在时钟脉冲到来时才会发生。总线周期与时钟频率成反比，要通过时钟控制事件的发生，因此任何时钟脉冲产生的相位漂移（clock skew）都不能过大，即总线周期时间不能短于信息在总线上传输所需要的时间，因此总线长度对总线的时钟频率和周期时间有一定限制。 异步（asynchronous）总线负责协调计算机各种操作，采用握手协议强制实现计算机其他操作同步。 总线仲裁（bus arbitration）：对于配备不止有一个主控设备的系统需要主线仲裁，为主控设备制定优先级别，保证各个主控设备都有机会使用总线。 菊花链仲裁方式：使用一条“出让总线”的控制线将总线使用权依次由最高级别向最低级别传递，简单但不能保证仲裁公平性。 集中式平行仲裁方式：每个设备有一个总线请求控制线，通过一个总线仲裁器选择设备。会导致使用过程中出现瓶颈效应。 自选择的分配式仲裁方式：由设备自己决定那个设备具有使用的最高优先级。 冲突检测的分配式仲裁方式：每个设备都允许发出总线使用请求，如果有冲突则这些设备都必须重新发出另一个使用请求。以太网采用这种方式。 课本上只分2种：集中式仲裁 ，分散式仲裁 时钟用于对系统各个部件协调同步，CPU的每条指令执行都是使用固定的时钟脉冲数目。计算机采用时钟周期量度系统指令的性能。 多数计算机系统都是同步计算机： 计算机只有一个主控时钟信号，按照规定的时间间隔发生脉动，各个寄存器必须等待时钟脉冲发生跃变才能输入新的数据。 最小的时钟周期时间至少应大于数据从每组寄存器的输出到下一组寄存器的输入所需要的传递时间，即电路的最大传输延迟时间。可以通过在输出寄存器和对应的输入寄存器之间增加寄存器的方法减小传输延迟，增加额外的寄存器等价于增加该条指令所需的时钟周期的数目。一般的，乘法比加法操作、浮点运算比整数运算要更多的时钟周期。 某些总线结构有自己的时钟，总线时钟通常比CPU时钟慢，造成了系统的瓶颈问题。有时会为了达到目的超越某些技术限制，如超频（over-clocking）使部件超出规定给出的时间频率或总线速度上限，CPU是最流行的超频组件（许多部件都可以超频运行）。对系统总线的超频可以大幅度改善系统性能，但同样可能损害与总线相连的各种部件。 存储器组成和寻址方式可以把存储器设想成一个数据位的方阵，方阵的每一行的位长度通常是机器的字大小。物理上通过一个寄存器实现存储器方阵的一行的数据存储。**每个寄存器（存储单元）都有一个唯一的地址编号。 存储器地址几乎都为无符号整数，正常情况下（目前流行的大部分）存储器采用按字节编址（byte-addressable），即每个字节有一个唯一的地址。有的机器的字大小超过一个字节（32位系统的每个字都是4个字节），可以采用按字编址（word-addressable），每个机器字有一个自己唯一的地址。字是计算机指令使用的基本单位，即使是按字节编址的计算机也可以从内存中直接读出一个字或将一个字写入存储器**。 存储器（内存）由随机访问存储器（RAM）芯片构成，使用符号L×W（长×宽）表示，如4M×16存储器表示存储器有4M（2^22个字）长和16位宽（每个字都是16位）。要对该存储器编址需要2^22个不同的地址，即22位二进制数。主存储器使用的RAM芯片数目大于1,通常利用多块芯片拼接成一定要求的单一存储器模块。单一共享存储器模块可能引起存储器访问上的顺序问题，存储器交叉存储技术从多个存储器模块中分离存储单元，低位交叉存储使用地址的低位选择存储器组，把连续的存储器地址分配到不同的存储器模块中，高位交叉存储使用地址的高位选择存储器组，把地址直接分配给具有连续地址的存储器模块。 1M = 2^20 byte 中断和输入输出子系统输入/输出是各种外围设备和主存储器（内存）之间的数据交换。中断（interrput）是改变或中断系统正常流程的各种事件。 输入/输出设备通常不与CPU直接相连，而采用某种接口（interface）来处理数据交换，接口将信号转化为总线和外设都可以接受的形式。 CPU通过输入输出寄存器和外设交流，有两种工作方式： 1）内存映射的输入输出（memory-mapped I/O），接口中的寄存器地址就在内存地址的分配表中，此时CPU对I/O设备的访问和对内存的访问完全相同，速度很快但要占用系统内存空间； 2）指令实现的输入输出（instruction-based I/O），CPU有专门的指令实现输入输出操作。 中断： 多种原因可以触发中断：I/O请求，算术错误，算术下溢或上溢，硬件故障，用户定义的中断点（如程序调试），页面错误，非法指令等。-不同中断类型的中断处理方法不同。由用户或系统发出（启动）的中断请求可以是屏蔽（maskable）中断（可以被禁止或忽略）或非屏蔽（maskable）中断（高优先级别的中断，不能被禁止，必须响应）。 计算机中有三种类型的中断：由外部事件（输入/输出，电源掉电等） 产生的外部中断；由于程序中的一些异常（被0除，堆栈溢出，保护系统侵犯等）产生的内部中断以及执行程序中的某条指令（例如，要求程序的执行从一种运行环境如用户层，转到另一个运行环境如内核层等）所引起的软件中断。 MARIEMARIE包含一个实际的工作计算机具备的全部功能部件，包括存储器和CPU。 体系结构、寄存器和总线特点： 使用二进制数和补码表示法 按字编址 主存：容量4K字，16bit数据，16bit指令，4bit操作码，12bit地址 16bit累加器（AC），16bit指令寄存器（IR），16bit存储缓冲寄存器（MBR），12bit程序计数器（PC），12bit存储器地址寄存器（MAR），8bit输入寄存器，8bit输出寄存器 MARIE的7种寄存器 AC：累加器（accumalator）用来保持数据值，是通用寄存器。 MAR：存储器地址寄存器（memory address register），用来保持被引用数据存储器地址。 MBR：存储器缓冲寄存器（memory buffer register），用来保持刚从存储器中读取或者将要写入存储器的数据。 PC：程序计数器（program counter），用来保持程序将要执行的下一条指令的地址。 IR：指令寄存器（instruction register），用来保持要执行的下一条指令。 InREG：输入寄存器（input register），用来保持来自输入设备的数据。 OutREG：输出寄存器（output register），用来保持将要输出到输出设备的数据。 其中，MAR、MBR、PC和IR寄存器为专用寄存器，不能作除上述规定外的其他目的。另外有一个状态或标志寄存器（status），保持显示各种状态信息。 指令系统体系结构指令系统体系结构（instruction set architecture，ISA）规定了计算机可以执行的每条指令和其格式。 MARIE的每条指令由16位二进制数构成，最左边（12～15）4位组成操作码，右边12位（0～11）形成地址。 实际操作中，数据先从源寄存器送到总线，然后脱离总线到达目的寄存器。下面叙述中没有包括总线的传送过程。 Load X MAR&lt;——X，MBR&lt;——M[MAR]，AC&lt;——MBR Store X MAR&lt;——X，MBR&lt;——AC，M[MAR]&lt;——MBR Add X MAR&lt;——X，MBR&lt;——M[MAR]，AC&lt;——AC+MBR Subt X MAR&lt;——X，MBR&lt;——M[MAR]，AC&lt;——AC-MBR Input AC&lt;——InREG Output OutREG&lt;——AC Halt 无 Jump X PC&lt;——X 指令执行过程：取指-译码-执行 将PC中的内容复制到MAR：MAR&lt;——PC。 CPU转向主存储器，提取由MAR给出的地址单元中的指令，并将指令放入指令寄存器IR，同时PC自动加1（MARIE按字编址，PC加一实际效果是下一个字的地址占据PC寄存器，如果MARIE按字节编制，则PC需要增量2,因为每条16位指令占据两个字节宽度），此时PC指向下一条指令：IR&lt;——M[MAR]，PC&lt;——PC+1。 IR最右边的12位地址复制到MAR，对IR最左边4位译码：MAR&lt;——IR[11-0]，IR[15-12]。 若需要，则CPU使用MAR中的地址转向存储器提取数据，并将数据放入MAR（可能是AC）中，然后执行指令。 6.指令系统每条计算机指令均有一个操作码和0或多个操作数。前章的MARIE指令长度为16位，至多只有一个操作数。根据ISA的不同，指令使用的二进制位数也可能不同（16位，32位，64位），每条指令允许使用的操作数的个数可能不同，指令类型和指令处理的操作数的类型也可能不同。具体在特征上可能存在以下差别： 1）操作数在CPU中的存储方式（堆栈结构或寄存器）； 2）指令直接作用的操作数数目（常用的操作数个数为0，1，2，3）； 3）操作数的位置（寄存器-寄存器，寄存器-存储器，存储器-存储器）； 4）操作（操作类型，指令是否可以访问存储器）； 5）操作数的类型和长度。 衡量标准ISA效能的衡量因素： 程序执行指令时占用内存空间的大小 指令系统复杂程度，主要指指令执行所需的译码数量和指令所执行任务的复杂性 指令长度 指令系统中指令的总数目 注意： 指令越短越好，占用空间就越少，提取速度越快；但短指令会闲置指令数量和操作数大小及数量 固定长度的指令的译码容易但浪费空间，但固定长度的指令系统不表示必须使用固定数量的操作数，可以设计一个指令总长度固定的ISA，但可以允许其操作数域的位数根据需要改变，称为拓展操作码（expanding opcode） 存储器的组成形势会影响指令的格式。如果存储器为16或32位字，如果不是按字节编址则很难访问到一个单一字符，因此有些16/32/64位机器也是按照字节编址。 存在多种不同类型的寻址方法；字节存储的小端大端位序问题；ISA需要多少寄存器并如何组织这些寄存器。 小端和大端位序问题位端（endian）指的是计算机体系结构中的“位序”（byte order），即计算机存储一个多字节数据时，多个字节的排列方式。 当今所有的计算机体系结构都是按字节编址，对于存储一个2字节整数，将低位的字节首先存放到低位地址，高位字节存放到高位地址，此时较低地址的字节就是数据的低位，这种方式称为小端（little-endian）；对于低位地址存放高位数据的方式称为大端（big-endian）。 大端位序存储方式更自然，便于阅读16进制编写的程序端。可以通过检查最高位的符号位判读数字正负，而小端需要知道数值长度，再跳过中间字节找到最高位。大端位序的机器存储整数和字符串采用相同次序，在某些字符串操作时更快。大部分位图映射格式图像都采用“最高位在字符串左边”的变换方法，即对于像素大于一个字节的数据可以直接按照大端位序处理，因此对于小端位序在处理较大图形对象时可能会性能受限。当对采用例如赫夫曼和LZW这类编码的压缩数据译码时，若数据采用大端位序存储，则实际的编码字可以被当作进入到某个查询表中的一个索引使用。 小端位序在高精度算术运算上速度更快更方便。大部分采用大端位序的体系结构都不允许计算机字按照非字地址边界的方法写数据字，例如一个字由2或4字节组成，写数据字时必须从偶数编号的字节地址开始，浪费空间。小端机器允许进行奇数地址的读写。 计算机网络都采用大端位序的体系结构。如果小端位序的机器要将整数数据（如网络设备地址）传送到网络上，必须将数据转换成网络要求的字节次序，从网络上接收数据时也需要将这些数据转换成本地表示形式。BMP图形格式是小端位序，如果在大端位序机器上查看BMP图形必须先反转数据位序。Adobe Photoshop采用大端格式，GIF是小端格式，JPEG是大端格式，Macpaint使用大端格式，PC Paintbrush是小端格式，RTP使用小端位序，Sun光栅文件是大端格式。WAV、AVI、TIFF、XWD等同时支持两种格式。 堆栈和寄存器CPU数据存储方式：区分不同指令系统体系结构的最基本方法，包含堆栈体系结构，累加器体系结构，通用寄存器体系结构。 堆栈体系结构（stack architecture）的计算机使用一个堆栈来执行各种指令，指令的操作数隐含的存放在堆栈顶部。按照堆栈体系结构设计的机器通常具有好的编码密度和一个简单的表达式估值模型。但因为不能对堆栈进行随机访问，使得采用堆栈结构的机器很难产生高效率的编码。如果存储器速度快，使用堆栈体系结构较好。 累加器体系结构（Accumulator architecture）计算机，如MARIE，将其中一个操作数隐含在累加器中，最大限度地降低机器内部复杂性，并且允许使用非常短的指令。由于累加器只是临时存储，需要对存储器的访问非常频繁。 通用寄存器体系结构（general purpose register architecture）计算机，采用多个通用寄存器组，是当今计算机体系结构最广泛的模型。寄存器组的访问速度比存储器快得多，也方便编译器处理。由于硬件价格急剧下降，现在可以以较小成本增加大量数目的寄存器。如果存储器速度较慢，通常采用通用寄存器体系结构。但由于所有操作数必须加以命名，因此使用寄存器结构会产生较长指令，导致较长的取指和译码时间。对ISA设计人员来说，实现短指令是一个非常重要的目标。 存储器-存储器（memory-memory）体系结构可以有两个或三个操作数位于存储器内，允许有一条执行某种操作而不需要有任何操作数的指令存放在某个寄存器中。 寄存器-存储器（register-memory）体系结构采用混合方式，要求至少有一个操作数在寄存器中和一个操作数在存储器中。 装入-存储（load-store）体系结构需要在任何对数据的操作前把数据装入寄存器中。 Intel和Motorola的计算机属于寄存器-存储器体系结构，数字仪器公司的VAX计算机体系结构实行的是存储器-存储器操作，SPARC、MIPS、ALPHA和PowerPC都是装入-存储式体系结构计算机。 操作数的数目和指令长度MARIE采用固定长度的指令，包括4位操作码和12位操作数。在现在的计算机体系结构中，指令构成的格式有固定长度和可变长度两种。 固定长度（fixed length）：会浪费一定存储空间，但执行速度更快。在采用指令层次的流水线结构时性能更好。 可变长度（variable length）：译码复杂，但节省存储空间。 实际设计中，通常会采用两到三种不同的指令长度，这样可以有不同的位的指令组合形式，便于简化指令的区分和译码。指令的长度必须配合机器字的长度，如果指令的长度严格等于机器字的长度，则将指令存储到主存储器时，指令间的对齐问题就会非常完美。因为存储器要编址，因此采用实际机器字长度的1/4，1/2，2倍或3倍长度的指令会浪费不少的存储空间。可变长度的指令同样也会造成存储空间的浪费。 最常用的指令格式包括0，1，2或3个操作数。通常情况下，算术运算或逻辑运算需要2个操作数，如果将累加器作为一个隐藏的操作数，则两个操作数的操作可以按照一个操作数指令的方式执行。同理，使用一个堆栈结构可以允许有不带操作数的指令。 只有操作码 0地址操作码+1个地址通常只有一个存储器地址操作码+2个地址通常两个寄存器地址，或一个寄存器地址加上一个存储器地址操作码+3个地址通常是三个寄存器地址或寄存器和存储器的某种组合 指令类型 数据移动：不同数据移动指令，MOVE，ADD 算术运算：整数和浮点运算的各种指令 布尔逻辑运算：逻辑与非或、异或 位操作：移位和循环换位 输入输出：程序控制的I/O，中断驱动的I/O，直接存储器访问（DMA） 控制转移：分支转移（branch）、跳过（skip）、进程调用（procedure call） 专门用途：如字符串处理指令，高级语言支持指令，保护和标志位控制指令，高速缓存指令等 寻址方式址方式是指定指令中操作数位置的方法。实际操作数的位置称为操作数的有效地址。 立即寻址（immediate addressing）：指令中操作码后的数值会被立刻引用，例如Load 008会直接将数值8装入累加器AC中。不灵活。 直接寻址（direct addressing）：指令之直接指定要饮用的数值的存储器地址，如Load 008将存储器地址为008的存储单元中的数值作为操作数装入累加器AC。 寄存器寻址（register addressing）：采用寄存器而不是存储器指定操作数，指令的地址域包含一个寄存器的引用。 间接寻址（indirect addressing）：地址域中的二进制数指定一个存储器地址，将该地址中的内容作为一个指针。例如Load 008，该操作表示在存储器地址为008的存储单元中存放的数值实际上是要用到的操作数的有效地址。如果008单元存放的数值是2A0，则2A0是真实地址，操作将地址为2A0的存储器单元中的内容装入AC。间接寻址也可用做寄存器。 变址寻址和基址寻址（indexed addressing）：一个变址寄存器用于存储一个偏移量，将这个偏移量与操作数相加，产生实际的有效地址。如Load X中的操作数X采用变址寻址方式，假定R1为变址寄存器，如果R1中存放着1，则Load X寻找到的有效地址是X+1。基址寻址与变址寻址类似，但基址寻址使用基地址寄存器而不是变址寄存器，操作数域中的内容表示的是偏移量。这两种寻址方式在访问数组元素和字符串中字符时非常有效，大部分汇编语言都提供专门的变址寄存器。 堆栈寻址（stack addressing）：操作数假定放在堆栈中。其他寻址方式：间接变址寻址（同时采用变址和间接寻址），基址/偏移量寻址（先将一个偏移量加到某个特定的基址寄存器中，再于指定的操作数相加产生有效地址），自动增/减量寻址等。 下表对于不同寻址方式给出实际装入累加器AC的值。 寻址方式 获取操作数方法 装入AC的值立即寻址 操作数数值直接包含在指令中 800直接寻址 指令的地址域是操作数的有效地址 900间接寻址 地址域的内容是实际操作数的地址 1000变址寻址 地址域数值与寄存器中数值相加产生有效地址 700 ISA体系结构案例Intel体系结构：Intel使用的是小端、双地址的体系结构，采用可变长度指令系统，寄存器-存储器结构，操作数句长度可以是1，2，4字节。 MIPS体系结构：小端、按字编址、3地址、采用固定长度，是装入/存储式体系结构。MIPS限制只有固定长度的操作，操作数句必须具有相同的字节数。 7.存储系统存储器类型高速缓存Cache：小容量，高速度，高价格；在频繁存取数据的过程中充当一个缓冲器，高性能的高速缓存存储器会掩盖掉较慢速度的存储器系统的作用。使用SRAM。 随机存储器RAM： 主存储器（primary memory），在执行程序时用来存储程序或数据。RAM是易失性存储器，存储器系统掉点时RAM中信息会全部丢失。现代计算机系统通常采用静态随机存储器（SRAM）和动态随机存储器（DRAM）存储器芯片来构造大规模RAM存储器。 SRAM速度比DRAM更快，价格更高。但DRAM的存储密度更高（单块芯片存储更多的位数），消耗功耗更低，比SRAM产生的热量小得多。通常将两种技术组合，DRAM用作主存储器，SRAM用作高速缓存存储器。 只读存储器（read-only memory，ROM）：ROM为非易失性存储器，采用硬连线，可长久保持所存放的数据，也可应用于嵌入式系统、计算机外围设备等，如激光打印机采用ROM保存打印字符点阵。 存储器层次结构采用存储器的分层组织结构，使不同层次的存储器具有不同的访问速度和存储容量。存储器分层结构系统基本类型包括：寄存器、高速缓存、主存储器、辅助存储器（硬盘、可移动存储介质等）。 名词解释： 命中（hit）：CPU请求的数据驻留在要访问的存储器层中。通常只在存储器的较高层才关注命中率问题。 缺失（miss）：CPU请求的数据不在要访问的存储器层。 命中率（hit rate）：访问某个特定存储器层，CPU找到所需数据的百分比。 缺失率（miss rate）：1 - 命中率。 命中时间（hit time）：在某个特定的存储器层，CPU取得所请求信息所需要的时间。 缺失损失（miss penalty）：CPU处理一次缺失时间所需要的时间，包括利用新数据块取代上层某个数据块所需要的时间、所需数据传递给处理器需要的附加时间。通常处理一次缺失事件花费的时间要比命中事件更多。 注：对于任何给定数据，处理器将访问数据请求传送给存储器中速度最快、规模最小的最高层，通常是高速缓存而不是寄存器，因为寄存器有专用用途。 局部性： 时间局部性：最近访问过的内容在不久的将来可能再次被访问。 空间局部性（Spatial locality）：对存储器地址空间的访问形成团簇的集中倾向（如数组或循环操作）。 顺序局部性（Sequential locality）：访问存储器的指令倾向于按顺序执行。 总结：局部性原理使系统在任意给定时刻只需要访问整个存储空间中非常小的部分，而且存储在该位置的数值会被重复读取。将大量信息存储在巨大的低成本的存储器中，再将部分数据复制到容量小、速度快的高层存储器中，就可以获得与高层存储器几乎相同的访问速度。 高速缓存同计算机的高速缓存容量有较大差别，通常PC机的L2大小为256或512KB，位于CPU和主存储器之间。L1为32KB（以i74790为例），集成在CPU中，分数据缓存和指令缓存。高速缓存存储器不通过地址访问，而是按照内容进行存取，因此又称按内容寻址的存储器（content addressable memory，CAM）。 映射模式和数据访问过程 主存储器块远多于高速缓存块，主存储器块需要竞争才能获取高速缓存中的对应位置。通过对主存储器地址的各个位划分并规定特殊意义来实现地址转换，将地址的二进制位分为不同的组（2～3个地址域）。主要有直接映射、全关联、组关联等。 数据访问过程：主存和高速缓存的存储空间都会被划分成相同大小的字块，当生成一个存储器地址时，CPU首先搜索高速缓存存储器判断需求的数据块是否存在，找不到则将主存储器中该字所在的整个块装入高速缓存。 cache的地址映射 全相联映射：内存 + cache = 主存 主存中任一块都可以映射到Cache中任一块；例如，设Cache共有2C块，主存共有2M块，当主存的某一块j需调进Cache中时，它可以存入Cache的块0、块1、…、块i、… 或块2C -1的任意一块上。 在全相联映射下，CPU访主存地址如图。M——主存块号，W——块内地址。C——Cache块号。 当 一个主存块调入Cache中时，会同时在一个存储主存块号和Cache块号映射表的相联存储器中进行登记。CPU访存时，首先，根据主存地址中的主存块号 M在相联存储器中查找Cache块号，若找到，则本次访Cache命中，于是将对应的Cache块号取出，并送访Cache地址的块号C字段；紧接着将主 存地址的块内字号W直接送Cache地址的块内字号W字段，从而形成一个访Cache的地址；最后根据该地址完成对Cache单元的访问. 优点是Cache的空间利用率高，但缺点是相联存储器庞大，比较电路复杂，因此只适合于小容量的Cache之用。 直接映射：主存按Cache容量分区 直接相联映射方式是指主存的某块j只能映射到满足如下特定关系的Cache块i中：如i＝j mod 2C T ——标志号，C——Cache块号，W——块内地址。一般来讲，主存的块数是Cache的块数的整数倍，也就是说主存的块数2M和Cache的块数2C满足关系式：2M＝n·2C 当一个主存块调入 Cache中时，会同时将主存地址的T标志存入Cache块的标志字段中。当CPU送来一个访存地址时，首先，根据该主存地址的C字段找到Cache的相 应块，然后将该块标志字段中存放的标志与主存地址的T标志进行比较，若相符，说明主存的块目前已调入该Cache块中，则命中，于是使用主存地址的W字段 访问该Cache块的相应字单元；若不相符，则未命中，于是使用主存地址直接访主存。 直接相联映射方式的优点 是比较电路最简单，但缺点是Cache块冲突率较高，从而降低了Cache的利用率。由于主存的每一块只能映射到Cache的一个特定块上，当主存的某块 需调入Cache时，如果对应的Cache特定块已被占用，而Cache中的其它块即使空闲，主存的块也只能通过替换的方式调入特定块的位置，不能放置到 其它块的位置上。 全相联映射方式来说为优点的恰是直接相联映射方式的缺点，而对于全相联映射方式来说为缺点的 恰是直接相联映射方式的优点。 组相联高速缓存：组内全相联，组间直接相联 将Cache分成2u组，每组包含2v块。主存的块与Cache的组之间采用直接相联映射，而与组内的各块则采用全相联映射。也就是说，主存的某块只能映射到Cache的特定组中的任意一块。 主存的某块j与Cache的组k之间满足如下关系：k＝j mod 2u 设主存共有2s×2u块（即M＝s+u），则它们的映射关系如下图： 主存的块0、2u、2u＋1、…、(2s-1)2u可以映射到Cache的第0组的任意一块，主存的块1、2u+1、2u＋1+1、…、(2s-1)2u+1可以映射到Cache的第1组的任意一块，… … ，主存的块2u-1、2u＋1-1、…、2M-1可以映射到Cache的第2u-1组的任意一块。 对主存地址的划分：主存组号（f）+组内块号（r）+组内地址（w） 其实，全相联映射和直接相联映射可以看成是组相联映射的两个极端情况。若u＝0，v＝C，则Cache只包含1组，此即全相联映射方式；若u＝C，v＝0，则组内的块数等于1，此即直接相联映射。 在实际应用中，相联映射方式每组的块数一般取值较小，典型值为2、4、8、16等，分别称为两路组相联、四路组相 联等。 置换策略最佳算法的目标是替换掉在未来最长时间段内不再使用的高速缓存块。 LRU（least recently used）算法：最近最少使用； 思想：如果数据最近被访问过，那么将来被访问的几率也很高 标准：根据使用时间差异 实现：链表 保护新数据 O(n) 1.新数据插入到链表头部； 每当缓存命中（即缓存数据被访问），则将数据移到链表头部； 当链表满的时候，将链表尾部的数据丢弃。 FIFO：First In First Out，先进先出； 类似队列 新访问的数据插入FIFO队列尾部，数据在FIFO队列中顺序移动；淘汰FIFO队列头部的数据； 如果一个数据最先进入缓存中，则应该最早淘汰掉。 LFU：Least Frequently Used，最不经常使用 根据使用次数差异 如果数据过去被访问多次，那么将来被访问的频率也更高 根据在一段时间里数据项被使用的次数选择出最少使用的数据项 对新数据保护不够 O(n) 新加入数据插入到队列尾部（因为引用计数为1）； 队列中的数据被访问后，引用计数增加，队列重新排序； 当需要淘汰数据时，将已经排序的列表最后的数据块删除。 写策略 写通：在每次对高速缓存的写操作时，处理器同步更新主存储器中对应的数据块。写通策略速度比回写速度满，但可以保证高速缓存与主存储器的数据始终一致。实际应用中，因为大多数存储器访问都是读操作，因此可以忽略写通策略对主存储器的写操作带来的系统速度减慢。 回写策略指只有某个高速缓存块被选择为牺牲块而必须从高速缓存移出时，处理器才更新主存储器中对应的数据块。缺点在于两者数值的不同步，如果某个进程在回写主存储器之前发生中断（或崩溃），则高速缓存中数据可能丢失。 8.输入输出协议：握手协议 在发送设备和接收设备之间交换的各种信号的具体形式和信号所代表的意义，包括命令信号、状态信号、数据传递信号。 接收设备对命令和发送来的数据做出应答的协议交换称为握手 I/O 控制方法1.程序控制IO 轮询：系统为每个I/O设备至少分配一个专用的寄存器，CPU持续不断的监视每个寄存器，等待数据到达；一旦CPU检测到某个“数据就绪”的条件，就为该寄存器准备指令执行等操作。 优点在于：可以通过编程控制每个外部设备的行为，改变程序就可以调整系统所控制的外部设备的数目和类型，以及轮询的权限和时间间隔。 缺点在于：不断对寄存器轮询使得CPU持续处于繁忙等待循环中，直到开始服务某个I/O请求。 如果没有任何I/O任务要处理，CPU就无法从事任何有用的操作。因此程序控制的I/O最适合用于自动提款机等一些用来控制或监视外部事件的系统 2.中断控制IO 在有数据发送需求时由外部设备通知CPU，如果没有外部设备发出服务请求来中断CPU，则CPU继续执行其他任务。 通常使用CPU的标志寄存器中的一个二进制位表示中断信号，该位称为中断标志。一旦中断标志置位，操作系统就会中断正在执行的程序，并保存该程序的状态和各种可变的信息，提取请求中断的I/O设备的地址矢量。在完成I/O操作后CPU会完全恢复到中断前状态并继续执行。 该方法和程序控制I/O的相似之处为：都可以对I/O服务程序进行修改以适应外部硬件的改变。许多主流操作系统均使用中断控制的I/O，为防止病毒制造者修改I/O设备地址矢量指向恶意代码，操作系统均提供了保护机制防止这类操作。 3.直接存储器存取（DMA）： 无论何种中断控制的I/O，CPU都需要从I/O设备移入和移出数据。 DMA方法是面向数据块的I/O处理方式，只在一组字节的传输结束后才中断CPU。当DMA发出I/O完成的信号后，CPU会给出下一个要读取或写入的内存地址，而传输失败时，CPU会独自做出适当的相应，因此DMA的I/O需要很少的CPU参与。 程序控制的I/O每次传输一个字节，中断控制的I/O每次可以按字节或小数据块形式传输，具体取决于I/O设备。 磁盘技术磁盘驱动器：RAM]]></content>
      <categories>
        <category>Computer Science</category>
        <category>Basis of Computer Engineering</category>
      </categories>
      <tags>
        <tag>Computer Science</tag>
        <tag>Basis of Computer Engineering</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机组成原理复习笔记1]]></title>
    <url>%2F2018%2F01%2F04%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B01%2F</url>
    <content type="text"><![CDATA[Abstract: 计算机组成原理期末复习知识点总结 （一版）~ 1.计算机概述基本组成： 存储器: 实现记忆功能的部件用来存放计算程序及参与运算的各种数据 运算器: 负责数据的算术运算和逻辑运算即数据的加工处理 控制器: 负责对程序规定的控制信息进行分析,控制并协调输入,输出操作或内存访问 输入设备:实现计算程序和原始数据的输入 输出设备:实现计算结果输出 计算机的工作过程: 用户打开程序 系统把程序代码段和数据段送入计算机的内存 取指令：控制器从存储器中取指令 执行指令：控制器分析,执行指令,为取下一条指令做准备 取下一条指令,分析执行,如此重复操作,直至执行完程序中全部指令,便可获得全部指令 冯·诺依曼机制: 程序存储 采用2进制 计算机系统体系结构： 内存储器： RAM:随机存储器 ROM：只读型存储器 2.数据概述数据信息的两种基本方法: 按值表示: 要求在选定的进位制中正确表示出数值，包括数字符号，小数点正负号 按形表示: 按一定的编码方法表示数据 信息的存储单位: 1KB=2^10B=1024Byte 1MB=2^20B=1024KB 1GB=2^30B=1o24MB 1TB=2^40B=1024GB 1024：byte——kb——MB——GB——TB 浮点表示法 R进制表示法: 进位制 二进制 八进制 十进制 十六进制 规则 逢二进一 逢八进一 逢十进一 逢十六进一基数 R=2 R=8 R=10 R=16数码 0、1 0…7 0…9 0…F权 2^i 8^i 10^i 16^i形式表示 B Q D H 十进制与R进制转换: 十进制转R进制: 整数的转化: “采用除R取余法”，从最后一次除得余数读取. 小数部分的转化:“采用乘R取整数”将所得小数从第一次乘得整数读起，就是这个十进制小数所对应的R进制小数（直到小数部分为0；结果顺取） R进制转十进制:使用权相加，即将各位进制数码与它对应的权相乘，其积相加，和数即为该R进制数相对应的十进制数 二进制，八进制，十六进制转化: （二进制 八进制）“三位并一位”（八进制 二进制）“一位拆三位”（二进制 十六进制）“四位并一位”（十六进制 二进制）“一位拆四位”（十六进制 八进制）“一位拆两位”（八进制 十六进制）“二位并一位” 原码,反码,补码,BCD码: 二进制的原码,反码及补码: 真值: 一个数的正号用“+”表示,负号用“—”表示,即为该数真值 机器数: 以0表示正数的符号,用1表示负数的符号,并且每一位数值也用0,1表示,这样的数叫机器数也叫机器码 原码: 数的原码表示在机器中用符号位的0和1表示数的正负号,而其余表示其数本身 反码: 正数：反码=原码 负数:反码 = 原码的符号位不变数值各位取反即0变1,1变0 补码: 正数:其补码与原码相同 负数:原码的符号位不变,数值各位取反,末尾加1 BCD码: (二→十进制) 用思维二进制代码对一位十进制数进行编码例：(931)10=(1001 0011 0001)2 运算公式: 【X】补+【Y】补=【X+Y】补 【X-Y】补=【X+(-Y)】补=【X】补+【-Y】补 逻辑运算: 定义: 实现了逻辑变量之间的运算 分类:逻辑加法 (‘或’运算)；逻辑乘法 (‘与’运算)；逻辑否定 (‘非’运算) 异或运算：相同为0，不同为1 逻辑代数常用公式 0-1律: A+0=A; A*0=0 重叠律: A+1=1; A1=A; A+A=1; AA=A 互补律: A*(!A)=0; A+(!A)=1 又拾律: !(!A)=A 交换律: A+B=B+A; AB=BA 结合律: A+(B+C)=(A+B)+C; A(BC)=(AB)C 分配率: A(B+C)=AB+AC; A+(BC)=(A+B)*(A+C) 摩尔定律: !(A+B)=(!A)(!B); !(AB)=(!A)+(!B) 3.总线定义: 连接计算机各部件之间或各计算机直接的一束公共信息线,它是计算机中传送信息代码的公共途径 特点: 同一组总线在同一时刻只能接受一个发送源,否则会发生冲突 信息的发送则可同时发送给一个或多个目的地 分类: 传送分类 串行总线 二进制各位在一条线上是一位一位传送的 并行总线 一次能同时传送多个二进制位数的总线 信息分类 数据总线 在中央处理器与内存或I/0设备之间传送数据 地址总线 用来传送单元或I/O设备接口信息 控制总线 负责在中央处理器或内存或外设之间传送信息 对象位置分类 片内总线 指计算机各芯片内部传送信息的通道 外部总线 微机和外部设备之间总线用了插件板一级互连 系统总线 微机中各插件与系统板 总线标准依据:物理尺寸,引线数组,信号含义,功能和时序,工作频率,总线协议。 4.中央处理器运算器组成: 算术逻辑单元(ALU) 通用寄存器组(R1 ~Rn) 多路选择器(Mn) 标志寄存器(FR) 控制器组成: 时标发生器(TGU) 主脉冲振荡器(MF) 地址形成器(AGU) 程序计数器(PC) 指令寄存器(IR) 指令译码器(ID) 总线: 数据总线(DBUS) 地址总线(ABUS) 控制总线(CBUS) CPU主要性能指标: 主频:CPU内部工作的时钟频率,是CPU运算时工作频率 外频:主板上提供一个基准节拍供各部件使用,主板提供的节拍成为外频 信频:CPU作频率以外频的若干倍工作,CPU主频是外频的倍数成为CPU的信频,这CPU工作频率=信频*外频 基本字长:CPU一次处理的二进制数的位数 地址总线宽度:地址总线宽度(地址总线的位数)决定了CPU可以访问的存储器的容量,不同型号的CPU总线宽度不同,因而使用的内存的最大容量也不一样 数据总线宽度:数据总线宽度决定了CPU与内存输入∕输出设备之间一次 数据传输的信息量 5.存储器定义: 计算机存储是存放数据和程序的设备 分类: 主存储器:也称内存,存储直接与CPU交换信息,由半导体存储器组成 辅助存储器: 也称外存,存放当前不立即使用的信息,它与主存储器批量交换信息,由磁带机,磁带盘及光盘组成 主存: 功能: 主存储器是能由CPU直接编写程序访问的存储器,它存放需要执行的程序与需要处理的数据,只能临时存放数据，不能长久保存数据 组成: 存储体(MPS): 由存储单元组成（每个单元包含若干个储存元件,每个元件可存一位二进制数）且每个单元有一个编号,称为存储单元地址（地址）,通常一个存储单元由8个存储元件组成 地址寄存器(MAR): 由若干个触发器组成,用来存放访问寄存器的地址,且地址寄存器长度与寄存器容量相匹配（即容量为1K,长度无2^10=1K） 地址译码器和驱动器 数据寄存器(MDR): 数据寄存器由若干个触发器组成,用来存放存储单元中读出的数据,或暂时存放从数据总线来的即将写入存储单元的数据【数据存储器的宽度（w）应与存储单元长度相匹配】 主要技术指标: 存储容量: 一般指存储体所包含的存储单元数量（N） 存取时间(TA): 指存储器从接受命令到读出∕写入数据并稳定在数据寄存器（MDP）输出端 存储周期(TMC): 两次独立的存取操作之间所需的最短时间,通常TMC比TA长 存取速率: 单位时间内主存与外部（如CPU）之间交换信息的总位数 可靠性: 用平均故障间隔时间MTBF来描述,即两次故障之间的平均时间间隔 高速缓冲存储器Cache： 定义：由存取速率较快的电路组成小容量存储单元,即在内存的基础上,再增加一层称为高速缓冲存储器 特点: 比主存快5 ~10倍 虚拟存储器: 它是建立在主存-辅存物理结构基础之上,由附加硬件装置及操作系统存储管理软件组成的一种存储体系,它将主存与辅存的地址空间统一编址,形成一个庞大的存储空间,因为实“际上CPU只能执行调入主存的程序,所以这样的存储体系成为“虚拟存储器” ROM与RAM： RAM：随机存储器，可读出可写入，随机存取；断电；意味着存取任一单元所需的时间相同,当断电后,存储内容立即消失,称为易失性 ROM：只读存储器；一旦有了信息,信息不易改变,结构简单,所以密度比可读写存储器高,具有易失性；分类: 固定掩模型ROM（不能再修改） PROM可编程之读存储器（由用户写入,但只允许编程一次） EPROM可擦除可编程只读存储器(可用紫外线照射擦除里面内容) E2PROM电擦除可编程只读存储器（由电便可擦除里面内容） 辅存（硬盘）： 说明: 是以铝合金圆盘为基片,上下两面涂有磁性材料而制成的磁盘 优点: 体积小,重量轻,防尘性好,可靠性高,存储量大,存取速度快,但多数它们固定于主机箱内,故不便携带,价格也高于软盘 性能指标: 转速,超频性能,缓存,单碟容量,传输模式,发热量,容量,平均等待时间 在整颗磁碟的第一个磁区特别的重要,因为他记录了整颗磁碟的重要资讯.扇区(Sector)为最小的物理储存单位，每个扇区为 512 bytes；将扇区组成一个圆，那就是磁柱(Cylinder)，磁柱是分割槽(partition)的最小单位；第一个扇区最重要，里面有：(1)主要启动区(Master boot record, MBR)及分割表(partition table)， 其中 MBR 占有 446 bytes，而 partition table 则占有 64 bytes。 6.输入输出设备输入设备分类: 字符: 键盘 图形: 鼠标器 , 操纵杆 , 光笔 模拟: 语音 , 模数转化 图像: 摄影机 , 扫描仪 , 传真机 光学阅读: 光学标记阅读机 , 光学字符阅读机 键盘分类(以接口类型): PS∕2接口的 USB接口的 无线的 鼠标分类: PS∕2接口 , USB接口 ( 以接口类型 ) 机械式鼠标 , 光电式鼠标 ( 以内部构造 ) 两键鼠标 , 三键鼠标 ( 以按键数 ) 语音输入设备: 主要部分: 输入器 , 模数转换器 , 语音识别器 输出设备 打印机: 分类: 击打式打印机 原理: 利用机械动作打击‘字体’使色带和打印纸相撞 分类: 活字式打印 , 点阵式打印 特点: 结构简单，价格便宜 非击打式打印机 原理: 用各种物理或化学的方法印刷字符 分类: 激光打印机 , 喷墨式打印 特点: 速度快，质量高，无噪声，但价格高 主要性能指标: 分辨率 , 接口类型 , 打印速度 显示器: 显示器分辨率: 屏幕上光栅的行数和列数 分类: 阴极射线管显示器; 液晶显示器; 等离子显示器 主要技术指标: 像素 , 分辨率 , 屏幕尺寸 , 刷新频率 , 点距 , 像素色彩 输入输出设备接口和控制方式 输入输出设备接口: 数据传送: 串行口; 并行口; 程序型接口; DMA型接口 通用性: 通用接口; 专用接口 功能选择: 可编程接口; 不可编程接口 输入输出控制方式: 程序查询方式 : 中断控制方式: 直接存储器存取方式 输入输出处理机方式 7.计算机的时标系统时序控制方式: 同步控制方式: 定义 将操作时间划分为许多时钟周期,周期长度固定,每个时间周期完成一步操作,各页操作应在规定时钟周期内完成 优缺点 优点：时序关系比较简单,控制部件在结构上易于集中,设计方便 缺点：在时间安排利用上不经济 在同步控制方式中,都有统一的时钟信号,各种微操作都是在这一时钟信息的同步下完成的,称这一时钟信号为计算机主频,其周期称为时钟周期,称完成一个基本操作所需要的时间为机器周期 异步控制方式: 定义：各项操作按其需要选择不同的时间,不受统一时钟周期的约束,各步操作间的衔接与各部件之间信息交换,采取应答的方式 优缺点: 优点：时间紧凑,能按不同部件,设备实际需求分配时间 缺点：是实际异步应答所需控制比较复杂 三级时标系统: 指令周期3：完成一个指令 机器周期2：完成一个基本操作 时钟周期1：完成一步操作 指令周期公式: 指令周期 = 时钟周期组成一个机械周期所需T的个数组成一个指令周期所需M个数]]></content>
      <categories>
        <category>Computer Science</category>
        <category>Basis of Computer Engineering</category>
      </categories>
      <tags>
        <tag>Computer Science</tag>
        <tag>Basis of Computer Engineering</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[囚徒困境]]></title>
    <url>%2F2018%2F01%2F03%2F%E5%9B%9A%E5%BE%92%E5%9B%B0%E5%A2%83%2F</url>
    <content type="text"><![CDATA[Abstract：囚徒困境反映在信息无法互通下个人最佳选择可能并非团体的最佳选择。如：大家都XXX，我又能怎么办呢，跟上呗。 解释囚徒困境（prisoner’s dilemma）是博弈论中非零和博弈的代表性例子，反映个人最佳选择可能并非团体的最佳选择。即两个被捕的囚徒在信息无法互通的情况下，明明合作对双方都有利，却因考虑个人的最佳选择而很难保持合作，从而导致整体利益受损。 两个共谋犯罪的人被关入监狱，不能互相沟通情况。如果两个人都不揭发对方，则由于证据不确定，每个人都坐牢一年；若一人揭发，而另一人沉默，则揭发者因为立功而立即获释，沉默者因不合作而入狱十年；若互相揭发，则因证据确实，二者都判刑八年。由于囚徒无法信任对方，因此倾向于互相揭发，而不是同守沉默。最终导致纳什均衡仅落在非合作点上的博弈模型. 举例1.政治学：军备竞赛 在政治学中，两国之间的军备竞赛可以用囚徒困境来描述。两国都可以声称有两种选择：增加军备（背叛）、或是达成削减武器协议（合作）。两国都无法肯定对方会遵守协议，因此两国最终会倾向增加军备。似乎自相矛盾的是，虽然增加军备会是两国的“理性”行为，但结果却显得“非理性”（例如会对经济造成损坏等）。 2.“不要让孩子输在起跑线上” 3.课程成绩 老师说不强制写论文，但是写论文可以加分，即使大家都约定好不写论文，也是会出现叛徒为了加分写论文，最后的结果就是所有人都写论文了。]]></content>
      <categories>
        <category>Communication Science</category>
        <category>Medium Study</category>
      </categories>
      <tags>
        <tag>Medium Study</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[“中印边境事件”舆情分析报告]]></title>
    <url>%2F2018%2F01%2F02%2F%E2%80%9C%E4%B8%AD%E5%8D%B0%E8%BE%B9%E5%A2%83%E4%BA%8B%E4%BB%B6%E2%80%9D%E8%88%86%E6%83%85%E5%88%86%E6%9E%90%E6%8A%A5%E5%91%8A%2F</url>
    <content type="text"><![CDATA[摘要：北京时间6月18日印度边防人员非法越界进入中国洞朗地区，引发中印边境局势持续紧张。本报告围绕“中印边境事件”，对2017年6月18日到2017年8月28日期间，互联网上采集到的1522740条信息进行了全方位的全网舆情分析。主要分析全网舆情特点、各大媒体平台舆情特点、微博舆情和传播特点、事件整体舆情特点，并对政府的舆情控制给出建议。其中关键使用LSTM深度学习模型对微博内容进行情感分析，以及使用LDA主题概论模型对微博、今日头条、微信公众号和知乎的事件相关数据进行文本聚类与典型意见挖掘。 关键词：热点事件发现，中印边境对峙，LSTM模型，LDA主题模型，Word2vec词向量分析，情感极性分析，文本聚类，典型意见挖掘，微博传播分析，政府舆情控制 注：若文中图片无法查看，则可通过Public Opinion Analysis of 2017 China–India border standoff .pdf查看和下载文章 ~ 1 事件简介及走势1.1 事件简介2017中印军队洞朗对峙事件[1]是中华人民共和国与印度共和国的边防部队于2017年6月18日至8月28日在洞朗地区进行的军事对峙，起因是印度边防人员非法越界进入中国洞朗地区。 2017年6月16日，中方在洞朗地区施工时，遭到印军越线阻拦，印方公然派军队越过双方承认的边界线进入中国境内，严重损害中国领土主权。期间中印双方多次交涉无果，在国内和国际上都引发了广泛热议，相关舆情量在7月5日达到顶峰。8月28日印方将越界人员和设备全部撤回，至此中印边境冲突平息。 1.2 事件走势回顾 上图是利用LSI(Latent Semantic Index)文本相似度索引算法、基于微博数据得到的事件走势回顾图，以时间轴顺序完整展示“中印边境事件”从发生到平息的各个节点的典型微博及其相同文章数量。 2 爬取数据说明2.1 爬取数据说明我们爬取了2017年6月18日至8月28日期间新浪微博、知乎、微信公众号、今日头条这四大主流社交媒体平台上与“中印边境事件”相关的的数据，数据分类及条数见下表： 数据分类 数据条数 数据分类 数据条数 微博 63837 知乎评论 20033 微博评论 535050 知乎用户 30667 微博转发 322383 微信文章 557 微博用户 459301 头条文章 161 知乎提问 100 头条评论 43118 知乎回答 2439 头条回复 5171 知乎专栏 38 头条用户 39885 每一条数据都有独一无二的 id 字段，相关数据之间通过外键关联在一起，保持了数据之间的关联结构。 下面是对所有爬取数据文件的具体说明。 2.2 微博数据(1)微博用户 weibo_user.csv 字段 说明 id 用户 id url 用户主页url name 用户昵称 gender 用户性别，1～男，0～女，-1～未知 location 用户所在地 description 用户简介 verified_reason 认证信息，为空表示没有认证 follow_count 关注人数 follower_count 粉丝数 status_count 发表的微博数 (2)微博 weibo_post.csv 字段 说明 id 微博 id url 微博 url content 微博内容 time 微博发布时间，用 UTC 时间戳表示 author 发表这篇微博的用户的 id comments_count 微博评论数 reposts_count 微博转发 likes_count 微博点赞数 (3)微博评论 weibo_comment.csv 字段 说明 id 评论 id content 评论内容 user 发表评论的用户的 id time 评论发表时间，用 UTC 时间戳表示 like 点赞数 target_type 评论对象的类型，一种是 “weibo_post”，表示针对原微博的评论，一种是 “weibo_comment”，表示回复某一个评论的评论 target_id 如果 target_type 是 “weibo_post”，则表示微博 id，如果 target_type 是 “weibo_comment”，则表示评论 id (4)微博转发 weibo_repost.csv 字段 说明 id 转发 id content 转发时发表的内容 like 点赞数 user 转发微博的用户的 id time 转发时间，用 UTC 时间戳表示 origin_post 原微博的 id 2.3 知乎数据(1) 知乎用户 zhihu_user.csv 字段 说明 字段 说明 id 用户 id articles_count 文章数 url_token 可以唯一标识用户的一个字符串 question_count 提问数 url 用户主页 url columns_count 专栏数 name 用户名 logs_count 参与公共编辑次数 gender 用户性别，1～男，0～女，-1～未知 voteup_count 用户获得的赞同数 locations 用户所在地 thanked_count 用户获得的感谢次数 educations 用户教育经历 follower_cont 粉丝数 employments 用户职业经历 following_count 关注的人数 headline 用户个性签名 favorite_count 收藏数 answer_count 回答数 favorited_count 被收藏数 (2) 知乎专栏 zhihu_zhuanlan.csv 字段 说明 id 专栏 id author 作者的 url_token title 专栏标题 content 专栏内容 url 专栏 url time 专栏发表时间，用 UTC 时间戳表示 like 点赞数 (3) 知乎提问 zhihu_question.csv 字段 说明 id 问题 id author 提问者的 url_token title 问题标题 content 问题详细 time 提问时间，用 UTC 时间戳表示 (4) 知乎回答 zhihu_answer.csv 字段 说明 id 用户 id url 回答 url author 回答者的 url_token content 回答内容 time 回答时间，用 UTC 时间戳表示 upvote 赞同数 target_type 回答对象类型，”question” 表示对问题的回答，”answer” 表示对某个回答的回答 target_id 如果 target_type 是 “question” 则表示问题 id，如果 target_type 是 “answer” 则表示回答 id 2.4 微信数据公众号文章 weixin_articles.csv 字段 说明 id 文章 id author 公众号名称 url 文章 url title 文章标题 content 文章内容 time 发表时间 2.5 头条数据(1) 头条用户 toutiao_users.csv 字段 说明 id 用户 id name 用户名 followers_count 粉丝数 followings_count 关注人数 verified_count 认证信息 (2) 头条文章 toutiao_articles.csv 字段 说明 id 文章 id media_name 媒体名称 title 文章标题 content 文章内容 datetime 发表时间 comments_count 评论数 digg_count 点赞数 bury_count 踩数 favorite_count 收藏数 (3) 头条评论 toutiao_comments.csv 字段 说明 id 评论 id article_id 文章 id user_id 发表评论的用户 id user_name 发表评论的用户名 text 评论内容 score 头条提供的影响力分数 digg_count 点赞数 bury_count 踩数 reply_count 回复数 create_time 评论时间 (4) 头条回复 toutiao_replies.csv 字段 说明 id 评论 id reply_id 回复 id text 回复内容 name 发表回复的用户名 user_id 发表回复的用户 id digg_count 点赞数 create_time 回复时间 3 关键技术3.1 基于Gibbs采样算法的LDA主题分类模型3.1.1 模型介绍LDA是一种非监督机器学习技术，本报告利用该模型识别微博、微信、今日头条和知乎四大媒体平台上的文章和评论中潜藏的主题信息，并将文章与评论进行分类，并分别用若干个关键词来描述每个主题，从中提取出舆情的主流观点。 LDA是一种典型的无监督、基于统计学习的词袋模型，一种文档主题生成模型，也称为一个三层贝叶斯概率模型。生成模型即我们认为一篇文章的每个词都是通过“以一定概率选择了某个主题，并从这个主题中以一定概率选择某个词语”。LDA模型包含词、主题和文档三层结构。即它认为一篇文档是由一组词构成的一个集合，词与词之间没有顺序以及先后的关系；一篇文档可以包含多个主题，文档中每一个词都由其中的一个主题生成。主题模型通过分析文本中的词来发现文档中的主题、主题之间的联系方式和主题的发展，通过主题模型可以使我们组织和总结无法人工标注的海量电子文档。 3.1.2 模型建立LDA模型基于贝叶斯网络结构，在LDA模型中一篇文档生成的方式如下: 从狄利克雷分布中取样生成文档i的主题分布\theta_{i} 从主题的多项式分布中取样生成文档i第j个词的主题Z_{i,j} 从狄利克雷分布中取样生成主题的词语分布\phi_{i,j} 从词语的多项式分布\phi_{i,j}中采样最终生成词语w_{i,j} 3.1.3 算法求解整个模型中所有可见变量以及隐藏变量的联合分布是: P(w_{i}, z_{i}, \theta_{i},\Phi | \alpha, \beta) = \prod_{j=1}^{N}P(\theta _{i}|\alpha)P(z_{i,j}|\theta _{i})P(\Phi|\beta)P(w_{i,j}|\theta_{z_{i,j}})最终一篇文档的单词分布的最大似然估计可以通过将上式的以及进行积分和对进行求和得到: P(w_{i}|\alpha, \beta) = \iint\sum_{z_{i}}P(w_{i}, z_{i}, \theta_{i}, \Phi|\alpha,\beta)根据上述p(w_{i,j}|α,β)的最大似然估计，通过吉布斯采样方法估计出模型中的参数，具体过程如下： 首先对所有文档中的词遍历一遍，为其都随机分配一个主题，z_{m,n} = k\sim Mult\frac{1}{K}，其中m表示第m篇文档，n表示文档中的第n个词，k表示主题，K表示主题的总数；n_{m}^{(k)}+1,n_{m}+1,n_{k}^{(t)}+1,n_{k}+1分别表示m文档中k主题出现的次数，m文档中主题数量的和，k主题对应的t词的次数，k主题对应的总次数 对下述操作进行重复迭代。 对所有文档中的所有词进行遍历，假如当前文档m的词t对应主题为k，则n_{m}^{(k)}-1,n_{m}-1,n_{k}^{(t)}-1,n_{k}-1, 即先拿出当前词，之后根据LDA中topic sample的概率分布采样出新主题，在对应的n_{m}^{(k)},n_{m},n_{k}^{(t)},n_{k}上分别+1. P(z_{i} = k|z_{-i}, w) \varpropto (n_{k,-i}^{(t)} + \beta_{t})(n_{m,-i}^{(k)} + \alpha_{k}) / \sum_{t=1}^{V}n_{k,-i}^{(t)} + \beta_{t} 迭代完成后输出主题-词参数矩阵和文档-主题矩阵θ 主题k中词t的概率分布：\phi_{k,t} = (n_{k}^{(t)} + \beta_{t})/(n_{k} + \beta_{t}) 文档m中主题k的概率分布：\theta _{m,k} = (n_{m}^{(k)} + \alpha_{k})/(n_{m} + \alpha_{k}) 3.2 基于LSTM模型的情感分析LSTM(长短期记忆神经网络)是RNN网络的拓展，具有时序建模的作用。LSTM通过输入门，遗忘门， 输出门保持和更新细胞状态，可以判断哪些信息是有用的，哪些是没用的，并把有用的信息在LSTM中保存。 传统情感分析方法需要提前建立好情感词典，而情感词典的建立不仅需要数据专家，而且需要语言学家的参与，这就带来了很大的问题。并且情感词典只是具有普适性，针对于特定问题并没有很好的效果。机器学习方法例如SVM，则不需要建立情感词典，但是它在分析句子时会把各个词语当成独立的单元去处理，并没有上下文关系，显然我们在语言中是需要上下文的。语言数据属于时间序列数据，每个词语的出现都依赖于它的前一个词语和后一个词语。由于这种依赖的存在，我们使用LSTM（长短期记忆网络）来进行情感分析。 3.2.1 数据标注由于国内暂时无法找到此类政治/军事事件的情感标注数据集，并且尝试其他领域情感标注数据集之后发现效果并不是很好，与预期相差较大，故我们决定进行对爬取的微博数据构建人工标注数据集。由于微博文章存在多种多样的情感较难使用语言描述，我们将其划分为4个情感等级，分别为0, 1, 2, 3，从0到3情感依次从平和到愤慨。我们人工标注了6000条左右的数据。 3.2.2 数据处理微博爬取到的数据有很大一部分与中印对峙无关，比如我们爬取数据时使用的“中印”关键词会爬到如“莞中印象”等无关数据。首先，我们采用标注好的数据中词频较多的关键词来进行粗筛，比如“中印“、“对峙”、”边境“、”战争“、”洞朗“等。之后我们采用TF-IDF进行词频统计，并进行停用词过滤，将与标注数据相似的文章提取出来。大概可以筛掉一半以上的数据，大大减小了噪声和训练成本。 3.2.3 基于LSTM模型的情感分析本报告的情感分析框架如下： (1) 数据分析 我们可视化人工标注的微博情感数据： 通过柱状图可知，中印对峙微博评论情感较为强烈。 (2) 模型选择 在文本向量化部分，我们选择了word2vec模型。情感是和上下文相关的，word2vec模型可以很好的根据上下文语境推断出每个词的词向量，如果两个个词在上下文的语境中，可以被互相替换，那么这两个词的距离就非常近。 在情感分类模型部分，我们采用了LSTM模型来训练。LSTM(长短期记忆神经网络)是RNN网络的拓展，具有时序建模的作用。LSTM通过输入门，遗忘门， 输出门保持和更新细胞状态，可以判断哪些信息是有用的，哪些是没用的，并把有用的信息在LSTM中保存。LSTM单元如下所示： (3) 模型训练与结果 我们采用jieba分词，对微博训练样本进行分割，并采用Google开源的Word2Vec训练，将分词词语表示为词向量，之后将每个微博样本向量化作为LSTM网络的输入。我们统计了每个微博样本的单词数： 根据直方图，我们选择将句子最大长度设置为85，之后我们将数据转化为索引矩阵，得到我们的输入数据，转换流程如下图： 首先，我们尝试进行二分类情感分析：即将标注数据的0,1归为一类，2,3 归为一类，放入如下的神经网络中进行训练： 单个GTX960显卡大概需要5分钟左右时间完成训练，训练准确率约为99%，测试集准确率约为90%： 之后我们进行多程度情感分析：将4种情感强弱类别作为输出，训练网络为： 我们最终得到训练集准确率在83%左右，测试集准确率在77%左右，效果不算很好。但是通过比对，测试集真实值与预测值在情感程度划分上差别较小，可能与数据标注的误差有关系。我们最终选择了使用多分类的情感模型。 3.3 其他算法 分析对象 算法 关联词分析 Word2vec词向量分析 全网事件走势回顾(相同主题文章数量分析) LSI(Latent Semantic Index)文本相似度索引算法 4 全网舆情分析4.1 全网关注度概况(1) 全网热度走势 上图是“中印边境事件”在整个周期内的全网热度走势图。自事件发生起，其全网热度总体呈波动上升趋势，出现十余个大小峰值，且在8月11日达到最高峰。对比本报告1.2节中的「事件走势回顾图」，易发现全网热度峰值时间点与此事件的每一步进展的时间点紧密吻合，说明此事件的热度增减与事件进展紧密相关。每当此事件相关新闻出现都会引发网民的舆论热议，由此可见网民对此类涉及国家安全和领土完整的政治问题是长期密切关注和参与的。 8月4日热度达次高峰，是由于“8月3日上午至4日凌晨，新华社 、解放军报、外交部、国防部、中国驻印度大使馆以及人民日报这中国6个国家部委和机构先后就印方越界事件发声，披露印方非法越界的性质，并强调中国将采取一切必要措施维护自己的正当合法权益”。由此可知，中方对待此事件的强硬态度引起网民巨大的舆论热议，可见网民的爱国之心被广泛激发。说明国家在面对国家领土和主权问题时态度强硬、立场坚定，能广泛振奋人心、给民众以信心和力量，激发民众的爱国之心。 8月11日热度达最高峰，是由于“印军进入高等级战备状态”的新闻爆出，这似乎是印度战争部署的预警。由此可见，中印边境事件紧张局势的进一步升级刺激到网民的敏感神经，引发了整个周期中最大的舆论热议，这体现网民在国家安全可能受战争威胁下的群体不安和“舆论骚动”。 8月28日热度达第二次高峰，是由于“印方将越界人员和设备全部撤回，中印边境冲突平息”。由此可见，政治事件的和平解决也会引发大量舆论关注。 (2) 全网文章和评论热度随时间变化① 全网文章热度随时间变化 ② 全网评论热度随时间变化 对比「全网文章热度随时间变化」「全网评论热度随时间变化」与「全网热度走势」三图，易发现三图的整体趋势十分相似，但全网评论的热度峰值时间点比全网文章的更接近于全网热度峰值时间点，这可能一是由于全网评论主要来源于微博，而微博相较于其他媒体平台及时性更高，二是相较于UGC特征强的微博，新闻文章从采编到发布到被阅读有较长的时间差，故导致全网文章的热度峰值普遍略迟。 (3) 全网关注度来源分布比例① 全网关注度来源分布比例 上图显示在全网对此事件的关注度来源中微博占比高达93.9%，而来自其他媒体平台的信息量只占6.1%, 可见基于微博庞大的用户量(能产生庞大的UGC内容)和不断凸显的社交媒体平台属性，微博至少在新闻舆论方面保持绝对影响力。 ② 除微博外其他媒体平台的报道量比例 由上图可看出，除微博外，微信和今日头条对此事件的信息量贡献最多。 微信的信息量大，一是基于微信庞大的公众号内容生产体系，二是得益于微信在今年新增的「搜索资讯」功能，使用户能够主动搜索关键词，浏览热点资讯、好友关注的资讯以及自己感兴趣的文章。这体现微信在社会化媒体内容生产方面的重要性日渐突出。 今日头条的信息量大，一是由于其强大的数据挖掘能力源源不断为平台输送内容，二是由于其庞大的用户基量产生巨大的资讯阅读需求和评论回复信息量(《2016移动资讯行业细分报告》[2]显示：截止2016年12月底，今日头条用户量达7.0亿，日活跃用户量达7800万)。 (4) 微博热度随时间变化 对比「微博热度随时间变化」与「全网热度变化」，易发现二者走势极为相似，这是由于此事件的微博信息量在全网信息量中占比极大，反映微博平台的强大舆论影响力。 (5) 除微博外媒体热度随时间变化 对比「除微博外媒体热度随时间变化」与「微博热度随时间变化」可发现：二者的热度最高峰不同。在此事件中微博的热度最高峰在8月11日(印军进入高等级战备状态)，而除微博外的媒体平台的热度最高峰在8月20-21日(“中方士兵在中印士兵肢体冲突中受伤)。 造成这种差异的原因可能与微博与其他平台的内容性质差异有关，微博是短文本、快餐式阅读，“印军进入高等级战备状态”这种信息量较小、事实明了又极易引起关注的新闻较容易在微博平台上引起广泛传播；而今日头条、微信公众号、知乎更偏向于长文本、较深层次阅读，“中方士兵在中印士兵肢体冲突中受伤”这种信息量较大(有文本有视频)、事实尚不明了且具话题性和争议性的新闻对它们来说更有内容创作的发挥空间。 4.2 全网关注点分析(1) 全网关键词词云 上图显示，国家、美国、哈哈哈、世界、战争、经济、越南、日本、不丹等词是与“中印边境事件”相关的全网信息中的高频词汇，反映网民在此事件上的关注焦点。下面是对部分词的高频原因的分析： “美国”：中印对峙期间，莫迪政府与美国开展“军事进购大单”、“幕后外交斡旋”、“双边及多边联合军演”等活动，反映美国可能存在拉拢印度以抗衡中国的意图以及印度可能存在寻求美国合作/援助的意图。若美印联手，则必将加剧中印边境冲突的紧张局势。 “日本”：事件期间据部分印媒报道日本政府支持印度立场。但日本驻印度大使馆随即否认了相关报道内容。因历史、领土争端等因素，中日关系素来易引起中国网民的关注。而此次中印边境事件亦涉及领土主权问题，故易让人联想到日本。 “越南”：中印边境争端持续之际，印度和同样与中国存在领土争端的越南越走越近(越南总理访问印度)。很难不让网民猜测其用意。 “不丹”：因为印度称自己在这场对峙中代表的是不丹。故在“中印对峙事件”中不丹的态度可能影响中、印、不丹三国关系。 “台湾”：因“中印对峙波及台湾，台两学生被印拒绝入境”新闻，触动中国网民关于台湾问题这一“历史遗留问题”的敏感神经。 “经济”：可能是担心中印冲突会影响两国经济合作，以及若发生战争会影响中国的经济建设和国家整体发展。 以上关键词反映此事件牵涉的相关主体和网民关注的焦点。可以看出，网民对事件背后的国家间博弈以及事件可能对本国经济造成的影响尤为关注。 (2) 全网关联词分析 关联分析：与主题相关的关联词分析，即与核心词共现频率(相关度)最高的词。关键词A 和 关键词B的相关度 = 同时包含关键词A和关键词B的文章的阅读数/包含关键词A的文章的阅读数 下面是分别对核心词“中印”和“中国”的关联词分析 ① 核心词“中印”的关联词分析 上表显示与核心词“中印”相关度最高的词语为：持续时间(41.3%)、中朝(38.8%)、退却(37.2%)、持久战(37.0%)。反映网民舆论较为关注中印对峙已经和将要持续的时间，且网民已有打“持久战”的猜测或心理准备。 其中“中朝”与“中印”相关度高是由于中印对峙期间中国突然增兵朝鲜边境，故引发网民舆论热议讨论。中国突然增兵朝鲜边境行为背后的含义其一可能是警告美国，显示出中国军事维护半岛“无战、无乱、无核”的原则和决心；其二可能是防止万一中印开战美军搞突然袭击。 ② 核心词“中国”的关联词分析 上表显示与核心词“中国”相关度最高的词语为：印度(45.2%)、扩张(45.1%)、中国政府(39.2%)、武力(38.7%)、出兵(38.1%)、威胁(38.0%)、入侵者(37.6%)、安全隐患(37.4%)。反映媒体报道和网民舆论十分关注中印关系，认为“中印边境事件”反映作为“入侵者”的印度扩张边境的野心，以及担忧其可能对我国西南边境造成安全隐患，并可能进行了是否使用武力解决冲突的讨论。 5 微博舆情分析5.1 微博传播分析(1) 核心传播用户 核心传播用户：机构/媒体人/网民 上表是在此事件中的微博核心传播用户。 核心传播机构中头条新闻的核心传播作用可能由于其庞大的用户基量，而紫光阁、环球时报、人民日报等机构则可能由于其官媒属性强，以及因有国家颁发的新闻采写资格证而掌握稀少的新闻采写资源。两个核心传播媒体人则主要因其军事视频自媒体的内容属性。核心传播用户中局座召忠也是军事报道与分析的自媒体。由此可见，在这类政治军事事件中的核心传播用户的共同特点是擅长军事属性内容创作或是有官方属性。 (2) 核心传播用户的影响力排名 影响力排名表 局座召忠 头条新闻 人民日报 环球时报 防务君 从上表中可看出，局座召忠作为一个专注于军事内容创作的自媒体，在“中印边境事件”中的传播影响力最强。 5.2 微博网民群体画像分析针对关注和参与“中印边境事件”的微博网民的用户画像，本报告从性别、国内地域分布、海内外参与比例三个数据维度进行分析，分析用户的不同群体特征对他们在舆论事件中的关注和参与度的影响。 (1) 微博关注用户的性别渗透率 渗透率:表示用户对特定事件关注度的比例.性别渗透率 = 某性别用户对关键词的关注度／该性别用户总关注度地域渗透率 = 某省份或城市用户对包含特定关键词文章的关注度/该省份或城市产生的总关注度 上图数据显示，微博关注此事件的男性网民与女性网民比例是71%：29%，可看出：男性网民对此事件的关注度远远高于女性网民。而根据微博数据中心发布的《2016微博用户发展报告》，2016年微博活跃用户中男性用户以55.5%：44.5%略高于女性用户，可知男性用户比例本身只是略高于女性用户。导致关注此事件的男性网民比例超出寻常地高于女性用户，可能是由于不同性别网民的兴趣差异，因为通常来说男性网民对“中印边境事件”这类国际、军事、时政、历史等性质的话题的关注度要远高于女性网民。 (2) 微博关注用户的地域渗透率 由上图可看出，北京和广东的微博网民对话题的关注度最高，其次是江苏、上海、山东、河南、云南等。 北京经济发达，人口众多，网民数量多，且作为我国政治中心，网民对政治事件的关注度和敏感度较高，故北京微博网民对“中印边境事件”的关注度最高。 广东省是中国的经济大省和改革开放前沿阵地，与印度的经贸合作一直十分紧密(去年粤印贸易总额达144亿美元，占中印贸易总额的1/5)；对印度来说广东的经济地位也十分突出，广东省与印度的古吉拉特邦已缔结友好省邦协议;再加上因互通贸易、人员流通等因素，广东省与印度文化上也有很大的渊源;且随中印双方更加紧密的发展伙伴关系的构建和“一带一路”建设的深入推进，广东和印度的经贸合作在未来会有更大的发展。所以担心“中印边境事件”会影响广东与印度的经济合作，可能是导致广东的微博网民对此事件的关注度最高的重要因素之一。 云南省作为中国通往南亚、东南亚开放的前沿省份，是中国面向印度的陆路窗口，与印度有地缘政治下的区域经济合作优势，且改革开放以来云南与印度的经济合作总体处于较快发展状态；以及在“一带一路”倡议和建设面向南亚、东南亚辐射中心的利好条件下，云南与印度的经济合作发展前景广阔。而且地域毗邻下若中印发生军事冲突，会首先影响到云南。故可以推测，担心中印边境冲突可能会影响云南的安定以及其与印度的经济合作，可能是导致云南的微博网民的对此事件关注度次高的重要因素之一。 据相关报道可知，河南为兵员大省，所以其微博网民对此事有较高的关注度。 (3) 微博海内外关注用户的参与统计 根据微博数据中心发布的《2016微博用户发展报告》[3]，港澳台及海外的微博用户占全体微博用户的2%。而上图显示，微博海内外关注用户中海外用户占比4.8%，高于2%。这说明海外微博用户对此事件较为关注。这可能是由于“中印边境事件”的国际属性，以及其事件本身在国际上就引起了较大的舆论争议。 5.3 微博内容情感分析本报告利用LSTM模型对事件相关的微博内容进行情感分析，将情感强度划分为四个情感等级，分别为0， 1， 2， 3，从0到3情感依次从平和到愤慨。 5.3.1 微博内容的情感极性时间变化图(1) 微博文章的情感极性时间变化图 对比上图与本报告1.1节的「事件走势回顾图」，易发现微博文章的情感极性峰值和谷值时间点与整个事件进展的关键时间节点十分吻合。分析上图可知： 1.事件进展的最新新闻(无论正面还是负面新闻)极易调动微博网民情绪； 2.事件周期内微博网民绝大部分时候处于愤慨情绪状态； 2.此事件中负面新闻(如：6月26日前印军阻挠中国边防部队在洞朗地区的正常活动 ；6月29日外交部展示印度边防人员非法越过中印边境进入中国领土照片)的爆出一次次刺激网民的敏感神经，接连不断引发网民的愤慨情绪峰值； 3.正面新闻的报道则会引发网民平和情绪谷值。 (2) 微博评论的情感极性时间变化图 对比上面两图，易发现微博评论的情感极性变化走势与微博文章十分相似，但波动幅度比微博文章小，峰值谷值没那么明显，且绝大部分时候处于愤慨情绪状态。 5.3.2 微博内容的情感地域分布(1) 微博文章的情感地域分布 上表显示“中印边境事件”下微博文章的情感强度的地域分布。由图可看出，新疆网民的微博文章的情感强度最高，表明最愤慨，其次是西藏、江西、云南等。 云南：由于印度、云南地域毗邻，故若中印发生军事冲突可能会首先影响到云南。故可以推测，担心中印边境冲突影响云南安定可能是云南微博网民较为愤慨的原因。 新疆：与此次事件争议的位于西藏的锡金段类似，绝大部分属于新疆管辖的阿克塞钦地区也是中印边界争议地区，且历史上中印多次边界冲突事件都牵涉到此地区。故印度与中国新疆的边界争议和曾爆发的多次冲突可能是引起新疆微博网民较为愤慨的原因。 (2) 微博评论的情感地域分布 上表显示此事件下微博网民评论的情感强度的地域分布。由图可看出，北京和重庆的微博网民的微博评论的情感强度最高，表明最愤慨，其次是辽宁、湖南、宁夏等。其中，由于北京首都和全国政治中心的性质，故北京微博网民对这类政治事件的敏感度相较更高、反应更为强烈，这可能是导致北京微博网民评论体现出最为愤慨的情感的原因。 通过对比上面两表，易发现二者特征差异很大。比如：微博文章的情感地域分布中，沿中印边界线的省份的微博网民情绪较为愤慨；而微博评论的情感地域分布中，中国中部和北方地区、东北地区的微博网民情绪较为愤慨，沿中印边界线的省份的微博网民情绪反而较为平和。 6 主流媒体平台的舆情特点对比分析6.1 主流媒体平台热度(声量大小)的变化趋势对比 分析上面7张图表可知，在“中印边境事件”中： 舆论热度大小比较：微博 &gt; 今日头条 &gt; 微信 &gt; 知乎 舆论热度的波动大小比较：今日头条 &gt; 微博 &gt; 微信 ≈ 知乎 微博舆论热度最高，声量最大，其中微博评论热度要远高于其文章热度 微博文章与微博评论、头条文章与头条评论、知乎回答与知乎评论这三对的热度变化趋势各自十分相似 今日头条与微博的舆论热度变化趋势较为相似，且波动都较大；而微信和知乎热度总体波动较小 四个媒体平台的热度最高峰出现的时间段不同，今日头条与微博的最高峰都是在8月11日(报道：印军进入高等级战备状态)，而微信和知乎的最高峰都是在8月03-04日(报道：中国六大部门集体就印方越界事件发声)。 由上可总结：在此事件中，微博与今日头条的舆论热度特征相似，微信与知乎的舆论热度特征相似。原因可推测：可能由于今日头条与微博的新闻即时性、社交化媒体属性、信息传播性更强，且内容都偏向于快餐式，故二者舆论热度特征相似；而微信和微博相对来说新闻即时性、社交化媒体属性、信息传播性偏弱，且内容都偏向于深层次，故二者舆论热度特征相似。 而热度最高峰的时间点的差异，实质上反映出这四个媒体平台对同一事件的关注点以及同一事件不同阶段进展的关注程度不同。头条和微博上“印军进入高等级战备状态”热度最高，可能是因它涉及国家安全，极具争议性和传播性，易在这两个平台上引起广泛传播和全民讨论；而微信和知乎上“中国六大部门集体就印方越界事件发声”热度最高，可能是由于它展现出中国对待这一事件不同以往的强硬态度和坚定立场，值得深入挖掘和讨论。 6.2 主流媒体平台的典型意见挖掘——基于LDA主题模型LDA是一种非监督机器学习技术，本报告利用该模型识别微博、微信、今日头条和知乎四大媒体平台上的文章和评论中潜藏的主题信息，并将文章与评论进行分类，并分别用若干个关键词来描述每个主题，从中提取出舆情的主流观点。 下列主题排名顺序表中：主题即反映主流观点；主题得分越高，说明其意见占比越大。 6.2.1 媒体平台文章的主流观点对比(1) 微博文档主题排名顺序 上图显示“中印边境事件”中微博文档的主流观点，其中{“印方”，“中方”，“越界”，“领土”，“边界”，“外交部”}主题得分最高(3800分)。由此可知，微博文档倾向于报道中印洞朗边界冲突发展状况，和中国外交部的多次回应。 这反映微博网民和媒体密切关注中印边境事件的最新进展。而密切关注中国外交部对此类政治敏感事件的表态，可能一是因为一般来说国家外交部对政治敏感事件的表态直接反映出国家解决事件的对策倾向，二是网民很在意且希望国家表现出强硬和坚定的维护国家主权的立场，这可给国民增加民族自信心和底气。 此外，微博文档还倾向于印度撤军、中国军事阅兵、尼泊尔立场等主题。 (2) 今日头条文章主题排名顺序 上图显示此事件中今日头条文章的主流观点，其中{“印度”，“不丹”，“中国”，“对峙”，“边界”，“领土”}主题得分最高(12800分)。由此可知，今日头条文章倾向于报道中印对峙暴露出的中、印、不丹三国领土边界争议，反映出今日头条文章在此事件中对这一历史遗留问题(边界争议)的关注。这可能是由于头条的文章普遍倾向于分析“中印边境事件”背后反映出的历史因素。 此外，今日头条文章还倾向于关注中印对峙下的中印经济合作问题等。 (3) 微信文章主题排名顺序 上图显示此事件中微信文章的主流观点，其中{“锡金”，“尼泊尔”，“边界”，“独立”，“地区”，“高原”，“对峙”}主题得分最高(110000分)。由此可知，微信文章倾向于关注中印锡金段边界对峙、尼泊尔立场、英国立场。其中英国立场受较大关注的可能原因，一是历史上印度与英国曾是殖民地与宗主国的关系，且印度现为英联邦成员国；二是英国表态不会介入中印争端。 此外，微信文章还倾向于分析在此事件中美国、日本、朝鲜、台湾、不丹、孟加拉国的态度和立场，解放军的力量，印军的亚东军事部署，中兴等中国手机公司在印度市场受影响等主题。这反映出微信文章将关注焦点更多聚焦于中印事件背后的国际政治关系博弈，以及事件对中印经济贸易的影响。 其中中兴等中国手机公司在印度市场受影响的主要原因，一是由于近年中兴、小米等中国手机品牌集体出征印度市场，印度市场对这些公司的海外市场战略很重要；二是由于事件期间印度国民出现的反华情绪和抵制“中国货”运动可能为这些中国手机厂商增加了变数。 (4) 知乎回答主题排名顺序 上图显示此事件中知乎回答的主流观点，其中{“印度”，“中国”，“国家”，“强硬”，“战争”，“经济”}主题得分最高(28000分)。由此可知，知乎回答的主流观点是：对印态度要强硬，中印经济发展，是否诉诸战争。 此外，知乎回答还倾向于谈论在中印边境事件中美国、不丹、朝鲜、南海的角色，中国的战略优劣势(后勤补给、海军、军队部署)等主题。由此可见，知乎回答更倾向于分析战争、军事战略层面，反映网民对此事件下国际多方立场的关注，对中国是否该诉诸武力、以及若发生战争中国的战略优劣势的思考。 6.2.2 媒体平台网民评论的主流观点对比(1) 微博评论主题排名顺序 上图显示“中印边境事件”中微博网民评论的主流观点，其中{“印度”，“中国”，“美国”，“日本”，“边境”，“士兵”}主题得分最高(18700分)。由此可知，微博网民评论更倾向于讨论中印边境事件中中、印、美、日各方的国际关系。 此外，微博评论还倾向于传播和平解决中印争端、印度非法入侵、小粉红、主战打印等主题。 以下是对“小粉红[4]”受到较大关注的分析： 小粉红是中国大陆自由派社交网络用户用以贬低中国大陆民族主义社交网络用户的负面标签。这部分群体具有简单粗暴的爱国主义情感和执行简单粗暴的爱国主义行为，如“抵制日货运动”。媒体对其历来褒贬不一。此次“中印边境事件”亦出现大量“主战派”“小粉红”，他们不满中国政府迟迟不肯开战，将印军入侵称为“非法越界”，主张诉诸武力捍卫国格与军威。 如：网友@透过本质看真相：别国的部队都过界了，平时那么强硬的媒体砖家还有爱国的小粉红去哪里了？ 但与以前不同，此次网上并未有对其的大量贬低、讽刺之声。由此可推测，对这类国家领土被非法入侵和国家安全受威胁的政治事件，诉诸武力是网民的考虑选项之一，支持者不在少数，而不是被看做“小粉红式“的“简单粗暴的爱国主义想法”。 (2) 今日头条评论主题排名顺序 上图显示此事件中今日头条网民评论的主流观点，其中{“印度”，“中国”，“美国”，“不丹”}主题得分最高(44000分)。由此可知，今日头条网民评论更倾向于谈论此事件下中、印、美、不丹四国的立场。 此外，今日头条评论还倾向于谈论印度抵制中国货，致敬中国解放军，印度非法入侵中国领土等主题。 (3) 知乎评论主题排名顺序 上图显示此事件中知乎网民评论的主流观点，其中{“中国”，“印度”，“国家”，“发展”，“经济”，“美国”}主题得分最高(34000分)。由此可知，知乎评论更倾向于谈论此事件下中印国家关系和中印经济发展等主题，这反映知乎网民评论中对此事件对中印国家关系和经济发展、经贸往来的影响的思考。 此外，知乎网民评论还倾向于分析此事件下中、美、朝、日、南海五者的角色和关系，以及所涉台湾问题(因中印冲突，台两学生被印拒绝入境)。 7 舆情总结与启示7.1 事件分析1.印度为何态度如此强硬？ 印度态度如此强硬，背后必然有着更深层次的原因。 一方面印度外债高筑，经济发展形势并不乐观，政府有无法掌控局势的趋势，这时候挑起与中国的冲突能有效转移国内矛盾; 另一方面，随着中国“一带一路”的推进，势必会改变沿线地区的地缘政治形势，并与其他大国的利益发生冲突，中东这里就有美国的核心利益石油；而“中巴经济走廊”项目将通过克什米尔地区更是让印度痛心疾首，在围堵中国的共同利益下，印度必然得到以美国为首的相关国家大力支持，更加有恃无恐。 2.印度最终为何选择和解？ 一是印度军事实力弱于中国; 二是印度现今经济情况不容乐观，经济情况可能无法承受战争的负担。 3.国际各国对此事件的态度是怎样的？ 各国均表示希望中印双方和解。 4.中国的态度？ 中国希望和平崛起，但在涉及领土的核心利益上，也决不会做任何妥协。从长远来看，军事冲突的代价对于双方都是难以承受，双方如果开战，伤亡在所难免，在谈判桌上和平解决争端是最好的选择。中国的在整次事件中的做法是在保证足够军事威慑的基础上，和平解决此次边境问题。 7.2 舆情整体特点总结基于对微博、微信公众号、今日头条和知乎四大媒体平台的事件相关数据的分析，我发现了此事件有如下舆论特点。 1.全网热度峰值时间点与此事件的每一步进展的时间点基本上紧密吻合，不同平台有所差别。 2.微博的舆论声量最大，今日头条、微信、知乎分别为第二、三、四。微博对信息量的贡献占绝大部分，可以说是中国第一大“社会公共舆论场”。 3.网民关注的焦点主要是：事件进展、中国外交部的回应、事件背后的国际政治关系博弈、战争对中国经济的影响、军事战略等，不同平台用户的关注焦点有所差别。- - 4.微博关注网民以男性居多，主流情感是愤怒，主战派声量较大且受到很多网友的赞同。微博的事件核心传播用户基本上都是官媒或军事自媒体背景，传播层级和覆盖面非常广，且传播周期短、效率高。 7.3 政府启示 微博是中国第一大“社会公共舆论场”，网民首选且重要的舆论参与平台。微博的每一部移动终端和微博空间的每一个网民，在其本质上都演绎成整个民间公共信息系统的释放点、采集点、延伸点、链接点；微博是网民在日常社交过程中通过碎片化传播随时随地参与完成对政府社会治理、公共事务和公共服务评议的“融合讯息通道”。 政府可以从这次舆情事件分析中得到什么启示？下次类似舆情事件发生，政府该如何应对？ 1.充分发挥微博大V的舆论影响力，借助相关大V来助力舆论引导，如利用官媒和军事自媒体的舆论影响力为官方舆论服务； 2.快速响应：热门微博的传播有时效性，政府需要密切关注原创发出的后几小时； 3.建立网络舆情预警机制，防止恶性信息的进一步扩散和传播，将负面舆情第一时间扼杀在摇篮中，及时疏导网络舆情，做到对其的有效应对； 4.创新正能量传播，多账号“抱团”垂直行业内部主动策划舆论传播，形成一定传播声量； 5.建立常态互动机制：主动回应舆情，从单向管理转向双向互动。 注释 [1] 维基百科.2017年中印军队洞朗对峙事件 https://zh.wikipedia.org/wiki/2017%E5%B9%B4%E4%B8%AD%E5%8D%B0%E5%86%9B%E9%98%9F%E6%B4%9E%E6%9C%97%E5%AF%B9%E5%B3%99%E4%BA%8B%E4%BB%B6 [2] 今日头条算数中心.2016移动资讯行业细分报告 http://www.199it.com/archives/559343.html [3] 微博数据中心.2016微博用户发展报告 http://data.weibo.com/report/reportDetail?id=346 [4] 维基百科.小粉红 https://zh.wikipedia.org/wiki/%E5%B0%8F%E7%B2%89%E7%B4%85]]></content>
      <categories>
        <category>Communication Science</category>
        <category>Public Opinion Analysis</category>
      </categories>
      <tags>
        <tag>Public Opinion Analysis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[品牌广告活动分析报告|2017网易考拉海购H5]]></title>
    <url>%2F2018%2F01%2F01%2F%E5%93%81%E7%89%8C%E5%B9%BF%E5%91%8A%E6%B4%BB%E5%8A%A8%E5%88%86%E6%9E%90%E6%8A%A5%E5%91%8A-2017%E7%BD%91%E6%98%93%E8%80%83%E6%8B%89%E6%B5%B7%E8%B4%ADH5%2F</url>
    <content type="text"><![CDATA[Abstract：广告创意与策划课程的小作业 ~ 从品牌传播和广告的角度分析了2017年11月网易考拉海购推出的一款风靡逗趣的H5广告。 一、广告简介广告形式：H5 广告对象：网易考拉海购 广告名称：《入职第一天，网易爸爸教我做人》 推广时间：2017年11月底 （扫码观看H5） 二、广告内容介绍网易考拉海购以“企业套路”为话题，推出这支名为《入职第一天，网易爸爸教我做人》的自黑H5。主角是一个第一天到网易上班的小职员“猪你丫”，H5里描述了猪你丫在网易上班第一天发生的被上司压榨、被迫加班到凌晨等等一系列事件，主角与总监斗智斗勇，内心戏也是极为丰富。 作为一支故事情节类H5，这支H5以第一人称的形式讲述了主人公“Julia”入职第一天的各种经历，其模拟的故事情节十分贴近人们日常的工作生活，结合当下最流行的群聊、表情包等元素，并在旁白配音的衬托下，使整支作品具有很强的趣味性。作品结尾也明确地点明主要目的，即为即将到来的双11购物节“带货”。 三、产品分析产品：网易考拉海购 1.产品定位：自营跨境电商的综合性电商平台，媒体驱动型电商。 2.产品核心特色：低价，高品质 3.产品slogan：只过1%的生活 4.用户痛点：对国内品牌不信任，低价，发货速度，正品 5.用户需求：希望以较优惠的价格买到可靠的正品（即低价海淘正品，用更少的钱过更好的生活） 6.用户定位：二线以上城市高学历高收入的女性群体,尤其是企业白领和年轻妈妈 12345678根据《2016 年中国跨境网购用户研究报告》热衷海淘的人群有以下几个特征： 年龄偏大，26 岁 ~40 岁占比 74.7% 学历整体较高，本科及以上学历占比 74.6% 收入较高,个人月收入万元以上的最多，占比超 1/4，平均月收入达 11043.9 元 企业员工最多，占比 55.6% 有孩子的最多，占比 66.5%,有一个孩子的占比 56.1% 故总体可把网易考拉海购的用户定位为：二线以上城市高学历高收入的女性群体,尤其是企业白领和年轻妈妈。 7.商品类别：主营母婴、美容彩妆、家居个护、进口美食、营养保健等品类。这类商品需求量大，且需求稳定可预测，也易进行批量处理。 四、广告亮点分析1.瞄准目标用户，设定鲜活且接地气的真实情境，引起用户代入感和共鸣，引发二次传播，引爆社会圈层 H5的主角“猪你丫”其实就像是平凡生活中互联网公司里的小职员。再具体一点，就是刚踏入职场的员工。而每个职场人士都经历过刚踏入职场的时候，普遍有那种“既不想加班，又迫于上司压力，同时还想邀功表现”的戏精心态，而网易考拉海购的主要用户群体就是这群年轻企业白领女性，可以说H5的角色和情节设定就是为这群用户“量身定制”。故很能引起她们强烈的代入感和情感共鸣，从而带来很强的传播效应，引爆该群体所在的社会圈层。 2.在故事中挖掘消费场景，在合适的场景中自然地为用户创造消费需求 H5 的结尾页设置了从H5到网页考拉的交互入口，按钮「帮猪你丫转正」会直接导流到网易考拉海购的购买页面，虽说只是对跳转的一个小小的故事化包装，但让用户还能沉浸在猪你丫的故事之中，即使到购买页面也并没有觉得生硬和出戏，可以说这个设置十分自然巧妙。本来用户是没有买东西的打算的，但在欣赏完这场“猪你丫”大戏后，心情愉悦，有可能也会顺便去海淘一下。并且从后期数据分析可知，这支H5的确为网易考拉带来了千万级的销售额。 3.娱乐化，坦诚自黑自侃，反而能获得消费者好感 我们以前看到的大多数广告都是自夸式的，不大会主动说自己的缺点，而H5中网易考拉把公司内的“企业套路”真实大胆地展现出来了，如「总监让小职员在第一天入职就联系 500 多个 KOL」、「本来想吃食堂晚餐，但凌晨还没下班」等等情节，甚至还做了夸张处理。看多了企业正正经经地宣扬品牌精神和文化，网易考拉海购这种放下身段，自黑恶搞的方式，反而显得更接地气，生动有趣，拉近了和年轻受众的距离。这种“脑洞大”、“会玩儿”的营销风格容易吸引一众年轻消费者的青睐，从而转化为对品牌的好感度。可以说，在新的消费趋势下，网易考拉海购真正抓住了用户的变化，敢出格，敢创意，这也让他们得到了消费者真正认可。 4.视觉设计上，多种风格混搭；情节设计上富有趣味性，迎合年轻人口味，制造层出不穷的笑点；听觉设计上，女主独白配音展现其丰富的“戏精”内心戏，也是精彩纷呈 H5的首页，女主的形象非常唯美女神，然后视频开始播放不到2秒，画面突然变为黑白简笔风，接着场景又到了非常真实的微信群聊中，还有表情包、快闪文字，魔性而又逗比 … 视觉形式丰富多样。再看剧情的设计，一波又一波猝不及防的剧情走向，甚至在最后，都狠狠地戳了一下用户的笑点。女声的独白配音也同样精彩，没有丝毫的尴尬或做作，在女神、女神经和萌妹纸之间，能轻松无压力切换。 5.自造IP，利用IP营销达到出奇制胜的营销效果 网易考拉海购品牌逐渐深植人心，这与其在营销传播策略上坚定地绑定头部内容（爆款IP）不无关系。通过场景化、有代入感的内容营销、事件营销，广泛触达影响目标人群，提升品牌知名度和好感度。比如《爸爸去哪儿5》中跨界之作“网易考拉海购洋屋民宿”、《花儿与少年3》中的毒鸡汤海报、《欢乐颂2》安心追剧不打折的神广告、《那年花开月正圆》的创意中插广告等等。 而这支H5则体现了网易考拉海购自造IP的功力，且达到了出奇制胜的营销效果。自造IP的一大优点在于可持续性营销，可以针对IP打造一系列的广告活动，比如继这支H5后，网易考拉又推出了2.0续集《入职半个月，网易爸爸让我怀疑人生》，且再度刷屏，点击量和带来的销售转化更超前作。 五、广告效果分析 H5 的结尾页设置了从H5到网页考拉的交互入口，按钮「帮猪你丫转正」会直接导流到网易考拉海购的购买页面。 这支H5为网易考拉带来了千万级的销售额。 六、总结网易考拉海购向来以“营销好手”、“情感收割机”著称，通过一系列社会化内容营销、IP营销、情感营销广泛触达影响目标人群，提升品牌知名度和好感度。这支H5正是其社会化营销、IP营销的典型案例，通过精准定位目标受众、鲜活接地气的情景设定、娱乐化年轻化的风格、自黑自侃的态度、新奇趣味的设计、自然巧妙的消费引导，从而完成这场成功的广告营销。]]></content>
      <categories>
        <category>Communication Science</category>
        <category>Advertising and Marketing</category>
      </categories>
      <tags>
        <tag>Communication Science</tag>
        <tag>Advertising and Marketing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构第九章：查找]]></title>
    <url>%2F2017%2F12%2F19%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E7%AC%AC%E4%B9%9D%E7%AB%A0%EF%BC%9A%E6%9F%A5%E6%89%BE%2F</url>
    <content type="text"><![CDATA[Abstract：查找。 9.1 查找的基本概念与术语查找表：用于查找操作的数据结构。由同类型数据元素组成的集合。 在查找表中查找某个具体的数据元素； 在查找表中插入数据元素； 从查找表中删除数据元素； 静态查找表：只做查找操作，不改动表中数据元素。 动态查找表：在做查找操作的同事，进行插入、删除操作。 关键字：属性 主关键字：即主码。唯一标识符。 次关键字：不具有唯一性 静态查找表是数据元素的线性表，可以是基于数组的顺序存储或以线性链表存储。 1234567891011/* 顺序存储结构*/typedef struct&#123;ElemType *elem； /* 数组基址*/int length； /* 表长度*/&#125;S_TBL；/* 链式存储结构结点类型*/typedef struct NODE&#123;ElemType elem； /* 结点的值域*/struct NODE *next； /* 下一个结点指针域*/&#125;NodeType； 9.2 静态查找表—顺序查找查找过程：顺序查找又称线性查找，是最基本的查找方法之一。其查找方法为：从表的一端开始，向另一端逐个按给定值kx 与关键码进行比较，若找到，查找成功，并给出数据元素在表中的位置；若整个表检测完，仍未找到与kx 相同的关键码，则查找失败，给出失败信息。 【算法9.1】以顺序存储为例，数据元素从下标为1 的数组单元开始存放，0 号单元留空。 12345678int s_search( S_TBL tbl KeyType kx )&#123; /*在表tbl 中查找关键码为kx 的数据元素，若找到返回该元素在数组中的下标，否则返回0 */ tbl.elem[0].key = kx ； /* 存放监测，这样在从后向前查找失败时，不必判表是否检测完， *//* 从而达到算法统一*/ for ( i = tbl.length; tbl.elem[i].key &lt; &gt; kx ； i-- ) ； /* 从标尾端向前找*/ return i ； &#125; 性能分析： 分析查找算法的效率，通常用平均查找长度ASL 来衡量。 定义：在查找成功时，平均查找长度ASL 是指为确定数据元素在表中的位置所进行的关键码比较次数的期望值。 其中：Pi 为表中第i 个数据元素的查找概率，Ci 为表中第i 个数据元素的关键码与给定值kx 相等时，按算法定位时关键码的比较次数。显然，不同的查找方法，Ci 可以不同。 就上述算法而言，对于n 个数据元素的表，给定值kx 与表中第i 个元素关键码相等，即定位第i 个记录时，需进行n-i+1 次关键码比较，即Ci=n-i+1。则查找成功时，顺序查找的平均查找长度为： 查找不成功时，关键码的比较次数总是n+1 次。 算法中的基本工作就是关键码的比较，因此，查找长度的量级就是查找算法的时间复杂度，其为O(n). 许多情况下，查找表中数据元素的查找概率是不相等的。为了提高查找效率，查找表需依据查找概率越高，比较次数越少；查找概率越低，比较次数就较多的原则来存储数据元素。 顺序查找缺点是当n 很大时，平均查找长度较大，效率低；优点是对表中数据元素的存储没有要求。另外，对于线性链表，只能进行顺序查找。 9.2 静态查找表—有序表的折半查找有序表即是表中数据元素按关键码升序或降序排列。 折半查找的思想为：在有序表中，取中间元素作为比较对象，若给定值与中间元素的关键码相等，则查找成功；若给定值小于中间元素的关键码，则在中间元素的左半区继续查找；若给定值大于中间元素的关键码，则在中间元素的右半区继续查找。不断重复上述查找过程，直到查找成功，或所查找的区域无数据元素，查找失败。 【步骤】① low=1；high=length； // 设置初始区间② 当low&gt;high 时，返回查找失败信息// 表空，查找失败③ low≤high，mid=(low+high)/2; // 取中点 a. 若kxtbl.elem[mid].key，low=mid+1；转② // 查找在右半区进行 c. 若kx=tbl.elem[mid].key，返回数据元素在表中位置// 查找成功 【例9.１】有序表按关键码排列如下：7，14，18，21，23，29，31，35，38，42，46，49，52在表中查找关键码为14 和22 的数据元素。 ⑴ 查找关键码为14 的过程 【算法9.2】 12345678910111213141516int Binary_Search( S_TBL tbl KEY kx )&#123; /* 在表tbl 中查找关键码为kx 的数据元素，若找到返回该元素在表中的位置，否则，返回0 */ int mid flag = 0 ； low = 1 ； high = length ； /* ①设置初始区间*/ while ( low &lt;= high ) /* ②表空测试*/ &#123; /* 非空，进行比较测试*/ mid = (low + high) / 2 ； /* ③得到中点*/ if ( kx &lt; tbl.elem[mid].key ) high = mid - 1 ； /* 调整到左半区*/ else if ( kx &gt; tbl.elem[mid].key ) low = mid + 1 ； /* 调整到右半区*/ else &#123; flag = mid ； break ； &#125; /* 查找成功，元素位置设置到flag 中*/ &#125; return(flag);&#125; 性能分析】从折半查找过程看，以表的中点为比较对象，并以中点将表分割为两个子表，对定位到的子表继续这种操作。所以，对表中每个数据元素的查找过程，可用二叉树来描述，称这个描述查找过程的二叉树为判定树。 可以看到，查找表中任一元素的过程，即是判定树中从根到该元素结点路径上各结点关键码的比较次数，也即该元素结点在树中的层次数。对于n 个结点的判定树，树高为k，则有2k-1 -1&lt;n≤2k-1，即k-1&lt;log2(n+1)≤k，所以k= 。因此，折半查找在查找成功时，所进行的关键码比较次数至多为。 接下来讨论折半查找的平均查找长度。为便于讨论，以树高为k 的满二叉树(n=2k-1)为例。假设表中每个元素的查找是等概率的，即Pi= ，则树的第i 层有2i-1 个结点，因此，折半查找的平均查找长度为： 所以，折半查找的时间效率为O(log2n)。 9.2 静态查找表—有序表的插值查找和斐波那契查找1.插值查找插值查找通过下列公式 求取中点，其中low 和high 分别为表的两个端点下标，kx 为给定值。若kxtbl.elem[mid].key，则low=mid+1，继续右半区查找；若kx=tbl.elem[mid].key，查找成功。 插值查找是平均性能最好的查找方法，但只适合于关键码均匀分布的表，其时间效率依然是O(log2n)。 2.斐波那契查找斐波那契查找通过斐波那契数列对有序表进行分割，查找区间的两个端点和中点都与斐波那契数有关。斐波那契数列定义如下： 设n 个数据元素的有序表，且n 正好是某个斐波那契数-1，即n=F(k)-1 时，可用此查找方法。 斐波那契查找分割的思想为：对于表长为F(i)-1 的有序表，以相对low 偏移量F(i-1)-1 取中点，即mid=low+F(i-1)-1，对表进行分割，则左子表表长为F(i-1)-1，右子表表长为F(i)-1-[F(i-1)-1]-1=F(i-2)-1。可见，两个子表表长也都是某个斐波那契数-1，因而，可以对子表继续分割。 【算法9.3】① low=1；high=F(k)-1； // 设置初始区间F=F(k)-1；f=F(k-1)-1； // F 为表长，f 为取中点的相对偏移量② 当low&gt;high 时，返回查找失败信息// 表空，查找失败③ low≤high，mid=low+f; // 取中点a. 若kxtbl.elem[mid].key，则 F=F-f-1； // 调整表长F f=f-F-1； // 计算取中点的相对偏移量 low=mid+1；转② // 查找在右半区进行c. 若kx=tbl.elem[mid].key，返回数据元素在表中位置// 查找成功 当n 很大时，该查找方法称为黄金分割法，其平均性能比折半查找好，但其时间效率仍为O(log2n)，而且，在最坏情况下比折半查找差，优点是计算中点仅作加、减运算。 9.2 静态查找表—分块查找分块查找：索引顺序查找，是对顺序查找的改进。 分块查找要求将查找表分成 若干个子表，并对子表建立索引表，查找表的每一个子表由索引表中的索引项确定。索引 项包括两个字段：关键码字段(存放对应子表中的最大关键码值) ；指针字段(存放指向对 应子表的指针) ，并且要求索引项按关键码字段有序。查找时，先用给定值kx 在索引表中 检测索引项，以确定所要进行的查找在查找表中的查找分块(由于索引项按关键码字段有序，可用顺序查找或折半查找) ，然后，再对该分块进行顺序查找。 【例9.2】关键码集合为：88，43，14，31，78，8，62，49，35，71，22，83，18，52 按关键码值31，62，88 分为三块建立的查找表及其索引表如下： 【性能分析】 9.3 动态查找表—二叉排序树（Binary Sort Tree）1.二叉排序树定义：或者是一棵空树；或者是具有下列性质的二叉树： 若左子树不空，则左子树上所有结点的值均小于根结点的值；若右子树不空，则右子树上所有结点的值均大于根结点的值。 左右子树也都是二叉排序树。 由图9.4 可以看出，对二叉排序树进行中序遍历，便可得到一个按关键码有序的序列，因此，一个无序序列，可通过构一棵二叉排序树而成为有序序列。 2.查找过程： 从其定义可见，二叉排序树的查找过程为： 若查找树为空，查找失败。 查找树非空，将给定值kx 与查找树的根结点关键码比较。 若相等，查找成功，结束查找过程，否则， a．当给kx 小于根结点关键码，查找将在以左子女为根的子树上继续进行，转① b．当给kx 大于根结点关键码，查找将在以右子女为根的子树上继续进行，转① 以二叉链表作为二叉排序树的存储结构，则查找过程算法程序描述如下： 1234typedef struct NODE&#123; ElemType elem; /*数据元素字段*/struct NODE *lc,*rc; /*左、右指针字段*/&#125;NodeType; /*二叉树结点类型*/ 【算法9.4】123456789101112131415161718int SearchElem( NodeType *t, NodeType **p, NodeType **q, KeyType kx )&#123; /*在二叉排序树t 上查找关键码为kx 的元素，若找到，返回1，且q 指向该结点，p 指向其父结点；*//*否则，返回0，且p 指向查找失败的最后一个结点*/ int flag = 0; *q = t; while ( *q ) /*从根结点开始 * 查找*/ &#123; if ( kx &gt; (*q)-&gt;elem.key ) /*kx 大于当前结点*q 的元素关键码*/ &#123; *p = *q; *q = (*q)-&gt;rc; &#125; /*将当前结点*q 的右子女置为新根*/ else&#123; if ( kx &lt; (*q)-&gt;elem.key ) /*kx 小于当前结点*q 的元素关键码*/ &#123; *p = *q; *q = (*q)-&gt;lc; &#125; /*将当前结点*q 的左子女置为新根*/ else &#123; flag = 1; break; &#125; /*查找成功，返回*/ &#125; &#125; /*while*/ return(flag);&#125; 3.插入操作和构造一棵二叉排序树 先讨论向二叉排序树中插入一个结点的过程：设待插入结点的关键码为kx，为将其插入，先要在二叉排序树中进行查找，若查找成功，按二叉排序树定义，待插入结点已存在，不用插入；查找不成功时，则插入之。因此，新插入结点一定是作为叶子结点添加上去的。 构造一棵二叉排序树则是逐个插入结点的过程。 【例9.3】记录的关键码序列为：63，90，70，55，67，42，98，83，10，45，58，则构造一棵二叉排序树的过程如下： 12345678910111213141516int InsertNode( NodeType **t, KeyType kx )&#123; /*在二叉排序树*t 上插入关键码为kx 的结点*/ NodeType *p = *t, *q, *s; int flag = 0; if ( !SearchElem( *t, &amp;p, &amp;q, kx ) ) ; /*在*t 为根的子树上查找*/ &#123; s = (NodeType *) )malloc( sizeof(NodeType) ); /*申请结点，并赋值*/ s-&gt;elem.key = kx; s-&gt;lc = NULL; s-&gt;rc = NULL; flag = 1; /*设置插入成功标志*/ if ( !p ) t = s; /*向空树中插入时*/ else&#123; if ( kx &gt; p-&gt;elem.key ) p-&gt;rc = s; /*插入结点为p 的右子女*/ else p-&gt;lc = s; /*插入结点为p 的左子女*/ &#125; &#125; return(flag);&#125; 4.删除操作 从二叉排序树中删除一个结点之后，使其仍能保持二叉排序树的特性即可。设待删结点为p（p 为指向待删结点的指针），其双亲结点为f，以下分三种情况进行讨论。p 结点为叶结点，由于删去叶结点后不影响整棵树的特性，所以，只需将被删结点的双亲结点相应指针域改为空指针。如图9.6。 p 结点只有右子树pr 或只有左子树pl，此时，只需将pr 或pl 替换f 结点的p 子树即可。如图9.7。p 结点既有左子树Pl 又有右子树Pr，可按中序遍历保持有序进行调整。设删除p 结点前，中序遍历序列为：P 为F 的左子女时有：…，Pl 子树，P，Pj ，S 子树，Pk，Sk子树，…，P2，S2子树，P1，S1子树，F，…P 为F 的右子女时有：…，F，Pl 子树，P，Pj ，S 子树，Pk，Sk子树，…，P2，S2子树，P1，S1子树，…则删除*p 结点后，中序遍历序列应为：P 为F 的左子女时有：…，Pl 子树，Pj ，S 子树，Pk，Sk子树，…，P2，S2子树，P1，S1子树，F，…P 为F 的右子女时有：…，F，Pl 子树，Pj ，S 子树，Pk，Sk子树，…，P2，S2子树，P1，S1子树，… 有两种调整方法：直接令pl 为f 相应的子树，以pr 为pl 中序遍历的最后一个结点pk 的右子树；令p 结点的直接前驱Pr 或直接后继（对Pl子树中序遍历的最后一个结点Pk）替换p 结点，再按⒉的方法删去Pr 或Pk。图9.8 所示的就是以p 结点的直接前驱Pr 替换*p。 123456789101112131415161718192021222324252627282930313233int DeleteNode( NodeType **t, KeyType kx )&#123; NodeType *p = *t, *q, *s, **f; int flag = 0; if ( SearchElem( *t, &amp;p, &amp;q, kx ) ) ; &#123; flag = 1; /*查找成功，置删除成功标志*/ if ( p = = q ) f = &amp;(*t); /*待删结点为根结点时*/ else&#123; /*待删结点非根结点时*/ f = &amp;(p-&gt;lc); if ( kx &gt; p-&gt;elem.key ) f = &amp;(p-&gt;rc); &#125; /*f 指向待删结点的父结点的相应指针域*/ if ( !q-&gt;rc ) *f = q-&gt;lc; /*若待删结点无右子树，以左子树替换待删结点*/ else&#123; if ( !q-&gt;lc ) *f = q-&gt;rc; /*若待删结点无左子树，以右子树替换待删结点*/ else&#123; /*既有左子树又有右子树*/ p = q-&gt;rc; s = p; while ( p-&gt;lc ) &#123; s = p; p = p-&gt;lc; &#125; /*在右子树上搜索待删结点的前驱p*/ *f = p; p-&gt;lc = q-&gt;lc; /*替换待删结点q，重接左子树*/ if ( s != p ) &#123; s-&gt;lc = p-&gt;rc; /*待删结点的右子女有左子树时，还要重接右子树*/ p-&gt;rc = q-&gt;rc; &#125; &#125; &#125; free( q ); &#125; return(flag);&#125; 对给定序列建立二叉排序树，若左右子树均匀分布，则其查找过程类似于有序表的折半查找。但若给定序列原本有序，则建立的二叉排序树就蜕化为单链表，其查找效率同顺序查找一样。因此，对均匀的二叉排序树进行插入或删除结点后，应对其调整，使其依然保持均匀。 9.3 动态查找表—平衡二叉树(AVL树)平衡二叉树或者是一棵空树，或者是具有下列性质的二叉排序树：它的左子树和右子树都是平衡二叉树，且左子树和右子树高度之差的绝对值不超过1。 图9.9 给出了两棵二叉排序树，每个结点旁边所注数字是以该结点为根的树中，左子树与右子树高度之差，这个数字称为结点的平衡因子。由平衡二叉树定义，所有结点的平衡因子只能取-1，0，1 三个值之一。若二叉排序树中存在这样的结点，其平衡因子的绝对值大于1，这棵树就不是平衡二叉树。如图9.9 （a）所示的二叉排序树。 在平衡二叉树上插入或删除结点后，可能使树失去平衡，因此，需要对失去平衡的树进行平衡化调整。设a 结点为失去平衡的最小子树根结点，对该子树进行平衡化调整归纳起来有以下四种情况： 一. 左单旋转 如图9.10 的图(a)为插入前的子树。其中，B 为结点a 的左子树，D、E 分别为结点c的左右子树，B、D、E 三棵子树的高均为h。图(a)所示的子树是平衡二叉树。在图(a)所示的树上插入结点x，如图(b)所示。结点x 插入在结点c 的右子树E 上，导致结点a 的平衡因子绝对值大于1，以结点a 为根的子树失去平衡。 【调整策略】调整后的子树除了各结点的平衡因子绝对值不超过1，还必须是二叉排序树。由于结点c 的左子树D 可作为结点a 的右子树，将结点a 为根的子树调整为左子树是B，右子树是D，再将结点a 为根的子树调整为结点c 的左子树，结点c 为新的根结点，如图(c)。 【平衡化调整操作判定】沿插入路径检查三个点a、c、E，若它们处于“\”直线上的同一个方向，则要作左单旋转，即以结点c 为轴逆时针旋转。 二、右单旋转 右单旋转与左单旋转类似，沿插入路径检查三个点a、c、E，若它们处于“/”直线上的同一个方向，则要作右单旋转，即以结点c 为轴顺时针旋转，如图9.11 所示。 三. 先左后右双向旋转 如图9.12 为插入前的子树，根结点a 的左子树比右子树高度高1，待插入结点x 将插入到结点b 的右子树上，并使结点b 的右子树高度增1，从而使结点a 的平衡因子的绝对值大于1，导致结点a 为根的子树平衡被破坏，如图9.13(a)、9.14(d)所示。 沿插入路径检查三个点a、b、c，若它们呈“&lt;”字形，需要进行先左后右双向旋转： 首先，对结点b 为根的子树，以结点c 为轴，向左逆时针旋转，结点c 成为该子树的新根，如图(b、e)； 由于旋转后，待插入结点x 相当于插入到结点b 为根的子树上，这样a、c、b 三点处于“/”直线上的同一个方向，则要作右单旋转，即以结点c 为轴顺时针旋转，如图(c、f)。 在平衡的二叉排序树T 上插入一个关键码为kx 的新元素，递归算法可描述如下： 若T 为空树，则插入一个数据元素为kx 的新结点作为T 的根结点，树的深度增1； 若kx 和T 的根结点关键码相等，则不进行插入； 若kx 小于T 的根结点关键码，而且在T 的左子树中不存在与kx 有相同关键码的结点， 则将新元素插入在T 的左子树上，并且当插入之后的左子树深度增加1 时，分别就下列情况进行处理： ① T 的根结点平衡因子为-1(右子树的深度大于左子树的深度)，则将根结点的平衡因子更改为0，T 的深度不变；② T 的根结点平衡因子为0(左、右子树的深度相等)，则将根结点的平衡因子更改为1，T 的深度增加1；③ T 的根结点平衡因子为1(左子树的深度大于右子树的深度)，则若T 的左子树根结点的平衡因子为1，需进行单向右旋平衡处理，并且在右旋处理之后，将根结点和其右子树根结点的平衡因子更改为0，树的深度不变；若T 的左子树根结点平衡因子为-1，需进行先左后右双向旋转平衡处理，并且在旋转处理之后，修改根结点和其左、右子树根结点的平衡因子，树的深度不变。 若kx 大于T 的根结点关键码，而且在T 的右子树中不存在与kx 有相同关键码的结点，则将新元素插入在T 的右子树上，并且当插入之后的右子树深度增加1 时，分别就不同情况处理之。其处理操作和(3.) 中所述相对称，读者可自行补充整理。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990typedef struct NODE &#123; ElemType elem; /*数据元素*/ int bf; /*平衡因子*/ struct NODE *lc, *rc; /*左右子女指针*/&#125;NodeType; /*结点类型*/void R_Rotate( NodeType **p )&#123; /*对以*p 指向的结点为根的子树，作右单旋转处理，处理之后，*p 指向的结点为子树的新根*/ lp = (*p)-&gt;lc; /*lp 指向*p 左子树根结点*/ (*p)-&gt;lc = lp-&gt;rc; /*lp 的右子树挂接*p 的左子树*/ lp-&gt;rc = *p; *p = lp; /* *p 指向新的根结点*/&#125;void L_Rotate( NodeType **p )&#123; /*对以*p 指向的结点为根的子树，作左单旋转处理，处理之后，*p 指向的结点为子树的新根*/ lp = (*p)-&gt;rc; /*lp 指向*p 右子树根结点*/ (*p)-&gt;rc = lp-&gt;lc; /*lp 的左子树挂接*p 的右子树*/ lp-&gt;lc = *p; *p = lp; /* *p 指向新的根结点*/&#125;#define LH 1 /* 左高*/#define EH 0 /* 等高*/#define RH 1 /* 右高*/void LeftBalance ( (NodeType * *p) &#123; /*对以*p 指向的结点为根的子树，作左平衡旋转处理，处理之后，*p 指向的结点为子树的新根*/ lp = (*p)-&gt;lc; /*lp 指向*p 左子树根结点*/ switch ( (*p)-&gt;bf ) /*检查*p 平衡度，并作相应处理*/ &#123; case LH: /*新结点插在*p 左子女的左子树上，需作单右旋转处理*/ (*p)-&gt;bf = lp-&gt;bf = EH; R_Rotate( p ); break; case EH: /*原本左、右子树等高，因左子树增高使树增高*/ (*p)-&gt;bf = LH; *paller = TRUE; break; case RH: /*新结点插在*p 左子女的右子树上，需作先左后右双旋处理*/ rd = lp-&gt;rc; /*rd 指向*p 左子女的右子树根结点*/ switch ( rd-&gt;bf ) /*修正*p 及其左子女的平衡因子*/ &#123; case LH: (*p)-&gt;bf = RH; lp-&gt;bf = EH; break; case EH: (*p)-&gt;bf = lp-&gt;bf = EH; break; case RH: (*p)-&gt;bf = EH; lp-&gt;bf = LH; break; &#125; /*switch(rd-&gt;bf)*/ rd-&gt;bf = EH; L_Rotate( &amp;( (*p)-&gt;lc) ); /*对*p 的左子树作左旋转处理*/ R_Rotate( p ); /*对*t 作右旋转处理*/ &#125; /*switch((*p)-&gt;bf)*/ &#125; /*LeftBalance*/ int InsertAVL( NodeType **t, ElemType e, Boolean *taller ) &#123; /*若在平衡的二叉排序树t 中不存在和e 有相同关键码的结点，则插入一个数据元素为e 的*//*新结点，并反回1，否则反回0。若因插入而使二叉排序树失去平衡，则作平衡旋转处理，*//*布尔型变量taller 反映t 长高与否*/ if ( !(*t) ) /*插入新结点，树“长高”，置taller 为TURE*/ &#123; *t = (NodeType *) malloc( sizeof(NodeType) ); (*T)-&gt;elem = e; (*t)-&gt;lc = (*t)-&gt;rc = NULL; (*t)-&gt;bf = EH; *taller = TRUE; &#125; /*if*/ else&#123; if ( e.key == (*t)-&gt;elem.key ) /*树中存在和e 有相同关键码的结点，不插入*/ &#123; taller = FALSE; return(0); &#125; if ( e.key &lt; (*t)-&gt;elem.key ) &#123; /*应继续在*t 的左子树上进行*/ if ( !InsertAVL( &amp;( (*t)-&gt;lc) ), e, &amp;taller ) ) return(0); /*未插入*/ if ( *taller ) /*已插入到(*t)的左子树中，且左子树增高*/ switch ( (*t)-&gt;bf ) /*检查*t 平衡度*/ &#123; case LH: /*原本左子树高，需作左平衡处理*/ LeftBalance( t ); *taller = FALSE; break; case EH: /*原本左、右子树等高，因左子树增高使树增高*/ (*t)-&gt;bf = LH; *taller = TRUE; break; case RH: /*原本右子树高，使左、右子树等高*/ (*t)-&gt;bf = EH; *taller = FALSE; break; &#125; &#125; /*if*/ else&#123; /*应继续在*t 的右子树上进行*/ if ( !InsertAVL( &amp;( (*t)-&gt;rc) ), e, &amp;taller ) ) return(0); /*未插入*/ if ( *taller ) /*已插入到(*t)的左子树中，且左子树增高*/ switch ( (*t)-&gt;bf ) /*检查*t 平衡度*/ &#123; case LH: /*原本左子树高，使左、右子树等高*/ (*t)-&gt;bf = EH; *taller = FALSE; break; case EH: /*原本左、右子树等高，因右子树增高使树增高*/ (*t)-&gt;bf = RH; *taller = TRUE; break; case RH: /*原本右子树高，需作右平衡处理*/ RightBalance( t ); *taller = FALSE; break; &#125; &#125; /*else*/ &#125; /*else*/ return(1); &#125; /*InsertAVL*/ 【平衡树的查找分析】在平衡树上进行查找的过程和二叉排序树相同，因此，在查找过程中和给定值进行比较的关键码个数不超过树的深度。那么，含有n 个关键码的平衡树的最大深度是多少呢？ 为解答这个问题，我们先分析深度为h 的平衡树所具有的最少结点数。假设以Nh 表示深度为h 的平衡树中含有的最少结点数。显然，N0=0，N1=1，N2=2，并且Nh=Nh-1+Nh-2+1。这个关系和斐波那契序列极为相似。利用归纳法容易证明：当h≥0间复杂度为O(logn)。 上述对二叉排序树和二叉平衡树的查找性能的讨论都是在等概率的提前下进行的。 9.3 动态查找表—B-树和B+树一.B-树及其查找B-树是一种平衡的多路查找树，它在文件系统中很有用。 二.B-树的插入和删除三. B+树B+树是应文件系统所需而产生的一种B-树的变形树。 9.4 哈希表查找(杂凑法)—哈希表与哈希方法前述查找法：由于数据元素的存储位置与关键码之间不存在确定的关系，因此，查找时，需要进行一系列对关键码的查找比较，即“查找算法”是建立在比较的基础上的，查找效率由比较一次缩小的查找范围决定。 哈希查找：依据关键码直接得到其对应的数据元素位置，即要求关键码与数据元素间存在一一对应关系，通过这个关系，能很快地由关键码得到对应的数据元素位置。 例9.6】11 个元素的关键码分别为18，27，1，20，22，6，10，13，41，15，25。选取关键码与元素位置间的函数为f(key)=key mod 11 通过这个函数对11 个元素建立查找表如下： 查找时，对给定值kx 依然通过这个函数计算出地址，再将kx 与该地址单元中元素的关键码比较，若相等，查找成功。 哈希表与哈希方法：选取某个函数，依该函数按关键码计算元素的存储位置，并按此存放；查找时，由同一个函数对给定值kx 计算地址，将kx 与地址单元中元素关键码进行比，确定查找是否成功，这就是哈希方法(杂凑法)；哈希方法中使用的转换函数称为哈希函数(杂凑函数)；按这个思想构造的表称为哈希表(杂凑表)。 对于n 个数据元素的集合，总能找到关键码与存放地址一一对应的函数。若最大关键为m，可以分配m 个数据元素存放单元，选取函数f(key)=key 即可，但这样会造成存储空间的很大浪费，甚至不可能分配这么大的存储空间。通常关键码的集合比哈希地址集合大得多，因而经过哈希函数变换后，可能将不同的关键码映射到同一个哈希地址上，这种现象称为冲突(Collision)，映射到同一哈希地址上的关键码称为同义词。可以说，冲突不可能避免，只能尽可能减少。所以，哈希方法需要解决以下两个问题： 1.构造好的哈希函数（1）所选函数尽可能简单，以便提高转换速度。（2）所选函数对关键码计算出的地址，应在哈希地址集中大致均匀分布，以减少空间浪费。 2.制定解决冲突的方案。 9.4 哈希表查找(杂凑法)—常用的哈希函数1.直接定址法Hash(key) = a.key + b（a，b为常数） 即取关键码的某个线性函数值为哈希地址，这类函数是一一对应函数，不会产生冲突，但要求地址集合与关键码集合大小相同，因此，对于较大的关键码集合不适用。 【例9.7】关键码集合为{100，300，500，700，800，900}，选取哈希函数为Hash(key)=key/100，则存放如下： 2.除留余数法Hash(key) = key mod p （p是一个整数） 即取关键码除以p 的余数作为哈希地址。使用除留余数法，选取合适的p 很重要，若哈希表表长为m，则要求p≤m，且接近m 或等于m。p 一般选取质数，也可以是不包含小于20 质因子的合数。 3.乘余取整法Hash(key)= ⎣B(Akey mod 1)⎦ (A、B 均为常数，且0&lt;A&lt;1，B 为整数)以关键码key 乘以A，取其小数部分(Akey mod 1 就是取Akey 的小数部分)，之后再用整数B 乘以这个值，取结果的整数部分作为哈希地址。 该方法B 取什么值并不关键，但A 的选择却很重要，最佳的选择依赖于关键码集合的特征。一般取A= 较为理想。 4.数字分析法设关键码集合中，每个关键码均由m 位组成，每位上可能有r 种不同的符号。【例9.8】若关键码是4 位十进制数，则每位上可能有十个不同的数符0～9，所以r=10。【例9.9】若关键码是仅由英文字母组成的字符串，不考虑大小写，则每位上可能有26 种不同的字母，所以r=26。 数字分析法根据r 种不同的符号，在各位上的分布情况，选取某几位，组合成哈希地址。所选的位应是各种符号在该位上出现的频率大致相同。 【例9.10】有一组关键码如下： 5.平方取中法对关键码平方后，按哈希表大小，取中间的若干位作为哈希地址。 6.折叠法Folding此方法将关键码自左到右分成位数相等的几部分，最后一部分位数可以短些，然后将这几部分叠加求和，并按哈希表表长，取后几位作为哈希地址。这种方法称为折叠法。 有两种叠加方法： 移位法── 将各部分的最后一位对齐相加。 间界叠加法── 从一端向另一端沿各部分分界来回折叠后，最后一位对齐相加。 【例9.11】关键码为key=05326248725，设哈希表长为三位数，则可对关键码每三位一部分来分割。 关键码分割为如下四组： 253 463 587 05 用上述方法计算哈希地址对于位数很多的关键码，且每一位上符号分布较均匀时，可采用此方法求得哈希地址。 9.4 哈希表查找(杂凑法)—处理冲突的方法1.开放定址法开放定址法：由关键码得到的哈希地址产生冲突，即改地址已存放了数据元素，则去寻找下一个空哈希地址，并存入。 下面介绍找空哈希地址的方法： 1.线性探测法 Hi=(Hash(key)+di) mod m ( 1≤i &lt; m )其中：Hash(key)为哈希函数m 为哈希表长度di 为增量序列1，2，……，m-1，且di=i 【例9.12】关键码集为{47，7，29，11，16，92，22，8，3}，哈希表表长为11，Hash(key)=key mod 11，用线性探测法处理冲突，建表如下： 2.二次探测法 Hi=(Hash(key)±di) mod m其中：Hash(key)为哈希函数m 为哈希表长度，m 要求是某个4k+3 的质数(k 是整数)di 为增量序列12，-12，22，-22，……，q2，-q2 且q≤1/2 (m-1) 仍以上例用二次探测法处理冲突，建表如下： 对关键码寻找空的哈希地址只有3 这个关键码与上例不同，Hash(3)=3，哈希地址上冲突，由H1=(Hash(3)+12) mod 11=4 仍然冲突；H2=(Hash(3)-12) mod 11=2 找到空的哈希地址，存入。 双哈希函数探测法Hi=(Hash(key)+i*ReHash(key)) mod m (i=1，2，……，m-1)其中：Hash(key)，ReHash(key)是两个哈希函数，m 为哈希表长度 双哈希函数探测法，先用第一个函数Hash(key)对关键码计算哈希地址，一旦产生地址冲突，再用第二个函数ReHash(key)确定移动的步长因子，最后，通过步长因子序列由探测函数寻找空的哈希地址。 比如，Hash(key)=a 时产生地址冲突，就计算ReHash(key)=b，则探测的地址序列为H1=(a+b) mod m，H2=(a+2b) mod m，……，Hm-1=(a+(m-1)b) mod m 2. 拉链法设哈希函数得到的哈希地址域在区间[0，m-1]上，以每个哈希地址作为一个指针，指向一个链，即分配指针数组ElemType eptr[m]；建立m 个空链表，由哈希函数对关键码转换后，映射到同一哈希地址i 的同义词均加入到eptr[i]指向的链表中。 【例9.l3】关键码序列为47,7,29,11,16,92,22,8,3,50,37,89,94,21，哈希函数为Hash(key)=key mod 11 用拉链法处理冲突，建表如图9.21。 3. 建立一个公共溢出区设哈希函数产生的哈希地址集为[0，m-1]，则分配两个表：一个基本表ElemType base_tbl[m]；每个单元只能存放一个元素；一个溢出表ElemType over_tbl[k]；只要关键码对应的哈希地址在基本表上产生冲突，则所有这样的元素一律存入该表中。查找时，对给定值kx 通过哈希函数计算出哈希地址i，先与基本表的base_tbl[i]单元比较，若相等，查找成功；否则，再到溢出表中进行查找。 9.4 哈希表查找(杂凑法)—哈希表的查找分析哈希表的查找过程基本上和造表过程相同。一些关键码可通过哈希函数转换的地址直接找到，另一些关键码在哈希函数得到的地址上产生了冲突，需要按处理冲突的方法进行查找。在介绍的三种处理冲突的方法中，产生冲突后的查找仍然是给定值与关键码进行比较的过程。所以，对哈希表查找效率的量度，依然用平均查找长度来衡量。 查找过程中，关键码的比较次数，取决于产生冲突的多少，产生的冲突少，查找效率就高，产生的冲突多，查找效率就低。因此，影响产生冲突多少的因素，也就是影响查找效率的因素。影响产生冲突多少有以下三个因素： 哈希函数是否均匀 处理冲突的方法 哈希表的装填因子 分析这三个因素，尽管哈希函数的“好坏”直接影响冲突产生的频度，但一般情况下，我们总认为所选的哈希函数是“均匀的”，因此，可不考虑哈希函数对平均查找长度的影响。 就线性探测法和二次探测法处理冲突的例子看，相同的关键码集合、同样的哈希函数，但在数据元素查找等概率情况下，它们的平均查找长度却不同： 线性探测法的平均查找长度ASL=(5×1+3×2+1×4)/9=5/3二次探测法的平均查找长度ASL=(5×1+3×2+1×2)/9=13/9 α是哈希表装满程度的标志因子。由于表长是定值，α与“填入表中的元素个数”成正比，所以，α越大，填入表中的元素较多，产生冲突的可能性就越大；α越小，填入表中的元素较少，产生冲突的可能性就越小。 实际上，哈希表的平均查找长度是装填因子α的函数，只是不同处理冲突的方法有不同的函数。以下给出几种不同处理冲突方法的平均查找长度： 哈希方法存取速度快，也较节省空间，静态查找、动态查找均适用，但由于存取是随机的，因此，不便于顺序查找。]]></content>
      <categories>
        <category>Computer Science</category>
        <category>Data Structure</category>
      </categories>
      <tags>
        <tag>Data Structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构第六章：二叉树]]></title>
    <url>%2F2017%2F12%2F18%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E7%AC%AC%E5%85%AD%E7%AB%A0%EF%BC%9A%E4%BA%8C%E5%8F%89%E6%A0%91%2F</url>
    <content type="text"><![CDATA[Abstract：二叉树。 6.1 二叉树的定义域性质6.1.1 二叉树的基本概念1.二叉树Binary Tree 有限个元素的集合，集合或为空、或由一个称为根（root）的元素和2个不想交的、被分别称为左子树和右子树的二叉树组成 结点：一个元素称为一个结点 有序：即左右子树颠倒即成为不同的二叉树；即使树中只有一棵子树也要区分 5种基本形态： 2.二叉树的相关概念 结点的度：结点拥有的子树个数 叶节点：度=0的结点，或称终端结点 分枝结点：度不为0的结点；除叶节点外的节点都是分支结点 左孩子，右孩子，双亲，兄弟 路径，路径长度：结点到结点所要经过的边数；eg.如果一棵树的一串结点n1,n2,…,nk 有如下关系：结点ni 是ni+1的父结点（1≤i&lt;k）,就把n1,n2,…,nk 称为一条由n1 至nk 的路径。这条路径的长度是k-1 祖先、子孙：在树中，如果有一条路径从结点M 到结点N，那么M 就称为N的祖先，而N 称为M 的子孙 结点的层数：规定树的根结点的层数为1，其余结点的层数等于它的双亲结点的层数加1。 树的深度。树中所有结点的最大层数称为树的深度。 树的度。树中各结点度的最大值称为该树的度。 满二叉树：所有分支结点都存在左子树和右子树，并且所有叶子结点都在同一层上。 完全二叉树：一棵深度为k 的有n 个结点的二叉树，对树中的结点按从上至下、从左到右的顺序进行编号，如果编号为i（1≤i≤n）的结点与满二叉树中编号为i 的结点在二叉树中的位置相同，则这棵二叉树称为完全二叉树。完全二叉树的特点是：叶子结点只能出现在最下层和次下层，且最下层的叶子结点集中在树的左部。显然，一棵满二叉树必定是一棵完全二叉树，而完全二叉树未必是满二叉树。 6.1.2 二叉树的主要性质性质1：一棵非空二叉树的第i 层上最多有2i-1 个结点（i≥1） 性质2：一棵深度为k 的二叉树中，最多具有2k－1 个结点 性质3：对于一棵非空的二叉树，如果叶子结点数为n0，度数为2 的结点数为n2，则有: n0＝n2＋1。 性质4：具有n个结点的完全二叉树的深度k = [log2n + 1]（2为下标） 性质5：对于具有n 个结点的完全二叉树，如果按照从上至下和从左到右的顺序对二叉树中的所有结点从1 开始顺序编号，则对于任意的序号为i 的结点，有： （1）如果i&gt;1，则序号为i 的结点的双亲结点的序号为i/2(“/”表示整除)；如果i＝1，则序号为i 的结点是根结点，无双亲结点。 （2）如果2i≤n，则序号为i 的结点的左孩子结点的序号为2i；如果2i&gt;n，则序号为i 的结点无左孩子。 （3）如果2i＋1≤n，则序号为i 的结点的右孩子结点的序号为2i＋1；如果2i＋1&gt;n，则序号为i 的结点无右孩子。 性质6：若对二叉树的根结点从0 开始编号，则相应的i 号结点的双亲结点的编号为（i -1）/2，左孩子的编号为2i＋1，右孩子的编号为2i＋2。 6.2 基本操作与存储—二叉树的存储1.顺序存储结构顺序存储：用一组连续的存储单元存放二叉树的结点，顺序为从上至下，从左至右。 一般的二叉树：数组元素下标的关系不能反映二叉树结点间的逻辑关系（结点在存储位置上的前驱后继关系并不一定就是它们在逻辑上的邻接关系），只有增添一些并不存在的空结点，使之成为一棵完全二叉树的形式，然后再用一维数组顺序存储（先增加空节点，再存储）。 完全二叉树和满二叉树：可以反映 改造：空间浪费；最坏的情况是右单支树，如图：一棵深度为k 的右单支树，只有k 个结点，却需分配2k－1 个存储单元。 2.链式存储结构二叉树的链式存储：链表表示二叉树，链指示元素的逻辑关系。 (1) 二叉链表存储【最常用】结点：三个域组成，数据域+指针域（分别指向该结点左孩子和右孩子所在结点的存储地址） data域：存放结点的数据信息；lchild 与rchild 分别存放指向左孩子和右孩子的指针，当左孩子或右孩子不存在时，相应指针域值为空（用符号∧或NULL 表示） 描述： 1234typedef struct BiTNode&#123; elemtype data; struct BiTNode *lchild; *rchild; &#125;BiTNode, *BiTree; // 将BiTree 定义为指向二叉链表结点结构的指针类型 (2) 三叉链表存储每个结点四个域。 parent 域为指向该结点双亲结点的指针 优点：便于寻找孩子结点和双亲 缺点：增加空间开销 6.2 基本操作与存储—二叉树的基本操作及实现1.二叉树的基本操作 initiate(bt): 建立一棵空二叉树 create（x，lbt，rbt）：生成一棵以x为根结点的数据域信息，以二叉树lbt和rbt为左子树和右子树的二叉树 Insert（bt，x，parent）：将数据域信息为x 的结点插入到二叉树bt 中作为结点parent 的左孩子结点。如果结点parent 原来有左孩子结点，则将结点parent 原来的左孩子结点作为结点x 的左孩子结点。 InsertR（bt，x，parent）将数据域信息为x 的结点插入到二叉树bt 中作为结点parent 的右孩子结点。如果结点parent 原来有右孩子结点，则将结点parent 原来的右孩子结点作为结点x 的右孩子结点。 DeleteL（bt，parent）在二叉树bt 中删除结点parent 的左子树。 DeleteR（bt，parent）在二叉树bt 中删除结点parent 的右子树。 Search（bt，x）在二叉树bt 中查找数据元素x。 Traverse（bt）按某种方式遍历二叉树bt 的全部结点。 2．算法的实现——基于二叉链表1.Initiate（bt）初始建立二叉树bt，并使bt 指向头结点。在二叉树根结点前建立头结点，就如同在单链表前建立的头结点，可以方便后边的一些操作实现。 12345678int Initiate (BiTree *bt)&#123;/*初始化建立二叉树*bt 的头结点*/if((*bt=(BiTNode *)malloc(sizeof(BiTNode)))==NULL) return 0;*bt-&gt;lchild=NULL;*bt-&gt;rchild=NULL;return 1;&#125; 2.Create（x，lbt，rbt）建立一棵以x 为根结点的数据域信息，以二叉树lbt 和rbt为左右子树的二叉树。建立成功时返回所建二叉树结点的指针；建立失败时返回空指针。123456789BiTree Create（elemtype x，BiTree lbt，BiTree rbt）&#123;/*生成一棵以x 为根结点的数据域值以lbt 和rbt 为左右子树的二叉树*/BiTree p;if ((p=(BiTNode *)malloc(sizeof(BiTNode)))==NULL) return NULL; p-&gt;data=x; p-&gt;lchild=lbt; p-&gt;rchild=rbt;return p;&#125; 3.InsertL（bt，x，parent） 1234567891011121314151617BiTree InsertL（BiTree bt，elemtype x，BiTree parent）&#123;/*在二叉树bt 的结点parent 的左子树插入结点数据元素x*/BiTree p;if (parent==NULL)&#123; printf(“\n 插入出错”)；return NULL;&#125;if ((p=(BiTNode *)malloc(sizeof(BiTNode)))==NULL) return NULL; p-&gt;data=x; p-&gt;lchild=NULL; p-&gt;rchild=NULL;if (parent-&gt;lchild==NULL) parent-&gt;lchild=p; else &#123;p-&gt;lchild=parent-&gt;lchild; parent-&gt;lchild=p;&#125;return bt;&#125; 4.DeleteL（bt，parent）在二叉树bt 中删除结点parent 的左子树。当parent 或parent的左孩子结点为空时删除失败。删除成功时返回根结点指针；删除失败时返回空指针。 12345678910111213BiTree DeleteL（BiTree bt，BiTree parent）&#123;/*在二叉树bt 中删除结点parent 的左子树*/BiTree p;if (parent==NULL||parent-&gt;lchild==NULL)&#123; printf(“\n 删除出错”)；return NULL’&#125; p=parent-&gt;lchild; parent-&gt;lchild=NULL; free(p); /*当p 为非叶子结点时，这样删除仅释放了所删子树根结点的空间，*//*若要删除子树分支中的结点，需用后面介绍的遍历操作来实现。*/return br;&#125; 6.3 二叉树的遍历—二叉树的遍历方法及递归实现二叉树的遍历: 指按照某种顺序访问二叉树中的每个结点，使每个结点被访问一次且仅被访问一次。 若以D、L、R分别访问根结点、遍历根结点的左子树、遍历根结点的右子树，则二叉树的遍历方式有六种：DLR、LDR、LRD、DRL、RDL 和RLD。如果限定先左后右，则只有前三种方式，即DLR（称为先序遍历）、LDR（称为中序遍历）和LRD（称为后序遍历）。 1.先序遍历DLR先序遍历的递归过程：若二叉树为空，遍历结束。否则： 访问根结点 先序遍历根结点的左子树 先序遍历根结点的右子树 递归算法： 1234567void PreOrder(BiTree bt)&#123;if (bt == NULL) return; Visite (bt-&gt;data); //遍历bt二叉树的数据域 PreOrder (bt-&gt;lchild);//preorder指先序 PreOrder (bt-&gt;rchild);&#125; 对上图b先序遍历：A-B-D-G-C-E-F(从根结点开始，先左后右) 2.中序遍历LDR递归过程：若二叉树为空，遍历结束。否则： 中序遍历根结点的左子树 访问根结点 中序遍历根结点的右子树 递归算法： 1234567void InOrder (BiTree bt)&#123;/*中序遍历二叉树bt*/if (bt==NULL) return; /*递归调用的结束条件*/InOrder（bt-&gt;lchild）; /*中序递归遍历bt 的左子树*/Visite（bt-&gt;data）; /*访问结点的数据域*/InOrder（bt-&gt;rchild）; /*中序递归遍历bt 的右子树*/&#125; 对上图b中序：D-G-B-A-C-E-F（从左子树开始） 3.后序遍历LRD递归：若二叉树为空，遍历结束。否则， （1）后序遍历根结点的左子树；（2）后序遍历根结点的右子树。（3）访问根结点； 算法： 1234567void PostOrder（BiTree bt）&#123;/*后序遍历二叉树bt*/if (bt==NULL) return; /*递归调用的结束条件*/ PostOrder（bt-&gt;lchild）; /*后序递归遍历bt 的左子树*/ PostOrder（bt-&gt;rchild）; /*后序递归遍历bt 的右子树*/ Visite（bt-&gt;data）; /*访问结点的数据域*/&#125; 对图b后序：G-D-B-E-F-C-A 4.层次遍历从根结点出发，由上至下由左至右逐个访问结点。如图b：A-B-C-D-E-F-G 算法： 在进行层次遍历时，可设置一个队列结构，遍历从二叉树的根结点开始，首先将根结点指针入队列，然后从对头取出一个元素，每取一个元素，执行下面两个操作： （1）访问该元素所指结点； （2）若该元素所指结点的左、右孩子结点非空，则将该元素所指结点的左孩子指针和右孩子指针顺序入队。 此过程不断进行，当队列为空时，二叉树的层次遍历结束。在下面的层次遍历算法中，二叉树以二叉链表存放，一维数组Queue[MAXNODE]用以实现队列，变量front 和rear 分别表示当前对首元素和队尾元素在数组中的位置。 12345678910111213141516171819202122232425void LevelOrder （ BiTree bt/*层次遍历二叉树bt*/&#123; BiTree Queue[MAXNODE]; int front, rear; if ( bt == NULL ) return; front = -1; rear = 0; queue[rear] = bt; while ( front != rear ) &#123; front++; Visite( queue[front]-&gt;data ); /*访问队首结点的数据域*/ if ( queue[front]-&gt;lchild != NULL ) /*将队首结点的左孩子结点入队列*/ &#123; rear++; queue[rear] = queue[front]-&gt;lchild; &#125; if ( queue[front]-&gt;rchild != NULL ) /*将队首结点的右孩子结点入队列*/ &#123; rear++; queue[rear] = queue[front]-&gt;rchild; &#125; &#125;&#125; 6.3 二叉树的遍历—二叉树遍历的非递归实现为什么要用非递归实现？ 并非所有程序设计语言都允许递归； 递归程序虽然简洁，但可读性一般不好，执行效率也不高。 因此，就存在如何把一个递归算法转化为非递归算法的问题。解决这个问题的方法可以通过对三种遍历方法的实质过程的分析得到。 先序遍历是在深入时遇到结点就访问，中序遍历是在从左子树返回时遇到结点访问，后序遍历是在从右子树返回时遇到结点访问。 在这一过程中，返回结点的顺序与深入结点的顺序相反，即后深入先返回，正好符合栈结构后进先出的特点。因此，可以用栈来帮助实现这一遍历路线。其过程如下。在沿左子树深入时，深入一个结点入栈一个结点，若为先序遍历，则在入栈之前访问之；当沿左分支深入不下去时，则返回，即从堆栈中弹出前面压入的结点，若为中序遍历，则此时访问该结点，然后从该结点的右子树继续深入；若为后序遍历，则将此结点再次入栈，然后从该结点的右子树继续深入，与前面类同，仍为深入一个结点入栈一个结点，深入不下去再返回，直到第二次从栈里弹出该结点，才访问之。 先序遍历的非递归实现下面算法：二叉树以二叉链表存放，一维数组stack[MAXNODE]实现栈，变量top表示当前栈顶的位置。 12345678910111213141516171819202122232425void NRPreOrder (BiTree bt)BiTree stack[MAXNODE],p;int top;if (bt==NULL) return;top=0;p=bt;while(!(p==NULL&amp;&amp;top==0))&#123; while(p!=NULL)&#123; Visite(p-&gt;data); /*访问结点的数据域*/if (top&lt;MAXNODE-1) /*将当前指针p 压栈*/&#123; stack[top]=p;top++;&#125;else &#123; printf(“栈溢出”)；return；&#125;p=p-&gt;lchild； /*指针指向p 的左孩子*/&#125;if (top&lt;=0) return; /*栈空时结束*/else&#123; top--;p=stack[top]; /*从栈中弹出栈顶元素*/p=p-&gt;rchild; /*指针指向p 的右孩子结点*/&#125;&#125;&#125; 对图b的二叉树，用上算法遍历，栈stack 和当前指针p 的变化情况以及树中各结点的访问次序如表所示。 中序遍历的非递归实现中序遍历的非递归算法的实现，只需将先序遍历的非递归算法中的Visite(p-&gt;data)移到p=stack[top]和p=p-&gt;rchild 之间即可。 后序遍历的非递归实现由前面的讨论可知，后序遍历与先序遍历和中序遍历不同，在后序遍历过程中，结点在第一次出栈后，还需再次入栈，也就是说，结点要入两次栈，出两次栈，而访问结点是在第二次出栈时访问。因此，为了区别同一个结点指针的两次出栈，设置一标志flag，令： 当结点指针进、出栈时，其标志flag 也同时进、出栈。因此，可将栈中元素的数据类型定义为指针和标志flag 合并的结构体类型。定义如下： typedef struct {BiTree link;int flag;}stacktype; 后序遍历二叉树的非递归算法如下。在算法中，一维数组stack[MAXNODE]用于实现栈的结构，指针变量p 指向当前要处理的结点，整型变量top 用来表示当前栈顶的位置，整型变量sign 为结点p 的标志量。 12345678910111213141516171819202122232425262728293031void NRPostOrder( BiTree bt )/*非递归后序遍历二叉树bt*/&#123; stacktype stack[MAXNODE]; BiTree p; int top, sign; if ( bt == NULL ) return; top = -1 /*栈顶位置初始化*/ p = bt; while ( !(p == NULL &amp;&amp; top == -1) ) &#123; if ( p != NULL ) /*结点第一次进栈*/ &#123; top++; stack[top].link = p; stack[top].flag = 1; p = p-&gt;lchild; /*找该结点的左孩子*/ &#125;else &#123; p = stack[top].link; sign = stack[top].flag; top--; if ( sign == 1 ) /*结点第二次进栈*/ &#123; top++; stack[top].link = p; stack[top].flag = 2; /*标记第二次出栈*/ p = p-&gt;rchild; &#125;else &#123; Visite( p-&gt;data ); /*访问该结点数据域值*/ &#125; &#125; &#125;&#125; 6.3 二叉树的遍历—由遍历序列恢复二叉树1.任何二叉树的先序和中序序列唯一，则可由其先序中序序列确定唯一的二叉树。 2.由二叉树的后序序列和中序序列也可唯一地确定一棵二叉树。 因为，依据后序遍历和中序遍历的定义，后序序列的最后一个结点，就如同先序序列的第一个结点一样，可将中序序列分成两个子序列，分别为这个结点的左子树的中序序列和右子树的中序序列，再拿出后序序列的倒数第二个结点，并继续分割中序序列，如此递归下去，当倒着取取尽后序序列中的结点时，便可以得到一棵二叉树。 已知一棵二叉树的先序序列与中序序列分别为： A B C D E F G H IB C A E D G H F I 试恢复该二叉树。 首先，由先序序列可知，结点A 是二叉树的根结点。其次，根据中序序列，在A 之前的所有结点都是根结点左子树的结点，在A 之后的所有结点都是根结点右子树的结点，由此得到图6.10 (a)所示的状态。然后，再对左子树进行分解，得知B 是左子树的根结点，又从中序序列知道，B 的左子树为空，B 的右子树只有一个结点C。接着对A 的右子树进行分解，得知A 的右子树的根结点为D；而结点D 把其余结点分成两部分，即左子树为E，右子树为F、G、H、I，如图6.10 (b)所示。接下去的工作就是按上述原则对D 的右子树继续分解下去，最后得到如图6.10 (c)的整棵二叉树。 上述过程是一个递归过程，其递归算法的思想是：先根据先序序列的第一个元素建立根结点；然后在中序序列中找到该元素，确定根结点的左、右子树的中序序列；再在先序序列中确定左、右子树的先序序列；最后由左子树的先序序列与中序序列建立左子树，由右子树的先序序列与中序序列建立右子树。下面给出用C 语言描述的该算法。假设二叉树的先序序列和中序序列分别存放在一维数组preod[ ]与inod[ ]中，并假设二叉树各结点的数据值均不相同。 12345void ReBiTree（char preod[ ],char inod[ ],int n,BiTree root）/*n 为二叉树的结点个数，root 为二叉树根结点的存储地址*/&#123; if (n≤0) root=NULL;else PreInOd(preod,inod,1,n,1,n,&amp;root);&#125; 12345678910void PreInOd（char preod[ ],char inod[ ],int i,j,k,h,BiTree *t）&#123;* t=(BiTNode *)malloc(sizeof(BiTNode));*t-&gt;data=preod[i];m=k;while (inod[m]!=preod[i]) m++;if (m==k) *t-&gt;lchild=NULLelse PreInOd(preod,inod,i+1,i+m-k,k,m-1,&amp;t-&gt;lchild);if (m==h) *t-&gt;rchild=NULLelse PreInOd(preod,inod,i+m-k+1,j,m+1,h,&amp;t-&gt;rchild);&#125; 注：数组preod 和inod 的元素类型可根据实际需要来设定，这里设为字符型。另外，如果只知道二叉树的先序序列和后序序列，则不能唯一地确定一棵二叉树。 6.3 二叉树的遍历—不用栈的二叉树遍历的非递归方法二叉树的遍历算法： 依据二叉树结构的递归性，采用递归调用的方式来实现 通过堆栈或队列来辅助实现 缺点：递归调用和栈都增加额外空间。 递归调用的深度与栈的大小是动态变化的，都与二叉树的高度有关。因此，在最坏的情况下，即二叉树退化为单支树的情况下，递归的深度或栈需要的存储空间等于二叉树中的结点数。 常用的不用栈的二叉树遍历的非递归方法有以下三种： 用三叉链表存放二叉树：即在二叉树的每个结点中增加一个双亲域parent，这样，在遍历深入到不能再深入时，可沿着走过的路径回退到任何一棵子树的根结点，并再向另一方向走。由于这一方法的实现是在每个结点的存储上又增加一个双亲域，故其存储开销就会增加。 逆转链：在遍历深入时，每深入一层，就将其再深入的孩子结点的地址取出，并将其双亲结点的地址存入，当深入不下去需返回时，可逐级取出双亲结点的地址，沿原路返回。虽然此种方法是在二叉链表上实现的，没有增加过多的存储空间，但在执行遍历的过程中改变子女指针的值，这既是以时间换取空间，同时当有几个用户同时使用这个算法时将会发生问题。 在线索二叉树上的遍历：利用具有n 个结点的二叉树中的叶子结点和一度结点的n＋1 个空指针域，来存放线索，然后在这种具有线索的二叉树上遍历时，就可不需要栈，也不需要递归了。 6.4 线索二叉树—线索二叉树的定义及结构1.线索二叉树的定义线索二叉树：带了线索的二叉树。线索thread：指向直接前驱结点和指向直接后继结点的指针。 为了保留结点在某种遍历序列中直接前驱和直接后继的位置信息，可以利用二叉树的二叉链表存储结构中的那些空指针域来指示。 2.线索二叉树的结构一个具有n 个结点的二叉树若采用二叉链表存储结构，在2n 个指针域中只有n－1 个指针域是用来存储结点孩子的地址，而另外n＋1 个指针域存放的都是NULL。因此，可以利用某结点空的左指针域（lchild）指出该结点在某种遍历序列中的直接前驱结点的存储地址，利用结点空的右指针域（rchild）指出该结点在某种遍历序列中的直接后继结点的存储地址；对于那些非空的指针域，则仍然存放指向该结点左、右孩子的指针。这样，就得到了一棵线索二叉树。 序列可由不同的遍历方法得到。线索树有先序线索二叉树、中序线索二叉树和后序线索二叉树三种。把二叉树改造成线索二叉树的过程称为线索化。 对图b进行线索化：得到先序线索二叉树、中序线索二叉树和后序线索二叉树分别如图(a)、(b)、(c)所示。图中实线表示指针，虚线表示线索。 如何区别某结点的指针域内存放的是指针还是线索？ （1）为每个结点增设两个标志位域ltag 和rtag，令： 这里我们按第一种方法来介绍线索二叉树的存储。为了将二叉树中所有空指针域都利用上，以及操作便利的需要，在存储线索二叉树时往往增设一头结点，其结构与其它线索二叉树的结点结构一样，只是其数据域不存放信息，其左指针域指向二叉树的根结点，右指针域指向自己。而原二叉树在某序遍历下的第一个结点的前驱线索和最后一个结点的后继线索都指向该头结点。 6.4 线索二叉树—线索二叉树的基本操作实现线索二叉树的结点结构： 12345678typedef char elemtype;typedef struct BiThrNode &#123; elemtype data; struct BiThrNode *lchild; struct BiThrNode *rchild; unsigned ltag : 1; unsigned rtag : 1;&#125;BiThrNodeType, *BiThrTree; 下面以中序线索二叉树为例，讨论线索二叉树的建立、线索二叉树的遍历以及在线索二叉树上查找前驱结点、查找后继结点、插入结点和删除结点等操作的实现算法。 1.建立一棵中序线索二叉树建立线索二叉树/线索化： 实质：遍历一棵二叉树 遍历过程中：访问结点的操作是检查当前结点的左、右指针域是否为空，若空，则将其改为指向前驱结点或后继结点的线索。为实现这一过程，设指针pre始终指向刚访问过的结点，即若指针p指向当前结点，则pre指向当前结点，则pre指向它的前驱，以便增设线索。 对二叉树加线索时，必须首先申请一个头结点，建立头结点与二叉树的根结点的指向关系，对二叉树线索化后，还需建立最后一个结点与头结点之间的线索。 递归算法：其中pre为全局变量 123456789101112131415//算法1int InOrderThr( BiThrTree *head, BiThrTree T )&#123; /*中序遍历二叉树T，并将其中序线索化，*head 指向头结点。*/ if ( !(*head = (BiThrNodeType *) malloc( sizeof(BiThrNodeType) ) ) ) return(0); (*head)-&gt;ltag = 0; (*head)-&gt;rtag = 1; /*建立头结点*/ (*head)-&gt;rchild = *head; /*右指针回指*/ if ( !T ) (*head)-&gt;lchild = *head; /*若二叉树为空，则左指针回指*/ else &#123; (*head)-&gt;lchild = T; pre = head; InThreading( T ); /*中序遍历进行中序线索化*/ pre-&gt;rchild = *head; pre-&gt;rtag = 1; /*最后一个结点线索化*/ (*head)-&gt;rchild = pre; &#125; return(1);&#125; 123456789101112131415161718//算法2void InTreading( BiThrTree p )&#123; /*中序遍历进行中序线索化*/ if ( p ) &#123; InThreading( p-&gt;lchild ); /*左子树线索化*/ if ( !p-&gt;lchild ) /*前驱线索*/ &#123; p-&gt;ltag = 1; p-&gt;lchild = pre; &#125; if ( !pre-&gt;rchild ) /*后继线索*/ &#123; pre-&gt;rtag = 1; pre-&gt;rchild = p; &#125; pre = p; InThreading( p-&gt;rchild ); /*右子树线索化*/ &#125;&#125; 2.在中序线索二叉树上查找任意结点的中序前驱结点对于中序线索二叉树上的任一结点，寻找其中序的前驱结点，有以下两种情况： （1）如果该结点的左标志为1，那么其左指针域所指向的结点便是它的前驱结点； （2）如果该结点的左标志为0，表明该结点有左孩子，根据中序遍历的定义，它的前驱结点是以该结点的左孩子为根结点的子树的最右结点，即沿着其左子树的右指针链向下查找，当某结点的右标志为1 时，它就是所要找的前驱结点。 在中序线索二叉树上寻找结点p 的中序前驱结点的算法如下： 12345678BiThrTree InPreNode（BiThrTree p）&#123;/*在中序线索二叉树上寻找结点p 的中序前驱结点*/BiThrTree pre;pre=p-&gt;lchild;if (p-&gt;ltag!=1) while (pre-&gt;rtag==0) pre=pre-&gt;rchild; return(pre);&#125; 3.在中序线索二叉树上查找任意结点的先序后继结点这一操作的实现依据是：若一个结点是某子树在中序下的最后一个结点，则它必是该子树在先序下的最后一个结点。该结论可以用反证法证明。 下面就依据这一结论，讨论在中序线索二叉树上查找某结点在先序下后继结点的情况。设开始时，指向此某结点的指针为p。 （1）若待确定先序后继的结点为分支结点，则又有两种情况：① 当p-&gt;ltag=0 时，p-&gt;lchild 为p 在先序下的后继；② 当p-&gt;ltag=1 时，p-&gt;rchild 为p 在先序下的后继。 （2）若待确定先序后继的结点为叶子结点，则也有两种情况：① 若p-&gt;rchild 是头结点，则遍历结束；② 若p-&gt;rchild 不是头结点，则p 结点一定是以p-&gt;rchild 结点为根的左子树中在中序遍历下的最后一个结点，因此p 结点也是在该子树中按先序遍历的最后一个结点。此时， 若p-&gt;rchild 结点有右子树， 则所找结点在先序下的后继结点的地址为p-&gt;rchild-&gt;rchild；若p-&gt;rchild 为线索，则让p＝p-&gt;rchild，反复情况（2）的判定。 在中序线索二叉树上寻找结点p 的先序后继结点的算法如下： 12345678910BiThrTree IPrePostNode（BiThrTree head,BiThrTree p）&#123;/*在中序线索二叉树上寻找结点p 的先序的后继结点,head 为线索树的头结点*/BiThrTree post;if (p-&gt;ltag==0) post=p-&gt;lchild;else &#123; post=p;while (post-&gt;rtag==1&amp;&amp;post-&gt;rchild!=head) post=post-&gt;rchild;post=post-&gt;rchild;&#125;return(post);&#125; 4.在中序线索二叉树上查找值为x的结点利用在中序线索二叉树上寻找后继结点和前驱结点的算法，就可以遍历到二叉树的所有结点。比如，先找到按某序遍历的第一个结点，然后再依次查询其后继；或先找到按某序遍历的最后一个结点，然后再依次查询其前驱。这样，既不用栈也不用递归就可以访问到二叉树的所有结点。 在中序线索二叉树上查找值为x 的结点，实质上就是在线索二叉树上进行遍历，将访问结点的操作具体写为那结点的值与x 比较的语句。下面给出其算法： 123456789101112BiThrTree Search (BiThrTree head,elemtype x)&#123;/*在以head 为头结点的中序线索二叉树中查找值为x 的结点*/BiThrTree p;p=head-&gt;lchild;while (p-&gt;ltag==0&amp;&amp;p!=head) p=p-&gt;lchild;while(p!=head &amp;&amp; p-&gt;data!=x) p=InPostNode(p);if (p==head)&#123; printf(“Not Found the data!\n”);return(0);&#125;else return(p);&#125; 5.在中序线索二叉树上查找任意结点在后序下的前驱这一操作的实现依据是：若一个结点是某子树在中序下的第一个结点，则它必是该子树在后序下的第一个结点。该结论可以用反证法证明。 下面就依据这一结论，讨论在中序线索二叉树上查找某结点在后序下前驱结点的情况。设开始时，指向此某结点的指针为p。 （1）若待确定后序前驱的结点为分支结点，则又有两种情况：① 当p-&gt;ltag=0 时，p-&gt;lchild 为p 在后序下的前驱；② 当p-&gt;ltag=1 时，p-&gt;rchild 为p 在后序下的前驱。 （2）若待确定后序前驱的结点为叶子结点，则也有两种情况：① 若p-&gt;lchild 是头结点，则遍历结束；② 若p-&gt;lchild 不是头结点，则p 结点一定是以p-&gt;lchild 结点为根的右子树中在中中序遍历下的第一个结点，因此p 结点也是在该子树中按后序遍历的第一个结点。此时，若p-&gt;lchild 结点有左子树， 则所找结点在后序下的前驱结点的地址为p-&gt;lchild-&gt;lchild；若p-&gt;lchild 为线索，则让p＝p-&gt;lchild，反复情况（2）的判定。 在中序线索二叉树上寻找结点p 的后序前驱结点的算法如下： 12345678910BiThrTree IPostPretNode（BiThrTree head,BiThrTree p）&#123;/*在中序线索二叉树上寻找结点p 的先序的后继结点，head 为线索树的头结点*/BiThrTree pre;if (p-&gt;rtag==0) pre=p-&gt;rchild;else &#123; pre=p;while (pre-&gt;ltag==1&amp;&amp; post-&gt;rchild!=head) pre=pre-&gt;lchild;pre=pre-&gt;lchild;&#125;return(pre);&#125; 6.在中序线索二叉树上查找值为x的结点利用在中序线索二叉树上寻找后继结点和前驱结点的算法，就可以遍历到二叉树的所有结点。比如，先找到按某序遍历的第一个结点，然后再依次查询其后继；或先找到按某序遍历的最后一个结点，然后再依次查询其前驱。这样，既不用栈也不用递归就可以访问到二叉树的所有结点。 在中序线索二叉树上查找值为x 的结点，实质上就是在线索二叉树上进行遍历，将访问结点的操作具体写为那结点的值与x 比较的语句。下面给出其算法： 123456789101112BiThrTree Search (BiThrTree head,elemtype x)&#123;/*在以head 为头结点的中序线索二叉树中查找值为x 的结点*/BiThrTree p;p=head-&gt;lchild;while (p-&gt;ltag==0&amp;&amp;p!=head) p=p-&gt;lchild;while(p!=head &amp;&amp; p-&gt;data!=x) p=InPostNode(p);if (p==head)&#123; printf(“Not Found the data!\n”);return(0);&#125;else return(p);&#125; 7.在中序线索二叉树上的更新线索二叉树的更新是指，在线索二叉树中插入一个结点或者删除一个结点。一般情况下，这些操作有可能破坏原来已有的线索，因此，在修改指针时，还需要对线索做相应的修改。一般来说，这个过程的代价几乎与重新进行线索化相同。这里仅讨论一种比较简单的情况，即在中序线索二叉树中插入一个结点p，使它成为结点s 的右孩子。 下面分两种情况来分析： （1）若s 的右子树为空，如图6.13 (a)所示，则插入结点p 之后成为图6.13 (b)所示的情形。在这种情况中，s 的后继将成为p 的中序后继，s 成为p 的中序前驱，而p 成为s 的右孩子。二叉树中其它部分的指针和线索不发生变化。 （2）若s 的右子树非空，如图6.14 (a)所示，插入结点p 之后如图6.14 (b)所示。S 原来的右子树变成p 的右子树，由于p 没有左子树，故s 成为p 的中序前驱，p 成为s 的右孩子；又由于s 原来的后继成为p 的后继，因此还要将s 原来的本来指向s 的后继的左线索，改为指向p。 下面给出上述操作的算法。 1234567891011121314void InsertThrRight(BiThrTree s,BiThrTree p)&#123;/*在中序线索二叉树中插入结点p 使其成为结点s 的右孩子*/BiThrTree w;p-&gt;rchild=s-&gt;rchild;p-&gt;rtag=s-&gt;rtag;p-&gt;lchild=s;p-&gt;ltag=1; /*将s 变为p 的中序前驱*/s-&gt;rchild=p;s-&gt;rtag=0; /*p 成为s 的右孩子*/if(p-&gt;rtag==0) /*当s 原来右子树不空时，找到s 的后继w，变w 为p 的后继，p 为w 的前驱*/&#123; w=InPostNode(p);w-&gt;lchild=p;&#125;&#125; 6.5 二叉树的应用—二叉树遍历的应用Viste（bt-&gt;data）：访问结点的数据域信息。即根据具体问题，对bt数据进行不同操作。 几个遍历操作的典型应用： 1.查找数据元素search (bt, x): 在bt为二叉树的根结点指针的二叉树中查找数据元素x。查找成功时返回该结点的指针；失败则返回空指针。 算法： 123456789101112131415//注意遍历算法中的Visite(bt-&gt;data)等同于其中的一组操作步骤。BiTree Search（BiTree bt，elemtype x）&#123;/*在bt 为根结点指针的二叉树中查找数据元素x*/BiTree p;if (bt-&gt;data==x) return bt; /*查找成功返回*/if (bt-&gt;lchild!=NULL) return(Search(bt-&gt;lchild,x));/*在bt-&gt;lchild 为根结点指针的二叉树中查找数据元素x*/if (bt-&gt;rchild!=NULL) return(Search(bt-&gt;rchild,x));/*在bt-&gt;rchild 为根结点指针的二叉树中查找数据元素x*/ return NULL; /*查找失败返回*/&#125; 2.统计出给定二叉树中叶子结点的数目1）顺序存储结构的实现1234567891011int CountLeaf1（SqBiTree bt，int k）&#123;/*一维数组bt[2k-1]为二叉树存储结构，k 为二叉树深度，函数值为叶子数。*/total=0;for(i=1;i&lt;=2k-1;i++)&#123; if (bt[i]!=0)&#123; if ((bt[2i]==0 &amp;&amp; bt[2i+1]==0) || (i&gt;(2k-1)/2))total++;&#125;&#125;return(total);&#125; （2）二叉链表存储结构的实现123456int CountLeaf2（BiTree bt）&#123;/*开始时，bt 为根结点所在链结点的指针，返回值为bt 的叶子数*/if (bt==NULL) return(0);if (bt-&gt;lchild==NULL &amp;&amp; bt-&gt;rchild==NULL) return(1);return(CountLeaf2(bt-&gt;lchild)+CountLeaf2(bt-&gt;rchild));&#125; 3.创建二叉树二叉链表存储，并显示设创建时，按二叉树带空指针的先序次序输入结点值，结点值类型为字符型。输出按中序。 CreateBinTree（BinTree *bt）是以二叉链表为存储结构建立一棵二叉树T 的存储，bt为指向二叉树T 根结点指针的指针。设建立时的输入序列为：AB0D00CE00F00。建立如图6.3 (b)所示的二叉树存储。 InOrderOut（bt）为按中序输出二叉树bt 的结点。算法实现如下，注意在创建算法中，遍历算法中的Visite(bt-&gt;data)被读入结点、申请空间存储的操作所代替；在输出算法中，遍历算法中的Visite(bt-&gt;data)被c 语言中的格式输出语句所代替。 123456789101112131415161718192021222324void CreateBinTree(BinTree *T)&#123;/*以加入结点的先序序列输入，构造二叉链表*/char ch;scanf(&quot;\n%c&quot;,&amp;ch);if (ch==&apos;0&apos;) *T=NULL; /*读入0 时，将相应结点置空*/else &#123;*T=(BinTNode*)malloc(sizeof(BinTNode)); /*生成结点空间*/(*T)-&gt;data=ch;CreateBinTree(&amp;(*T)-&gt;lchild); /*构造二叉树的左子树*/CreateBinTree(&amp;(*T)-&gt;rchild); /*构造二叉树的右子树*/&#125;&#125;void InOrderOut(BinTree T)&#123;/*中序遍历输出二叉树T 的结点值*/if (T)&#123; InOrderOut(T-&gt;lchild); /*中序遍历二叉树的左子树*/printf(&quot;%3c&quot;,T-&gt;data); /*访问结点的数据*/InOrderOut(T-&gt;rchild); /*中序遍历二叉树的右子树*/&#125;&#125;main()&#123;BiTree bt;CreateBinTree(&amp;bt);InOrderOut(bt);&#125; 4.表达式运算我们可以把任意一个算数表达式用一棵二叉树表示，图6.15 所示为表达式3x2+x-1/x+5的二叉树表示。在表达式二叉树中，每个叶结点都是操作数，每个非叶结点都是运算符。对于一个非叶子结点，它的左、右子树分别是它的两个操作数。 对该二叉树分别进行先序、中序和后序遍历，可以得到表达式的三种不同表示形式。 前缀表达式+-+3xxx/1x5中缀表达式3xx+x-1/x+5后缀表达式3xx**x+1x/-5+ 6.5 二叉树的应用—最优二叉树(哈夫曼树)1.哈夫曼树的基本概念最优二叉树：对一组带有确定权值的叶结点，构造的具有最小带权路径长度的二叉树。 路径长度：根结点到所有叶结点的路径长度之和。 带权路径长度：从根结点到各个叶结点的路径长度与相应结点权值的乘积之和。 其中Wk 为第k 个叶结点的权值，Lk 为第k 个叶结点的路径长度。如图所示的二叉树，它的带权路径长度值WPL＝2×2＋4×2＋5×2＋3×2＝28。 给定一组具有确定权值的叶结点，可以构造出不同的带权二叉树。例如，给出4 个叶结点，设其权值分别为1，3，5，7，我们可以构造出形状不同的多个二叉树。这些形状不同的二叉树的带权路径长度将各不相同。 这五棵树的带权路径长度分别为：（a）WPL＝1×2＋3×2＋5×2＋7×2＝32（b）WPL＝1×3＋3×3＋5×2＋7×1＝29（c）WPL＝1×2＋3×3＋5×3＋7×1＝33（d）WPL＝7×3＋5×3＋3×2＋1×1＝43（e）WPL＝7×1＋5×2＋3×3＋1×3＝29 由此可见，由相同权值的一组叶子结点所构成的二叉树有不同的形态和不同的带权路径长度，那么如何找到带权路径长度最小的二叉树（即哈夫曼树）呢？根据哈夫曼树的定义，一棵二叉树要使其WPL 值最小，必须使权值越大的叶结点越靠近根结点，而权值越小的叶结点越远离根结点。哈夫曼（Haffman）依据这一特点提出了一种方法，这种方法的基本思想是： （1）由给定的n 个权值{W1，W2，…，Wn}构造n 棵只有一个叶结点的二叉树，从而得到一个二叉树的集合F＝{T1，T2，…，Tn}； （2）在F 中选取根结点的权值最小和次小的两棵二叉树作为左、右子树构造一棵新的二叉树，这棵新的二叉树根结点的权值为其左、右子树根结点权值之和； （3）在集合F 中删除作为左、右子树的两棵二叉树，并将新建立的二叉树加入到集合F 中； （4）重复（2）（3）两步，当F 中只剩下一棵二叉树时，这棵二叉树便是所要建立的哈夫曼树。 对于同一组给定叶结点所构造的哈夫曼树，树的形状可能不同，但带权路径长度值是相同的，一定是最小的。 2.哈夫曼树的构造算法在构造哈夫曼树时，可以设置一个结构数组HuffNode 保存哈夫曼树中各结点的信息，根据二叉树的性质可知，具有n 个叶子结点的哈夫曼树共有2n－1 个结点，所以数组HuffNode 的大小设置为2n －1，数组元素的结构形式如下： 为了判定一个结点是否已加入到要建立的哈夫曼树中，可通过parent 域的值来确定。初始时parent 的值为－1，当结点加入到树中时，该结点parent 的值为其双亲结点在数组HuffNode 中的序号，就不会是－1 了。 构造哈夫曼树时，首先将由n 个字符形成的n 个叶结点存放到数组HuffNode 的前n个分量中，然后根据前面介绍的哈夫曼方法的基本思想，不断将两个小子树合并为一个较大的子树，每次构成的新子树的根结点顺序放到HuffNode 数组中的前n 个分量的后面。 算法：1234567891011121314151617181920212223242526272829303132333435363738394041424344#define MAXVALUE 10000 /*定义最大权值*/#define MAXLEAF 30 /*定义哈夫曼树中叶子结点个数*/#define MAXNODE MAXLEAF * 2 - 1typedef struct &#123; int weight; int parent; int lchild; int rchild;&#125;HNodeType;void HaffmanTree( HNodeType HuffNode[] )&#123; /*哈夫曼树的构造算法*/ int i, j, m1, m2, x1, x2, n; scanf( “ % d ”, &amp;n ); /*输入叶子结点个数*/ for ( i = 0; i &lt; 2 * n - 1; i++ ) /*数组HuffNode[ ]初始化*/ &#123; HuffNode[i].weight = 0; HuffNode[i].parent = -1; HuffNode[i].lchild = -1; HuffNode[i].rchild = -1; &#125; for ( i = 0; i &lt; n; i++ ) scanf( “ % d ”, &amp;HuffNode[i].weight ); /*输入n 个叶子结点的权值*/ for ( i = 0; i &lt; n - 1; i++ ) /*构造哈夫曼树*/ &#123; m1 = m2 = MAXVALUE; x1 = x2 = 0; for ( j = 0; j &lt; n + i; j++ ) &#123; if ( HuffNode[j].weight &lt; m1 &amp;&amp; HuffNode[j].parent == -1 ) &#123; m2 = m1; x2 = x1; m1 = HuffNode[j].weight; x1 = j; &#125;else if ( HuffNode[j].weight &lt; m2 &amp;&amp; HuffNode[j].parent == -1 ) &#123; m2 = HuffNode[j].weight; x2 = j; &#125; &#125;/*将找出的两棵子树合并为一棵子树*/ HuffNode[x1].parent = n + i; HuffNode[x2].parent = n + i; HuffNode[n + i].weight = HuffNode[x1].weight + HuffNode[x2].weight; HuffNode[n + i].lchild = x1; HuffNode[n + i].rchild = x2; &#125;&#125; 3.哈夫曼树在编码问题中的应用编码：数据通讯中，将传送的文字转换成二进制符号0，1组成的二进制串。 如何使电文编码尽可能短？：让出现频率高的字符采用尽可能短的编码，出现频率低的字符采用稍长的编码，构造一种不等长编码。 哈夫曼树可用于构造使电文的编码总长最短的编码方案。具体做法如下：设需要编码的字符集合为{d1，d2，…，dn}，它们在电文中出现的次数或频率集合为{w1，w2，…，wn}，以d1，d2，…，dn 作为叶结点，w1，w2，…，wn 作为它们的权值，构造一棵哈夫曼树，规定哈夫曼树中的左分支代表0，右分支代表1，则从根结点到每个叶结点所经过的路径分支组成的0 和1 的序列便为该结点对应字符的编码，我们称之为哈夫曼编码。 树的带权路径长度——电文代码总长：各字符码长与其出现次数的乘积之和。采用哈夫曼树构造的编码是一种能使电文代码总长最短的不等长编码。 在建立不等长编码时，必须使任何一个字符的编码都不是另一个字符编码的前缀，这样才能保证译码的唯一性。 如表 (d)的编码方案，字符A 的编码01 是字符B 的编码010 的前缀部分，这样对于代码串0101001，既是AAC 的代码，也是ABD 和BDA 的代码，因此，这样的编码不能保证译码的唯一性，我们称之为具有二义性的译码。 采用哈夫曼树进行编码，则不会产生上述二义性问题。因为，在哈夫曼树中，每个字符结点都是叶结点，它们不可能在根结点到其它字符结点的路径上，所以一个字符的哈夫曼编码不可能是另一个字符的哈夫曼编码的前缀，从而保证了译码的非二义性。 实现哈夫曼编码算法： 1.构造哈夫曼树；2.在哈夫曼树上求叶结点的编码。 求哈夫曼编码，实质上就是在已建立的哈夫曼树中，从叶结点开始，沿结点的双亲链域回退到根结点，每回退一步，就走过了哈夫曼树的一个分支，从而得到一位哈夫曼码值，由于一个字符的哈夫曼编码是从根结点到相应叶结点所经过的路径上各分支所组成的0，1 序列，因此先得到的分支代码为所求编码的低位码，后得到的分支代码为所求编码的高位码。我们可以设置一结构数组HuffCode 用来存放各字符的哈夫曼编码信息，数组元素的结构如下： 其中，分量bit 为一维数组，用来保存字符的哈夫曼编码，start 表示该编码在数组bit中的开始位置。所以，对于第i 个字符，它的哈夫曼编码存放在HuffCode[i].bit 中的从HuffCode[i].start 到n 的分量上。 哈夫曼编码算法描述如下： 12345678910111213141516171819202122232425262728293031323334#define MAXBIT 10 /*定义哈夫曼编码的最大长度*/typedef struct &#123; int bit[MAXBIT]; int start;&#125;HCodeType;void HaffmanCode()&#123; /*生成哈夫曼编码*/ HNodeType HuffNode[MAXNODE]; HCodeType HuffCode[MAXLEAF], cd; int i, j, c, p; HuffmanTree( HuffNode ); /*建立哈夫曼树*/ for ( i = 0; i &lt; n; i++ ) /*求每个叶子结点的哈夫曼编码*/ &#123; cd.start = n - 1; c = i; p = HuffNode[c].parent; while ( p != 0 ) /*由叶结点向上直到树根*/ &#123; if ( HuffNode[p].lchild == c ) cd.bit[cd.start] = 0; else cd.bit[cd.start] = 1; cd.start--; c = p; p = HuffNode[c].parent; &#125; for ( j = cd.start + 1; j &lt; n; j++ ) /*保存求出的每个叶结点的哈夫曼编码和编码的起始位*/ HuffCode[i].bit[j] = cd.bit[j]; HuffCode[i].start = cd.start; &#125; for ( i = 0; i &lt; n; i++ ) /*输出每个叶子结点的哈夫曼编码*/ &#123; for ( j = HuffCode[i].start + 1; j &lt; n; j++ ) printf( “ % ld ”, HuffCode[i].bit[j] ); printf( “ \ n ” ); &#125;&#125; 3.哈夫曼在判定问题中的应用如：编制一个百分制转五级分制的程序。条件语句即可完成： 12345if (a&lt;60) b=”bad”;else if (a&lt;70) b=”pass”else if (a&lt;80) b=”general”else if (a&lt;90) b=”good”else b=”excellent”; 这个判定过程可以图(a)所示的判定树来表示。如果上述程序需反复使用，而且每次的输入量很大，则应考虑上述程序的质量问题，即其操作所需要的时间。因为在实际中，学生的成绩在五个等级上的分布是不均匀的，假设其分布规律如下表所示： 则80％以上的数据需进行三次或三次以上的比较才能得出结果。假定以5，15，40，30 和10 为权构造一棵有五个叶子结点的哈夫曼树，则可得到如图(b)所示的判定过程，它可使大部分的数据经过较少的比较次数得出结果。但由于每个判定框都有两次比较，将这两次比较分开，得到如图6.19 (c)所示的判定树，按此判定树可写出相应的程序。假设有10000 个输入数据，若按图(a)的判定过程进行操作，则总共需进行31500 次比较；而若按图6.19 (c)的判定过程进行操作，则总共仅需进行22000 次比较。]]></content>
      <categories>
        <category>Computer Science</category>
        <category>Data Structure</category>
      </categories>
      <tags>
        <tag>Data Structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构第八章：图]]></title>
    <url>%2F2017%2F12%2F17%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E7%AC%AC%E5%85%AB%E7%AB%A0%EF%BC%9A%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[Abstract：图。 8.1 图的基本概念图状结构：比树形结构更复杂的非线性结构。 结点间的邻接关系任意 用于描述各种复杂的数据对象 8.1.1 图的定义和术语1.图的定义 图（Graph）：由非空顶点集合 + 一个描述顶点间关系——边的集合组成；形式化定义为：G = （V，E）；V = {vi| vi ∈ dataobject}; E = {(vi, vj) | vi ∈ V ∧P(vi, vj)}; G表示图，V是顶点的集合，E是边的集合。 2.图的相关术语： 无向图：(vi, vj)无序； 有向图：（vi, vj）∈E 有序； 顶点：vi 边：（vi, vj），一般称无向图中的连线为边 弧：有序偶对表示，一般称有向图中的连线为弧 始点/弧尾：有序偶对的第一个结点vi，在图中就是不带箭头的一端 终点/弧头: 有序偶对的第二个结点vj 被称为终点（或弧头），在图中就是带箭头的一端 有向完全图：任意两顶点之间都有方向互反的两条弧相连接；在一个含有n 个顶点的有向完全图中，有n(n-1)条边。 无向完全图：无向图中，任意两顶点都有一条直接边相连接；在一个含有n 个顶点的无向完全图中，有n(n-1)/2 条边。 稠密图、稀疏图：若一个图接近完全图，称为稠密图；称边数很少的图为稀疏图。 权weight：与边有关的数据信息 网图/网络network：边上带权的图 路径：顶点vp 到顶点vq 之间的路径（path）是指顶点序列vp,vi1,vi2, …,vim,vq.。其中，（vp,vi1），(vi1,vi2)，…,(vim,.vq)分别为图中的边。路径上边的数目称为路径长度。 简单路径：序列中顶点不重复出现的路径 简单回路(环)：除第一个顶点与最后一个顶点之外，其他顶点不重复出现的回路 子图：对于图G=（V，E），G’=（V’，E’），若存在V’是V 的子集，E’是E的子集，则称图G’是G 的一个子图。 连通的、连通图、连通分量。在无向图中，如果从一个顶点vi 到另一个顶点vj(i≠j)有路径，则称顶点vi 和vj 是连通的。如果图中任意两顶点都是连通的，则称该图是连通图。无向图的极大连通子图称为连通分量。 强连通图、强连通分量：对于有向图来说，若图中任意一对顶点vi 和vj(i≠j)均有从一个顶点vi 到另一个顶点vj 有路径，也有从vj 到vi 的路径，则称该有向图是强连通图。有向图的极大强连通子图称为强连通分量。 生成树：连通图G 的生成树，是G 的包含其全部n 个顶点的一个极小连通子图；必定包含且仅包含G 的n-1 条边。在生成树中添加任意一条属于原图中的边必定会产生回路，因为新添加的边使其所依附的两个顶点之间有了第二条路径。若生成树中减少任意一条边，则必然成为非连通的。 生成森林。在非连通图中，由每个连通分量都可得到一个极小连通子图，即一棵生成树。这些连通分量的生成树就组成了一个非连通图的生成森林。 8.1.2 图的基本操作（1） CreatGraph（G）输入图G 的顶点和边，建立图G 的存储。（2）DestroyGraph（G）释放图G 占用的存储空间。（3）GetVex（G，v）在图G 中找到顶点v，并返回顶点v 的相关信息。（4）PutVex（G，v，value）在图G 中找到顶点v，并将value 值赋给顶点v。（5）InsertVex（G，v）在图G 中增添新顶点v。（6）DeleteVex（G，v）在图G 中，删除顶点v 以及所有和顶点v 相关联的边或弧。（7）InsertArc（G，v，w）在图G 中增添一条从顶点v 到顶点w 的边或弧。（8）DeleteArc（G，v，w）在图G 中删除一条从顶点v 到顶点w 的边或弧。（9）DFSTraverse（G，v）在图G 中，从顶点v 出发深度优先遍历图G。（10）BFSTtaverse（G，v）在图G 中，从顶点v 出发广度优先遍历图G。在一个图中，顶点是没有先后次序的，但当采用某一种确定的存储方式存储后，存储结构中顶点的存储次序构成了顶点之间的相对次序，这里用顶点在图中的位置表示该顶点的存储顺序；同样的道理，对一个顶点的所有邻接点，采用该顶点的第i 个邻接点表示与该顶点相邻接的某个顶点的存储顺序，在这种意义下，图的基本操作还有： （11）LocateVex（G，u）在图G 中找到顶点u，返回该顶点在图中位置。（12）FirstAdjVex（G，v）在图G 中，返回v 的第一个邻接点。若顶点在G 中没有邻接顶点，则返回“空”。（13）NextAdjVex（G，v，w）在图G 中，返回v 的（相对于w 的）下一个邻接顶点。若w 是v 的最后一个邻接点，则返回“空”。 8.1.2 图的基本操作（1） CreatGraph（G）输入图G 的顶点和边，建立图G 的存储。（2）DestroyGraph（G）释放图G 占用的存储空间。（3）GetVex（G，v）在图G 中找到顶点v，并返回顶点v 的相关信息。（4）PutVex（G，v，value）在图G 中找到顶点v，并将value 值赋给顶点v。（5）InsertVex（G，v）在图G 中增添新顶点v。（6）DeleteVex（G，v）在图G 中，删除顶点v 以及所有和顶点v 相关联的边或弧。（7）InsertArc（G，v，w）在图G 中增添一条从顶点v 到顶点w 的边或弧。（8）DeleteArc（G，v，w）在图G 中删除一条从顶点v 到顶点w 的边或弧。（9）DFSTraverse（G，v）在图G 中，从顶点v 出发深度优先遍历图G。（10）BFSTtaverse（G，v）在图G 中，从顶点v 出发广度优先遍历图G。在一个图中，顶点是没有先后次序的，但当采用某一种确定的存储方式存储后，存储结构中顶点的存储次序构成了顶点之间的相对次序，这里用顶点在图中的位置表示该顶点的存储顺序；同样的道理，对一个顶点的所有邻接点，采用该顶点的第i 个邻接点表示与该顶点相邻接的某个顶点的存储顺序，在这种意义下，图的基本操作还有： （11）LocateVex（G，u）在图G 中找到顶点u，返回该顶点在图中位置。（12）FirstAdjVex（G，v）在图G 中，返回v 的第一个邻接点。若顶点在G 中没有邻接顶点，则返回“空”。（13）NextAdjVex（G，v，w）在图G 中，返回v 的（相对于w 的）下一个邻接顶点。若w 是v 的最后一个邻接点，则返回“空”。 8.1.2 图的基本操作（1） CreatGraph（G）输入图G 的顶点和边，建立图G 的存储。（2）DestroyGraph（G）释放图G 占用的存储空间。（3）GetVex（G，v）在图G 中找到顶点v，并返回顶点v 的相关信息。（4）PutVex（G，v，value）在图G 中找到顶点v，并将value 值赋给顶点v。（5）InsertVex（G，v）在图G 中增添新顶点v。（6）DeleteVex（G，v）在图G 中，删除顶点v 以及所有和顶点v 相关联的边或弧。（7）InsertArc（G，v，w）在图G 中增添一条从顶点v 到顶点w 的边或弧。（8）DeleteArc（G，v，w）在图G 中删除一条从顶点v 到顶点w 的边或弧。（9）DFSTraverse（G，v）在图G 中，从顶点v 出发深度优先遍历图G。（10）BFSTtaverse（G，v）在图G 中，从顶点v 出发广度优先遍历图G。在一个图中，顶点是没有先后次序的，但当采用某一种确定的存储方式存储后，存储结构中顶点的存储次序构成了顶点之间的相对次序，这里用顶点在图中的位置表示该顶点的存储顺序；同样的道理，对一个顶点的所有邻接点，采用该顶点的第i 个邻接点表示与该顶点相邻接的某个顶点的存储顺序，在这种意义下，图的基本操作还有： （11）LocateVex（G，u）在图G 中找到顶点u，返回该顶点在图中位置。（12）FirstAdjVex（G，v）在图G 中，返回v 的第一个邻接点。若顶点在G 中没有邻接顶点，则返回“空”。（13）NextAdjVex（G，v，w）在图G 中，返回v 的（相对于w 的）下一个邻接顶点。若w 是v 的最后一个邻接点，则返回“空”。 8.1.2 图的基本操作（1） CreatGraph（G）输入图G 的顶点和边，建立图G 的存储。（2）DestroyGraph（G）释放图G 占用的存储空间。（3）GetVex（G，v）在图G 中找到顶点v，并返回顶点v 的相关信息。（4）PutVex（G，v，value）在图G 中找到顶点v，并将value 值赋给顶点v。（5）InsertVex（G，v）在图G 中增添新顶点v。（6）DeleteVex（G，v）在图G 中，删除顶点v 以及所有和顶点v 相关联的边或弧。（7）InsertArc（G，v，w）在图G 中增添一条从顶点v 到顶点w 的边或弧。（8）DeleteArc（G，v，w）在图G 中删除一条从顶点v 到顶点w 的边或弧。（9）DFSTraverse（G，v）在图G 中，从顶点v 出发深度优先遍历图G。（10）BFSTtaverse（G，v）在图G 中，从顶点v 出发广度优先遍历图G。在一个图中，顶点是没有先后次序的，但当采用某一种确定的存储方式存储后，存储结构中顶点的存储次序构成了顶点之间的相对次序，这里用顶点在图中的位置表示该顶点的存储顺序；同样的道理，对一个顶点的所有邻接点，采用该顶点的第i 个邻接点表示与该顶点相邻接的某个顶点的存储顺序，在这种意义下，图的基本操作还有： （11）LocateVex（G，u）在图G 中找到顶点u，返回该顶点在图中位置。（12）FirstAdjVex（G，v）在图G 中，返回v 的第一个邻接点。若顶点在G 中没有邻接顶点，则返回“空”。（13）NextAdjVex（G，v，w）在图G 中，返回v 的（相对于w 的）下一个邻接顶点。若w 是v 的最后一个邻接点，则返回“空”。 8.2 图的存储表示——邻接矩阵图的信息：包括图中顶点的信息以及描述顶点之间的关系――边或者弧的信息。因此无论采用什么方法建立图的存储结构，都要完整、准确地反映这两方面的信息。 邻接矩阵Adjacency Matrix：一维数组表示各顶点的邻接关系，矩阵表示各顶点的邻接关系；假设图G＝（V，E）有n 个确定的顶点，即V＝{v0,v1,…,vn-1},则表示G 中各顶点相邻关系为一个n×n 的矩阵，矩阵的元素为： wij 表示边(vi,vj)或上的权值；∞表示一个计算机允许的、大于所有边上权值的数。 邻接矩阵特点： ① 无向图的邻接矩阵一定是一个对称矩阵。因此，在具体存放邻接矩阵时只需存放上（或下）三角矩阵的元素即可。② 对于无向图，邻接矩阵的第i 行（或第i 列）非零元素（或非∞元素）的个数正好是第i 个顶点的度TD(vi)。③ 对于有向图，邻接矩阵的第i 行（或第i 列）非零元素（或非∞元素）的个数正好是第i 个顶点的出度OD(vi)（或入度ID(vi)）。④用邻接矩阵方法存储图，很容易确定图中任意两个顶点之间是否有边相连；但是，要确定图中有多少条边，则必须按行、按列对每个元素进行检测，所花费的时间代价很大。 在用邻接矩阵存储图时，除了用一个二维数组存储用于表示顶点间相邻关系的邻接矩阵外，还需用一个一维数组来存储顶点信息，另外还有图的顶点数和边数。故可将其形式描述如下： 123456789101112131415161718192021222324252627282930#define MaxVertexNum 100 /*最大顶点数设为100*/typedef char VertexType; /*顶点类型设为字符型*/typedef int EdgeType; /*边的权值设为整型*/typedef struct &#123; VertexType vexs[MaxVertexNum]; /*顶点表*/ EdeType edges[MaxVertexNum][MaxVertexNum]; /*邻接矩阵，即边表*/ int n, e; /*顶点数和边数*/&#125;Mgragh; /*Maragh 是以邻接矩阵存储的图类型*/建立 一 个 图的邻接矩阵存储的算法如 ：void CreateMGraph( MGraph *G )&#123; /*建立有向图G 的邻接矩阵存储*/ int i, j, k, w; char ch; printf( &quot;请输入顶点数和边数(输入格式为:顶点数,边数):\n&quot; ); scanf( &quot;%d,%d&quot;, &amp;(G-&gt;n), &amp;(G-&gt;e) ); /*输入顶点数和边数*/ printf( &quot;请输入顶点信息(输入格式为:顶点号&lt;CR&gt;):\n&quot; ); for ( i = 0; i &lt; G-&gt;n; i++ ) scanf( &quot;\n%c&quot;, &amp;(G-&gt;vexs[i]) ); /*输入顶点信息，建立顶点表*/ for ( i = 0; i &lt; G-&gt;n; i++ ) for ( j = 0; j &lt; G-&gt;n; j++ ) G-&gt;edges[i][j] = 0; /*初始化邻接矩阵*/ printf( &quot;请输入每条边对应的两个顶点的序号(输入格式为:i,j):\n&quot; ); for ( k = 0; k &lt; G-&gt;e; k++ ) &#123; scanf( &quot;\n%d,%d&quot;, &amp;i, &amp;j ); /*输入e 条边，建立邻接矩阵*/ G-&gt;edges[i][j] = 1; /*若加入G-&gt;edges[j][i]=1;，*//*则为无向图的邻接矩阵存储建立*/ &#125;&#125; /*CreateMGraph*/ 8.2 图的存储表示—邻接表邻接表(Adjacency List)：图的一种顺序存储与链式存储结构组合的存储方法。类似树的孩子链表表示法。 对于图G 中的每个顶点vi，将所有邻接于vi 的顶点vj 链成一个单链表，这个单链表就称为顶点vi 的邻接表，再将所有点的邻接表表头放到数组中，就构成了图的邻接表。 一种是顶点表的结点结构，它由顶点域（vertex）和指向第一条邻接边的指针域（firstedge）构成，另一种是边表（即邻接表）结点，它由邻接点域(adjvex)和指向下一条邻接边的指针域(next)构成。对于网图的边表需再增设一个存储边上信息（如权值等）的域（info），网图的边表结构如图： 1234567891011121314151617181920212223242526272829303132333435363738＃ define MaxVerNum 100 /*最大顶点数为100*/typedef struct node &#123; /*边表结点*/ int adjvex; /*邻接点域*/ struct node * next; /*指向下一个邻接点的指针域*//*若要表示边上信息，则应增加一个数据域info*/&#125;EdgeNode;typedef struct vnode &#123; /*顶点表结点*/ VertexType vertex; /*顶点域*/ EdgeNode * firstedge; /*边表头指针*/&#125;VertexNode;typedef VertexNode AdjList[MaxVertexNum]; /*AdjList 是邻接表类型*/typedef struct &#123; AdjList adjlist; /*邻接表*/ int n, e; /*顶点数和边数*/&#125;ALGraph; /*ALGraph 是以邻接表方式存储的图类型*/建立 一 个 向 图的邻接表存储的算法如 ：void CreateALGraph( ALGraph *G )&#123; /*建立有向图的邻接表存储*/ int i, j, k; EdgeNode * s; printf( &quot;请输入顶点数和边数(输入格式为:顶点数,边数)：\n&quot; ); scanf( &quot;%d,%d&quot;, &amp;(G-&gt;n), &amp;(G-&gt;e) ); /*读入顶点数和边数*/ printf( &quot;请输入顶点信息(输入格式为:顶点号&lt;CR&gt;)：\n&quot; ); for ( i = 0; i &lt; G-&gt;n; i++ ) /*建立有n 个顶点的顶点表*/ &#123; scanf( &quot;\n%c&quot;, &amp;(G-&gt;adjlist[i].vertex) ); /*读入顶点信息*/ G-&gt;adjlist[i].firstedge = NULL; /*顶点的边表头指针设为空*/ &#125; printf( &quot;请输入边的信息(输入格式为:i,j)：\n&quot; ); for ( k = 0; k &lt; G-&gt;e; k++ ) /*建立边表*/ &#123; scanf( &quot;\n%d,%d&quot;, &amp;i, &amp;j ); /*读入边&lt;Vi,Vj&gt;的顶点对应序号*/ s = (EdgeNode *) malloc( sizeof(EdgeNode) ); /*生成新边表结点s*/ s-&gt;adjvex = j; /*邻接点序号为j*/ s-&gt;next = G-&gt;adjlist[i].firstedge; /*将新边表结点s 插入到顶点Vi 的边表头部*/ G-&gt;adjlist[i].firstedge = s; &#125;&#125; /*CreateALGraph*/ 若无向图中有n 个顶点、e 条边，则它的邻接表需n 个头结点和2e 个表结点。显然，在边稀疏(e&lt;&lt;n(n-1)/2)的情况下，用邻接表表示图比邻接矩阵节省存储空间，当和边相关的信息较多时更是如此。 在无向图的邻接表中，顶点vi 的度恰为第i 个链表中的结点数；而在有向图中，第i个链表中的结点个数只是顶点vi 的出度，为求入度，必须遍历整个邻接表。在所有链表中其邻接点域的值为i 的结点的个数是顶点vi 的入度。有时，为了便于确定顶点的入度或以顶点vi 为头的弧，可以建立一个有向图的逆邻接表，即对每个顶点vi 建立一个链接以vi为头的弧的链表。例如图8.12 所示为有向图G2（图8.2）的邻接表和逆邻接表。 在建立邻接表或逆邻接表时，若输入的顶点信息即为顶点的编号，则建立邻接表的复杂度为O（n+e），否则，需要通过查找才能得到顶点在图中位置，则时间复杂度为O（n·e）。 在邻接表上容易找到任一顶点的第一个邻接点和下一个邻接点，但要判定任意两个顶点（vi 和vj）之间是否有边或弧相连，则需搜索第i 个或第j 个链表，因此，不及邻接矩阵方便。 8.2 图的存储表示—十字链表十字链表（Orthogonal List）是有向图的一种存储方法，它实际上是邻接表与逆邻接表的结合，即把每一条边的边结点分别组织到以弧尾顶点为头结点的链表和以弧头顶点为头顶点的链表中。在十字链表表示中，顶点表和边表的结点结构分别如图： 在弧结点中有五个域：其中尾域(tailvex)和头(headvex)分别指示弧尾和弧头这两个顶点在图中的位置，链域hlink 指向弧头相同的下一条弧，链域tlink 指向弧尾相同的下一条弧，info 域指向该弧的相关信息。弧头相同的弧在同一链表上，弧尾相同的弧也在同一链表上。它们的头结点即为顶点结点，它由三个域组成：其中vertex 域存储和顶点相关的信息，如顶点的名称等；firstin 和firstout 为两个链域，分别指向以该顶点为弧头或弧尾的第一个弧结点。 十字链表形式描述： 1234567891011121314#define MAX_VERTEX_NUM 20typedef struct ArcBox &#123; int tailvex, headvex; /*该弧的尾和头顶点的位置*/ struct ArcBox * hlink, tlink; / 分 别 为 弧 头 相 和 弧 尾 相 财 的 弧 的链域 * / InfoType info; /*该弧相关信息的指针*/&#125;ArcBox;typedef struct VexNode &#123; VertexType vertex : ArcBox fisrin, firstout; /*分别指向该顶点第一条入弧和出弧*/&#125;VexNode;typedef struct &#123; VexNode xlist[MAX_VERTEX_NUM]; /*表头向量*/ int vexnum, arcnum; /*有向图的顶点数和弧数*/&#125;OLGraph; 下面给出建立一个有向图的十字链表存储的算法。通过该算法，只要输入n 个顶点的信息和e 条弧的信息，便可建立该有向图的十字链表，其算法内容如下。 1234567891011121314151617void CreateDG(LOGraph **G)/*采用十字链表表示，构造有向图G(G.kind=DG)*/&#123; scanf (&amp;(*G-&gt;brcnum),&amp;(*G-&gt;arcnum),&amp;IncInfo); /*IncInfo 为0 则各弧不含其实信息*/for (i=0;i&lt;*G-&gt;vexnum;++i) /*构造表头向量*/&#123; scanf(&amp;(G-&gt;xlist[i].vertex)); /*输入顶点值*/*G-&gt;xlist[i].firstin=NulL;*G-&gt;xlist[i].firstout =NULL; /*初始化指针*/&#125;for(k=0;k&lt;G.arcnum;++k) /*输入各弧并构造十字链表*/&#123; scanf(&amp;v1,&amp;v2); /*输入一条弧的始点和终点*/i=LocateVex(*G,v1); j=LocateVex(*G,v2); /*确定v1 和v2 在G 中位置*/p=(ArcBox*) malloc (sizeof(ArcBox)); /*假定有足够空间*/*p=&#123; i,j,*G-&gt;xlist[j].fistin,*G-&gt;xlist[i].firstout,NULL&#125; /*对弧结点赋值*//*&#123;tailvex,headvex,hlink,tlink,info&#125;*/*G-&gt;xlist[j].fisrtin=*G-&gt;xlist[i].firstout=p; /*完成在入弧和出弧链头的插入*/if (IncInfo) Input( p-&gt;info); /*若弧含有相关信息，则输入*/&#125;&#125;/*CreateDG*/ 在十字链表中既容易找到以为尾的弧，也容易找到以vi 为头的弧，因而容易求得顶点的出度和入度（或需要，可在建立十字链表的同时求出）。同时，由上述算法可知，建立十字链表的时间复杂度和建立邻接表是相同的。在某些有向图的应用中，十字链表是很有用的工具。 8.3 图的遍历图的遍历是指从图中的任一顶点出发，对图中的所有顶点访问一次且只访问一次。 由于图结构本身的复杂性，所以图的遍历操作也较复杂，主要表现在以下四个方面： ① 在图结构中，没有一个“自然”的首结点，图中任意一个顶点都可作为第一个被访问的结点。② 在非连通图中，从一个顶点出发，只能够访问它所在的连通分量上的所有顶点，因此，还需考虑如何选取下一个出发点以访问图中其余的连通分量。③ 在图结构中，如果有回路存在，那么一个顶点被访问之后，有可能沿回路又回到该顶点。④ 在图结构中，一个顶点可以和其它多个顶点相连，当这样的顶点访问过后，存在如何选取下一个要访问的顶点的问题。 图的遍历通常有深度优先搜索和广度优先搜索两种方式。 8.3.1 深度优先搜索（Depth_Fisrst Search）深度优先搜索（Depth_Fisrst Search）遍历类似于树的先根遍历，是树的先根遍历的推广。 访问过程： 假设初始状态是图中所有顶点未曾被访问，则深度优先搜索可从图中某个顶点发v 出发，访问此顶点 然后依次从v 的未被访问的邻接点出发深度优先遍历图，直至图中所有和v 有路径相通的顶点都被访问到； 若此时图中尚有顶点未被访问，则另选图中一个未曾被访问的顶点作起始点，重复上述过程，直至图中所有顶点都被访问到为止【递归过程】。 为了在遍历过程中便于区分顶点是否已被访问，需附设访问标志数组visited[0:n-1], ，其初值为FALSE ，一旦某个顶点被访问，则其相应的分量置为TRUE。 12345678910111213141516171819202122232425262728293031323334从图的某一点v 出发，递归地进行深度优先遍历的过程如算法8.4 所示。void DFS(Graph G,int v )&#123; /*从第v 个顶点出发递归地深度优先遍历图G*/visited[v]=TRUE;VisitFunc(v); /*访问第v 个顶点*/for(w=FisrAdjVex(G,v);w; w=NextAdjVex(G,v,w))if (!visited[w]) DFS(G,w); /*对v 的尚未访问的邻接顶点w 递归调用DFS*/&#125;算法8.4算法8.5 和算法8.6 给出了对以邻接表为存储结构的整个图G 进行深度优先遍历实现的C 语言描述。void DFSTraverseAL(ALGraph *G)&#123;/*深度优先遍历以邻接表存储的图G*/int i;for (i=0;i&lt;G-&gt;n;i++)visited[i]=FALSE; /*标志向量初始化*/for (i=0;i&lt;G-&gt;n;i++)if (!visited[i]) DFSAL(G,i); /*vi 未访问过，从vi 开始DFS 搜索*/&#125;/*DFSTraveseAL*/算法8.5void DFSAL(ALGraph *G,int i)&#123;/*以Vi 为出发点对邻接表存储的图G 进行DFS 搜索*/EdgeNode *p;printf(&quot;visit vertex:V%c\n&quot;,G-&gt;adjlist[i].vertex);/*访问顶点Vi*/visited[i]=TRUE; /*标记Vi 已访问*/p=G-&gt;adjlist[i].firstedge; /*取Vi 边表的头指针*/while(p) /*依次搜索Vi 的邻接点Vj，j=p-&gt;adjva*/&#123;if (!visited[p-&gt;adjvex]) /*若Vj 尚未访问，则以Vj 为出发点向纵深搜索*/DFSAL(G,p-&gt;adjvex);p=p-&gt;next; /*找Vi 的下一个邻接点*/&#125;&#125;/*DFSAL*/算法8.6 分析上述算法，在遍历时，对图中每个顶点至多调用一次DFS 函数，因为一旦某个顶点被标志成已被访问，就不再从它出发进行搜索。因此，遍历图的过程实质上是对每个顶点查找其邻接点的过程。 其耗费的时间则取决于所采用的存储结构。当用二维数组表示邻接矩阵图的存储结构时，查找每个顶点的邻接点所需时间为O(n2) ，其中n 为图中顶点数。而当以邻接表作图的存储结构时，找邻接点所需时间为O(e)，其中e 为无向图中边的数或有向图中弧的数。由此，当以邻接表作存储结构时，深度优先搜索遍历图的时间复杂度为O(n+e) 。 8.3.2 广度优先搜索（Breadth_First Search） 类似于树的按层次遍历的过程（一个层次遍历完再遍历下一个层次） 假设从图中某顶点v 出发，在访问了v 之后依次访问v 的各个未曾访问过和邻接点，然后分别从这些邻接点出发依次访问它们的邻接点，并使“先被访问的顶点的邻接点”先于“后被访问的顶点的邻接点”被访问，直至图中所有已被访问的顶点的邻接点都被访问到 广度优先搜索遍历图的过程中以v 为起始点，由近至远，依次访问和v 有路径相通且路径长度为1,2,…的顶点。 和深度优先搜索类似，在遍历的过程中也需要一个访问标志数组。并且，为了顺次访问路径长度为2、3、…的顶点，需附设队列以存储已被访问的路径长度为1、2、… 的顶点。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162从图的某 一 点v 出发 递 归地进行 广 度 优 先遍历的过 如算法8 .7 所 示 。void BFSTraverse( Graph G, Status (*Visit)( int v ) )&#123; /*按广度优先非递归遍历图G。使用辅助队列Q 和访问标志数组visited*/ for ( v = 0; v &lt; G, vexnum; ++v ) visited[v] = FALSE InitQueue( Q ); /*置空的国债队列Q*/ if ( !visited[v] ) /*v 尚未访问*/ &#123; EnQucue( Q, v ); /*v 入队列*/ while ( !QueueEmpty( Q ) ) &#123; DeQueue( Q, u ); /*队头元素出队并置为u*/ visited[u] = TRUE; visit( u ); /*访问u*/ for ( w = FistAdjVex( G, u ); w; w = NextAdjVex( G, u, w ) ) if ( !visited[w] ) EnQueue( Q, w ); /*u 的尚未访问的邻接顶点w 入队列Q*/ &#125; &#125;&#125; /*BFSTraverse*/算法8 .7算法8 .8 和算法8 .9 给出了对以邻接矩阵 为 存储结构的整 个 图G 进行深度 优 先遍历实现的C 语 言 描述 。void BFSTraverseAL( MGraph *G )&#123; /*广度优先遍历以邻接矩阵存储的图G*/ int i; for ( i = 0; i &lt; G-&gt;n; i++ ) visited[i] = FALSE; /*标志向量初始化*/ for ( i = 0; i &lt; G-&gt;n; i++ ) if ( !visited[i] ) BFSM( G, i ); /* vi 未访问过，从vi 开始BFS 搜索*/&#125; /*BFSTraverseAL*/算法8 .8void BFSM( MGraph *G, int k )&#123; /*以Vi 为出发点，对邻接矩阵存储的图G 进行BFS 搜索*/ int i, j; CirQueue Q; InitQueue( &amp;Q ); printf( &quot;visit vertex:V%c\n&quot;, G-&gt;vexs[k] ); /*访问原点Vk*/ visited[k] = TRUE; EnQueue( &amp;Q, k ); /*原点Vk 入队列*/ while ( !QueueEmpty( &amp;Q ) ) &#123; i = DeQueue( &amp;Q ); /*Vi 出队列*/ for ( j = 0; j &lt; G-&gt;n; j++ ) /*依次搜索Vi 的邻接点Vj*/ if ( G-&gt;edges[i][j] == 1 &amp;&amp; !visited[j] ) /*若Vj 未访问*/ &#123; printf( &quot;visit vertex:V%c\n&quot;, G-&gt;vexs[j] ); /*访问Vj */ visited[j] = TRUE; EnQueue( &amp;Q, j ); /*访问过的Vj 入队列*/ &#125; &#125;&#125; /*BFSM*/算法8 .9 分析上述算法，每个顶点至多进一次队列。遍历图的过程实质是通过边或弧找邻接点的过程，因此广度优先搜索遍历图的时间复杂度和深度优先搜索遍历相同，两者不同之处仅仅在于对顶点访问的顺序不同。 8.4 图的连通性——无向图的连通性在对无向图进行遍历时，对于连通图，仅需从图中任一顶点出发，进行深度优先搜索或广度优先搜索，便可访问到图中所有顶点。对非连通图，则需从多个顶点出发进行搜索，而每一次从一个新的起始点出发进行搜索过程中得到的顶点访问序列恰为其各个连通分量中的顶点集。 要想判定一个无向图是否为连通图，或有几个连通分量，就可设一个计数变量count，初始时取值为0，在算法8.5 的第二个for 循环中，每调用一次DFS，就给count 增1。这样，当整个算法结束时，依据count 的值，就可确定图的连通性了。 8.4 图的连通性—有向图的连通性连通图 在无向图中，如果一个顶点到另一个顶点存在至少一条路径，称它们之间是连通的。 如果图中任意两个顶点之间都是连通的，则此图为连通图。 如果一个图本身不是连通图，但是图中某个子图是连通图，那么这个子图又被称为“连通分量”。 注意：这里的“子图”指的是无向图中最大的连通子图。 在有向图中，如果任意一对顶点 Vi 和 Vj，从 Vi 到 Vj 和从 Vj 到 Vi 都含有至少一条通路，那么称此图为强连通图。如果有向图的连通分量也具有此特征，则为强连通分量。 有向图的连通性不同于无向图的连通性，可分为弱连通、单侧连通和强连通。 深度优先搜索是求有向图的强连通分量的一个有效方法。假设以十字链表作有向图的存储结构，则求强连通分量的步骤如下： （1）在有向图G 上，从某个顶点出发沿以该顶点为尾的弧进行深度优先搜索遍历，并按其所有邻接点的搜索都完成(即退出DFS 函数)的顺序将顶点排列起来。此时需对8.3.1中的算法作如下两点修改：（a）在进入DFSTraverseAL 函数时首先进行计数变量的初始化，即在入口处加上count=0 的语句；（b）在退出函数之前将完成搜索的顶点号记录在另一个辅助数组finished[vexnum] 中，即在函数DFSAL 结束之前加上finished[++count]=v 的语句。 （2）在有向图G 上，从最后完成搜索的顶点（即finished[vexnum-1]中的顶点）出发，沿着以该顶点为头的弧作逆向的深度搜索遍历，若此次遍历不能访问到有向图中所有顶点，则从余下的顶点中最后完成搜索的那个顶点出发，继续作逆向的深度优先搜索遍历，依次类推，直至有向图中所有顶点都被访问到为止。此时调用DFSTraverseAL 时需作如下修改：函数中第二个循环语句的边界条件应改为v 从finished[vexnum-1]至finished[0]。 由此，每一次调用DFSAL 作逆向深度优先遍历所访问到的顶点集便是有向图G 中一个强连通分量的顶点集。 上述求强连通分量的第二步，其实质为：（1）构造一个有向图Gr，设G=(V,{A})，则Gr=(Vr，{Ar})对于所有&lt; vi,,vj&gt;∈A，必有&lt; vj, vi &gt;∈Ar。即Gr 中拥有和G 方向相反的弧；（2）在有向图Gr 上，从顶点finished[vexnum-1] 出发作深度优先遍历。可以证明，在Gr 上所得深度优先生成森林中每一棵树的顶点集即为G 的强连通分量的顶点集。 显然，利用遍历求强连通分量的时间复杂度亦和遍历相同。 8.4 图的连通性—生成树和生成森林 对于非连通图，通过这样的遍历，将得到的是生成森林。例如，图8.20 (b) 所示为图8.20 (a)的深度优先生成森林，它由三棵深度优先生成树组成。 12345678910111213141516171819202122232425262728293031323334353637383940假设以孩子兄 弟 链表 作 生 成 森林的存储结构 则 算法8 .10 生 成 非 连 通 图的深度 优 先 生 成 森林 其 中 DFSTree 函数如算法8 .11 所 示 。 显 然 算法8 .10 的时间 杂度和遍历相 。void DESForest( Graph G, CSTree *T )&#123; /*建立无向图G 的深度优先生成森林的孩子兄弟链表T*/ T = NULL; for ( v = 0; v &lt; G.vexnum; ++v ) if ( !visited[v] = FALSE; for ( v = 0; v &lt; G.vexnum; ++v ) if ( !visited[v] ) /*顶点v 为新的生成树的根结点*/ &#123; p = (CSTree) malloc( sixeof( CSNode ) ); /*分配根结点*/ p = &#123; GetVex( G, v ).NULL, NULL &#125;; /*给根结点赋值*/ if ( !T ) (*T) = p; /*T 是第一棵生成树的根*/ else q-&gt;nextsibling = p; /*前一棵的根的兄弟是其它生成树的根*/ q = p; /*q 指示当前生成树的根*/ DFSTree( G, v, &amp;p ); /*建立以p 为根的生成树*/ &#125; &#125; 算法8 .10 void DFSTree( Graph G, int v, CSTree * T ) &#123; /*从第v 个顶点出发深度优先遍历图G，建立以*T 为根的生成树*/ visited[v] = TRUE; first = TRUE; for ( w = FirstAdjVex( G, v ); w; w = NextAdjVex( G, v, w ) ) if ( !visited[w] ) &#123; p = (CSTree) malloc( sizeof ) CSNode) ); /*分配孩子结点*/ *p = &#123; GetVex( G, w ), NULL, NULL &#125;; if ( first ) /*w 是v 的第一个未被访问的邻接顶点，作为根的左孩子结点*/ &#123; T-&gt;lchild = p; first = FALSE; &#125;else &#123; /*w 是v 的其它未被访问的邻接顶点，作为上一邻接顶点的右兄弟*/ q-&gt;nextsibling = p; &#125; q = p; DFSTree( G, w, &amp;q ); /*从第w 个顶点出发深度优先遍历图G，建立生成子树*q*/ &#125; &#125; 8.4 图的连通性—关节点和重连通分量关节点：假若在删去顶点v 以及和v 相关联的各边之后，将图的一个连通分量分割成两个或两个以上的连通分量，则称顶点v 为该图的一个关节点(articulation point) 。关节点亦称为割点。 一个没有关节点的连通图称为重连通图(biconnected graph) 。在重连通图上，任意一对顶点之间至少存在两条路径，则在删去某个顶点以及依附于该顶点的各边时也不破坏图的连通性。若在连通图上至少删去k 个顶点才能破坏图的连通性，则称此图的连通度为k。 一个表示通信网络的图的连通度越高，其系统越可靠，无论是哪一个站点出现故障或遭到外界破坏，都不影响系统的正常工作；又如，一个航空网若是重连通的，则当某条航线因天气等某种原因关闭时，旅客仍可从别的航线绕道而行；再如，若将大规模的集成电路的关键线路设计成重连通的话，则在某些元件失效的情况下，整个片子的功能不受影响，反之，在战争中，若要摧毁敌方的运输线，仅需破坏其运输网中的关节点即可。 求关节点的时间复杂度为O（n+e）。 利用深度优先搜索便可求得图的关节点，并由此可判别图是否是重连通的。 图8.21 (b)所示为从顶点A 出发深优先生成树，图中实线表示树边，虚线表示回边（即不在生成树上的边）。对树中任一顶点v 而言，其孩子结点为在它之后搜索到的邻接点，而其双亲结点和由回边连接的祖先结点是在它之前搜索到的邻接点。由深度优先生成树可得出两类关节点的特性： （1）若生成树的根有两棵或两棵以上的子树，则此根顶点必为关节点。因为图中不存在联结不同子树中顶点的边，因此，若删去根顶点，生成树便变成生成森林。如图8.21(b)中的顶点A。（2）若生成树中某个非叶子顶点v，其某棵子树的根和子树中的其它结点均没有指向v 的祖先的回边，则v 为关节点。因为，若删去v，则其子树和图的其它部分被分割开来。如图8.21(b)中的顶点B 和G 。 若对图Graph＝(V，{Edge}) 重新定义遍历时的访问函数visited，并引入一个新的函数low，则由一次深度优先遍历便可求得连通图中存在的所有关节点。 8.5 最小生成树—最小生成树的基本概念 无向连通图的生成树不唯一 连通图的一次遍历所经过的边的集合及图中所有顶点的集合 = 构成一棵生成树 对连通图的不同遍历可得到不同生成树 对于有n 个顶点的无向连通图，无论其生成树的形态如何，所有生成树中都有且仅有n－1 条边。 最小生成树：权值总和最小。 8.5 最小生成树—构造最小生成树的Prim算法假设G＝（V，E）为一网图，其中V 为网图中所有顶点的集合，E 为网图中所有带权边的集合。设置两个新的集合U 和T，其中集合U 用于存放G 的最小生成树中的顶点，集合T 存放G 的最小生成树中的边。令集合U 的初值为U＝{u1}（假设构造最小生成树时，从顶点u1 出发），集合T 的初值为T＝{}。 Prim 算法的思想是，从所有u∈U，v∈V－U 的边中，选取具有最小权值的边（u，v），将顶点v 加入集合U 中，将边（u，v）加入集合T 中，如此不断重复，直到U＝V 时，最小生成树构造完毕，这时集合T 中包含了最小生成树的所有边。 图8.23 (a)所示的一个网图，按照Prim 方法，从顶点1 出发，该网的最小生成树的产生过程如图8.23 (b)、(c)、(d)、(e)、(f)和(g)所示。 为实现Prim 算法，需设置两个辅助一维数组lowcost 和closevert，其中lowcost 用来保存集合V－U 中各顶点与集合U 中各顶点构成的边中具有最小权值的边的权值；数组closevertex 用来保存依附于该边的在集合U 中的顶点。假设初始状态时，U＝{u1}(u1 为出发的顶点)，这时有lowcost[0]=0，它表示顶点u1 已加入集合U 中，数组lowcost 的其它各分量的值是顶点u1 到其余各顶点所构成的直接边的权值。然后不断选取权值最小的边（ui，uk）（ui∈U，uk∈V－U），每选取一条边，就将lowcost（k）置为0，表示顶点uk 已加入集合U 中。由于顶点uk 从集合V－U 进入集合U 后，这两个集合的内容发生了变化，就需依据具体情况更新数组lowcost 和closevertex 中部分分量的内容。最后closevertex 中即为所建立的最小生成树。 123456789101112131415161718192021222324252627282930313233343536当无 向 网采 用二维数组存储的邻接矩阵存储时 Prim 算法的C 语 言 实现 为 ： void Prim （ int gm[] [MAXNODE] int n int closevertex[] &#123; /*用Prim 方法建立有n 个顶点的邻接矩阵存储结构的网图gm 的最小生成树*//*从序号为0 的顶点出发；建立的最小生成树存于数组closevertex 中*/ int lowcost[100], mincost; int i, j, k; for ( i = 1; i &lt; n; i++ ) /*初始化*/ &#123; lowcost[i] = gm[0][i]; closevertex[i] = 0; &#125; lowcost[0] = 0; /*从序号为0 的顶点出发生成最小生成树*/ closevertex[0] = 0; for ( i = 1; i &lt; n; i++ ) /*寻找当前最小权值的边的顶点*/ &#123; mincost = MAXCOST; /*MAXCOST 为一个极大的常量值*/ j = 1; k = 1; while ( j &lt; n ) &#123; if ( lowcost[j] &lt; mincost &amp;&amp; lowcost[j] != 0 ) &#123; mincost = lowcost[j]; k = j; &#125; j++; &#125; printf( “ 顶点的序号 ＝ % d 边的权 值 ＝ % d \ n ”, k, mincost ); lowcost[k] = 0; for ( j = 1; j &lt; n; j++ ) /*修改其它顶点的边的权值和最小生成树顶点序号*/ if ( gm[k][j] &lt; lowcost[j] ) &#123; lowcost[j] = gm[k][j]; closevertex[j] = k; &#125; &#125; &#125; 图8.24 给出了在用上述算法构造网图8.23 (a)的最小生成树的过程中，数组closevertex、lowcost 及集合U，V－U 的变化情况，读者可进一步加深对Prim 算法的了解。 在Prim 算法中，第一个for 循环的执行次数为n－1，第二个for 循环中又包括了一个while 循环和一个for 循环，执行次数为2(n-1)2，所以Prim 算法的时间复杂度为O(n2)。 8.5 最小生成树—构造最小生成树的Kruskal算法Kruskal 算法是一种按照网中边的权值递增的顺序构造最小生成树的方法。 其基本思想是：设无向连通网为G＝（V，E），令G 的最小生成树为T，其初态为T＝（V，{}），即开始时，最小生成树T 由图G 中的n 个顶点构成，顶点之间没有一条边，这样T 中各顶点各自构成一个连通分量。然后，按照边的权值由小到大的顺序，考察G 的边集E 中的各条边。若被考察的边的两个顶点属于T 的两个不同的连通分量，则将此边作为最小生成树的边加入到T 中，同时把两个连通分量连接为一个连通分量；若被考察边的两个顶点属于同一个连通分量，则舍去此边，以免造成回路，如此下去，当T 中的连通分量个数为1 时，此连通分量便为G 的一棵最小生成树。 n 个结点的生成树，有n－1 条边，故反复上述过程，直到选取了n－1 条边为止，就构成了一棵最小生成树。 设置一个结构数组Edges 存储网中所有的边，边的结构类型包括构成的顶点信息和边权值，定义如下： 1234567#define MAXEDGE &lt;图中的最大边数&gt;typedef struct &#123;elemtype v1;elemtype v2;int cost;&#125; EdgeType;EdgeType edges[MAXEDGE]; 123456789101112131415161718192021222324252627282930313233343536下面用C 语言实现Kruskal 算法，其中函数Find 的作用是寻找图中顶点所在树的根结点在数组father 中的序号。需说明的是，在程序中将顶点的数据类型定义成整型，而在实际应用中，可依据实际需要来设定。typedef int elemtype;typedef struct &#123; elemtype v1; elemtype v2; int cost;&#125;EdgeType;void Kruskal （ EdgeType edges[] int n/*用Kruskal 方法构造有n 个顶点的图edges 的最小生成树*/&#123; int father[MAXEDGE]; int i, j, vf1, vf2; for ( i = 0; i &lt; n; i++ ) father[i] = -1; i = 0; j = 0; while ( i &lt; MAXEDGE &amp;&amp; j &lt; n - 1 ) &#123; vf1 = Find( father, edges[i].v1 ); vf2 = Find( father, edges[i].v2 ); if ( vf1 != vf2 ) &#123; father[vf2] = vf1; j++; printf( “ % 3d % 3d \ n ”, edges[i].v1, edges[i].v2 ); &#125; i++; &#125;&#125;算法8 .15int Find （ int father[] int v/*寻找顶点v 所在树的根结点*/&#123; int t; t = v; while ( father[t] &gt;= 0 ) t = father[t]; return(t); &#125; 在Kruskal 算法中，第二个while 循环是影响时间效率的主要操作，其循环次数最多为MAXEDGE 次数，其内部调用的Find 函数的内部循环次数最多为n，所以Kruskal 算法的时间复杂度为O（n·MAXEDGE）。 8.6 最短路径—从一个源点到其它各点的最短路径网图中：边的权值之和最短的路径为最短路径； 非网图中：两点间经历的边数最少的路径。 一般情况下，下一条长度次短的最短路径的长度必是： D[j]=Min{D[i]| vi∈V-S} 其中，D[i] 或者弧（v, vi）上的权值，或者是D[k]( vk∈S 和弧（vk, vi）上的权值之和。 根据以上分析，可以得到如下描述的算法： （1）假设用带权的邻接矩阵edges 来表示带权有向图，edges[i][j] 表示弧〈vi, vj〉上的权值。若〈vi, vj〉不存在，则置edges[i][j]为∞（在计算机上可用允许的最大值代替）。S 为已找到从v 出发的最短路径的终点的集合，它的初始状态为空集。那么，从v 出发到图上其余各顶点（终点）vi 可能达到最短路径长度的初值为：D[i]= edges[Locate Vex(G,v)][i] vi∈V （2）选择vj，使得D[j]=Min{D[i]| vi∈V-S}vj 就是当前求得的一条从v 出发的最短路径的终点。令S＝S∪{j} （3）修改从v 出发到集合V-S 上任一顶点vk 可达的最短路径长度。如果D[j]+ edges[j][k]&lt;D[k]则修改D[k]为D[k]=D[j]+ edges[j][k]重复操作（2）、（3）共n-1 次。由此求得从v 到图上其余各顶点的最短路径是依路径长度递增的序列。 1234567891011121314151617181920212223242526272829303132333435void ShortestPath_1( Mgraph G, int v0, PathMatrix *p, ShortPathTable *D )&#123; /*用Dijkstra 算法求有向网G 的v0 顶点到其余顶点v 的最短路径P[v]及其路径长度D[v]*//*若P[v][w]为TRUE，则w 是从v0 到v 当前求得最短路径上的顶点*//*final[v] 为TRUE 当且仅当v∈S, ，即已经求得从v0 到v 的最短路径*//*常量INFINITY 为边上权值可能的最大值*/ for ( v = 0; v &lt; G.vexnum; ++v ) &#123; fianl[v] = FALSE; D[v] = G.edges[v0][v]; for ( w = 0; w &lt; G.vexnum; ++w ) P[v][w] = FALSE; /*设空路径*/ if ( D[v] &lt; INFINITY ) &#123; P[v][v0] = TRUE; P[v][w] = TRUE; &#125; &#125; D[v0] = 0; final[v0] = TRUE ； /*初始化，v0 顶点属于S 集*//*开始主循环，每次求得v0 到某个v 顶点的最短路径，并加v 到集*/ for ( i = 1; i &lt; G.vexnum; ++i ) /*其余G.vexnum-1 个顶点*/ &#123; min = INFINITY; /*min 为当前所知离v0 顶点的最近距离*/ for ( w = 0; w &lt; G.vexnum; ++w ) if ( !final[w] ) /*w 顶点在V－S 中*/ if ( D[w] &lt; min ) &#123; v = w; min = D[w]; &#125; final[v] = TRUE /*离v0 顶点最近的v 加入S 集合*/ for ( w = 0; w &gt; G.vexnum; ++w ) /*更新当前最短路径*/ if ( !final[w] &amp;&amp; (min + G.edges[v][w] &lt; D[w]) ) /*修改D[w]和P[w],w∈V-S*/ &#123; D[w] = min + G.edges[v][w]; P[w] = P[v]; P[w][v] = TRUE; /*P[w]=P[v]+P[w]*/ &#125; &#125;&#125; /*ShortestPath._1*/ 下面分析一下这个算法的运行时间。第一个for 循环的时间复杂度是O(n)，第二个for循环共进行n-1 次，每次执行的时间是O(n)。所以总是的时间复杂度是O(n2)。如果用带权的邻接表作为有向图的存储结构，则虽然修改D 的时间可以减少，但由于在D 向量中选择最小的分量的时间不变，所以总的时间仍为O(n2)。 如果只希望找到从源点到某一个特定的终点的最短路径，但是，从上面我们求最短路径的原理来看，这个问题和求源点到其它所有顶点的最短路径一样复杂，其时间复杂度也是O(n2)。 8.6 最短路径—每一对顶点之间的最短路径解决这个问题的一个办法是：每次以一个顶点为源点，重复招待迪杰斯特拉算法次。这样，便可求得每一结顶点的最短路径。总的执行时间为O(n3)。 这里要介绍由弗洛伊德(Floyd)提出的另一个算法。这个算法的时间复杂度也是O(n3),但形式上简单些。 弗洛伊德算法仍从图的带权邻接矩阵cost 出发，其基本思想是：假设求从顶点vi 到vj 的最短路径。如果从vi 到vi 有弧，则从vi 到vj 存在一条长度为edges[i][j]的路径，该路径不一定是最短路径，尚需进行n 次试探。首先考虑路径（vi, v0,vj）是否存在（即判别弧（vi, v0）和(v0, vj)是否存在）。如果存在，则比较（vi, vj）和（vi,v0, vj）的路径长度取长度较短者为从vi 到vj 的中间顶点的序号不大于0 的最短路径。假如在路径上再增加一个顶点v1，也就是说，如果（vi, …, v1）和（v1, …, vj）分别是当前找到的中间顶点的序号不大于0 的最短路径，那么（vi, …, v1, … , vj） 就有可能是从vi 到vj 的中间顶点的序号不大于1 的最短路径。将它和已经得到的从vi 到vj 中间顶点序号不大于0 的最短路径相比较，从中选出中间顶点的序号不大于1 的最短路径之后，再增加一个顶点v2，继续进行试探。依次类推。在一般情况下，若（vi, …, vk）和（vk, …, vj）分别是从vi 到vk 和从vk 到vj 的中间顶点的序号不大于k-1 的最短路径，则将（vi, …,vk, …, vj）和已经得到的从vi 到vj 且中间顶点序号不大于k-1 的最短路径相比较，其长度较短者便是从vi 到vj 的中间顶点的序号不大于k 的最短路径。这样，在经过n 次比较后，最后求得的必是从vi 到vj 的最短路径。 按此方法，可以同时求得各对顶点间的最短路径。 现定义一个n 阶方阵序列。D(-1)，D(0)，D(1)，…，D(k)，D(n-1)其中D(-1)[i][j]=edges[i][j]D( k)[i][j]=Min{D( k-1)[i][j], D( k-1)[i][k]+D( k-1)[k][j]} 0≦k≦n-1 从上述计算公式可见，D(1)[i][j]是从vi 到vj 的中间顶点的序号不大于1 的最短路径的长度；D( k)[i][j] 是从vi 到vj 的中间顶点的个数不大于k 的最短路径的长度；D( n-1)[i][j] 就是从vi 到vj 的最短路径的长度。 12345678910111213141516171819202122232425由 此得到求任 意 两顶点间的 最 短路径的算法8 .18 。void ShortestPath_2( Mgraph G, PathMatrix *P[], DistancMatrix *D )&#123; /*用Floyd 算法求有向网G 中各对顶点v 和w 之间的最短路径P[v][w]及其带权长度D[v][w]。*//*若P[v][w][u]为TRUE，则u 是从v 到w 当前求得的最短路径上的顶点。*/ for ( v = 0; v &lt; G.vexnum; ++v ) /*各对顶点之间初始已知路径及距离*/ for ( w = 0; w &lt; G, vexnum; ++w ) &#123; D[v][w] = G.arcs[v][w]; for ( u = 0; u &lt; G, vexnum; ++u ) P[v][w][u] = FALSE; if ( D[v][w] &lt; INFINITY ) /*从v 到w 有直接路径*/ &#123; P[v][w][v] = TRUE; &#125; &#125; for ( u = 0; u &lt; G.vexnum; ++u ) for ( v = 0; v &lt; G.vexnum; ++v ) for ( w = 0; w &lt; G.vexnum; ++w ) if ( D[v][u] + D[u][w] &lt; D[v][w] ) /*从v 经u 到w 的一条路径更短*/ &#123; D[v][w] = D[v][u] + D[u][w]; for ( i = 0; i &lt; G.vexnum; ++i ) P[v][w][i] = P[v][u][i] || P[u][w][i]; &#125;&#125; /* ShortestPath_2*/ 图8.28 给出了一个简单的有向网及其邻接矩阵。图8.29 给出了用Floyd 算法求该有向网中每对顶点之间的最短路径过程中，数组D 和数组P 的变化情况。 8.7 有向无环图（DAG）及其应用—有向无环图的概念有向无环图是描述含有公共子式的表达式的有效工具。例如下述表达式： ((a+b)*(b*(c+d)+(c+d)*e)*((c+d)*e) 可以用第六章讨论的二叉树来表示，如图8.31 所示。仔细观察该表达式，可发现有一些相同的子表达式，如(c+d)和(c+d)*e 等，在二叉树中，它们也重复出现。若利用有向无环图，则可实现对相同子式的共享，从而节省存储空间。例如图8.32 所示为表示同一表达式的有向无环图。 检查一个有向图是否存在环要比无向图复杂。对于无向图来说，若深度优先遍历过程中遇到回边（即指向已访问过的顶点的边），则必定存在环；而对于有向图来说，这条回边有可能是指向深度优先生成森林中另一棵生成树上顶点的弧。但是，如果从有向图上某个顶点v 出发的遍历，在dfs(v)结束之前出现一条从顶点u 到顶点v 的回边，由于u 在生成树上是v 的子孙，则有向图必定存在包含顶点v 和u 的环。 有向无环图是描述一项工程或系统的进行过程的有效工具。除最简单的情况之外，几乎所有的工程（project）都可分为若干个称作活动（activity）的子工程，而这些子工程之间，通常受着一定条件的约束，如其中某些子工程的开始必须在另一些子工程完成之后。 对整个工程和系统，人们关心的是两个方面的问题：一是工程能否顺利进行：二是估算整个工程完成所必须的最短时间。以下两小节将详细介绍这样两个问题是如何通过对有向图进行拓扑排序和关键路径操作来解决的。 8.7 有向无环图及其应用—AOV网（(Activity on vertex network)）与拓扑排序1．AOV网(Activity on vertex network)所有的工程或者某种流程可以分为若干个小的工程或阶段，这些小的工程或阶段就称为活动。若以图中的顶点来表示活动，有向边表示活动之间的优先关系，则这样活动在顶点上的有向图称为AOV 网。在AOV 网中，若从顶点i 到顶点j 之间存在一条有向路径，称顶点i 是顶点j 的前驱，或者称顶点j 是顶点i 的后继。若是图中的弧，则称顶点i是顶点j 的直接前驱，顶点j 是顶点i 的直接后驱。 AOV 网中的弧表示了活动之间存在的制约关系。例如，计算机专业的学生必须完成一系列规定的基础课和专业课才能毕业。学生按照怎样的顺序来学习这些课程呢？这个问题可以被看成是一个大的工程，其活动就是学习每一门课程。这些课程的名称与相应代号如表8.1 所示。 表中，C1、C12 是独立于其它课程的基础课，而有的课却需要有先行课程，比如，学完程序设计导论和数值分析后才能学数据结构……，先行条件规定了课程之间的优先关系。这种优先关系可以用图8.33 所示的有向图来表示。其中，顶点表示课程，有向边表示前提条件。若课程i 为课程j 的先行课，则必然存在有向边〈i,j〉。在安排学习顺序时，必须保证在学习某门课之前，已经学习了其先行课程。 类似的AOV 网的例子还有很多，比如大家熟悉的计算机程序，任何一个可执行程序也可以划分为若干个程序段（或若干语句），由这些程序段组成的流程图也是一个AOV 网。 2．拓扑排序偏序与全序： 若集合A 中的二元关系R 是自反的、非对称的和传递的，则R 是A 上的偏序关系。集合A 与关系R 一起称为一个偏序集合。 若R 是集合A 上的一个偏序关系，如果对每个a、b∈A 必有aRb 或bRa ，则R 是A上的全序关系。集合A 与关系R 一起称为一个全序集合。 偏序关系经常出现在我们的日常生活中。例如，若把A 看成一项大的工程必须完成的一批活动，则aRb 意味着活动a 必须在活动b 之前完成。比如，对于前面提到的计算机专业的学生必修的基础课与专业课，由于课程之间的先后依赖关系，某些课程必须在其它课程以前讲授，这里的aRb 就意味着课程a 必须在课程b 之前学完。 AOV 网所代表的一项工程中活动的集合显然是一个偏序集合。为了保证该项工程得以顺利完成，必须保证AOV 网中不出现回路；否则，意味着某项活动应以自身作为能否开展的先决条件，这是荒谬的。测试AOV 网是否具有回路（即是否是一个有向无环图）的方法，就是在AOV 网的偏序集合下构造一个线性序列，该线性序列具有以下性质： 1、在AOV 网中，若顶点i 优先于顶点j ，则在线性序列中顶点i 仍然优先于顶点j；2、对于网中原来没有优先关系的顶点与顶点，如图8.33 中的C1 与C13，在线性序列中也建立一个先后关系，或者顶点i 优先于顶点j ，或者顶点j 优先于i。 满足这样性质的线性序列称为拓扑有序序列。构造拓扑序列的过程称为拓扑排序。也可以说拓扑排序就是由某个集合上的一个偏序得到该集合上的一个全序的操作。 若某个AOV 网中所有顶点都在它的拓扑序列中，则说明该AOV 网不会存在回路，这时的拓扑序列集合是AOV 网中所有活动的一个全序集合。以图8.21 中的AOV 网例，可以得到不止一个拓扑序列，C1、C12、C4、C13、C5、C2、C3、C9、C7、C10、C11、C6、C8 就是其中之一。显然，对于任何一项工程中各个活动的安排，必须按拓扑有序序列中的顺序进行才是可行的。 3.拓扑排序算法对AOV 网进行拓扑排序的方法和步骤是：1、从AOV 网中选择一个没有前驱的顶点（该顶点的入度为0）并且输出它；2、从网中删去该顶点，并且删去从该顶点发出的全部有向边；3、重复上述两步，直到剩余的网中不再存在没有前驱的顶点为止。 这样操作的结果有两种：一种是网中全部顶点都被输出，这说明网中不存在有向回路；另一种就是网中顶点未被全部输出，剩余的顶点均不前驱顶点，这说明网中存在有向回路。 这样得到一个拓扑序列：v2,v5,v1,v4,v3,v7,v6。 为了实现上述算法，对AOV 网采用邻接表存储方式，并且邻接表中顶点结点中增加一个记录顶点入度的数据域，即顶点结构设为： 顶点表结点结构的描述改为：typedef struct vnode{ /顶点表结点/int count /存放顶点入度/VertexType vertex; /顶点域/EdgeNode firstedge; /边表头指针*/}VertexNode; 当然也可以不增设入度域，而另外设一个一维数组来存放每一个结点的入度。算法中可设置了一个堆栈，凡是网中入度为0 的顶点都将其入栈。为此，拓扑排序的算法步骤为：1、将没有前驱的顶点（count 域为0）压入栈；2、从栈中退出栈顶元素输出，并把该顶点引出的所有有向边删去，即把它的各个邻接顶点的入度减1；3、将新的入度为0 的顶点再入堆栈；4、重复②～④，直到栈为空为止。此时或者是已经输出全部顶点，或者剩下的顶点中没有入度为0 的顶点。 下面给出用C 语言描述的拓扑排序算法的实现。 从上面的步骤可以看出，栈在这里的作用只是起到一个保存当前入度为零点的顶点，并使之处理有序。这种有序可以是后进先出，也可以是先进先出，故此也可用队列来辅助实现。在下面给出用C 语言描述的拓扑排序的算法实现中，我们采用栈来存放当前未处理过的入度为零点的结点，但并不需要额外增设栈的空间，而是设一个栈顶位置的指针将当前所有未处理过的入度为零的结点连接起来，形成一个链式栈。 1234567891011121314151617181920212223242526272829303132333435void Topo_Sort( AlGraph *G )&#123; /*对以带入度的邻接链表为存储结构的图G，输出其一种拓扑序列*/ int top = -1 ； /* 栈顶指针初始化*/ for ( i = 0 ； i &lt; n ； i++ ) /* 依次将入度为0 的顶点压入链式栈*/ &#123; if ( G-&gt;adjlist[i].Count = = 0 ) &#123; G-&gt;adjlist[i].count = top ； top = i ； &#125; &#125; for ( i = 0; i &lt; n; i++ ) &#123; if ( t0p = -1 ) &#123; printf( “ The network has a cycle ” ); return; &#125; j = top; top = G-&gt;adjlist[top].count; /* 从栈中退出一个顶点并输出*/ printf( “ % c ”, G-&gt;adjlist[j].vertex ); ptr = G-&gt;adjlist[j].firstedge; while ( ptr != null ) &#123; k = ptr-&gt;adjvex; G-&gt;adjlist[k].count--; /*当前输出顶点邻接点的入度减1*/ if ( G-&gt;adjlist[k].count = = 0 ) /*新的入度为0 的顶点进栈*/ &#123; G-&gt;adjlist[k].count = top; top = k; &#125; ptr = ptr-&gt;next; /*找到下一个邻接点*/ &#125; &#125;&#125; 算法8.19 对一个具有n 个顶点、e 条边的网来说，整个算法的时间复杂度为O（e+n）。下面结合图8.34 (a)给出的AOV 网以及图8.35 所示的邻接表，观察算法的执行情况。 图8.36 给出了邻接表的顶点结点的变化情况。其中，图8.36 (a)示出了算法开始时堆栈的初始状态；图8.36（b）～（h）给出了每输出一个顶点后堆栈的状态。]]></content>
      <categories>
        <category>Computer Science</category>
        <category>Data Structure</category>
      </categories>
      <tags>
        <tag>Data Structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构第七章：树]]></title>
    <url>%2F2017%2F12%2F17%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E7%AC%AC%E4%B8%83%E7%AB%A0%EF%BC%9A%E6%A0%91%2F</url>
    <content type="text"><![CDATA[Abstract：树。 7.1 树的概念与表示对具有更一般意义的树结构进行讨论。本章所讨论的树结构，其结点可以有任意数目的子结点，这使其在存储以及操作实现上要比二叉树更复杂。 7.1.1 树的定义和相关术语1.树的定义 树：n（n&gt;=0）个有限数据元素的集合。n = 0时，为空树。 在一棵非空树T 中： （1）有一个特殊的数据元素称为树的根结点，根结点没有前驱结点。 （2）若n&gt;1，除根结点之外的其余数据元素被分成m（m&gt;0）个互不相交的集合T1，T2，…，Tm，其中每一个集合Ti（1≤i≤m）本身又是一棵树。树T1，T2，…，Tm 称为这个根结点的子树。 递归概念：用树定义树；类同二叉树结构算法——递归。 描述——二元组：T = （D，R） D：结点的集合；R：结点间关系的集合。 空树时：D=Φ；当T不为空时，D = {Root}∪DF Root：根结点；DF：子树集合，DF＝D1∪D2∪…∪Dm 且Di∩Dj＝Φ（i≠j，1≤i≤m，1≤j≤m） 当T中结点个数n &lt;= 1时，R = Φ；当树T 中结点个数n&gt;1 时有：R＝{，i＝1，2，…，m} Root：根结点；ri：根结点Root的子树Ti的根结点。 树的两个特点： 根结点无前驱结点，除根结点外的所有结点有且只有一个前驱； 所有结点有0个或多个后继结点。 b、c、d都不是树结构。 有序树与无序树：如果一棵树中结点的各子树丛左到右是有次序的，即若交换了某结点各子树的相对位置，则构成不同的树，称这棵树为有序树；反之，则称为无序树。 森林：零棵或有限棵不相交的树的集合称为森林。自然界中树和森林是不同的概念，但在数据结构中，树和森林只有很小的差别。任何一棵树，删去根结点就变成了森林。 7.1.2 树的表示树的表示方法： 1.直观表示法：如上图a，对树的逻辑结构的直观描述，最常用。 2.嵌套集合表示法：嵌套集合指一些集合的集体，对于其中任何两个集合，或者不相交，或者一个包含另一个；用嵌套集合的形式表示树，就是将根结点视为一个大的集合，其若干棵子树构成这个大集合中若干个互不相交的子集，如此嵌套下去，即构成一棵树的嵌套集合表示。下图(a)就是一棵树的嵌套集合表示。 3.凹入表示法：如下图c，用于树的屏幕和打印输出； 4.广义表表示法：将根作为由子树森林组成的表的名字写在表的左边，这样依次将树表示出来。下图 (b)就是一棵树的广义表表示。 7.2 树的基本操作与存储 Initiate(t):初始化一棵空树t Root(x):求结点x所在树的根结点 Parent(t, x):求树t种结点x的双亲结点 Child(t, x, i):求树t中结点x的第i个孩子结点 RightSibling（t，x）求树t 中结点x 的第一个右边兄弟结点。 Insert（t，x，i，s）把以s 为根结点的树插入到树t 中作为结点x 的第i 棵子树。 Delete（t，x，i）在树t 中删除结点x 的第i 棵子树。 Tranverse（t）是树的遍历操作，即按某种方式访问树t 中的每个结点，且使每个结点只被访问一次。 7.2.2 树的存储结构存储结构要求： 存储data——各结点的数据信息 唯一反映各结点之间的逻辑关系 1.双亲表示法：树中的每个结点都有唯一的一个双亲结点，根据这一特性，可用一组连续的存储空间（一维数组）存储树中的各个结点，数组中的一个元素表示树中的一个结点，数组元素为结构体类型，其中包括结点本身的信息以及结点的双亲结点在数组中的序号，树的这种存储方法称为双亲表示法。 123456#define MAXNODEtypedef struct&#123;elemtype data;int parent;&#125;NodeType;NodeType t[MAXNODE] 缺点： 求某结点的孩子结点，即实现Child（t，x，i）操作时，需要查询整个数组 不能反映各兄弟结点的关系，难以实现RightSibling（t，x）操作 解决：在结点结构中增设存放第一个孩子的域和存放第一个右兄弟的域； 树a的双亲表示法表示如下图：图中用parent 域的值为-1 表示该结点无双亲结点，即该结点是一个根结点。 2.孩子表示法： (1) 多重链表法： 由于树中的每个结点都有0个或多个孩子结点，因此可令每个结点包括1个结点信息域data和多个指针域；每个指针域指向该结点的一个孩子结点，通过各指针域反映树中各结点间的逻辑关系。 多重链表：每个结点都有多个指针域，形成多条链表。 在一棵树中，各结点度数差异，结点指针域个数的设置有2种表示方法： 每个结点指针域的个数 = 该结点的度数：节约存储空间；但由于各结点不同构，各操作不易实现； 每个结点指针域个数 = 树的度数：结点同构，但浪费存储空间（适用：各结点度数相差不大的情况）； 树中结点存储表示： 12345#define MAXSON&lt;树的度数&gt;typedefine struct TreeNode&#123;elemtype data;struct TreeNode *son[MAXSON];&#125;NodeType; 对任意一棵树t，可定义：NodeType *t；使变量t为指向树的根结点的指针。 （2）孩子链表表示法： 将树按下图形式存储。单链表的结构也由两个域组成，一个存放孩子结点在一维数组中的序号，另一个是指针域，指向下一个孩子。 存储表示描述： 12345678910#define MAXNODE &lt;树中结点的最大个数&gt;typedef struct ChildNode&#123;int childcode;struct ChildNode *nextchild;&#125;typedef struct &#123;elemtype data;struct ChildNode *firstchild;&#125;NodeType;NodeType t[MAXNODE]; 3.双亲孩子表示法双亲表示法是将双亲表示法和孩子表示法相结合的结果。其仍将各结点的孩子结点分别组成单链表，同时用一维数组顺序存储树中的各结点，数组元素除了包括结点本身的信息和该结点的孩子结点链表的头指针之外，还增设一个域，存储该结点双亲结点在数组中的序号。如下图： 4.孩子兄弟表示法 一种常用的存储结构。 方法：在树中，每个结点除其信息域外，再增加两个分别指向该结点的第一个孩子结点和下一个兄弟结点的指针。在这种存储结构下，树中结点的存储表示可描述为： 12345typedef struct TreeNode &#123;elemtype data;struct TreeNode *son;struct TreeNode *next;&#125;NodeType; 7．3 树、森林与二叉树的转换从树的孩子兄弟表示法可以看到，如果设定一定规则，就可用二叉树结构表示树和森林，这样，对树的操作实现就可以借助二叉树存储，利用二叉树上的操作来实现。本节将讨论树和森林与二叉树之间的转换方法。 7.3.1 树转换成二叉树树与二叉树的区别：无序树的各孩子结点词序无关紧要；而二叉树的左、右孩子结点有区别。 转换方法： 树中所有相邻兄弟之间加一条连线 对树中的每个结点，只保留它与第一个孩子结点之间的连线，删去它与其它孩子结点之间的连线。 以树的根结点为轴心，将整棵树顺时针转动一定的角度，使之结构层次分明。 在二叉树中，左分支上的各结点在原来的树中是父子关系，而右分支上的各结点在原来的树中是兄弟关系。由于树的根结点没有兄弟，所以变换后的二叉树的根结点的右孩子必为空。一棵树采用孩子兄弟表示法所建立的存储结构与它所对应的二叉树的二叉链表存储结构是完全相同的。 7.3.2 森林转换为二叉树森林是若干棵树的集合，只要将森林中各棵树的根视为兄弟，每棵树又可以用二叉树表示，这样，森林也同样可以用二叉树表示。森林转换为二叉树的方法如下： 将森林中的每棵树转换成相应的二叉树。 第一棵二叉树不动，从第二棵二叉树开始，依次把后一棵二叉树的根结点作为前一棵二叉树根结点的右孩子，当所有二叉树连起来后，此时所得到的二叉树就是由森林转换得到的二叉树。 这一方法可形式化描述为： 如果F＝{ T1，T2，…，Tm }是森林，则可按如下规则转换成一棵二叉树B＝（root，LB，RB）。 （1）若F 为空，即m＝0，则B 为空树； （2）若F 非空，即m≠0，则B 的根root 即为森林中第一棵树的根Root(T1)；B 的左子树LB 是从T1 中根结点的子树森林F1＝{ T11，T12，…，T1m1 }转换而成的二叉树；其右子树RB 是从森林F’＝{ T2，T3，…，Tm }转换而成的二叉树。 森林及其转换为二叉树的过程： 7.3.3 二叉树转换为树和森林树和森林都可以转换为二叉树，二者不同的是树转换成的二叉树，其根结点无右分支，而森林转换后的二叉树，其根结点有右分支。显然这一转换过程是可逆的，即可以依据二叉树的根结点有无右分支，将一棵二叉树还原为树或森林，具体方法如下： （1）若某结点是其双亲的左孩子，则把该结点的右孩子、右孩子的右孩子……都与该结点的双亲结点用线连起来； （2）删去原二叉树中所有的双亲结点与右孩子结点的连线； （3）整理由（1）、（2）两步所得到的树或森林，使之结构层次分明。 这一方法可形式化描述为： 如果B＝（root，LB，RB）是一棵二叉树，则可按如下规则转换成森林F＝{ T1，T2，…，Tm }。 （1）若B 为空，则F 为空； （2）若B 非空，则森林中第一棵树T1 的根ROOT（T1）即为B 的根root；T1 中根结点的子树森林F1 是由B 的左子树LB 转换而成的森林；F 中除T1 之外其余树组成的森林F’＝{ T2，T3，…，Tm }是由B 的右子树RB 转换而成的森林。 一棵二叉树还原为森林的过程: 7．4 树和森林的遍历7.4.1 树的遍历1.先根遍历 定义： 访问根结点 按照从左至右顺序先根遍历根结点的每一棵子树 对上图先根遍历，顺序：ABEFCDG 2.后根遍历 定义： 按照从左至右顺序后根遍历根结点的每一棵子树 访问根结点 对上图后根遍历，顺序：EFBCGDA 7.4.2 森林的遍历1.前序遍历 （1）访问森林中第一棵树的根结点；（2）前序遍历第一棵树的根结点的子树；（3）前序遍历去掉第一棵树后的子森林。 对上图森林前序遍历，得出结果序列：ABCDEFGHJIK 2.中序遍历 （1）中序遍历第一棵树的根结点的子树；（2）访问森林中第一棵树的根结点；（3）中序遍历去掉第一棵树后的子森林。 对上图森林中序遍历，得出结果序列：B A D E F C J H K I G 森林的前序遍历和中序遍历与所转换的二叉树的先序遍历和中序遍历的结果序列相同。 7．5 树的应用树的应用十分广泛，在后面的排序和查找常用的两项技术中，就有以树结构组织数据进行操作的。本节仅讨论树在判定树和集合表示与运算方面的应用。 7．5．1 判定树最优二叉树：哈夫曼在判定问题中的应用 判定树：用于判定问题的描述与解决；解决过程中的一系列判断构成了树结构 Q：设有八枚硬币，分别表示为a，b，c，d，e，f，g，h，其中有一枚且仅有一枚硬币是伪造的，假硬币的重量与真硬币的重量不同，可能轻，也可能重。现要求以天平为工具，用最少的比较次数挑选出假硬币，并同时确定这枚硬币的重量比其它真硬币是轻还是重。 A：用H 和L 分别表示假硬币较其它真硬币重或轻。下面对这一判定方法加以说明，并分析它的正确性。 从八枚硬币中任取六枚，假设是a，b，c，d，e 和f，在天平两端各放三枚进行比较。 假设a，b，c 三枚放在天平的一端，d，e，f 三枚放在天平的另一端，可能出现三种比较结果：（1）a＋b＋c&gt;d＋e＋f（2）a＋b＋c＝d＋e＋f（3）a＋b＋c&lt;d＋e＋f 这里，只以第一种情况为例进行讨论。若a＋b＋c&gt;d＋e＋f，根据题目的假设，可以肯定这六枚硬币中必有一枚为假币，同时也说明g，h 为真币。这时可将天平两端个去掉一枚硬币，假设它们是c 和f，同时将天平两端的硬币各换一枚，假设硬币b，d 作了互换，然后进行第二次比较，那么比较的结果同样可能有三种： ①a＋d&gt;b＋e 这种情况表明天平两端去掉硬币c，f 且硬币b，d 互换后，天平两端的轻重关系保持不变，从而说明了假币必然是a，e 中的一个，这时我们只要用一枚真币（b，c，d，f，g，h）和a 或e 进行比较，就能找出假币。例如，用b 和a 进行比较，若a&gt;b，则a 是较重的假币；若a＝b，则e 为较轻的假币；不可能出现a&lt;b 的情况。 ②a＋d＝b＋e 此时天平两端由不平衡变为平衡，表明假币一定在去掉的两枚硬币c，f 中，a，b，d，e，g，h 必定为真硬币，同样的方法，用一枚真币和c 或f 进行比较，例如，用a 和c 进行比较，若c&gt;a，则c 是较重的假币；若a＝c，则f 为较轻的假币；不可能出现c&lt;a 的情况. ③a＋d]]></content>
      <categories>
        <category>Computer Science</category>
        <category>Data Structure</category>
      </categories>
      <tags>
        <tag>Data Structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构第三章：栈]]></title>
    <url>%2F2017%2F12%2F16%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E7%AC%AC%E4%B8%89%E7%AB%A0%EF%BC%9A%E6%A0%88%2F</url>
    <content type="text"><![CDATA[Abstract：栈和队列。 3.1 栈—栈的定义及基本运算栈和队列：逻辑结构与线性表相同，特点在于受限制的运算 栈：后进先出（秤砣） 队列：先进先出（管道） 栈：限制在表的一端进行插删操作的线性表；后进先出的线性表，简称LIFO 表 栈顶：允许插删的一端 栈底：固定端 空栈：表中无元素时 基本操作： ⑴ 栈初始化：Init_Stack(s)初始条件：栈s 不存在操作结果：构造空栈。 ⑵ 判栈空：Empty_Stack(s)初始条件：栈s 已存在操作结果：若s 为空栈返回为1，否则返回为0。 ⑶ 入栈： Push_Stack(s，x)初始条件：栈s 已存在操作结果：在栈s 的顶部插入一个新元素x， x 成为新的栈顶元素。栈发生变化。 ⑷ 出栈：Pop_Stack(s)初始条件：栈s 存在且非空操作结果：栈s 的顶部元素从栈中删除，栈中少了一个元素。栈发生变化。 ⑸ 读栈顶元素：Top_Stack(s)初始条件：栈s 存在且非空操作结果：栈顶元素作为结果返回，栈不变化。 3.1 栈—栈的存储实现和运算实现1.顺序栈顺序栈：利用顺序存储方式实现的栈；类似于顺序表的定义,栈中的数据元素用一个预设的足够长度的一维数组来实现：datatype data[MAXSIZE]，栈底位置可以设置在数组的任一个端点，而栈顶是随着插入和删除而变化的，用一个int top 来作为栈顶的指针，指明当前栈顶的位置，同样将data 和top 封装在一个结构中，顺序栈的类型描述如下： 12345#define MAXSIZE 1024typedef struct&#123; datatype data[MAXSIZE]; int top;&#125;SeqStack 定义一个指向顺序栈的指针：SeqStack *s; 通常0 下标端设为栈底，这样空栈时栈顶指针top=-1; 入栈时，栈顶指针加１，即s-&gt;top++; 出栈时，栈顶指针减１，即s-&gt;top—。栈操作的示意图如图所示 图(a)是空栈，图(c)是A、B、C、D、E 5 个元素依次入栈之后，图(d)是在图(c)之后E、D 相继出栈，此时栈中还有3 个元素，或许最近出栈的元素D、E 仍然在原先的单元存储着，但top 指针已经指向了新的栈顶，则元素D、E 已不在栈中了 栈顶元素：s-&gt;data[s-&gt;top] 基本操作： 12345678910111213141516171819202122232425262728293031323334353637⑴ 置空栈：首先建立栈空间，然后初始化栈顶指针。SeqStack *Init_SeqStack()&#123; SeqStack *s;s=malloc(sizeof(SeqStack));s-&gt;top= -1; return s;&#125;⑵ 判空栈int Empty_SeqStack(SeqStack *s)&#123; if (s-&gt;top= = -1) return 1;else return 0;&#125;⑶ 入栈int Push_SeqStack (SeqStack *s, datatype x)&#123;if (s-&gt;top= =MAXSIZE-1) return 0; /*空栈可入，但栈满不能入栈*/else &#123; s-&gt;top++;s-&gt;data[s-&gt;top]=x;return 1;&#125;&#125;⑷ 出栈int Pop_SeqStack(SeqStack *s, datatype *x)&#123; if (Empty_SeqStack ( s ) ) return 0; /*栈空不能出栈*/else &#123; *x=s-&gt;data[s-&gt;top];s-&gt;top--; return 1; &#125; /*栈顶元素存入*x，返回*/&#125;⑸ 取栈顶元素datatype Top_SeqStack(SeqStack *s)&#123; if ( Empty_SeqStack ( s ) ) return 0; /*栈空*/else return (s-&gt;data[s-&gt;top] );&#125;1. 对于顺序栈，入栈时，首先判栈是否满了，栈满的条件为：s-&gt;top= =MAXSIZE-1，栈满时，不能入栈; 否则出现空间溢出，引起错误，这种现象称为上溢。2. 出栈和读栈顶元素操作，先判栈是否为空，为空时不能操作，否则产生错误。通常栈空时常作为一种控制转移的条件。 2.链栈链栈：链式存储结构实现的栈，通常用单链表表示； 12345typedef struct node&#123; datatype data; struct node *next;&#125;StackNode, *LinkList; 说明top 为栈顶指针： LinkStack top ;因为栈中的主要运算是在栈顶插入、删除，显然在链表的头部做栈顶是最方便的，而且没有必要象单链表那样为了运算方便附加一个头结点。 基本操作： 123456789101112131415161718192021222324252627282930313233⑴ 置空栈LinkStack Init_LinkStack（）&#123; return NULL;&#125;⑵ 判栈空int Empty_LinkStack（LinkStack top ）&#123; if（top==-1） return 1;else return 0;&#125;⑶ 入栈LinkStack Push_LinkStack（LinkStack top, datatype x）&#123; StackNode *s;s=malloc（sizeof（StackNode））;s-&gt;data=x;s-&gt;next=top;top=s;return top;&#125;⑷ 出栈LinkStack Pop_LinkStack (LinkStack top, datatype *x)&#123; StackNode *p;if （top= =NULL） return NULL;else &#123; *x = top-&gt;data;p = top;top = top-&gt;next;free (p);return top;&#125;&#125; 3.2 栈的应用举例1.简单应用：数制转换问题将十进制数N 转换为r 进制的数，其转换方法利用辗转相除法：以N=3456，r=8 为例。 算法思想如下：当N&gt;0 时重复1，2 1． 若N≠0，则将N % r 压入栈s 中，执行2;若N=0，将栈s 的内容依次出栈，算法结束。2． 用N / r 代替N 1234567891011121314151617#define L 10void conversation(int N, int r)&#123; int s[L], top int x; top = -1; while(N) &#123; s[++top]=N%r; N = N/r; &#125; while (top!=-1) &#123; x=s[top--]; printf(&quot;%d&quot;, x); &#125;&#125; 2.利用栈实现迷宫的求解——回溯法 试探方向：在上述表示迷宫的情况下，每个点有8 个方向去试探，如当前点的坐标(x,y)，与其相邻的8 个点的坐标都可根据与该点的相邻方位而得到，如图所示。因为出口在（m，n），因此试探顺序规定为：从当前位置向前试探的方向为从正东沿顺时针方向进行。为了简化问题，方便的求出新点的坐标，将从正东开始沿顺时针进行的这8 个方向的坐标增量放在一个结构数组move [ 8 ]中，在move 数组中，每个元素有两个域组成，x：横坐标增量，y：纵坐标增量。 迷宫求解算法思想如下：1． 栈初始化;2． 将入口点坐标及到达该点的方向（设为－１）入栈3． while (栈不空){ 栈顶元素＝＞（x , y , d）出栈;求出下一个要试探的方向d++ ;while （还有剩余试探方向时）{ if （d 方向可走）则{ （x , y , d）入栈;求新点坐标(i, j ) ;将新点（i , j）切换为当前点（x , y） ;if ( (x ,ｙ)= =(ｍ,n) ) 结束;else 重置d=0 ;}else d++ ;}} 123456789101112131415161718192021222324252627282930int path( maze move )int maze[m][n];item move[8];&#123; SeqStack s; datetype temp; int x, y, d, i, j; temp.x = 1; temp.y = 1; temp.d = -1; Push - _SeqStack( s temp ); while ( !Empty_SeqStack( s ) ) &#123; Pop_SeqStack( s, ＆ temp ); x = temp.x; y = temp.y; d = temp.d + 1; while ( d &lt; 8 ) &#123; i = x + move[d].x; j = y + move[d].y; if ( maze[i][j] = = 0 ) &#123; temp = &#123; x, y, d &#125;; Push_SeqStack( s, temp ); x = i; y = j; maze[x][y] = -1; if ( x == m &amp;&amp; y = = n ) return(1); /*迷宫有路*/ else d = 0; &#125;else d++; &#125; /*while (d&lt;8)*/ &#125; /*while */ return(0); /*迷宫无路*/&#125; 栈中保存的就是一条迷宫的通路. 3. 表达式求值表达式是由运算对象、运算符、括号组成的有意义的式子。运算符从运算对象的个数上分，有单目运算符和双目运算符；从运算类型上分，有算术运算、关系运算、逻辑运算。在此仅限于讨论只含二目运算符的算术表达式。 1． 中缀表达式求值：中缀表达式：每个二目运算符在两个运算量的中间，假设所讨论的算术运算符包括：+ 、- 、*、/、%、^（乘方）和括号（）。 设运算规则为：．运算符的优先级为：（）——&gt; ^ ——&gt;＊、/、%——&gt; +、- ；．有括号出现时先算括号内的，后算括号外的，多层括号，由内向外进行；．乘方连续出现时先算最右面的； 表达式作为一个满足表达式语法规则的串存储，如表达式“32^（4+22-１3）-5”,它的的求值过程为：自左向右扫描表达式，当扫描到32 时不能马上计算，因为后面可能还有更高的运算，正确的处理过程是：需要两个栈：对象栈s1 和算符栈s2。当自左至右扫描表达式的每一个字符时，若当前字符是运算对象，入对象栈，是运算符时，若这个运算符比栈顶运算符高则入栈，继续向后处理，若这个运算符比栈顶运算符低则从对象栈出栈两个运算量，从算符栈出栈一个运算符进行运算，并将其运算结果入对象栈，继续处理当前字符，直到遇到结束符。 2.后缀表达式： 不包含括号，运算符放在两个运算对象的后面，所有的计算按运算符出现的顺序，严格从左向右进行（不再考虑运算符的优先规则）。 运用后缀表达式进行计算的具体做法： 建立一个栈S 。从左到右读表达式，如果读到操作数就将它压入栈S中，如果读到n元运算符(即需要参数个数为n的运算符)则取出由栈顶向下的n项按操作数运算，再将运算的结果代替原栈顶的n项，压入栈S中 。如果后缀表达式未读完，则重复上面过程，最后输出栈顶的数值则为结束。 3.前缀表达式：运算符位于操作数之前。 4.中缀表达式：常用的算术表示方法。 举例：(3 + 4) × 5 - 6 就是中缀表达式 × + 3 4 5 6 前缀表达式3 4 + 5 × 6 - 后缀表达式 CSDN.前缀、中缀、后缀表达式 3.3 队列—队列的定义及基本运算队列：先进先出FIFO，允许插入的一端叫队尾rear，允许删除的一端叫队头front（插入与删除操作分属两端）。 基本操作： ⑴ 队列初始化：Init_Queue(q)初始条件： 队q 不存在。操作结果： 构造了一个空队。 ⑵ 入队操作： In_Queue(q,x),初始条件： 队q 存在。操作结果： 对已存在的队列q，插入一个元素x 到队尾，队发生变化。 ⑶ 出队操作： Out_Queue(q,x)初始条件: 队q 存在且非空操作结果： 删除队首元素，并返回其值，队发生变化。 ⑷ 读队头元素：Front_Queue(q,x)初始条件: 队q 存在且非空操作结果： 读队头元素，并返回其值，队不变； ⑸ 判队空操作：Empty_Queue(q)初始条件： 队q 存在操作结果： 若q 为空队则返回为1，否则返回为0。 3.3 队列—队列的存储实现及运算实现与线性表、栈类似，队列也有顺序存储和链式存储两种存储方法。 1.顺序队队头队尾都是活动的，有队头队尾两个指针。 12345define MAXSIZE 1024 //队列的最大容量typedef struct&#123; datatype data[MAXSIZE]; //队员的存储空间 int rear, front; //定义队头队尾的指针&#125;SeQueue; 定义一个指向队的指针变量：SeQueue sq;申请一个顺序队的存储空间： s= malloc(sizeof(SeQueue));队列的数据区为：sq-&gt;data[0]——sq-&gt;data[MAXSIZE - 1]*队头队尾指针：sq-&gt;front; sq-&gt;rear;（若垂直方向，则下面队头上面队尾） 设队头指针指向队头元素前面一个位置，队尾指针指向队尾元素（这样的设置是为了某些运算的方便，并不是唯一的方法）。 置空队则为：sq-&gt;front=sq-&gt;rear=-1; 在不考虑溢出的情况下，入队操作队尾指针加1，指向新位置后，元素入队。操作如下：sq-&gt;rear++;sq-&gt;data[sq-&gt;rear]=x; /原队尾元素送x 中/ 在不考虑队空的情况下，出队操作队头指针加1，表明队头元素出队。操作如下： sq-&gt;front++;x=sq-&gt;data[sq-&gt;front]; 队中元素的个数：m=(sq-&gt;rear)-(q-&gt;front);队满时：m= MAXSIZE； 队空时：m=0。 循环队列按照上述思想建立的空队及入队出队示意图如图所示，设MAXSIZE=10。从图中可以看到，随着入队出队的进行，会使整个队列整体向后，这样就出现了图中的现象：队尾指针已经移到了最后,再有元素入队就会出现溢出，而事实上此时队中并未真的“满员”，这种现象为“假溢出”，这是由于“队尾入队头出”这种受限制的操作所造成。解决假溢出的方法之一是将队列的数据区data[0..MAXSIZE-1]看成头尾相接的循环结构，头尾指针的关系不变，将其称为“循环队”，“循环队”的示意图如图所示。 因为是头尾相接的循环结构，入队时的队尾指针加1 操作修改为：sq-&gt;rear=(sq-&gt;rear+1) % MAXSIZE; 出队时的队头指针加1 操作修改为：sq-&gt;front=(sq-&gt;front+1) % MAXSIZE; 设MAXSIZE=10，下图是循环队列操作示意图： 从循环队可以看出，(a)中具有a5 、a6 、a7 、a8 四个元素，此时front=4,rear=8；随着a9~a14 相继入队，队中具有了10 个元素—-队满，此时front=4,rear=4,如（b）所示，可见在队满情况下有：front==rear。若在（a）情况下，a5~a8 相继出队，此时队空， front=4，rear=4，如（c）所示，即在队空情况下也有：front==rear。就是说“队满”和“队空”的条件是相同的了。这显然是必须要解决的一个问题。 方法之一是附设一个存储队中元素个数的变量如num，当num==0 时队空，当num==MAXSIZE 时为队满。 另一种方法是少用一个元素空间，把图（d）所示的情况就视为队满，此时的状态是队尾指针加1 就会从后面赶上队头指针，这种情况下队满的条件是： (rear+1) %MAXSIZE==front，也能和空队区别开。 循环队列基本操作typedef struct {datatype data[MAXSIZE]; /数据的存储区/int front,rear; /队头队尾指针/int num; /队中元素的个数/}c_SeQueue; /循环队/ ⑴ 置空队c_SeQueue* Init_SeQueue(){ q=malloc(sizeof(c_SeQueue));q-&gt;front=q-&gt;rear=MAXSIZE-1;q-&gt;num=0;return q;} ⑵ 入队int In_SeQueue ( c_SeQueue q , datatype x){ if (num==MAXSIZE){ printf(＂队满＂);return –1; /队满不能入队/}else{ q-&gt;rear=(q-&gt;rear+1) % MAXSIZE;q-&gt;data[q-&gt;rear]=x;num++;return 1; /入队完成*/}} ⑶ 出队int Out_SeQueue (c_SeQueue q , datatype x){ if (num==0){ printf(＂队空＂)；return –1; /队空不能出队/}else{ q-&gt;front=(q-&gt;front+1) % MAXSIZE;x=q-&gt;data[q-&gt;front]; /读出队头元素/num—;return 1; /出队完成*/}} ⑷ 判队空int Empty_SeQueue(c_SeQueue *q){ if (num==0) return 1;else return 0;} 2.链队链队：链式存储的队；和链栈类似，用单链表来实现链队，根据队的FIFO 原则，为了操作上的方便，我们分别需要一个头指针和尾指针，如： 头指针front 和尾指针rear 是两个独立的指针变量，从结构性上考虑，通常将二者封装在一个结构中。 链队的描述： 1234567typedef struct node&#123; datatype data; struct node *next;&#125;QNode;/*链队结点的类型*/ typedef struct &#123; QNode *front, *rear;&#125;LQueue; /*将头尾指针封装在一起的链队*/ 定义一个指向链队的指针：LQueue *q; 带头结点的链队： 链队的基本运算如下： (1) 创建一个带头结点的空队：LQueue Init_LQueue(){ LQueue q,p;q=malloc(sizeof(LQueue)); /申请头尾指针结点/p=malloc(sizeof(QNode)); /申请链队头结点*/p-&gt;next=NULL; q-&gt;front=q-&gt;rear=p;return q;} (2) 入队void In_LQueue(LQueue q , datatype x){ QNode p;p=malloc(sizeof(QNnode)); /申请新结点/p-&gt;data=x; p-&gt;next=NULL;q-&gt;rear-&gt;next=p;q-&gt;rear=p;} (3) 判队空int Empty_LQueue( LQueue *q){ if (q-&gt;front==q-&gt;rear) return 0;else return 1;} (4) 出队int Out_LQueue(LQueue q , datatype x){ QNnode p;if (Empty_LQueue(q) ){ printf (＂队空＂)； return 0;} /队空，出队失败/else{ p=q-&gt;front-&gt;neat;q-&gt;front-&gt;next=p-&gt;next; x=p-&gt;data;/队头元素放x 中/free(p);if (q-&gt;front-&gt;next==NULL)q-&gt;rear=q-&gt;front;/只有一个元素时，出队后队空，此时还要要修改队尾指针参考图/return 1;}} 3.5 队列应用举例例：求迷宫的最短路径： 现要求设计一个算法找一条从迷宫入口到出口的最短路径。本算法要求找一条迷宫的最短路径，算法的基本思想为：从迷宫入口点（1,1）出发，向四周搜索，记下所有一步能到达的坐标点；然后依次再从这些点出发，再记下所有一步能到达的坐标点，…，依此类推，直到到达迷宫的出口点(m,n)为止，然后从出口点沿搜索路径回溯直至入口。这样就找到了一条迷宫的最短路径，否则迷宫无路径。 有关迷宫的数据结构、试探方向、如何防止重复到达某点以避免发生死循环的问题与之前的例子处理相同，不同的是：如何存储搜索路径。在搜索过程中必须记下每一个可到达的坐标点，以便从这些点出发继续向四周搜索。由于先到达的点先向下搜索，故引进一个“先进先出”数据结构———队列来保存已到达的坐标点。到达迷宫的出口点(m,n)后，为了能够从出口点沿搜索路径回溯直至入口，对于每一点，记下坐标点的同时，还要记下到达该点的前驱点，因此，用一个结构数组sq[num]作为队列的存储空间，因为迷宫中每个点至多被访问一次，所以num 至多等于m*n。sq 的每一个结构有三个域：x,y 和pre，其中x,y 分别为所到达的点的坐标，pre 为前驱点在sq 中的坐标，是一个静态链域。除sq 外，还有队头、队尾指针：front 和rear 用来指向队头和队尾元素。 队的定义如下： typedef struct { int x,y; int pre; }sqtype; sqtype sq[num]; int front,rear; 初始状态，队列中只有一个元素sq[１]记录的是入口点的坐标（１,１），因为该点是出发点，因此没有前驱点，pre 域为－１，队头指针front 和队尾指针rear 均指向它，此后搜索时都是以front 所指点为搜索的出发点，当搜索到一个可到达点时，即将该点的坐标及front 所指点的位置入队，不但记下了到达点的坐标，还记下了它的前驱点。front 所指点的８个方向搜索完毕后，则出队，继续对下一点搜索。搜索过程中遇到出口点则成功，搜索结束，打印出迷宫最短路径，算法结束；或者当前队空即没有搜索点了，表明没有路径算法也结束。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546void path( maze move )int maze[m][n] ； /*迷宫数组*/item move[8] ； /*坐标增量数组*/&#123; sqtype sq[NUM]; int front, rear ； int x, y, i, j, v; front = rear = 0; sq[０].x = 1; sq[０].y = 1; sq[０].pre = -1; /*入口点入队*/ maze[1, 1] = -1; while ( front &lt;= rear ) /*队列不空*/ &#123; x = sq[front].x; y = sq[front].y; for ( v = 0; v &lt; 8; v++ ) &#123; i = x + move[v].x; j = x + move[v].y; if ( maze[i][j] == 0 ) &#123; rear++; sq[rear].x = i; sq[rear].y = j; sq[rear].pre = front; maze[i][j] = -1; &#125; if ( i == m &amp;&amp; j == n ) &#123; printpath( sq, rear ); /*打印迷宫*/ restore( maze ); /*恢复迷宫*/ return(1); &#125; &#125; /*for v*/ front++; /*当前点搜索完，取下一个点搜索*/ &#125; /*while*/ return(0);&#125; /*path*/void printpath( sqtype sq[], int rear ) /*打印迷宫路径*/&#123; int i; i = rear; do &#123; printf( ＂ (% d, % d)􀃅 ＂, sq[i].x, sq[i].y ); i = sq[i].pre; /*回溯*/ &#125; while ( i != -1 );&#125; /*printpath*/ 迷宫搜索过程如图： 运行结果： (6,8)􀃅 (5,7)􀃅 (4,6)􀃅(4,5)􀃅 (3,4)􀃅 (3,3)􀃅 (2,2)􀃅(1,1) 在上面的例子中，不能采用循环队列，因为在本问题中，队列中保存了探索到的路径序列，如果用循环队列，则把先前得到的路径序列覆盖掉。而在有些问题中，如持续运行的实时监控系统中，监控系统源源不断的收到监控对象顺序发来的信息如报警，为了保持报警信息的顺序性，就要按顺序一一保存，而这些信息是无穷多个，不可能全部同时驻留内存，可根据实际问题，设计一个适当大的向量空间，用作循环队列，最初收到的报警信息一一入队，当队满之后，又有新的报警到来到时，新的报警则覆盖掉了旧的报警，内存中始终保持当前最新的若干条报警，以便满足快速查询。]]></content>
      <categories>
        <category>Computer Science</category>
        <category>Data Structure</category>
      </categories>
      <tags>
        <tag>Data Structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构第十章：排序]]></title>
    <url>%2F2017%2F12%2F14%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[Abstract：排序。 排序初导排序：使序列成为一个按关键字有序的序列。 排序(Sorting)是计算机程序设计中的一种重要操作，其功能是对一个数据元素集合或序列重新排列成一个按数据元素某个项值有序的序列。作为排序依据的数据项称为“排序码”，也即数据元素的关键码。为了便于查找，通常希望计算机中的数据表是按关键码有序的。如有序表的折半查找，查找效率较高。还有，二叉排序树、B-树和B+树的构造过程就是一个排序过程。若关键码是主关键码，则对于任意待排序序列，经排序后得到的结果是唯一的；若关键码是次关键码，排序结果可能不唯一，这是因为具有相同关键码的数据元素，这些元素在排序结果中，它们之间的的位置关系与排序前不能保持。 若对任意的数据元素序列，使用某个排序方法，对它按关键码进行排序：若相同关键码元素间的位置关系，排序前与排序后保持一致，称此排序方法是稳定的；而不能保持一致的排序方法则称为不稳定的。 排序分为两类：内排序和外排序。 内排序：指待排序列完全存放在内存中所进行的排序过程，适合不太大的元素序列。在排序整个过程中，待排序的所有记录全部被放置在内存中。 外排序：指排序过程中还需访问外存储器，足够大的元素序列，因不能完全放入内存，只能使用外排序。外排序是由于排序的记录个数太多，不能同时放置在内存，整个排序过程需要在内外存之间多次交换数据才能进行。 基本概念和分类： 在排序问题中，通常将数据元素称为记录 排序的依据是关键字之间的大小关系，那么，对同一个记录集合，针对不同的关键字进行排序，可以得到不同序列。 关键字kiki可以是记录rr的主关键字，也可以是次关键字，甚至是若干数据项的组合。 排序的稳定性： 定义：假设ki=kj (1≤i≤n,1≤j≤n,i≠j)ki=kj (1≤i≤n,1≤j≤n,i≠j)，且在排序前的序列中riri领先于rjrj（即i&lt;ji&lt;j）。如果排序后riri仍领先于rjrj，则称所用的排序方法是稳定的；反之，若可能使得排序后的序列中rjrj领先于riri，则称所用的排序方法是不稳定的。 不稳定的排序算法有：希尔、快速、堆排和选择排序。 时间性能： 内排序：主要是比较 + 移动 2个操作 高效率的内排序算法应该是具有尽可能少的关键字比较次数和尽可能少的记录移动次数 辅助空间：辅助存储空间是除了存放待排序所占用的存储空间之外，执行算法所需要的其他存储空间。 算法的复杂性：指算法本身的复杂度，非时间复杂度。 根据排序过程中借助的主要操作，我们把内排序分为：插入排序、交换排序、选择排序和归并排序。 排序用到的结构与函数：顺序表 123456#define MAXSIZE 10typedef struct&#123;int r[MAXSIZE];int length;&#125;SqList; 此外，由于排序最常用到的操作是数组两元素的交换，这里写成一个函数，如下所示： 12345// 交换L中数组r的下标为i和j的值void swap(sqlist *L, int i, int j)&#123; int temp = L-&gt;r[i]; L-&gt;r[i] = L-&gt;r[j]; L-&gt;r[j] = temp;&#125; 冒泡排序冒泡：交换排序；两两比较相邻记录的关键字，如果反序则交换，知道没有反序的记录为止。 123456789101112// 冒泡排序初级版void BubbleSort0(SqList *L)&#123; int i, j; for (i = 0; i &lt; L-&gt;length - 1; i++) &#123; for (j = i + 1; j &lt;= L-&gt;length - 1; j++)&#123; if (L-&gt;r[i] &gt; L-&gt;r[j])&#123; // 实现递增排序 swap(L, i, j); &#125; &#125; &#125;&#125; 这段代码不算是标准的冒泡排序算法，因为不满足“两两比较相邻记录”的冒泡排序思想，它更应该是最简单的交换排序。它的思路是让每一个关键字都和后面的每一个关键字比较，如果大或小则进行交换，这样关键字在一次循环后，第一个位置的关键字会变成最大值或者最小值。 12345678910111213// 正宗的冒泡排序算法实现代码void BubbleSort(SqList *L)&#123; int i, j; for (i = 0; i &lt; L-&gt;length; i++) &#123; for (j = L-&gt;length - 2; j &gt;= i; j--)&#123; // j是从后往前循环 if (L-&gt;r[j] &gt; L-&gt;r[j + 1])&#123; // 实现递增排序 swap(L, j, j + 1); &#125; &#125; &#125;&#125; 这里改变的地方是在内循环中，j是从数组最后往前进行比较，并且是逐个往前进行相邻记录的比较，这样最大值或者最小值会在第一次循环过后，从后面浮现到第一个位置，如同气泡一样浮到上面。 这段实现代码其实还是可以进行优化的，例如待排序数组是{2,1,3,4,5,6,7,8,9},需要进行递增排序，可以发现其实只需要交换前两个元素的位置即可完成，但是上述算法还是会在交换完这两者位置后继续进行循环，这样效率就不高了，所以可以在算法中增加一个标志，当有一次循环中没有进行数据交换，就证明数组已经是完成排序的，此时就可以退出算法，实现代码如下： 123456789101112131415161718// 改进版冒泡算法void BubbleSortOptimz(SqList *L)&#123; int i, j; bool flag = true; for (int i = 0; i &lt; L-&gt;length &amp;&amp; flag; i++)&#123; // 若 flag为false则退出循环 flag = false; for (j = L-&gt;length - 2; j &gt;= i; j--)&#123; // j是从后往前循环 if (L-&gt;r[j] &gt; L-&gt;r[j + 1])&#123; // 实现递增排序 swap(L, j, j + 1); // 如果有数据交换，则flag是true flag = true; &#125; &#125; &#125;&#125; 冒泡排序算法的时间复杂度是O(n2)。 简单选择排序简单选择排序算法(Simple Selection Sort)就是通过n−i次关键字间的比较，从n−i+1个记录中选出关键字中最小的记录，并和第i(1≤i≤n)个记录进行交换。 123456789101112131415// 简单选择排序算法void SelectSort(SqList *L)&#123; int i, j, min; for (i = 0; i &lt; L-&gt;length - 1; i++)&#123; // 将当前下标定义为最小值下标 min = i; for (j = i + 1; j &lt;= L-&gt;length - 1; j++)&#123; if (L-&gt;r[j] &lt; L-&gt;r[min]) min = j; &#125; // 若min不等于i，说明找到最小值，进行交换 if (min != i) swap(L, i, min); &#125;&#125; 简单选择排序的最大特点就是交换移动数据次数相当少。分析其时间复杂度发现，无论最好最差的情况，比较次数都是一样的，都需要比较∑n−1i=1(n−i)=(n−1)+(n−2)+⋯+2+1=n(n−1)2∑i=1n−1(n−i)=(n−1)+(n−2)+⋯+2+1=n(n−1)2次。对于交换次数，最好的时候是交换0次，而最差的情况是n−1n−1次。因此，总的时间复杂度是O(n2)O(n2)，虽然与冒泡排序一样的时间复杂度，但是其性能上还是略好于冒泡排序。 直接插入排序直接插入排序(Straight Insertion Sort)的基本操作是将一个记录插入到已经排好序的有序表中，从而得到一个新的、记录数增加1的有序表。 123456789101112131415// 直接插入排序void InsertSort(SqList *L)&#123; int i, j,val; for (i = 1; i &lt;= L-&gt;length - 1; i++)&#123; if (L-&gt;r[i] &lt; L-&gt;r[i - 1])&#123; // 将L-&gt;r[i]插入有序表中,使用val保存待插入的数组元素L-&gt;r[i] val = L-&gt;r[i]; for (j = i - 1; L-&gt;r[j]&gt;val; j--) // 记录后移 L-&gt;r[j + 1] = L-&gt;r[j]; // 插入到正确位置 L-&gt;r[j + 1] =val; &#125; &#125;&#125; 直接插入排序算法是需要有一个保存待插入数值的辅助空间。 在时间复杂度方面，最好的情况是待排序的表本身就是有序的，如{2,3,4,5,6}，比较次数则是n−1n−1次，然后不需要进行移动，时间复杂度是O(n)O(n)。 最差的情况就是待排序表是逆序的情况，如{6,5,4,3,2},此时需要比较$\sum{i=2}^{n} i = \frac{(n+2)(n-1)}{2}次，而记录的移动次数也达到最大值次，而记录的移动次数也达到最大值\sum{i=2}^{n} (i+1) = \frac{(n+4)(n-1)}{2}$次。 如果排序记录是随机的，那么根据概率相同的原则，平均比较和移动次数约为n_^2/4。因此，可以得出直接插入排序算法的时间复杂度是O(n2)。同时也可以看出，直接插入排序算法会比冒泡排序和简单选择排序算法的性能要更好一些。 希尔排序上述三种排序算法的时间复杂度都是O(n2)，而希尔排序是突破这个时间复杂度的第一批算法之一。 其实直接插入排序的效率在某些情况下是非常高效的，这些情况是指记录本来就很少或者待排序的表基本有序的情况，但是这两种情况都是特殊情况，在现实中比较少见。而希尔排序就是通过创造条件来改进直接插入排序的算法。 希尔排序的做法是将原本有大量记录数的记录进行分组，分割成若干个序列，这样每个子序列待排序的记录就比较少了，然后就可以对子序列分别进行直接插入排序，当整个序列基本有序时，再对全体记录进行一次直接插入排序。 这里的基本有序是指小的关键字基本在前面，大的基本在后面，不大不小的在中间。像{2,1,3,6,4,7,5,8,9}可以称为基本有序。 这里的关键就是如何进行分割，希尔排序采取的是跳跃分割的策略：将相距某个“增量”的记录组成一个子序列，这样才能保证在子序列内分别进行直接插入排序后得到的结果是基本有序而不是局部有序。 12345678910111213141516171819// 希尔排序void ShellSort(SqList *L)&#123; int i, j,val; int increment = L-&gt;length; do&#123; // 增量序列 increment = increment / 3 + 1; for (i = increment; i &lt;= L-&gt;length - 1; i++)&#123; if (L-&gt;r[i]&lt;L-&gt;r[i - increment])&#123; // 将L-&gt;r[i]插入有序表中,使用val保存待插入的数组元素L-&gt;r[i] val = L-&gt;r[i]; for (j = i - increment; j &gt;= 0 &amp;&amp; L-&gt;r[j]&gt;val; j -= increment) // 记录后移，查找插入位置 L-&gt;r[j + increment] = L-&gt;r[j]; L-&gt;r[j + increment] = val; &#125; &#125; &#125; while (increment &gt; 1);&#125; 上述代码中增量的选取是increment = increment / 3 + 1，实际上增量的选取是非常关键的，现在还没有人找到一种最好的增量序列，但是大量研究表明，当增量序列是δ[k]=2t−k+1−1(0≤k≤t≤⌊log2(n+1)⌋)δ[k]=2t−k+1−1(0≤k≤t≤⌊log2(n+1)⌋)时，可以获得不错的效率，其时间复杂度是O(n32)O(n32)，要好于直接插入排序的O(n2)O(n2)。当然，这里需要注意的是增量序列的最后一个增量值必须等于1才行。此外，由于记录是跳跃式的移动，希尔排序是不稳定的排序算法。 桶排序有一个数量为Size个数的数组A，数组的值范围为(0 - Max)，然后创建一个大小为Max+1的数组B，每个元素都为0.从头遍历A，当读取到A[i]的时候，B[A[i]]的值+1，这样所有的A数组被遍历后，直接扫描B之后，输出表B就可以了。然后再根据B来对A进行排序。 12345678910111213141516171819202122232425262728293031323334353637//获得未排序数组中最大的一个元素值int GetMaxVal(int* arr, int len)&#123; int maxVal = arr[0]; //假设最大为arr[0] for (int i = 1; i &lt; len; i++) //遍历比较，找到大的就赋值给maxVal &#123; if (arr[i] &gt; maxVal) maxVal = arr[i]; &#125; return maxVal; //返回最大值&#125;void BucketSort(int *numbers, int length)&#123; if (numbers == NULL || length &lt;= 0)&#123; cout &lt;&lt; &quot;wrong input!&quot;; return; &#125; int size = GetMaxVal(numbers,length) + 1; vector&lt;int&gt; bucket(size); for (int i = 0; i &lt; length + 1; i++)&#123; bucket[i] = 0; &#125; // 计算数组中每个元素出现的次数 for (int i = 0; i &lt; length; i++)&#123; int j = numbers[i]; bucket[j] += 1; &#125; // 排序 int count = 0; for (int i = 0; i &lt; size; i++)&#123; if (bucket[i] &gt; 0)&#123; for (int j = 0; j &lt; bucket[i]; j++)&#123; numbers[count] = i; count++; &#125; &#125; &#125;&#125; 堆排序简单选择排序在待排序的nn个记录中选择一个最小的记录需要比较n−1n−1次，这是查找第一个数据，所以需要比较这么多次是比较正常的，但是可惜的是它没有把每一趟的比较结果保存下来，这导致在后面的比较中，实际有许多比较在前一趟中已经做过了。因此，如果可以做到每次在选择到最小记录的同时，并根据比较结果对其他记录做出相应的调整，那样排序的总体效率就会变得很高了。 堆排序(Heap Sort)就是对简单选择排序进行的一种改进，并且效果非常明显。 堆是具有下列性质的完全二叉树：每个结点的值都大于或等于其左右孩子结点的值，称为最大堆或者大顶堆；或者每个结点的值都小于或等于其左右孩子结点的值，称为最小堆或者小顶堆。 下图是一个例子，左边的是大顶堆，而右边的是小顶堆。 根结点一定是堆中所有结点最大或者最小者。 堆排序的基本思想是，将待排序的序列构成一个最大堆。此时，整个序列的最大值就是堆顶的根结点。将它移走（其实就是将其与堆数组的末尾元素进行交换，此时末尾元素就是最大值），然后将剩余的n−1n−1个序列重新构成一个堆，这样就会得到nn个元素中的次最大值。如此反复执行，便能得到一个有序序列。 1234567891011121314151617181920212223242526272829303132333435// 已知L-&gt;r[s...m]中记录的关键字除L-&gt;r[s]之外均满足堆的定义// 本函数调整L-&gt;r[s]的关键字，使L-&gt;r[s..m]成为一个大顶堆void HeapAdjust(SqList *L, int s, int m)&#123; int temp, j; temp = L-&gt;r[s]; for (j = 2 * s; j &lt;= m - 1; j *= 2)&#123; // 沿关键字较大的孩子结点向下筛选 if (j &lt; m-1 &amp;&amp; L-&gt;r[j] &lt; L-&gt;r[j + 1]) // j是关键字中较大的记录的下标 ++j; if (temp &gt;= L-&gt;r[j]) // 当前值不需要进行调整 break; L-&gt;r[s] = L-&gt;r[j]; s = j; &#125; // 插入 L-&gt;r[s] = temp;&#125;// 堆排序void HeapSort(SqList *L)&#123; int i; for (i = L-&gt;length / 2; i &gt;= 0; i--)&#123; // 将待排序的序列构成一个最大堆 HeapAdjust(L, i, L-&gt;length); &#125; // 开始进行排序 for (i = L-&gt;length - 1; i &gt; 0; i--)&#123; // 将堆顶记录与当前未经排序的子序列的最后一个记录交换 swap(L, 0, i); // 重新调整为最大堆 HeapAdjust(L, 0, i - 1); &#125;&#125; 从代码中可以看出，堆排序分两步走，首先是将待排序的序列构造成最大堆，这也是HeapSort()中第一个循环所做的事情，第二个循环也就是第二步，进行堆排序，逐步将每个最大值的根结点和末尾元素进行交换，然后再调整成最大堆，重复执行。 而在第一步中构造最大堆的过程中，是从⌊n2⌋⌊n2⌋的位置开始进行构造，这是从下往上、从右到左，将每个非叶结点当作根结点，将其和其子树调整成最大堆。 接下来就是分享堆排序的效率了。堆排序的运行时间主要是消耗在初始构造堆和在重建堆时的反复筛选上。 在构建堆的过程中，因为是从完全二叉树的最下层最右边的非叶结点开始构建，将它与其孩子进行比较和若有必要的交换，对每个非叶结点，最多进行两次比较和互换操作，这里需要进行这种操作的非叶结点数目是⌊n2⌋⌊n2⌋个，所以整个构建堆的时间复杂度是O(n)O(n)。 在正式排序的时候，第ii取堆顶记录重建堆需要用O(logi)的时间(完全二叉树的某个结点到根结点的距离是⌊log2i⌋+1)，并且需要取n−1n−1次堆顶记录，因此，重建堆的时间复杂度是O(nlogn)。 所以，总体上来说，堆排序的时间复杂度是O(nlogn)。由于堆排序对原始记录的排序状态并不敏感，因此它无论最好、最坏和平均时间复杂度都是O(nlogn)。同样由于记录的比较与交换是跳跃式进行，堆排序也不是稳定的排序算法。 另外，由于初始构建堆需要的比较次数较多，因此，它并不适合待排序序列个数较少的情况。 归并排序 归并排序(Merging Sort)就是利用归并的思想实现的排序方法，它的原理是假设初始序列有nn个记录，则可以看成是nn个有序的子序列，每个子序列的长度为1，然后两两合并，得到⌈n2⌉⌈n2⌉(⌈x⌉⌈x⌉表示不小于xx的最小整数)个长度为2或1的有序子序列；再两两合并，⋯⋯⋯⋯，如此重复，直至得到一个长度为nn的有序序列为止，这种排序方法称为2路归并排序。 12345678910111213141516171819202122232425262728293031323334353637383940// 归并排序,使用递归void MergeSort(SqList *L)&#123; MSort(L-&gt;r, L -&gt;r, 0, L-&gt;length-1);&#125;// 将SR[s..t]归并排序为TR1[s..t]void MSort(int SR[], int TR1[], int s, int t)&#123; int m; int TR2[MAXSIZE]; if (s == t) TR1[s] = SR[s]; else&#123; // 将SR[s..t]平分为SR[s...m-1]和SR[m...t] m = (s + t) / 2+1; MSort(SR, TR2, s, m-1); MSort(SR, TR2, m, t); // 将TR2[s..m-1]和TR2[m..t]归并到TR1[s..t] Merge(TR2, TR1, s, m-1, t); &#125;&#125;// 将有序的SR[i..m]和SR[m+1..n]归并为有序的TR[i..n]void Merge(int SR[], int TR[], int i, int m, int n)&#123; int j, k, l; for (j = m+1, k = i; i &lt;= m &amp;&amp; j &lt;= n; k++)&#123; // 将SR中记录由小到大并入TR if (SR[i] &lt; SR[j]) TR[k] = SR[i++]; else TR[k] = SR[j++]; &#125; if (i &lt;= m)&#123; for (l = 0; l &lt;= m - i; l++) // 将剩余的SR[i..m]复制到TR TR[k + l] = SR[i + l]; &#125; if (j &lt;= n)&#123; for (l = 0; l &lt;= n - j; l++) // 将剩余的SR[j..n-1]复制到TR TR[k + l] = SR[j + l]; &#125;&#125; 上述代码是一个递归版本的归并排序实现算法，其中函数MSort()的作用是将待排序序列进行分割，然后Merge()函数会对需要归并的序列进行排序并两两归并在一起。 归并排序的时间复杂度是O(nlogn)，并且无论是最好、最坏还是平均都是同样的时间性能。另外，在归并过程中需要与原始记录序列同样数量的存储空间存放归并结果，并且递归时需要深度为log2n的栈空间，因此空间复杂度是O(n+logn)。 另外，归并排序是使用两两比较，不存在跳跃，这在Merge()中的语句if(SR[i]&lt;SR[j])可以看出，所以归并排序是一个稳定的排序算法。 总体来说，归并排序是一个比较占用内存，但效率高且稳定的算法。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152// 非递归版本的归并排序void MergeSort2(SqList *L)&#123; // 申请额外空间 int* TR = (int *)malloc(L-&gt;length * sizeof(int)); int k = 1; while (k &lt; L-&gt;length)&#123; MergePass(L-&gt;r, TR, k, L-&gt;length); // 子序列长度加倍 k = 2 * k; MergePass(TR, L-&gt;r, k, L-&gt;length); k = 2 * k; &#125;&#125;// 将SR[]中相邻长度为s的子序列两两归并到TR[]void MergePass(int SR[], int TR[], int s, int n)&#123; int i = 0; int j; while (i &lt;= n - 2 * s)&#123; // 两两归并 Merge(SR, TR, i, i + s - 1, i + 2 * s - 1); i = i + 2 * s; &#125; if (i &lt; n - s + 1) // 归并最后两个子序列 Merge(SR, TR, i, i + s - 1, n - 1); else&#123; // 若最后剩下单个子序列 for (j = i; j &lt;= n - 1; j++) TR[j] = SR[j]; &#125;&#125;// 将有序的SR[i..m]和SR[m+1..n]归并为有序的TR[i..n]void Merge(int SR[], int TR[], int i, int m, int n)&#123; int j, k, l; for (j = m+1, k = i; i &lt;= m &amp;&amp; j &lt;= n; k++)&#123; // 将SR中记录由小到大并入TR if (SR[i] &lt; SR[j]) TR[k] = SR[i++]; else TR[k] = SR[j++]; &#125; if (i &lt;= m)&#123; for (l = 0; l &lt;= m - i; l++) // 将剩余的SR[i..m]复制到TR TR[k + l] = SR[i + l]; &#125; if (j &lt;= n)&#123; for (l = 0; l &lt;= n - j; l++) // 将剩余的SR[j..n-1]复制到TR TR[k + l] = SR[j + l]; &#125;&#125; 非递归版本的归并排序算法避免了递归时深度为log2n的栈空间，空间复杂度是O(n)，并且避免递归也在时间性能上有一定的提升。应该说，使用归并排序时，尽量考虑用非递归方法。 快速排序在前面介绍的几种排序算法，希尔排序相当于直接插入排序的升级，它们属于插入排序类，而堆排序相当于简单选择排序的升级，它们是属于选择排序类，而接下来介绍的快速排序就是冒泡排序的升级，它们属于交换排序类。 快速排序(Quick Sort)的基本思想是：通过一趟排序将待排序记录分割成独立的两部分，其中一部分记录的关键字均比另一部分记录的关键字小，则可分别对这两部分记录继续进行排序，以达到整个序列有序的目的。 12345678910111213141516171819202122232425262728293031323334// 快速排序void QuickSort(SqList *L)&#123; QSort(L, 0, L-&gt;length - 1);&#125;// 对待排序序列L中的子序列L-&gt;r[low...high]做快速排序void QSort(SqList *L, int low, int high)&#123; int pivot; if (low &lt; high)&#123; // 将L-&gt;r[low...high]一分为二，算出枢轴值pivot pivot = Partition(L, low, high); // 对低子序列递归排序 QSort(L, low, pivot - 1); // 对高子序列递归排序 QSort(L, pivot + 1, high); &#125;&#125;// 交换待排序序列L中子表的记录，使枢轴记录到位，并返回其所在位置// 并使得其之前位置的值小于它，后面位置的值大于它int Partition(SqList *L, int low, int high)&#123; int pivot_key; // 初始值设置为子表的第一个记录 pivot_key = L-&gt;r[low]; while (low &lt; high)&#123; while (low &lt; high &amp;&amp; L-&gt;r[high] &gt;= pivot_key) high--; // 将小于枢轴记录的值交换到低端 swap(L, low, high); while (low &lt; high &amp;&amp; L-&gt;r[low] &lt;= pivot_key) low++; // 将大于枢轴记录的值交换到高端 swap(L, low, high); &#125; return low;&#125; 上述代码同样是使用了递归，其中Partition()函数要做的就是先选取待排序序列中的一个关键字，然后将其放在一个位置，这个位置左边的值小于它，右边的值都大于它，这样的值被称为枢轴。 快速排序的时间性能取决于快速排序递归的深度。在最优情况下，Partition()每次都划分得很均匀，如果排序nn个关键字，其递归树的深度技术⌊logn⌋+1⌊logn⌋+1，即需要递归log2nlog2n次，其时间复杂度是O(nlogn)O(nlogn)。而最坏的情况下，待排序的序列是正序或逆序，得到的递归树是斜树，最终其时间复杂度是O(n2)O(n2)。 平均情况可以得到时间复杂度是O(nlogn)O(nlogn)，而空间复杂度的平均情况是O(logn)O(logn)。但是由于关键字的比较和交换是跳跃进行的，所以快速排序也是不稳定排序。 根据排序过程中借助的主要操作，将内排序分为：插入排序、交换排序、选择排序和归并排序。 事实上，目前还没有十全十美的排序算法，都是各有优点和缺点，即使是快速排序算法，也只是整体上性能优越，它也存在排序不稳定、需要大量辅助空间、对少量数据排序无优势等不足。 从算法的简单性来看，可以分为两类： 简单算法：冒泡、简单选择、直接插入。改进算法：快速、堆、希尔、归并。从平均情况看，快速、堆、归并三种改进算法都优于希尔排序，并远远胜过3种简单算法。 从最好情况看，冒泡和直接插入排序要更好一点，即当待排序序列是基本有序的时候，应该考虑这两种排序算法，而非4种复杂的改进算法。 从最坏情况看，堆和归并排序比其他排序算法都要更好。 从空间复杂度看，归并排序和快速排序都对空间有要求，而其他排序反而都只是O(1)O(1)的复杂度。 从稳定性上看，归并排序是改进算法中唯一稳定的算法。而不稳定的排序算法有“快些选堆”，即快速、希尔、选择和堆排序四种算法（书中给出的简单选择排序是不稳定的，但是从网上查找资料看到选择排序是一个不稳定的算法）。 排序算法的总结就到这里，实际上还是要根据实际问题来选择适合的排序算法。]]></content>
      <categories>
        <category>Computer Science</category>
        <category>Data Structure</category>
      </categories>
      <tags>
        <tag>Data Structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构第四章：串]]></title>
    <url>%2F2017%2F12%2F12%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E7%AC%AC%E5%9B%9B%E7%AB%A0%EF%BC%9A%E4%B8%B2%2F</url>
    <content type="text"><![CDATA[Abstract：串。 4.1 串及其基本运算4.1.1 串的基本概念1.串的定义 串：由0个或N个任意字符组成的字符序列，记作：s=”s1,s2,..,sn＂＂ 数据元素类型为字符型的线性表 特点：字符的特殊性，字符串经常作为一个整体来处理 s：串名 双引号:串的定界符，引号引起来的字符序列为串值，引号本身不属于串的内容 ai（1&lt;=i&lt;=n）：任意字符，是串的元素，构成串的基本单位；i是它在整个串中的序号，n为串的长度（表示包含的字符个数） n=0时，为空串，记为Φ 子串与主串：串中任意连续的字符组成的子序列为子串；包含子串的串为主串 子串的位置：子串的第一个字符在主串中的序号称为子串的位置。 串相等：称两个串是相等的，是指两个串的长度相等且对应字符都相等。 4.2 串的基本运算 1.求串长：StrLength(s) 操作条件：串s存在 结果：求出串s的长度 2.串赋值StrAssign(s1,s2) 操作条件： s1 是一个串变量，s2 或者是一个串常量，或者是一个串变量（通常s2 是一个串常量时称为串赋值，是一个串变量称为串拷贝）。 操作结果：将s2 的串值赋值给s1， s1 原来的值被覆盖掉。 s1一定是串变量，s2可常可变 3.连接操作：StrConcat(s1, s2, s)或StrConcat(s1, s2) 操作条件：串s1,s2 存在。 操作结果：两个串的联接就是将一个串的串值紧接着放在另一个串的后面，连接成一个串。前者是产生新串s，s1 和s2 不改变; 后者是在s1 的后面联接s2 的串值，s1 改变， s2不改变。 例如： s1=＂he＂，s2=＂ bei＂，前者操作结果是s=＂he bei＂；后者操作结果是s1=＂he bei＂。 区别：前者的结果是s，后者结果是s1 4.求子串SubStr(s, i, len) 操作条件：串s 存在，1≤i≤StrLength(s)，0≤len≤StrLength(s)-i+1。 操作结果：返回从串s 的第i 个字符开始的长度为len 的子串。len=0 得到的是空串。 例如：SubStr(＂abcdefghi＂,3,4)= ＂cdef＂ 5.串比较StrCmp(s1, s2) 操作条件：串s1,s2 存在。操作结果：若s1==s2，操作返回值为0；若s1]]></content>
      <categories>
        <category>Computer Science</category>
        <category>Data Structure</category>
      </categories>
      <tags>
        <tag>Data Structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构第五章：数组]]></title>
    <url>%2F2017%2F12%2F10%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E7%AC%AC%E4%BA%94%E7%AB%A0%EF%BC%9A%E6%95%B0%E7%BB%84%2F</url>
    <content type="text"><![CDATA[Abstract: 数组。 5.1 多维数组数组与广义表：数据元素仍为表。本章讨论多维数组的逻辑结构和存储结构、特殊矩阵、矩阵的压缩存储、广义表的逻辑结构和存储结构等。 5.1.1 数组的逻辑结构数组：可看做是线性表的推广；数组作为一种数据结构其特点是结构中元素本身可具有的某种结构的数据，但属于同一数据类型。如： 一维数组可以看作一个线性表 二维数组可以看作“数据元素是一维数组”的一维数组 三维数组可以看作“数据元素是二维数组”的一维数组，依此类推 m 行n列的二维数组： 数组是一个具有固定格式和数量的数据有序集，每一个数据元素有唯一的一组下标来标识，因此，在数组上不能做插入、删除数据元素的操作。通常在各种高级语言中数组一旦被定义，每一维的大小及上下界都不能改变。在数组中通常做下面两种操作： (1) 取值操作：给定一组下标，读其对应的数据元素。 (2) 赋值操作：给定一组下标，存储或修改与其相对应的数据元素。 5.1.2 数组的内存映像数组的存储表示：数组在内存被映象为向量，即用向量作为数组的一种存储结构，这是因为内存的地址空间是一维的，数组的行列固定后，通过一个映象函数，则可根据数组元素的下标得到它的存储地址。 对于一维数组：按下标顺序分配 对多维数组：要把它的元素映象存储在一维存储器中，一般有两种存储方式： 以行为主序的顺序存放：如BASIC、PASCAL、COBOL、C 等程序设计语言中用的是以行为主的顺序分配，即**一行分配完了接着分配下一行。 以列为主序（先列后行）的顺序存放：如FORTRAN 语言中，用的是以列为主序的分配顺序，即一列一列地分配。 以行为主序的分配规律是：最右边的下标先变化，即最右下标从小到大，循环一遍后，右边第二个下标再变，…，从右向左，最后是左下标。以列为主序分配的规律恰好相反：最左边的下标先变化，即最左下标从小到大，循环一遍后，左边第二个下标再变，…，从左向右，最后是右下标。 如一个2×3 二维数组，逻辑结构可以用图1表示。以行为主序的内存映象如图2.1，分配顺序为：a11 ，a12 ，a13 ，a21 ，a22 ，a23 ; 以列为主序的分配顺序为：a11 ，a21 ，a12 ，a22 ，a13 ，a23 ; 它的内存映象如图2.2所示。 设有m×n 二维数组Amn，下面我们看按元素的下标求其地址的计算：以“以行为主序”的分配为例：设数组的基址为LOC(a11)，每个数组元素占据p个地址单元，那么aij 的物理地址可用一线性寻址函数计算： LOC(aij) = LOC(a11) + ( (i-1)*n + j-1 ) * p 这是因为数组元素aij 的前面有i-1 行，每一行的元素个数为n，在第i 行中它的前面还有j-1 个数组元素。 在C 语言中，数组中每一维的下界定义为0，则： LOC(aij) = LOC(a00) + ( i*n + j ) * p 推广到一般的二维数组：A[c1..d1] [c2..d2]，则aij 的物理地址计算函数为： LOC(aij)=LOC(a c1 c2)+( (i- c1) *( d2 - c2 + 1)+ (j- c2) )*p 同理对于三维数组Amnp，即m×n×p 数组，对于数组元素aijk 其物理地址为： LOC(aijk)=LOC(a111)+( ( i-1) *n*p+ (j-1)*p +k-1)*p 推广到一般的三维数组：A[c1..d1] [c2..d2] [c3..d3]，则aijk 的物理地址为： LOC(i,j)=LOC(a c1 c2 c3)+( (i- c1) *( d2 - c2 + 1)* (d3- c3 + 1)+ (j- c2) *( d3- c3 + 1)+(k- c3))*p 三维数组的逻辑结构和以行为主序的分配示意图如图所示。 例：若矩阵Am×n 中存在某个元素aij 满足：aij 是第i 行中最小值且是第j 列中的最大值，则称该元素为矩阵A 的一个鞍点。试编写一个算法，找出A 中的所有鞍点。 基本思想：在矩阵A中求出每一行的最小值元素，然后判断该元素是否是它所在列的最大值，是则打印出，接着处理下一行。矩阵A用一个二维数组表示。 12345678910111213141516171819202122void saddle( int A[][], int m, int n )/*m,n 是矩阵A 的行和列*/&#123; int i, j, min; for ( i = 0; i &lt; m; i++ ) /*按行处理*/ &#123; min = A[i][0] for ( j = 1; j &lt; n; j++ ) if ( A[i][j] &lt; min ) min = A[I][j]; /*找第I 行最小值*/ for ( j = 0; j &lt; n; j++ ) /*检测该行中的每一个最小值是否是鞍点*/ if ( A[I][j] == min ) &#123; k = j; p = 0; while ( p &lt; m &amp;&amp; A[p][j] &lt; min ) p++; if ( p &gt;= m ) printf( ＂ % d, % d, % d \ n ＂, i, k, min ); &#125; /* if */ &#125; /*for i*/&#125; 时间性能为O(m(n+mn))。 5.2 特殊矩阵的压缩存储—对称矩阵矩阵结构：二维数组表示很恰当 对称矩阵： aij = aji，其中1≤i , j≤n； 关于对角线对称，故只需存储上三角或下三角即可 【压缩存储】节省存储资源：原来需要n*n 个存储单元，现在只需要n(n+1)/2 个存储单元了，节约了n(n-1)/2个存储单元 如何只存储下三角？ 对下三角部分以行为主序顺序存储到一个向量中去，在下三角中共有n*(n+1)/2 个元素，因此，不失一般性，设存储到向量SA[n(n+1)/2]中，存储顺序可用下图示意，这样，原矩阵下三角中的某一个元素aij 则具体对应一个sak，下面的问题是要找到k 与i、j 之间的关系。 角中的元素aij，其特点是：i≥j 且1≤i≤n，存储到SA 中后，根据存储原则，它前面有i-1行，共有1+2+…+i-1=i(i-1)/2 个元素，而aij 又是它所在的行中的第j 个，所以在上面的排列顺序中，aij 是第i(i-1)/2+j 个元素，因此它在SA 中的下标k 与i、j 的关系为： k=i*(i-1)/2+j-1 (０≤k&lt;n*(n+1)/2 ) 若i&lt;j，则aij 是上三角中的元素，因为aij=aji ，这样，访问上三角中的元素aij 时则去访问和它对应的下三角中的aji 即可，因此将上式中的行列下标交换就是上三角中的元素在SA 中的对应关系： k=j*(j-1)/2+i-1 (０≤k&lt;n*(n+1)/2 ) 综上所述，对于对称矩阵中的任意元素aij，若令I=max(i,j)，J=min(i,j)，则将上面两个式子综合起来得到： k=i*(i-1)/2+J-1 5.2 特殊矩阵的压缩存储—三角矩阵1. 下三角矩阵下三角矩阵：一共存储n*(n+1)+1 个元素，设存入向量：SA[n(n+1)+1]中，这种的存储方式可节约n(n-1)-1 个存储单元，sak 与aji 的对应关系为： 2.上三角矩阵类似下三角，以行为主序顺序存储上三角部分，最后存储对角线下方的常量。对于第1 行，存储n 个元素，第2 行存储n-1 个元素，…，第p 行存储(n-p+1)个元素，aij 的前面有i-1 行，共存储： 个元素，而aij 是它所在的行中要存储的第（j-i+1）个；所以，它是上三角存储顺序中的第(i-1)(2n-i+2)/2+(j-i+1)个，因此它在SA中的下标为：k=(i-1)(2n-i+2)/2+j-i。综上， sak 与aji 的对应关系为： 5.2 特殊矩阵的压缩存储—带状矩阵n 阶矩阵A 称为带状矩阵，如果存在最小正数m ，满足当∣i-j∣≥m 时，aij =0，这时称w=2n-1 为矩阵A 的带宽。如图是一个w=3(m=2)的带状矩阵。带状矩阵也称为对角矩阵。由图可看出，在这种矩阵中，所有非零元素都集中在以主对角线为中心的带状区域中，即除了主对角线和它的上下方若干条对角线的元素外，所有其他元素都为零(或同一个常数c)。 带状矩阵A也可采用压缩存储： 压缩法1：将A压缩到一个n行w列的二维数组B中，如图，当某行非零元素的个数小于带宽w时，先存放非零元素后补零。则aij映射为b i’j’，映射关系为： 压缩法2：将带状矩阵压缩到向量C中，按以行为主序，顺序存储其非零元素。按其压缩规律，找到相应的映像函数。如当w = 3时，映像函数为：k = 2*i + j - 3； 5.3 稀疏矩阵—稀疏矩阵的三元组表存储稀疏矩阵：有t个非零元素（t &lt;&lt; mn）的mn矩阵。 解决的问题：很多科学管理及工程计算中，常会遇到阶数很高的大型稀疏矩阵。如果按常规分配方法，顺序分配在计算机内，相当浪费内存。 方法：用三元组仅存放非零元素，然后再按某种规律存储这些三元组，目的是节省存储空间。 三元组（i, j, v）:记下非零元素所在的行、列和值。 12345678910define SMAX 1024 /*一个足够大的数*/typedef struct&#123; int i, j; /*非零元素的行、列*/ datatype v; /*非零元素值*/&#125;SPNode; /*三元组类型*/&#123; int mu,nu,tu; /*矩阵的行、列及非零元素的个数*/ SPNode data[SMAX]; /*三元组表*/&#125;SPMatrix; /*三元组表的存储类型*/ 下面讨论这种存储方式下的稀疏矩阵的两种运算：转置和相乘。 1.稀疏矩阵的转置算法思路：①A 的行、列转化成B 的列、行；②在A.data 中依次找第一列的、第二列的、直到最后一列，并将找到的每个三元组的行、列交换后顺序存储到B.data 中即可。 12345678910111213141516171819202122void TransM1( SPMatrix *A )&#123; SPMatrix *B; int p, q, col; B = malloc( sizeof(SPMatrix) ); /*申请存储空间*/ B-&gt;mu = A-&gt;nu; B-&gt;nu = A-&gt;mu; B-&gt;tu = A-&gt;tu;/*稀疏矩阵的行、列、元素个数*/ if ( B-&gt;tu &gt; 0 ) /*有非零元素则转换*/ &#123; q = 0; for ( col = 1; col &lt;= (A-&gt;nu); col++ ) /*按A 的列序转换*/ for ( p = 1; p &lt;= (A-&gt;tu); p++ ) /*扫描整个三元组表*/ if ( A-&gt;data[p].j == col ) &#123; B-&gt;data[q].i = A-&gt;data[p].j; B-&gt;data[q].j = A-&gt;data[p].i; B-&gt;data[q].v = A-&gt;data[p].v; q++; &#125; /*if*/ &#125; /*if(B-&gt;tu&gt;0)*/ return(B); /*返回的是转置矩阵的指针*/&#125; /*TransM1*/ 分析该算法 其时间主要耗费在col 和p 的二重循环上，所以时间复杂性为O(n*t)，(设m、n 是原矩阵的行、列，t 是稀疏矩阵的非零元素个数) 显然当非零元素的个数t 和mn 同数量级时，算法的时间复杂度为O(mn2) 通常存储方式下矩阵转置算法相比，可能节约了一定量的存储空间，但算法的时间性能更差一些。 时间性能差的原因：反复搜索A表。若能直接确定A 中每一三元组在B 中的位置，则对A 的三元组表扫描一次即可。这是可以做到的，因为A 中第一列的第一个非零元素一定存储在B.data[1]，如果还知道第一列的非零元素的个数，那么第二列的第一个非零元素在B.data 中的位置便等于第一列的第一个非零元素在B.data 中的位置加上第一列的非零元素的个数，如此类推，因为A 中三元组的存放顺序是先行后列，对同一行来说，必定先遇到列号小的元素，这样只需扫描一遍A.data 即可。 根据这个想法，需引入两个向量来实现：num[n+1]和cpot[n+1]，num[col]表示矩阵A中第col 列的非零元素的个数（为了方便均从1 单元用起），cpot［col］初始值表示矩阵A中的第col 列的第一个非零元素在B.data 中的位置。于是cpot 的初始值为： cpot[1]=1； cpot[col]=cpot[col-1]+num[col-1]； 2≤col≤n 如矩阵A的num和cpot的值如下： 依次扫描A.data，当扫描到一个col 列元素时，直接将其存放在B.data 的cpot[col]位置上，cpot[col]加１，cpot[col]中始终是下一个col 列元素在B.data 中的位置。下面按以上思路改进转置算法如下： 1234567891011121314151617181920212223242526272829303132SPMatrix * TransM2( SPMatrix *A )&#123; SPMatrix *B; int i, j, k; int num[n + 1], cpot[n + 1]; B = malloc( sizeof(SPMatrix) ); /*申请存储空间*/ B-&gt;mu = A-&gt;nu; B-&gt;nu = A-&gt;mu; B-&gt;tu = A-&gt;tu;/*稀疏矩阵的行、列、元素个数*/ if ( B-&gt;tu &gt; 0 ) /*有非零元素则转换*/ &#123; for ( i = 1; i &lt;= A-&gt;nu; i++ ) num[i] = 0; for ( i = 1; i &lt;= A-&gt;tu; i++ ) /*求矩阵A 中每一列非零元素的个数*/ &#123; j = A-&gt;data[i].j; num[j]++; &#125; cpot[1] = 1; /*求矩阵A 中每一列第一个非零元素在B.data 中的位置*/ for ( i = 2; i &lt;= A-&gt;nu; i++ ) cpot[i] = cpot[i - 1] + num[i - 1]; for ( i = 1; i &lt;= (A-&gt;tu); i++ ) /*扫描三元组表*/ &#123; j = A-&gt;data[i].j; /*当前三元组的列号*/ k = cpot[j]; /*当前三元组在B.data 中的位置*/ B-&gt;data[k].i = A-&gt;data[i].j; B-&gt;data[k].j = A-&gt;data[i].i; B-&gt;data[k].v = A-&gt;data[i].v; cpot[j]++; &#125; /*for i */ &#125; /*if (B-&gt;tu&gt;0)*/ return(B); /*返回的是转置矩阵的指针*/&#125; /*TransM2*/ 分析这个算法的时间复杂度：这个算法中有四个循环，分别执行n，t，n-1，t 次，在每个循环中，每次迭代的时间是一常量，因此总的计算量是O（n+t）。当然它所需要的存储空间比前一个算法多了两个向量。 2.稀疏矩阵的乘积已知稀疏矩阵A(m1× n1)和B(m2× n2)，求乘积C(m1× n2)。稀疏矩阵A、B、C 及它们对应的三元组表A.data、B.data、C.data 如图所示。 a11 只有可能和B 中第1 行的非零元素相乘，a12 只有可能和B 中第2 行的非零元素相乘，…，而同一行的非零元是相邻存放的，所以求c11 和c12 同时进行：求a11b11 累加到c11，求a11b12 累加到c12，再求a12b21 累加到c11，再求a12b22 累加到c22.，…，当然只有aik 和bkj(列号与行号相等)且均不为零（三元组存在）时才相乘，并且累加到cij 当中去。 为了运算方便，设一个累加器：datatype temp[n+1]；用来存放当前行中cij 的值，当前行中所有元素全部算出之后，再存放到C.data 中去。 为了便于B.data 中寻找B 中的第k 行第一个非零元素，与前面类似，在此需引入num和rpot 两个向量。num[k]表示矩阵B 中第k 行的非零元素的个数；rpot[k]表示第k 行的第一个非零元素在B.data 中的位置。于是有 rpot[1]=1 rpot[k]=rpot[k-1]+num[k-1] 2≤k≤n 矩阵B的num和rpot值： 稀疏矩阵的乘法运算的粗略步骤如下： 1.初始化：清理一些单元，准备按行顺序存放乘积矩阵； 2.求B的num，rpot； 3.做矩阵乘法：将A.data 中三元组的列值与B.data 中三元组的行值相等的非零元素相乘，并将具有相同下标的乘积元素相加。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253SPMatrix *MulSMatrix( SPMatrix *A, SPMatrix *B )/*稀疏矩阵A(m1× n1)和B(m2× n2) 用三元组表存储，求A×B */&#123; SPMatrix *C; /* 乘积矩阵的指针*/ int p, q, i, j, k, r; datatype temp[n + 1]; int num[B-&gt;nu + 1], rpot[B-&gt;nu + 1]; if ( A-&gt;nu != B-&gt;mu ) return(NULL); /*A 的列与B 的行不相等*/ C = malloc( sizeof(SPMatrix) ); /*申请C 矩阵的存储空间*/ C-&gt;mu = A-&gt;mu; C-&gt;nu = B-&gt;nu; if ( A-&gt;tu * B-&gt;tu == 0 ) &#123; C-&gt;tu = 0; return(C); &#125; for ( i = 1; i &lt;= B-&gt;mu; i++ ) num[i] = 0; /*求矩阵B 中每一行非零元素的个数*/ for ( k = 1; k &lt;= B-&gt;tu; k++ ) &#123; i = B-&gt;data[k].i; num[i]++; &#125; rpot[1] = 1; /*求矩阵B 中每一行第一个非零元素在B.data 中的位置*/ for ( i = 2; i &lt;= B-&gt;mu; i++ ) rpot[i] = rpot[i - 1] + num[i - 1]; r = 0; /*当前C 中非零元素的个数*/ p = 1; /*指示A.data 中当前非零元素的位置*/ for ( i = 1; i &lt;= A-&gt;mu; i++ ) &#123; for ( j = 1; j &lt;= B-&gt;nu; j++ ) temp[j] = 0; /*cij 的累加器初始化*/ while ( A-&gt;data[p].i == i ) . /*求第i 行的*/ &#123; k = A-&gt;data[p].j; /*A 中当前非零元的列号*/ if ( k &lt; B-&gt;mu ) t = rpot[k + 1]; else t = B-&gt;tu + 1; /*确定B 中第k 行的非零元素在B.data 中的下限位置*/ for ( q = rpot[k]; q &lt; t; q++; ) /* B 中第k 行的每一个非零元素*/ &#123; j = B-&gt;data[q].j; temp[j] += A-&gt;data[p].v * B-&gt;data[q].v &#125; p++; &#125; /* while */ for ( j = 1; j &lt;= B-&gt;nu; j++ ) if ( temp[j] ) &#123; r++;; C-&gt;data[r] = &#123; i, j, temp[j] &#125;; &#125; &#125; /*for i*/ C-&gt;tu = r; return(C);&#125; /* MulSMatrix */ 分析上述算法的时间性能如下： （1）求num 的时间复杂度为O(B-&gt;nu+B-&gt;tu)； （2）求rpot 时间复杂度为O(B-&gt;mu)； （3）求temp 时间复杂度为O(A-&gt;mu*B-&gt;nu)； （4）求C的所有非零元素的时间复杂度为O(A-&gt;tu*B-&gt;tu/B-&gt;mu)； （5）压缩存储时间复杂度为O(A-&gt;muB-&gt;nu)；所以总的时间复杂度为O(A-&gt;muB-&gt;nu+(A-&gt;tu*B-&gt;tu)/B-&gt;nu)。 5.3 稀疏矩阵—稀疏矩阵的十字链表存储三元组：稀疏矩阵顺序存储 不便性：在做一些操作（如加法、乘法）时，非零项数目及非零元素的位置会发生变化，这时这种表示十分不便 十字链表：稀疏矩阵的一种链式存储结构，它同样具备链式存储的特点，因此，在某些情况下，采用十字链表表示稀疏矩阵很方便。 一个稀疏矩阵的十字链表： 基本思想：对每个非零元素存储为一个结点，结点由5个域组成，其结构如图。其中：row域存储非零元素的行号，col域存储列号，v域存储本元素的值，right、down是2个指针域。 稀疏矩阵中每一行的非零元素结点按其列号从小到大顺序由right 域链成一个带表头结点的循环行链表，同样每一列中的非零元素按其行号从小到大顺序由down 域也链成一个带表头结点的循环列链表。即每个非零元素aij 既是第i 行循环链表中的一个结点，又是第j 列循环链表中的一个结点。行链表、列链表的头结点的row 域和col 域置0。每一列链表的表头结点的down 域指向该列链表的第一个元素结点，每一行链表的表头结点的right域指向该行表的第一个元素结点。由于各行、列链表头结点的row 域、col 域和v 域均为零，行链表头结点只用right 指针域，列链表头结点只用right 指针域，故这两组表头结点可以合用，也就是说对于第i 行的链表和第i 列的链表可以共用同一个头结点。为了方便地找到每一行或每一列，将每行（列）的这些头结点们链接起来，因为头结点的值域空闲，所以用头结点的值域作为连接各头结点的链域，即第i 行（列）的头结点的值域指向第i+1行（列）的头结点，… ，形成一个循环表。这个循环表又有一个头结点，这就是最后的总头结点，指针HA 指向它。总头结点的row 和col 域存储原矩阵的行数和列数。 因为非零元素结点的值域是datatype 类型，在表头结点中需要一个指针类型，为了使整个结构的结点一致，我们规定表头结点和其它结点有同样的结构，因此该域用一个联合来表示；改进后的结点结构如图。 综上，结点的结构定义如下:12345678typedef struct node&#123; int row, col;struct node *down , *right;union v_next&#123; datatype v;struct node *next;&#125;&#125; MNode，*MLink; 基于这种存储结构的稀疏矩阵的运算: 1.建立稀疏矩阵A 的十字链表: 首先输入的信息是：m（A 的行数），n（A 的列数），r（非零项的数目），紧跟着输入的是r 个形如（i,j,aij）的三元组。 算法的设计思想是：首先建立每行（每列）只有头结点的空链表，并建立起这些头结点拉成的循环链表；然后每输入一个三元组（i，j，aij），则将其结点按其列号的大小插入到第i 个行链表中去，同时也按其行号的大小将该结点插入到第j 个列链表中去。在算法中将利用一个辅助数组MNode *hd[s+1]; 其中s=max(m , n) , hd [i]指向第i 行(第i 列)链表的头结点。这样做可以在建立链表时随机的访问任何一行（列），为建表带来方便。 算法如下：12345678910111213141516171819202122232425262728293031323334353637MLink CreatMLink( ) /* 返回十字链表的头指针*/｛MLink H;Mnode *p,*q,*hd[s+1];int i,j,m,n,t;datatype v;scanf(“%d,%,%d”,&amp;m,&amp;n,&amp;t);H=malloc(sizeof(MNode)); /*申请总头结点*/H-&gt;row=m; H-&gt;col=n;hd[0]=H;for(i=1; i&lt;S; i++)&#123; p=malloc(sizeof(MNode)); /*申请第i 个头结点*/p-&gt;row=0; p-&gt;col=0;p-&gt;right=p; p-&gt;down=p;hd[i]=p;hd[i-1]-&gt;v_next.next=p;&#125;hd[S]-&gt;v_next.next=H; /*将头结点们形成循环链表*/for (k=1;k&lt;=t;k++)&#123; scanf (“%d,%d,%d”,&amp;i,&amp;j,&amp;v); /*输入一个三元组，设值为int*/p=malloc(sizeof(MNode));p-&gt;row=i ; p-&gt;col=j; p-&gt;v_next.v=v/*以下是将*p 插入到第i 行链表中去，且按列号有序*/q=hd[i];while ( q-&gt;right!=hd[i] &amp;&amp; (q-&gt;right-&gt;col)&lt;j ) /*按列号找位置*/q=q-&gt;right;p-&gt;right=q-&gt;right; /*插入*/q-&gt;right=p;/*以下是将*p 插入到第j 行链表中去，且按行号有序*/q=hd[i];while ( q-&gt;down!=hd[j] &amp;&amp; (q-&gt;down-&gt;row)&lt;i ) /*按行号找位置*/q=q-&gt;down;p-&gt; down =q-&gt; down; /*插入*/q-&gt; down =p;&#125; /*for k*/return H;&#125; /* CreatMLink */ 上述算法中，建立头结点循环链表时间复杂度为O(S)，插入每个结点到相应的行表和列表的时间复杂度是O(tS)，这是因为每个结点插入时都要在链表中寻找插入位置，所以**总的时间复杂度为O(tS)。该算法对三元组的输入顺序没有要求。如果我们输入三元组时是按以行为主序（或列）输入的，则每次将新结点插入到链表的尾部的，改进算法后，时间复杂度为O(S+t)**。 2.两个十字链表表示的稀疏矩阵的加法 已知两个稀疏矩阵A 和B，分别采用十字链表存储，计算C=A+B，C 也采用十字链表方式存储，并且在A 的基础上形成C。 由矩阵的加法规则知，只有A 和B 行列对应相等，二者才能相加。C 中的非零元素cij 只可能有３种情况：或者是aij+bij，或者是aij (bij=0)，或者是bij (aij=0)，因此当B 加到A 上时，对A 十字链表的当前结点来说，对应下列四种情况：或者改变结点的值（aij+bij≠０），或者不变（bij＝０），或者插入一个新结点（aij＝０），还可能是删除一个结点（aij+bij＝０）。整个运算从矩阵的第一行起逐行进行。对每一行都从行表的头结点出发，分别找到A 和B 在该行中的第一个非零元素结点后开始比较，然后按４种不同情况分别处理。设pa和pb 分别指向A 和B 的十字链表中行号相同的两个结点，４种情况如下： (1) 若pa-&gt;col=pb-&gt;col 且pa-&gt;v+pb-&gt;v≠0，则只要用aij+bij 的值改写pa 所指结点的值域即可。 (2) 若pa-&gt;col=pb-&gt;col 且pa-&gt;v+pb-&gt;v=0，则需要在矩阵A 的十字链表中删除pa 所指结点，此时需改变该行链表中前趋结点的right 域，以及该列链表中前趋结点的down 域。 (3) 若pa-&gt;col &lt; pb-&gt;col 且pa-&gt;col≠0（即不是表头结点），则只需要将pa 指针向右推进一步，并继续进行比较。 (4) 若pa-&gt;col &gt; pb-&gt;col 或pa-&gt;col＝0（即是表头结点），则需要在矩阵A 的十字链表中插入一个pb 所指结点。 由前面建立十字链表算法知，总表头结点的行列域存放的是矩阵的行和列，而各行（列）链表的头结点其行列域值为零，当然各非零元素结点的行列域其值不会为零，下面分析的4 种情况利用了这些信息来判断是否为表头结点。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556//十字链表表示的稀疏矩阵相加MLink AddMat (Ha,Hb)MLink Ha,Hb;&#123; Mnode *p,*q,*pa,*pb,*ca,*cb,*《中国知识付费行业发展白皮书2016》;if (Ha-&gt;row!=Hb-&gt;row || Ha-&gt;col!=Hb-&gt;col) return NULL;ca=Ha-&gt;v_next.next; /*ca 初始指向A 矩阵中第一行表头结点*/cb=Hb-&gt;v_next.next; /*cb 初始指向B 矩阵中第一行表头结点*/do &#123; pa=ca-&gt;right; /*pa 指向A 矩阵当前行中第一个结点*/《中国知识付费行业发展白皮书2016》=ca; /*《中国知识付费行业发展白皮书2016》 是pa 的前驱*/pb=cb-&gt;right; /*pb 指向B 矩阵当前行中第一个结点*/while (pb-&gt;col!=0) /*当前行没有处理完*/&#123;if (pa-&gt;col &lt; pb-&gt;col &amp;&amp; pa-&gt;col !=0 ) /*第三种情况*/&#123; 《中国知识付费行业发展白皮书2016》=pa;pa=pa-&gt;right;&#125;elseif (pa-&gt;col &gt; pb-&gt;col || pa-&gt;col ==0 ) /*第四种情况*/&#123;p=malloc(sizeof(MNode));p-&gt;row=pb-&gt;row; p-&gt;col=pb-&gt;col; p-&gt;v=pb-&gt;v;p-&gt;right=pa;《中国知识付费行业发展白皮书2016》-&gt;right=p; /* 新结点插入*pa 的前面*/pa=p;/*新结点还要插到列链表的合适位置，先找位置，再插入*/q=Find_JH(Ha,p-&gt;col); /*从列链表的头结点找起*/while(q-&gt;down-&gt;row!=0 &amp;&amp; q-&gt;down-&gt;row&lt;p-&gt;row)q=q-&gt;down;p-&gt;down=q-&gt;down; /*插在*q 的后面*/q-&gt;down=p;pb=pb-&gt;right;&#125; /* if */else /*第一、二种情况*/&#123;x= pa-&gt;v_next.v+ pb-&gt;v_next.v;if (x==0) /*第二种情况*/&#123; 《中国知识付费行业发展白皮书2016》-&gt;right=pa-&gt;right; ./*从行链中删除*//*还要从列链中删除，找*pa 的列前驱结点*/q= Find_JH (Ha,pa-&gt;col); /*从列链表的头结点找起*/while ( q-&gt;down-&gt;row &lt; pa-&gt;row )q=q-&gt;down;q-&gt;down=pa-&gt;down;free (pa);pa=《中国知识付费行业发展白皮书2016》;&#125; /*if (x==0)*/else /*第一种情况*/&#123; pa-&gt;v_next.v=x;《中国知识付费行业发展白皮书2016》=pa;&#125;pa=pa-&gt;right;pb=pb-&gt;right;&#125;&#125; /*while*/ca=ca-&gt;v_next.next; /*ca 指向A 中下一行的表头结点*/cb=cb-&gt;v_next.next; /*cb 指向B 中下一行的表头结点*/&#125; while (ca-&gt;row==0) /*当还有未处理完的行则继续*/return Ha;&#125; 为了保持算法的层次，在上面的算法，用到了一个函数findjH。函数Mlink Find_JH(MLink H, int j)的功能是：返回十字链表H 中第j 列链表的头结点指针，很简单。 5.4 广义表—广义表的定义和基本运算广义表：线性表的推广。也称列表（Lists，用复数形式以示与统称的表List 的区别） 1.广义表的定义和性质线性表是由n 个数据元素组成的有限序列。其中每个组成元素被限定为单元素，有时这种限制需要拓宽。例如，中国举办的某体育项目国际邀请赛，参赛队清单可采用如下的表示形式：（俄罗斯，巴西，（国家，河北，四川），古巴，美国，（），日本） 在这个拓宽了的线性表中，韩国队应排在美国队的后面，但由于某种原因未参加，成为空表。国家队、河北队、四川队均作为东道主的参赛队参加，构成一个小的线性表，成为原线性表的一个数据项。这种拓宽了的线性表就是广义表。 广义表（Generalized Lists）是n（n≥0）个数据元素a1，a2，…，ai，…，an 的有序序列，一般记作： ls＝（a1，a2，…，ai，…，an） 其中：ls 是广义表的名称，n 是它的长度。每个ai（1≤i≤n）是ls 的成员，它可以是单个元素，也可以是一个广义表，分别称为广义表ls 的单元素和子表。当广义表ls 非空时，称第一个元素a1 为ls 的表头（head），称其余元素组成的表（a2，…，ai，…，an）为ls 的表尾（tail）。显然，广义表的定义是递归的。 通常用大写字母表示广义表，用小写字母表示单个数据元素，广义表用括号括起来，括号内的数据元素用逗号分隔开。下面是一些广义表的例子： A ＝（）B ＝（e）C ＝（a，（b，c，d））D ＝（A，B，C）E ＝（a，E）F ＝（（）） 2.广义表的性质⑴广义表是一种多层次的数据结构。广义表的元素可以是单元素，也可以是子表，而子表的元素还可以是子表，…。 ⑵广义表可以是递归的表。广义表的定义并没有限制元素的递归，即广义表也可以是其自身的子表。例如表E 就是一个递归的表。 ⑶广义表可以为其他表所共享。例如，表A、表B、表C 是表D 的共享子表。在D中可以不必列出子表的值，而用子表的名称来引用。 广义表可以看成是线性表的推广，线性表是广义表的特例/子集。 广义表的结构相当灵活，在某种前提下，它可以兼容线性表、数组、树和有向图等各种常用的数据结构。 当二维数组的每行（或每列）作为子表处理时，二维数组即为一个广义表。另外，树和有向图也可以用广义表来表示。由于广义表不仅集中了线性表、数组、树和有向图等常见数据结构的特点，而且可有效地利用存储空间，因此在计算机的许多应用领域都有成功使用广义表的实例。 3.广义表基本运算重要基本操作： 取头Head 取尾Tail 对于任意一个非空的列表，其表头可能是单元素也可能是列表，而表尾必为列表。 Head(B) = e Tail(B) = ()Head（C）＝ a Tail（C）＝（（b，c，d））Head（D）＝ A Tail（D）＝（B，C）Head（E）＝ a Tail（E）＝（E）Head（F）＝（） Tail（F）＝（） 此外，在广义表上可以定义与线性表类似的一些操作，如建立、插入、删除、拆开、连接、复制、遍历等。 CreateLists(ls)：根据广义表的书写形式创建一个广义表ls。IsEmpty(ls)：若广义表ls 空，则返回True；否则返回False。Length(ls)：求广义表ls 的长度。Depth(ls)：求广义表ls 的深度。Locate(ls，x)：在广义表ls 中查找数据元素x。Merge(ls1，ls2)：以ls1 为头、ls2 为尾建立广义表。CopyGList(ls1，ls2)：复制广义表，即按ls1 建立广义表ls2。Head(ls)：返回广义表ls 的头部。Tail(ls)：返回广义表的尾部。 5.4 广义表—广义表的存储 广义表的数据远古三结构不同，故很难用顺序存储结构表示 链式存储结构分配较灵活，易于解决广义表的共享与递归问题，故通常采用链式存储结构来存储广义表，其中每个数据元素可用一个结点表示 按结点形式的不同，广义表的链式存储结构又可以分为不同的两种存储方式。一种称为头尾表示法，另一种称为孩子兄弟表示法。 1.头尾表示法 若广义表不空，则可分解为表头 + 表尾 一对确定的表头和表尾可唯一确定一个广义表 由于广义表中的数据元素既可能是列表也可能是单元素，相应地在头尾表示法中结点的结构形式有两种： 一种是表结点，用以表示列表； 另一种是元素结点，用以表示单元素。 在表结点中应该包括一个指向表头的指针和指向表尾的指针；而在元素结点中应该包括所表示单元素的元素值。为了区分这两类结点，在结点中还要设置一个标志域，如果标志为1，则表示该结点为表结点；如果标志为0，则表示该结点为元素结点。其形式定义说明如下： 1234567891011typedef enum &#123;ATOM, LIST&#125; Elemtag; /*ATOM=0：单元素；LIST=1：子表*/typedef struct GLNode &#123; Elemtag tag; /*标志域，用于区分元素结点和表结点*/ union &#123; /*元素结点和表结点的联合部分*/ datatype data; /*data 是元素结点的值域*/struct &#123; struct GLNode *hp, *tp&#125;ptr; /*ptr 是表结点的指针域，ptr.hp 和ptr.tp 分别*//*指向表头和表尾*/ &#125;;&#125;*GList; /*广义表类型*/ 头尾表示法的结点形式: 对于前面所列举的广义表A、B、C、D、E、F，若采用头尾表示法的存储方式，其存储结构如图所示。 从上述存储结构示例中可以看出，采用头尾表示法容易分清列表中单元素或子表所在的层次。例如，在广义表D 中，单元素a 和e 在同一层次上，而单元素b、c、d 在同一层次上且比a 和e 低一层，子表B 和C 在同一层次上。另外，最高层的表结点的个数即为广义表的长度。例如，在广义表D 的最高层有三个表结点，其广义表的长度为3。 2.孩子兄弟表示法在孩子兄弟表示法中，也有两种结点形式： 一种是有孩子结点，用以表示列表； 另一种是无孩子结点，用以表示单元素。 在有孩子结点中包括一个指向第一个孩子（长子）的指针和一个指向兄弟的指针；而在无孩子结点中包括一个指向兄弟的指针和该元素的元素值。为了能区分这两类结点，在结点中还要设置一个标志域。如果标志为1，则表示该结点为有孩子结点；如果标志为0，则表示该结点为无孩子结点。其形式定义说明如下： 123456789typedef enum &#123;ATOM, LIST&#125; Elemtag; /*ATOM=0：单元素；LIST=1：子表*/typedef struct GLENode &#123;Elemtag tag; /*标志域，用于区分元素结点和表结点*/union &#123; /*元素结点和表结点的联合部分*/datatype data; /*元素结点的值域*/struct GLENode *hp; /*表结点的表头指针*/&#125;;struct GLENode *tp; /*指向下一个结点*/&#125;*EGList; /*广义表类型*/ 孩子兄弟表示法的结点形式： 对于前面所列举的广义表A、B、C、D、E、F，若采用孩子兄弟表示法的存储方式，其存储结构如图所示。 采用孩子兄弟表示法时，表达式中的左括号“（”对应存储表示中的tag=1 的结点，且最高层结点的tp 域必为NULL。 5.4 广义表—广义表基本操作的实现以头尾表示法存储广义表，讨论广义表的有关操作的实现。由于广义表的定义是递归的，因此相应的算法一般也都是递归的。 ⒈广义表的取头、取尾123456789101112131415GList Head(GList ls)&#123;if ls-&gt;tag = = 1then p = ls-&gt;hp;return p;&#125;算法5.6GList Tail(GList ls)&#123;if ls-&gt;tag = = 1then p = ls-&gt;tp;return p;&#125;算法5.7 ⒉建立广义表的存储结构1234567891011121314151617181920212223242526272829int Create(GList *ls, char * S)&#123; Glist p; char *sub;if StrEmpty(S) *ls = NULL;else &#123;if (!(*ls = (GList)malloc(sizeof(GLNode)))) return 0;if (StrLength(S) = = 1) &#123;(*ls)-&gt;tag = 0;(*ls)-&gt;data = S;&#125;else &#123;(*ls)-&gt;tag = 1;p = *ls;hsub =SubStr(S,2,StrLength(S)-2);do &#123;sever(sub,hsub);Create(&amp;(p-&gt;ptr.hp), sub);q = p;if (!StrEmpty(sub))&#123;if (!(p = (GList)malloc(sizeof(GLNode)))) return 0;;p-&gt;tag = 1;q-&gt;ptr.tp = p;&#125;&#125;while (!StrEmpty(sub));q-&gt;ptr.tp = NULL;&#125;&#125;return 1;&#125;算法5.8 123456789101112131415161718192021int sever(char *str, char *hstr)&#123;int n = StrLength(str);i= 1; k = 0;for (i = 1, k = 0; i &lt;= n || k != 0; ++i)&#123;ch=SubStr(str,i,1);if (ch = = &apos;(&apos;) ++k;else if (ch = = &apos;)&apos;) --k;&#125;if (i &lt;= n)&#123;hstr =SubStr(str,1,i-2);str= SubStr(str,i,n-i+1);&#125;else &#123;StrCopy(hstr,str);ClearStr(str);&#125;&#125;算法5.9 ⒊以表头、表尾建立广义表123456789int Merge(GList ls1,GList ls2, Glist *ls)&#123;if (!(*ls = (GList)malloc(sizeof(GLNode)))) return 0;*ls-&gt;tag = 1;*ls-&gt;hp = ls1;*ls-&gt;tp = ls2;return 1;&#125;算法5.10 ⒋求广义表的深度12345678910111213int Depth(GList ls)&#123;if (!ls)return 1; /*空表深度为1*/if (ls-&gt;tag = = 0)return 0; /*单元素深度为0*/for (max = 0,p = ls; p; p = p-&gt;ptr.tp) &#123;dep = Depth(p-&gt;ptr.hp); /*求以p-&gt;ptr.hp 尾头指针的子表深度*/if (dep &gt; max) max = dep;&#125;return max+1; /*非空表的深度是各元素的深度的最大值加1*/&#125;算法5.11 ⒌复制广义表123456789101112131415int CopyGList(GList ls1, GList *ls2)&#123;if (!ls1) *ls2 = NULL; /*复制空表*/else &#123;if (!(*ls2 = (Glist)malloc(sizeof(Glnode)))) return 0; /*建表结点*/(*ls2)-&gt;tag = ls1-&gt;tag;if (ls1-&gt;tag = = 0) (*ls2)-&gt;data = ls1-&gt;data; /*复制单元素*/else &#123;CopyGList(&amp;((*ls2)-&gt;ptr.hp), ls1-&gt;ptr.hp); /*复制广义表ls1-&gt;ptr.hp 的一个副本*/CopyGList(&amp;((*ls2)-&gt;ptr.tp) , ls1-&gt;ptr.tp); /*复制广义表ls1-&gt;ptr.tp 的一个副本*/&#125;&#125;return 1;&#125;算法5.12]]></content>
      <categories>
        <category>Computer Science</category>
        <category>Data Structure</category>
      </categories>
      <tags>
        <tag>Data Structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构第二章：线性表]]></title>
    <url>%2F2017%2F12%2F09%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E7%BA%BF%E6%80%A7%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[Abstract：线性表。 线性结构1.特点： 存在唯一的首位元素a1 存在唯一的末位元素an 除第一个元素外，每个元素有且仅有一个直接前驱 除最后一个元素外，每个元素有且仅有一个直接后继 线性表2.1 线性表的逻辑结构1.线性表：由n(n&gt;=0)个类型相同的数据元素（a1，a2,…,an）构成的有限序列。记作L = （a1，a2,…,an）。 同一类型的数据元素构成的线性结构 两种存储结构：顺序存储 &amp; 链式存储 基本操作：插入，删除，检索 数据元素之间是一种线性关系，数据元素“一个接一个的排列” 2.表长（表的长度）：线性表中数据元素的数目。3.空表：不含数据元素的线性表。 线性表的特点 有限性，元素个数有限，n &gt; = 0 元素具有逻辑上的顺序性，在序列中各元素排列有先后次序（但不必按大小排列） 元素都是数据元素，每个表元素都是单个元素 元素的数据类型相同，意味着每个表元素占有相同数量的存储空间 元素具有抽象性：我们仅关注元素间的逻辑关系，不考虑元素表示什么内容 线性表、顺序表、链表的本质区别：线性表是一种**逻辑结构，表示元素之间一对一的相邻关系。顺序表和链表是指存储结构**，两者属于不同层面的概念，因此不要将其混淆。 抽象数据类型的线性表 ADT：三元组，包括数据对象，数据关系，基本操作。 ADT List{ 数据对象：D={ai|ai∈ElemSet,i=1,2,,…n,n&gt;=0}数据关系：R1={| ai-1,ai∈D,i=2,,…n}基本操作：1.IniList(&amp;L) //构造空表L。2.DestroyList(&amp;L) //销毁线性表L3.ClearList(&amp;L) //置L为空表4.ListEmpty(L) //判断L是否为空表。若L为空表，则返回true,否则返回false。5.ListLength(L) //求表L的长度6.GetElem(L,i,&amp;e) //取元素ai,由e返回ai7.LocateElem(L,e,compare()) //查找符合条件的元素8.ListInsert(&amp;L,i,e) //元素ai之前插入新元素e9.ListDelete(&amp;L,i,&amp;e) //删除第i个元素10.DeleteElem(&amp;L,x) //删除元素值为x的元素 … … }ADT List 为什么有的在L前加了&amp;而有的没加：当操作对线性表或其中的元素作了修改，则是&amp;L；若无修改，则为L。 定义：一个数学模型以及定义在该模型上的一组操作。 作用：抽象数据类型可以使我们更容易描述现实世界。 关键：使用它的人可以只关心它的逻辑特征，不需要了解它的存储方式。定义它的人同样不必要关心它如何存储。 操作 基本操作 可利用基本操作组成更复杂的操作 EG.将线性表Lb中的且不在线性表La中的数据元素合并到La中。 123456789101112//算法：判断两个线性表的长度——用循环结构将符合条件的元素插入La中（判断Lb中的每个元素是否在La中，不在的话就插入） void union(List &amp;La, List &amp;Lb) //合并线性表 &#123; La_len = ListLength(La); Lb_len = ListLength(Lb); for (i=1;i&lt;=Lb_len;i++) &#123; GetElem(Lb,i,e) //取Lb的第i个数据元素赋给e；即依次取出所有Lb中的元素 if(!LocateElem(La,e,equal)) ListInsert(La,++La_len,e); //判断e在La中是否存在，不存在则插入 &#125;&#125; 2.2 线性表的顺序存储及运算（顺序表）顺序表 顺序分配：将线性表中的元素依次存放到计算机存储器中一组连续地址的存储单元中，这种分配方式称为顺序分配或顺序映像。由此得到的存储结构称为顺序存储结构或向量(一维数组）。 逻辑上相邻的两个元素在物理位置上也相邻，但物理相邻不一定逻辑相邻。 一般形式：i——an——存储地址：LOC(i) = a1+（i-1） d （1&lt;=i&lt;=n）a1：表的首地址/元素a1的地址；d：1个数据元素所占存储单元的数目；maxleng：最大长度，为某个常数 [序号maxleng——存储地址：b+(maxleng-1)p] 只要知道顺序表首地址和每个数据元素所占地址单元的个数就可求出第i个数据元素的地址来，这也是顺序表具有按数据元素的序号随机存取的特点。需用一个变量last 记录当前线性表中最后一个元素在数组中的位置，即last 起一个指针的作用，始终指向线性表中最后一个元素，因此，表空时last=-1.线性表中元素的位序是从1开始的，而数组中元素的下标是从0开始的。假设表L的起始位置为LOC(A)，sizeof(ElemType)是每个数据元素所占用存储空间的大小。 顺序访问 随机访问 从结构性上考虑，通常将data 和last 封装成一个结构作为顺序表的类型： typedef struct { datatype data[MAXSIZE]; int last; } SeqList; 定义一个顺序表：SeqList L ; 表长＝L.last+1，线性表中的数据元素a1至an分别存放在L.data[0]至L.data[L.last]中;有时定义一个指向SeqList 类型的指针更为方便：SeqList L ；L是一个指针变量，线性表的存储空间通过L=malloc(sizeof(SeqList)) 操作来获得, L中存放的是顺序表的地址. 表长表示为（L）.last或L－＞last+1，线性表的存储区域为L-&gt;data ，线性表中数据元素的存储空间为：L－＞data[0] ~ L－＞data[L－＞last]。 线性表顺序结构在C语言中的定义（静态分配）? 123456789101112#define maxleng 100 typedef struct &#123; ElemType elem[maxleng]；//下标:0,1,...,maxleng-1 int length； //表长 &#125; SqList； SqList La； .......... 其中：typedef---别名定义，SqList----结构类型名 La----结构类型变量名 La.length---表长 La.elem[0]----a1 La.elem[La.length-1]---an 线性表顺序结构在C语言中的定义（动态分配） 123456789101112 #define LIST_INIT_SIZE 100 #define LISTINCREMENT 10 typedef struct &#123; ElemType *elem；//存储空间基地址 int length； //表长 int listsize; //当前分配的存储容量 //（以sizeof(ElemType)为单位 &#125; SqList； SqList Lb； 其中： typedef--别名定义， SqList---结构类型名 Lb---结构类型变量名 Lb.length---表长 Lb.elem[0]---a1 Lb.elem[Lb.length-1]--an 顺序表上的基本运算1.初始化即构造一个空表；将L设为指针参数，首先动态分配存储空间，然后，将表中last 指针置为－1，表示表中没有数据元素。 SeqList *init_SeqList( ) { SeqList *L; L=malloc(sizeof(SeqList)); L-&gt;last=-1; return L; } 2.插入运算在表的第i个位置上插入一个值为x 的新元素，插入后使原表长为n的表:(a1，a2，… ，ai-1，ai，ai+1，… ，an)成为表长为n+1 表：(a1，a2，…，ai-1，x，ai，ai+1，…，an ) ，1&lt;=i&lt;=n+1。 步骤：移-让-放-改 (1) 将ai～an 顺序向下移动，为新元素让出位置； (2) 将x 置入空出的第i个位置； (3) 修改last 指针(相当于修改表长)，使之仍指向最后一个元素。 表长Maxsize = L.last+1；L.last = Maxsize -1;若L.last = Maxsize -1，则表满 12345678910111213int Insert_SeqList(SeqList *L，int i，datatype x)&#123; int j;if (L-&gt;last==MAXSIZE－1)&#123; printf(＂表满＂); return(-1); &#125; /*表空间已满，不能插入*/if (i&lt;1 || i&gt;L-&gt;last+2) /*检查插入位置的正确性*/&#123; printf(＂位置错＂);return(0); &#125;for(j=L-&gt;last;j&gt;=i-1;j--)L-&gt;data[j+1]=L-&gt;data[j]; /* 结点移动*/L-&gt;data[i-1]=x; /*新元素插入*/L-&gt;last++; /*last仍指向最后元素*/return (1); /*插入成功，返回*/&#125;//先处理异常情况，再处理循环 注意： (1) 顺序表中数据区域有MAXSIZE个存储单元，所以在向顺序表中做插入时先检查表空间是否满了，在表满的情况下不能再做插入，否则产生溢出错误。 (2) 要检验插入位置的有效性，这里i 的有效范围是：1&lt;=i&lt;=n+1，其中n 为原表长。 (3) 注意数据的移动方向。 插入算法的时间性能分析：顺序表上的插入运算，时间主要消耗在了数据的移动上，在第i个位置上插入x ，从ai 到an 都要向下移动一个位置，共需要移动n－i＋1个元素，而i 的取值范围为：1&lt;= i&lt;= n+1，即有n＋1个位置可以插入。设在第i个位置上作插入的概率为Pi，则平均移动数据元素的次数： 在顺序表上做插入操作需移动表中一半的数据元素。显然时间复杂度为Ｏ(n) 3. 删除运算DeleteList(L,i)将表中第i 个元素从线性表中去掉，删除后使原表长为n 的线性表：(a1，a2，… ，ai-1，ai，ai+1，…，an)成为表长为n－1 的线性表：(a1，a2，… ，ai-1， ai+1，… ，an)。i 的取值范围为：1&lt;=i&lt;=n 。 123456789//算法：先判断元素ai的合法性——若不合法则返回Error/若合法则继续——定位ai元素（在第i-1号地址）并赋给变量e——用循环结构将ai后的所有元素往前挪一位（共L.length-1个元素；赋值法）int ListDelete_Sq(SqList &amp;L,int I,ElemType &amp;e) &#123;if (i&lt;1 || i&gt;L.length) return ERROR; e=L.elem[i-1]; for(j=i;j&lt;=L.length-1;j++) L.elem[j-1]=L.elem[j]; L.length--; return OK; &#125; 步骤： 将ai+1～an 顺序向上移动。 修改last指针(相当于修改表长)使之仍指向最后一个元素。 12345678int Delete_SeqList(*SeqList *L, int i)&#123;int j;if (i &lt; 1 || i &gt; L-&gt;last+1) /*检查空表及删除位置的合法性*/&#123;print (&quot;不存在第i个元素&quot;), return(0);&#125;for(j=i;j&lt;=L-&gt;last;j++)L-&gt;data[j-1]=L-&gt;data[j]; /*向上移动*/L-&gt;last--;return(1); /*删除成功*/&#125; 注意： （1）删除第i个元素，i的取值为1&lt;=i&lt;=n ,否则第i个元素不存在，因此，要检查删除位置的有效性。 （2）当表空时不能做删除，因表空时L-&gt;last的值为-1，条件（iL-&gt;last+1）也包括了对表空的检查。 （3）删除ai 之后，该数据已不存在，如果需要，先取出ai ，再做删除。 删除算法的时间性能分析：与插入运算相同，其时间主要消耗在了移动表中元素上，删除第i个元素时，其后面的元素ai+1～an 都要向上移动一个位置，共移动了n-i 个元素，所以平均移动数据元素的次数： 顺序表上作删除运算时大约需要移动表中一半的元素，显然该算法的时间复杂度为Ｏ(n)。 4.按值查找线性表中的按值查找是指在线性表中查找与给定值x相等的数据元素。在顺序表中完成该运算最简单的方法是：从第一个元素a1 起依次和x比较，直到找到一个与x相等的数据元素，则返回它在顺序表中的存储下标或序号（二者差一）；或者查遍整个表都没有找到与x 相等的元素，返回-1。 1234567int Location_SeqList(SeqList *L, datatype x)&#123; int i=0;while(i&lt;=L.last &amp;&amp; L-&gt;data[i]!= x)i++;if (i&gt;L-&gt;last) return -1;else return i; /*返回的是存储位置*/&#125; 主要运算是比较。显然比较的次数与x在表中的位置有关，也与表长有关。当a1=x 时，比较一次成功。当an=x 时比较n 次成功。平均比较次数为（n+1）/2，时间性能为O(n)。 顺序表应用举例1.顺序表的划分Q：将顺序表(a1，a2，… ，an) 重新排列为以a1 为界的两部分：a1 前面的值均比a1 小，a1 后面的值都比a1 大(这里假设数据元素的类型具有可比性, 不妨设为整型)，这一操作称为划分。a1 也称为基准。 思路：从第二个元素开始到最后一个元素，逐一向后扫描： （1）当前数据元素aI 比a1 大时，表明它已经在a1 的后面，不必改变它与a1 之间的位置，继续比较下一个。 （2）当前结点若比a1 小，说明它应该在a1 的前面，此时将它上面的元素都依次向下移动一个位置，然后将它置入最上方。 1234567891011&#123; int i,j;datatype x,y;x=L-&gt;data[0]; /* 将基准置入x 中*/for(i=1;i&lt;=L-&gt;last;i++)if(L-&gt;data[i]&lt;x) /*当前元素小于基准*/&#123; y = L-&gt;data[i];for(j=i-1;j&gt;=0;j--) /*移动*/L－&gt;data[j+1]=L-&gt;data[j];L-&gt;data[0]=y;&#125;&#125; 本算法中，有两重循环，外循环执行n－1次，内循环中移动元素的次数与当前数据的大小有关，当第ｉ个元素小于a1 时，要移动它上面的i-1个元素，再加上当前结点的保存及置入，所以移动i-1+2次，在最坏情况下，a1 后面的结点都小于a1 ，故总的移动次数为： 最坏情况下移动数据时间性能为Ｏ(ｎ２) 2.合并顺序表并按顺序排列Q：有顺序表A和B，其元素均按从小到大的升序排列，编写一个算法将它们合并成一个顺序表C，要求C的元素也是从小到大的升序排列。 算法思路：依次扫描通过A和B的元素，比较当前的元素的值，将较小值的元素赋给C，如此直到一个线性表扫描完毕，然后将未完的那个顺序表中余下部分赋给C即可。C的容量要能够容纳A、B两个线性表相加的长度。 12345678910111213void merge(SeqList A, SeqList B, SeqList *C)&#123; int i,j,k;i=0;j=0;k=0;while ( i&lt;=A.last &amp;&amp; j&lt;=B.last )if (A.date[i]&lt;B.date[j])C-&gt;data[k++]=A.data[i++];else C-&gt;data[k++]=B.data[j++];while (i&lt;=A.last )C-&gt;data[k++]= A.data[i++];while (j&lt;=B.last )C-&gt;data[k++]=B.data[j++];C-&gt;last=k-1;&#125; 算法的时间性能是O(m+n)，其中m是A的表长，n是B的表长。 3.比较两个线性表的大小Q：比较两个线性表的大小。两个线性表的比较依据下列方法：设A、B是两个线性表，均用向量表示，表长分别为m和n。A′和B′分别为A 和B 中除去最大共同前缀后的子表。 例如A=(x,y,y,z,x,z)， B=(x,y,y,z,y,x,x,z)，两表最大共同前缀为(x,y,y,z) 。则A′=（x,z）， B′=（y,x,x,z），若A′=B′= 空表，则A=B；若A′=空表且B′≠空表，或两者均不空且A′首元素小于B′首元素，则AB。 算法思路：首先找出A、B的最大共同前缀；然后求出A′和B′，之后在按比较规则进行比较，A&gt;B 函数返回1；A=B返回0；A&lt;B返回-1。算法如下： 12345678910111213int compare( A,B,m,n)int A[],B[];int m,n;&#123; int i=0,j,AS[],BS[],ms=0,ns=0; /*AS,BS作为A′,B′*/while (A[i]==B[i]) i++; /*找最大共同前缀*/for (j=i;j&lt;m;j++)&#123; AS[j-i]=A[j];ms++; &#125; /*求A′,ms为A′的长度*/for (j=i;j&lt;n;j++)&#123; BS[j-i]=B[j];ns++; &#125; /*求B′,ms为B′的长度*/if (ms==ns&amp;&amp;ms==0) return 0;else if (ms==0&amp;&amp;ns&gt;0 || ms&gt;0 &amp;&amp; ns&gt;0 &amp;&amp; AS[0]&lt;BS[0]) return –1;else return 1;&#125; 算法的时间性能是O( m+n ) 顺序结构的评价优点： 随机存取结构，存取任何元素的时间都是一个常数，速度快 结构简单，元素逻辑相邻也物理相邻 不适用指针，节省存储空间 缺点： 插入和删除元素需要移动大量元素，消耗时间 需要一个连续的存储空间 插入元素可能发生“溢出” 自由区中的存储空间不能被其他数据占用/共享 2.3 线性表的链式存储结构及运算 顺序表：存贮特点是用物理上的相邻实现了逻辑上的相邻，要求用连续的存储单元顺序存储线性表中各元素，因此，对顺序表插入、删除时需要通过移动数据元素来实现，影响了运行效率。 链表：用地址连续的存储单元来实现，因为它不要求逻辑上相邻的两个数据元素物理上也相邻，它是通过“链”建立起数据元素之间的逻辑关系来，因此对线性表的插入、删除不需要移动数据元素。 单链表链表：通过一组任意的存储单元来存储线性表中的数据元素 为建立起数据元素之间的线性关系，对每个数据元素ai，除了存放数据元素的自身的信息ai 之外，还需要和ai一起存放其后继ai+1 所在的存贮单元的地址，这两部分信息组成一个“结点”。 data：存放ai自身的信息 next：存放ai的直接后继ai+1的地址（用指针存储地址；*next） 数据域：存放数据元素信息 指针域：存放其后继地址 链表：n个元素的线性表通过每个结点的指针域拉成了一个“链子”；链表由一个个结点构成。 单链表：每个结点只有一个指向后继的指针的链表 typedef struct node { datatype data; struct node *next; } LNode，*LinkList； //定义头指针变量： LinkList H； 注意： LNode是结点的类型，LinkList是指向Lnode类型结点的指针类型 最后一个结点没有后继, 其指针域必需置空 我们关心的是结点间的逻辑结构，而对每个结点的实际地址并不关心 通常我们用“头指针”来标识一个单链表，如单链表L、单链表H等，是指某链表的第一个结点的地址放在了指针变量L、H 中， 头指针为“NULL”则表示一个空表 通常将标识一个链表的头指针说明为LinkList类型的变量，如LinkList L ; 当L有定义时，值要么为NULL，则表示一个空表；要么为第一个结点的地址，即链表的头指针；将操作中用到指向某结点的指针变量说明为LNode 类型，如LNode p， 则语句： p=malloc(sizeof(LNode))； 则完成了申请一块Lnode 类型的存储单元的操作，并将其地址赋值给变量p。 P所指的结点为p，p的类型为LNode型，所以该结点的数据域为(p).data 或p-&gt;data，指针域为(p).next 或p-&gt;next。free(p)则表示释放p 所指的结点。 单链表基本运算1.建立单链表(1)在链表的头部插入结点建立单链表： 链表：动态管理的存储结构；链表中的每个结点占用的存储空间不是预先分配，而是运行时系统根据需求而生成的，因此建立单链表从空表开始，每读入一个数据元素则申请一个结点，然后插在链表的头部。 因为是在链表的头部插入，故读入数据的顺序和线性表中的逻辑顺序是相反的。 12345678910111213LinkList Creat_LinkList1( )&#123; LinkList L=NULL；/*空表*/Lnode *s;int x; /*设数据元素的类型为int*/scanf(＂%d＂,&amp;x);while (x!=flag)&#123; s=malloc(sizeof(LNode));s-&gt;data=x;s-&gt;next=L; L=s;Scanf (＂%d＂,&amp;x);&#125;return L;&#125; (2)在单链表的尾部插入结点建立单链表： 头插入与尾插入的区别： 头插入：读入数据元素的顺序与生成链表中的元素顺序相反 尾插入：使词序一致；每次将新结点插入到链表的尾部，所以需加入一个指针r 用来始终指向链表中的尾结点，以便能够将新结点插入到链表的尾部 算法思路： 初始状态：头指针H=NULL，尾指针r=NULL; 按线性表中元素的顺序依次读入数据元素，不是结束标志时，申请结点，将新结点插入到r 所指结点的后面，然后r 指向新结点。H=NULL r=NULL /初始状态/ 123456789101112131415LinkList Creat_LinkList2( )&#123; LinkList L=NULL;Lnode *s,*r=NULL;int x; /*设数据元素的类型为int*/scanf(＂%d＂,&amp;x);while (x!=flag)&#123; s=malloc(sizeof(LNode)); s-&gt;data=x;if (L==NULL) L=s; /*第一个结点的处理*/else r-&gt;next=s; /*其它结点的处理*/r=s; /*r 指向新的尾结点*/scanf(＂%d＂,&amp;x);&#125;if ( r!=NULL) r-&gt;next=NULL; /*对于非空表，最后结点的指针域放空指针*/return L;&#125; 算法解释： 第一个结点的处理和其它结点不同，原因是第一个结点加入时链表为空，它没有直接前驱结点，它的地址就是整个链表的指针， 需要放在链表的头指针变量中；而其它结点有直接前驱结点，其地址放入直接前驱结点的指针域。 “第一个结点”的问题在很多操作中都会遇到，如在链表中插入结点时，将结点插在第一个位置和其它位置是不同的，在链表中删除结点时，删除第一个结点和其它结点的处理也是不同的 头结点的加入使得“第一个结点”的问题不再存在，也使得“空表”和“非空表”的处理成为一致 头结点：数据域无定义，指针域中存放的是第一个数据结点的地址，空表时为空 2.求表长算法思路：设一个移动指针ｐ和计数器ｊ，初始化后，ｐ所指结点后面若还有结点，ｐ向后移动，计数器加1。 (1)设L是带头结点的单链表(线性表的长度不包括头结点): 1234567int Length_LinkList1 (LinkList L)&#123; Lnode * p=L; /* p指向头结点*/int j=0;while (p-&gt;next)&#123; p=p-&gt;next; j++ &#125; /* p所指的是第j 个结点*/return j;&#125; (2)设L是不带头结点的单链表： 123456789int Length_LinkList2 (LinkList L)&#123; Lnode * p=L;int j;if (p==NULL) return 0; /*空表的情况*/j=1; /*在非空表的情况下，p所指的是第一个结点*/;while (p-&gt;next )&#123; p=p-&gt;next; j++ &#125;return j;&#125; 不带头结点的单链表空表情况要单独处理，而带上头结点之后则不用。 时间复杂度均为O(n)。 3.查找操作(1) 按序号查找Get_Linklist(L,i)：算法思路：从链表的第一个元素结点起，判断当前结点是否是第i个，若是，则返回该结点的指针，否则继续后一个，表结束为止。没有第ｉ个结点时返回空。 123456789Lnode * Get_LinkList(LinkList L, Int i);/*在单链表L中查找第i个元素结点，找到返回其指针，否则返回空*/&#123; Lnode * p=L;int j=0;while (p-&gt;next !=NULL &amp;&amp; j&lt;i )&#123; p=p-&gt;next; j++; &#125;if (j==i) return p;else return NULL;&#125; (2) 按值查找即定位Locate_LinkList(L,x) 算法思路：从链表的第一个元素结点起，判断当前结点其值是否等于x，若是，返回该结点的指针，否则继续后一个，表结束为止。找不到时返回空。算法如下： 1234567Lnode * Locate_LinkList( LinkList L, datatype x)/*在单链表L中查找值为x的结点，找到后返回其指针，否则返回空*/&#123; Lnode * p=L-&gt;next;while ( p!=NULL &amp;&amp; p-&gt;data != x)p=p-&gt;next;return p;&#125; 4.插入(1)后插结点： 设p指向单链表中某结点，s指向待插入的值为x的新结点，将s插入到p的后面，插入示意图如图2.13。操作如下： ①s-&gt;next=p-&gt;next;②p-&gt;next=s; 注意：两个指针的操作顺序不能交换。 (2)前插结点： 设ｐ指向链表中某结点，ｓ指向待插入的值为x的新结点，将s插入到p的前面，插入示意图如图，与后插不同的是：首先要找到p的前驱q，然后再完成在q之后插入s，设单链表头指针为L，操作如下： 12345q=L;while (q-&gt;next!=p)q=q-&gt;next; /*找*p的直接前驱*/s-&gt;next=q-&gt;next;q-&gt;next=s; 后插操作的时间复杂性为O(1)，前插操作因为要找p 的前驱，时间性能为O(n)；其实我们关心的更是数据元素之间的逻辑关系，所以仍然可以将s 插入到*p 的后面，然后将ｐ-&gt;data与s-&gt;data交换即可，这样即满足了逻辑关系，也能使得时间复杂性为O(1)。 (3)插入运算Insert_LinkList(L,i,x) 算法思路：1.找到第i-1个结点；若存在继续2，否则结束2.申请、填装新结点；3.将新结点插入。结束。 算法如下： 12345678910111213int Insert_LinkList( LinkList L, int i, datatype x)/*在单链表L的第i个位置上插入值为x的元素*/&#123; Lnode * p,*s;p=Get_LinkList(L,i-1); /*查找第i-1个结点*/if (p==NULL)&#123; printf(＂参数i错＂);return 0; &#125; /*第i-1个不存在不能插入*/else &#123;s=malloc(sizeof(LNode)); /*申请、填装结点*/s-&gt;data=x;s-&gt;next=p-&gt;next; /*新结点插入在第i-1个结点的后面*/p-&gt;next=sreturn 1;&#125; 5. 删除(1)删除结点： 设p指向单链表中某结点，删除*p。操作示意图如图所示。 通过示意图可见，要实现对结点p的删除，首先要找到p的前驱结点*q，然后完成指针的操作即可。指针的操作由下列语句实现： q-&gt;next=p-&gt;next; free(p); 显然找p前驱的时间复杂性为O(n)。若要删除p的后继结点(假设存在)，则可以直接完成： s=p-&gt;next; p-&gt;next=s-&gt;next; free(s); 该操作的时间复杂性为O(1) 。 (2)删除运算：Del_LinkList(L,i) 算法思路：1.找到第i-1个结点；若存在继续2，否则结束；2.若存在第i个结点则继续3，否则结束；3.删除第i个结点，结束。 算法如下：int Del_LinkList(LinkList L，int i)/删除单链表L上的第i个数据结点/{ LinkList p,s;p=Get_LinkList(L,i-1); /查找第i-1个结点/if (p==NULL) { printf(＂第i-1个结点不存在＂);return -1; }else { if (p-&gt;next==NULL){ printf(＂第i个结点不存在＂);return 0; }else{ s=p-&gt;next; /s指向第i个结点/p-&gt;next=s-&gt;next; /从链表中删除/free(s); /释放s */return 1;}算法2.13。算法2.13的时间复杂度为O(n)。 通过上面的基本操作我们得知： (1) 在单链表上插入、删除一个结点，必须知道其前驱结点。 (2) 单链表不具有按序号随机访问的特点，只能从头指针开始一个个顺序进行。 顺序表和链表的比较顺序存储有三个优点： (1) 方法简单，各种高级语言中都有数组，容易实现。(2) 不用为表示结点间的逻辑关系而增加额外的存储开销。(3) 顺序表具有按元素序号随机访问的特点。(4)顺序表存储密度大 两个缺点： (1) 在顺序表中做插入删除操作时，平均移动大约表中一半的元素，因此对n较大的顺序表效率低。(2) 需要预先分配足够大的存储空间，估计过大，可能会导致顺序表后部大量闲置；预先分配过小，又会造成溢出。 如何选取存储结构： 基于存储的考虑 顺序表容易实现，任何高级语言中都有数组类型，链表的操作是基于指针的，相对来讲前者简单些，也是用户考虑的一个因素。顺序表的存储空间是静态分配的，在程序执行之前必须明确规定它的存储规模，也就是说事先对＂MAXSIZE＂要有合适的设定，过大造成浪费，过小造成溢出。可见对线性表的长度或存储规模难以估计时，不宜采用顺序表；链表不用事先估计存储规模，但链表的存储密度较低，存储密度是指一个结点中数据元素所占的存储单元和整个结点所占的存储单元之比。显然链式存储结构的存储密度是小于１的。 基于运算的考虑 在顺序表中按序号访问ai的时间性能时O(1)，而链表中按序号访问的时间性能O(n)，所以如果经常做的运算是按序号访问数据元素，显然顺序表优于链表；而在顺序表中做插入、删除时平均移动表中一半的元素，当数据元素的信息量较大且表较长时，这一点是不应忽视的；在链表中作插入、删除，虽然也要找插入位置，但操作主要是比较操作，从这个角度考虑显然后者优于前者。 基于环境的考虑 顺序表容易实现，任何高级语言中都有数组类型，链表的操作是基于指针的，相对来讲前者简单些，也是用户考虑的一个因素。 循环链表单循环链表：最后一个结点的指针域是空指针，若将该链表头指针置入该指针域，则使链表头尾结点相连，即构成单循环链表。（头尾相连，尾指针域指向头结点地址） 单循环链表上的操作基本上与非循环链表相同，只是将原来判断指针是否为NULL变为是否是头指针而已，没有其它较大的变化。 对于单链表只能从头结点开始遍历整个链表，而对于单循环链表则可以从表中任意结点开始遍历整个链表 有时对链表常做的操作是在表尾、表头进行，此时可以改变一下链表的标识方法，不用头指针而用一个指向尾结点的指针R 来标识，可以使得操作效率得以提高 例如：对两个单循环链表H1 、H2 的连接操作，是将H2 的第一个数据结点接到H1 的尾结点，如用头指针标识，则需要找到第一个链表的尾结点，其时间复杂性为O(n)，而链表若用尾指针R1 、R2 来标识，则时间性能为O(1) 1234p= R1 –&gt;next; /*保存R1 的头结点指针*/R1-&gt;next=R2-&gt;next-&gt;next; /*头尾连接*/free(R2-&gt;next); /*释放第二个表的头结点（因为一个链表只能有一个头结点，而头结点本身作用在于存放第一个结点数据域的地址）*/R2-&gt;next=p; /*组成循环链表*/ 双向链表 单链表的结点只有一个指向后继结点的指针域，因此若已知某结点的指针为p，其后继结点的指针则为p-&gt;next ，而找其前驱则只能从该链表的头指针开始，顺着各结点的next域进行，也就是说找后继的时间性能是O(1)，找前驱的时间性能是O(n)，如果也希望找前驱的时间性能达到O(1)，则只能付出空间的代价：每个结点再加一个指向前驱的指针域，结点的结构为如图所示，用这种结点组成的链表称为双向链表 1234typedef struct dlnode&#123; datatype data;struct dlnode *prior,*next;&#125;DLNode,*DLinkList; 和单链表类似，双向链表通常也是用头指针标识，也可以带头结点和做成循环结构。 设p 指向双向循环链表中的某一结点，即p 中是该结点的指针，则p-&gt;prior-&gt;next 表示的是p 结点之前驱结点的后继结点的指针，即与p 相等；类似，p-&gt;next-&gt;prior 表示的是p 结点之后继结点的前驱结点的指针，也与p 相等，所以有以下等式： 双向链表中结点的插入：设p 指向双向链表中某结点，s 指向待插入的值为x 的新结点，将s 插入到p 的前面，插入示意图如图所示。操作如下： 12341 s-&gt;prior=p-&gt;prior;2 p-&gt;prior-&gt;next=s;3 s-&gt;next=p;4 p-&gt;prior=s; 双向链表中结点的删除：设p 指向双向链表中某结点，删除*p。操作如下：①p-&gt;prior-&gt;next=p-&gt;next;②p-&gt;next-&gt;prior=p-&gt;prior;free(p); 静态链表优点：采用静态空间分配方式，且插入和删除操作时不需要移动元素 规模较大的结构数组sd[MAXSIZE] 中有两个链表: 其中链表SL是一个带头结点的单链表，表示了线性表(a1, a2, a3, a4, a5)，而另一个单链表AV是将当前sd 中的空结点组成的链表。 1234567#define MAXSIZE … /*足够大的数*/typedef struct&#123;datatype data;int next;&#125;SNode; /*结点类型*/SNode sd[MAXSIZE];int SL,AV; /*两个头指针变量*/ 这种链表的结点中也有数据域data和指针域next，与前面所讲的链表中的指针不同的是，这里的指针是结点的相对地址(数组的下标)，称之为静态指针，这种链表称之为静态链表，空指针用-1表示，因为上面定义的数组中没有下标为-1的单元。 SL是用户的线性表，AV模拟的是系统存储池中空闲结点组成的链表，当用户需要结点时，例如向线性表中插入一个元素，需自己向AV申请，而不能用系统函数malloc来申请，相关的语句为： if(AV!=-1) { t=AV; AV=sd[AV].next; } 所得到的结点地址(下标)存入了t 中；不难看出当AV表非空时，摘下了第一个结点给用户。当用户不再需要某个结点时，需通过该结点的相对地址t 将它还给AV，相关语句为： sd[t].next=AV;AV=t;而不能调用系统的free 函数。交给AV表的结点链在了AV的头部。 下面通过线性表插入这个例子看静态链表操作 在带头结点的静态链表SL的第i个结点之前插入一个值为x的新结点。设静态链表的存储区域sd为全局变量。 12345678910111213141516171819int Insert_SList( int SL, datatype x, int i)&#123; int p,s;p=SL; j=0;while(sd[p].next!=-1 &amp;&amp; j&lt;i-1)&#123;p=sd[p].next;j++;&#125; /*找第i-1个结点*/if(j==i-1)&#123; if(AV!=-1) /*若AV表还有结点可用*/&#123;t=AV;AV=sd[AV].next; /*申请、填装新结点*/sd[t].data=x;sd[t].next=sd[p].next; /*插入*/sd[p].next=t;return 1; /*正常插入成功返回*/&#125;else&#123;printf(＂存储池无结点＂);return 0;&#125;/*未申请到结点，插入失败*/else&#123;printf(＂插入的位置错误＂);return -1;&#125;/*插入位置不正确，插入失败*/&#125; 单链表应用举例1.已知单链表H，写一算法将其倒置。即实现如图操作。(a)为倒置前，(b)为倒置后。 算法思路：从右往左依次取原链表中的每个结点，将其作为第一个结点插入新链表，指针p用来指向当前结点，p为空时结束。 12345678910void reverse (Linklist H)&#123; LNode *p;p=H-&gt;next; /*p指向第一个数据结点*/H-&gt;next=NULL; /*将原链表置为空表H*/while (p)&#123; q=p; p=p-&gt;next;q-&gt;next=H-&gt;next; /*将当前结点插到头结点的后面*/H-&gt;next=q;&#125;&#125; 该算法只是对链表中顺序扫描一边即完成了倒置，所以时间性能为O(n)。 2.已知单链表L，写一算法，删除其重复结点，即实现如图操作。(a)为删除前，(b)为删除后。 算法思路：用指针p 指向第一个数据结点，从它的后继结点开始到表的结束，找与其值相同的结点并删除之；p 指向下一个；依此类推，p 指向最后结点时算法结束。 12345678910111213141516171819202122void pur_LinkList( LinkList H )&#123; LNode *p, *q, *r; p = H-&gt;next; /*p指向第一个结点*/ if ( p == NULL ) return; while ( p-&gt;next ) &#123; q = p; while ( q-&gt;next ) /* 从*p的后继开始找重复结点*/ &#123; if ( q-&gt;next-&gt;data == p-&gt;data ) &#123; r = q-&gt;next; /*找到重复结点，用r指向，删除*r */ q-&gt;next = r-&gt;next; free( r ); &#125; /*if*/ else q = q-&gt;next; &#125; /*while(q-&gt;next)*/ p = p-&gt;next; /*p指向下一个，继续*/ &#125; /*while(p-&gt;next)*/&#125; 该算法的时间性能为O(n2) 3.设有两个单链表A、B，其中元素递增有序，编写算法将A、B归并成一个按元素值递减（允许有相同值）有序的链表C，要求用A、B中的原结点形成，不能重新申请结点。 算法思路：利用A、B两表有序的特点，依次进行比较，将当前值较小者摘下，插入到C表的头部，得到的C表则为递减有序的。 1234567891011121314151617181920212223242526LinkList merge( LinkList A, LinkList B )/*设A、B均为带头结点的单链表*/&#123; LinkList C; LNode *p, *q; p = A-&gt;next; q = B-&gt;next; C = A; /*C表的头结点*/ C-&gt;next = NULL; free( B ); while ( p &amp;&amp; q ) &#123; if ( p-&gt;data &lt; q-&gt;data ) &#123; s = p; p = p-&gt;next; &#125;else &#123; s = q; q = q-&gt;next; &#125; /*从原AB表上摘下较小者*/ s-&gt;next = C-&gt;next; /*插入到C表的头部*/ C-&gt;next = s; &#125; /*while */ if ( p == NULL ) p = q; while ( p ) /* 将剩余的结点一个个摘下，插入到C表的头部*/ &#123; s = p; p = p-&gt;next; s-&gt;next = C-&gt;next; C-&gt;next = s; &#125;&#125; 该算法的时间性能为O(m+n) 纠错p = H -&gt; next :指向H表的首结点 前后之分：next方向是后，prior方向是前。 在单链表中，增加一个头节点的目的是为了方便运算的实现。 将长度为m的单链表链接在长度为n的单链表之后的算法时间复杂度为 O(n)_。 在一个长度为n（n&gt;1）的带头节点的单链表上，另设有尾指针r（指向尾节点），执行删除单链表的尾节点_操作与链表的长度有关。 已知一个长度为n的单链表中所有节点是递增有序的，找最小值节点的算法的时间复杂度为 O(1). 带表头结点的双循环链表L为空表的条件是 :L -&gt; next == L 某线性表最常用的操作是在尾元素之后插入一个元素和删除尾元素，则采用 循环双链表_ 存储方式最节省运算时间 如果对含有n（n&gt;1）个元素的线性表的运算只有4种，即删除第一个元素、删除尾元素、在第一个元素前面插入新元素、在尾元素的后面插入新元素，则最好使用只有开始数据节点指针没有尾节点指针的循环双链表_。 在长度为n的 只有表头指针的不带表头节点的循环单链表_ 上，删除第一个元素，其算法的时间复杂度为O(n)。 在单链表中，要删除某一指定的节点，必须找到该节点的 _前驱 节点。 求一个单链表长度的算法的时间复杂度为 O(n)_。 疑问？循环单链表与循环双链表的区别？ 双链表的插入删除操作算法？]]></content>
      <categories>
        <category>Computer Science</category>
        <category>Data Structure</category>
      </categories>
      <tags>
        <tag>Data Structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构第一章：绪论]]></title>
    <url>%2F2017%2F12%2F08%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E7%BB%AA%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[Abstract：今天《数据结构》开课，本文是第一章绪论的笔记。 什么是数据结构 计算机中存储和组织数据的方式，意味着接口或封装：一个数据结构可被视为两个函数之间的接口，或者是由数据类型联合组成的存储内容的访问方法封装。 why接口：设计出DS封装成接口，以不同的应用程序能够安全地重用这些DS 一个设计良好的数据结构，应该在尽可能使用较少的时间与空间资源的前提下，为各种临界状态下的运行提供支持。数据结构可通过编程语言所提供的数据类型、引用及其他操作加以实现。 系统构造的关键因素是数据结构而非算法 相互之间存在一种或多种特定关系的数据元素的集合，数据元素间的关系为结构。 程序设计 = 数据结构 + 算法 基本概念和术语 数据data：所有能输入到计算机中并被计算机程序加工、处理的符号的总称; 如整数、实数、字符串、图像、声音等。信息的载体，它能够被计算机识别、存储和加工处理。 数据元素data element：数据的基本单位（元素、记录、结点、顶点），在计算机程序中通常作为一个整体进行考虑和处理。 数据项data item：一个数据元素可由若干个数据项组成，数据项是数据的不可分割的最小单位。如：姓名、年龄等； 初等项：不可分割，如性别、籍贯 组合项：可分割，如学生成绩可划分为数学、物理、语文等 数据对象data object：由性质/类型相同的数据元素组成的集合；是数据的一个子集。 数据结构data structure：相互之间存在一种或多种特定关系的数据元素的集合。数据元素间的关系为结构。四类基本结构：集合（元素同属于一个集合的关系），线性结构（一对一的关系），树状结构（一对多的关系）和图状结构（多对多的关系）。 数据的逻辑结构：各元素间的逻辑关系，可以看作是从具体问题抽象出来的数学模型 物理结构：指数据的逻辑结构在计算机中的存储形式；逻辑结构面向问题，物理结构面向计算机，其基本目标是将数据及其逻辑关系存储到计算机的内存中。研究的是数据结构在计算机中的实现方法，包括数据结构中元素的表示及元素间关系的表示。 数据的存储结构：数据结构在计算机存储器中的映象(mapping)；存储结构也称为：存储表示,物理结构,物理表示。 结点/元素：计算机中由若干位bit组成的一个位串表示一个元素，这个位串即元素或结点（node）。 顺序存储结构：用元素在存储器中的物理相对位置来表示数据元素之间的逻辑关系。非顺序/链式存储结构：用指示元素存储地址的指针表示数据元素间的逻辑关系；把数据元素存放在任意的存储单元里，这组存储单元可以是连续的也可以是不连续的。数据元素的存储关系不反映其逻辑关系，用指针存放数据元素的地址，我们通过地址可以找到相关联数据元素的位置。【即逻辑相邻的元素物理位置任意，以指针地址寻访相关联元素】如： 数据类型data type：是一个值的集合和定义在这个值上的一组操作的总称。 抽象数据类型(Abstract Data Type):与计算机的实现无关的数据类型，指一个数学模型以及定义在该模型上的一组操作，其定义只取决于它的一组逻辑特性，与其在计算机内部如何表示和实现无关。一个抽象数据类型定义了一个数据对象、数据对象中各数据元素之间的关系及对数据元素的操作。 抽象数据类型（Abstract Data Type, ADT）可用三元组表示（D，S，T）：ADT抽象数据类型名{ 数据对象：（数据对象的定义） 数据关系：（数据关系的定义） 基本操作：（基本操作的定义）} 基本操作的定义格式：基本操作名（参数表） 初始条件：（初始条件描述） 操作结果：（操作结果描述） 数据结构的层次： C语言核心子集 预定义变量和类型：#define typedef表示数据结构，元素类型为ElemType 基本操作语句 赋值 选择：if …else; switch…case…default 循环：for; while; do…while 结束：return (函数结束语句)；break（case结束）；exit（异常结束） 输入输出：scanf([格式串]，变量1，…，变量n)；printf 注释：// 基本函数：max;min;abs; floor; ceil; eof; eoln 逻辑运算约定：&amp;&amp;；|| 算法Algorithm和算法分析定义：对特定问题求解步骤的描述。 算法的描述工具：自然语言；程序设计语言（C）；流程图；伪码语言；类C； 算法的时间复杂度 是什么：算法中基本操作重复执行的次数的总和 语句频度：所有语句的执行次数f(n) 时间复杂度：T(n)=O(f(n)) 根据时间复杂度衡量算法效率的前提是：针对同一问题 算法改进 举例：冒泡排序的C语言算法 算法的存储空间需求 空间复杂度S(n) = O(f(n)); 程序运行所需的存储空间包括以下两部分： ⑴固定部分。这部分空间与所处理数据的大小和个数无关，或者称与问题的实例的特征无关。主要包括程序代码、常量、简单变量、定长成分的结构变量所占的空间。 ⑵可变部分。这部分空间大小与算法在某次执行中处理的特定数据的大小和规模有关。例如100 个数据元素的排序算法与1000 个数据元素的排序算法所需的存储空间显然是不同的。 其他学习目的学习数据结构的目的是为了了解计算机处理对象的特性，将实际问题中所涉及的处理对象在计算机中表示出来并对它们进行处理。 课程内容数据结构课程内容体系：三个层次五个要素 核心技术：分解和抽象 1.抽象层次： 从具体（即具体问题）到抽象（即数据结构）的过程 数据表示 通过分解：划分出数据的三个层次 通过抽象：舍弃数据元素的具体内容，得到逻辑结构 数据处理 通过分解：将处理要求划分为各种功能 通过抽象：舍弃实现细节，得到运算的定义 2.实现层次： 从抽象（即数据结构）到具体（即具体实现）的过程 通过增加对实现细节的考虑进一步得到存储结构和实现运算，从而完成设计任务 抽象数据类型1.数据类型： 原子类型：不可分解，如int，float，string 结构类型：由若干成分按某种结构组成的，因此是可分解的，并且它的成分可以是非结构的，也可以是结构的 在某种意义上，数据结构可以看成是“一组具有相同结构的值”，而数据类型则可被看成是由一种数据结构和定义在其上的一组操作所组成的。 2.抽象数据类型 ~ 数据类型: 一个数学模型以及定义在该模型上的一组操作 定义取决于它的一组逻辑特性，而与其在计算机内部如何表示和实现无关。即不论其内部结构如何变化，只要它的数学特性不变，都不影响其外部的使用。 特征是使用与实现相分离，实行封装和信息隐蔽 算法与算法分析算法与数据结构的关系紧密，在算法设计时先要确定相应的数据结构，而在讨论某一种数据结构时也必然会涉及相应的算法。下面就从算法特性、算法描述、算法性能分析与度量等三个方面对算法进行介绍。 算法（Algorithm）是对特定问题求解步骤的一种描述，是指令的有限序列。其中每一条指令表示一个或多个操作。一个算法应该具有下列特性： 算法必须具备的特性： ⑴有穷性。一个算法必须在有穷步之后结束，即必须在有限时间内完成。 ⑵确定性。算法的每一步必须有确切的定义，无二义性。算法的执行对应着的相同的输入仅有唯一的一条路经。 ⑶可行性。算法中的每一步都可以通过已经实现的基本运算的有限次执行得以实现。 ⑷输入。一个算法具有零个或多个输入，这些输入取自特定的数据对象集合。 ⑸输出。一个算法具有一个或多个输出，这些输出同输入之间存在某种特定的关系。 一个算法若用程序设计语言来描述，则它就是一个程序 好算法：正确，可读，健壮（当输入不合法数据时，应能作适当处理，不至引起严重后果），高效 当我们将一个算法转换成程序并在计算机上执行时，其运行所需要的时间取决于下列因素： ⑴硬件的速度。例如使用486 机还是使用586 机。 ⑵书写程序的语言。实现语言的级别越高，其执行效率就越低。 ⑶编译程序所生成目标代码的质量。对于代码优化较好的编译程序其所生成的程序质量较高。 ⑷问题的规模。例如，求100 以内的素数与求1000 以内的素数其执行时间必然是不同的。 *在各种因素都不能确定的情况下，很难比较出算法的执行时间, 但可算出时间复杂度/问题规模*。也就是说，使用执行算法的绝对时间来衡量算法的效率是不合适的。为此，可以将上述各种与计算机相关的软、硬件因素都确定下来，这样一个特定算法的运行工作量的大小就只依赖于问题的规模（通常用正整数n 表示），或者说它是问题规模的函数。 错题 数据的运算与采用何种存储结构有关 算法必须具备：输入，输出，可行性，有穷性，确定性 算法的时间复杂度为O(n2)，表明该算法的执行时间与成正比 【误！】在相同的问题规模下n下，时间复杂度为O(nlog2n)的算法在执行时间上总是优于时间复杂度为O()的算法 简单的嵌套循环的时间复杂度一般为O(n^2) 程序与算法：程序不一定是算法，算法不一定是程序]]></content>
      <categories>
        <category>Computer Science</category>
        <category>Data Structure</category>
      </categories>
      <tags>
        <tag>Data Structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[中印对峙事件热门微博传播分析]]></title>
    <url>%2F2017%2F12%2F03%2F%E4%B8%AD%E5%8D%B0%E5%AF%B9%E5%B3%99%E4%BA%8B%E4%BB%B6%E7%83%AD%E9%97%A8%E5%BE%AE%E5%8D%9A%E4%BC%A0%E6%92%AD%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[Abstract：补充中印对峙事件舆情分析报告 ~ 这篇是对事件周期中最热门微博的整体传播分析。 前面的舆情分析中我们发现，此事件在微博上的声量最高，故我们选取了最热门的微博(转发量最大)进行详细的传播分析——@局座召忠【中印边境对峙持续一个多月了…】。该条微博截图如下： 造成微博声量最大的原因：1.微博的“注意力资源”分配极不均匀，少数大V博主拥有庞大的粉丝资源，其发布的内容往往能引起微博上广大民众的讨论，造成极大的影响力；2.用户参与转发或评论了某条微博，每一条转发和评论都被视为一个用户UGC。 可看出，此微博是主战观点，认为不用重拳不足以显我军威。 热门微博信息介绍(1) 微博信息表 Item Detail 时间区间 2017.8.03 17:52——2017.8.28 23:59 总转发数 13962 总评论数 18744 总赞数 70510 覆盖人次 116,991,845 关键传播用户 江宁婆婆(带动再次转发数：387) 截止分析时间，@局座召忠的此条微博共收获转发数13962次，评论数18744条，点赞数70510个。覆盖人次116,991,845，达亿级，包括原创者（@局座召忠）和转发者的粉丝数的叠加，当然也免不了重复计算（亦未排除水军），但可看出整体的传播效果非常惊人。 其中，江宁婆婆是此条微博的关键传播用户，共带起再次转发数387条。 (2) 博主信息表 Item Detail 微博账号名称 局座召忠 微博认证等级 金V认证 粉丝数 762万 微博数 269 个人背景 张召忠，中国著名军事理论家、军事评论家；中国人民解放军海军少将，副军职 ；致力于普及国防教育，通过新媒体与年轻人沟通，在沟通过程中做爱国主义、国防科普。 强大的军事背景和专业知识、十几年从事国防普及教育，这可能是局座召忠具有强大微博舆论影响力的原因。 传播路径传播路径图 由传播路径图可看出，该微博的传播路径以原创节点——局座召忠为中心向外幅散，并经过多个传播节点（江宁婆婆、paingod等）增强扩散力；长尾节点则有很多（多半是被动的信息接收者，传播层级到此戛然而止）。其中，原创节点本身被许多互动节点所包围，形成一个近似实心的圆形，可见该微博的影响力大与其博主本身的粉丝量大有很大关系。该传播路径图形象直观地反映了该微博传播范围之广、幅散能力之强、传播层级之深。 转发层级转发层级可以看出某个微博传播渗透力的强弱，层级越多，代表话题的渗透性和传播性越强，微博粉丝的参与度也就越高。 转发层级详情分析表 层级(9) 转发数 (13962) 转发占比 覆盖人数 核心转发Top3 第1层 10521 75.35% 50991752 (江宁婆婆,387)；(paingod,247)；(老刀99,86) 第2层 1086 7.78% 36803587 (专业戳轮胎熊律师,78)；(默虹_美海军学习小站,76)；(俄罗斯什么值得买,67) 第3层 465 3.33% 13718736 (刘耘博士,37)；(杜长军,29)；(GeniusVczh,26) 第4层 240 1.72% 1528817 (平凡6028527276,22)；(某某某和某某某的,13)；(可是我没有_我以为我会暴富…,11) 第5层 98 0.70% 3000754 (平凡6028527276,21)；(乐乐的小团子QY,12)；(军迷张晓宇,10) 第6层 43 0.31% 1827061 (平凡6028527276, 21);(雨农谈股，9);(我是B型小百搭，8) 第7层 19 0.14% 1467036 (平凡6028527276, 21);(臭流氓调戏村干部，7);(柳橙汁雪梨，5) 第8层 10 0.07% 14257 (平凡6028527276, 21);(松鼠频道道道主，6);(Akira卍大天使加护，4) 第9层 11 0.08% 8521 (平凡6028527276, 21);(海绵_小漾游 ，2);(HelenGhosny良辰，1) 上图是本条微博的转发层级详情分析，可以看到其传播层级达9级，联想著名的“六度空间”理论（你和任何一个陌生人之间所间隔的人不会超过六个），其说明传播的稀缺性；再结合“转发数”、“转发占比”和“覆盖人数”这3项指标来看，这个层级已接近微博转发能达到的极限了。在所分析时间区间里其覆盖人数达1亿多人（116,991,845），可见其惊人的传播速率。 同时，我们还可以看到，传播层级从第1层到第9层，传播者从微博大V逐渐过渡到普通微博用户，其转发量也呈现逐级衰减的趋势，间接反映了传播者影响力由强变弱的趋势。 转发/评论趋势图事件传播有其生命周期，上图清晰展示了该微博转发和评论的随时间变化趋势、微博互动及散播活跃与否、以及处于生命周期的哪个阶段（引发期、酝酿期、发生期、发展期、高潮期、处理期、平息期和反馈期），对于及时、准确研判事件及舆情走向起到至关重要的作用。 上图清晰展示了该微博转发和评论的随时间变化趋势、微博互动及散播活跃度。该微博于08-03 17:52发布后，于08-04 00:00达到转发、评论高峰，转发峰值6006条、评论峰值8256条，此后微博传播速度逐渐降低。 可以看出，转发评论行为总体上呈先增后减趋势，48小时后基本偃旗息鼓；粉丝互动强，散播活跃度高；意见领袖传播集中在微博发出的前6个小时，且主要流量都是由意见领袖带来的，可见微博传播的效率高、速度快。所以政府在微博舆论管控时，需要快速响应，在原创发布后几小时（关键传播时间段）及时研判事件和舆情走向，并及时采取措施。 互动粉丝画像分析转发/评论者地域分析 该条微博转评用户在地域分布上较为集中，转发者主要分布于北京、广东、江苏，评论者主要分布于北京、广东、江苏。 转发/评论者性别分析 上图显示转评用户中男女比接近8：2，男性粉丝占绝大多数，应该是由于男性对军事主题的内容更关注。 转发/评论者兴趣标签 上面两图显示，转评用户对旅游、美食、搞笑幽默、美女、军事、IT数码最感兴趣，可看出转评用户具有鲜明的爱生活、爱搞笑幽默、对军事和IT数码感兴趣等特征，他们转评这条微博，其原因既与“中印边境事件”的军事性质有关，又与“局座召忠”逗比幽默的语言风格有关；至于“军事”、“IT数码”、“美女”标签则与男性用户的兴趣点有关。 粉丝数量区间分布 上图横轴是互动粉丝拥有的粉丝量，纵轴是互动粉丝数量，从粉丝拥有的粉丝量多寡可以看出该粉丝的微博活跃度。其中，粉丝量10-49这个区间的互动粉丝最多，再次是50-99, 100-199这两个粉丝区间互动用户。以及500-999区间的用户活跃度也较高。整体来看，该微博的互动粉丝的质量比较高，出现水军的几率不大。 典型意见挖掘关键词分析 在用户转评内容中，”局座”提及量3097次，成为网友提及次数最多的词，”阿三”排名第二、”印度”排名第三。其中“阿三”指“印度阿三”，指印度人，是“十里洋场”时期的吴语上海话（瘪三、十三点、猪头三），带有种族歧视的贬义意味。从这个词的高提及率可见网友对印度在此事件中的表现的不满、气愤，也与局座的这条微博的“主战观点的引导作用有关。 主流观点分析 对用户转评内容进行文本聚类和典型意见挖掘，得出上图。由图可看出，转评该微博的主流观点都是赞同局座的“主战”观点。 转发/评论表情分析 表情包是现代网民表达情绪的重要方式。从上图可看出，转评该微博的网友的情感态度主要是：看好戏、赞许、高兴。可见该微博的观点说出了大家的心声（不满印度作为，希望我国以军威震慑印度），故得到赞许。 ReferenceWiki.张召忠]]></content>
      <categories>
        <category>Communication Science</category>
        <category>Public Opinion Analysis</category>
      </categories>
      <tags>
        <tag>Public Opinion Analysis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何在hexo中书写LaTex数学公式？]]></title>
    <url>%2F2017%2F11%2F30%2F%E5%A6%82%E4%BD%95%E5%9C%A8hexo%E4%B8%AD%E4%B9%A6%E5%86%99LaTex%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[Abstract：用kramed替代marked，就可以在hexo里愉快写LaTex辣 ~ hexo支持Latex方法操作步骤1.两句命令：卸了marked（hexo原生库不支持Latex），安装kramed库（是对marked的改进） 123$ npm uninstall hexo-renderer-marked --save$ npm install hexo-renderer-kramed --save 2.改theme的config文件里的mathjax的支持为true 1234# MathJax Supportmathjax: enable: true per_page: true 3.hexo clean &amp;&amp; hexo g 4.hexo s:查看静态文件, 看是否能正常显示数学公式 ReferenceKramed插件GitHub]]></content>
      <categories>
        <category>Computer Science</category>
        <category>Coding</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[离散数学：理解图论]]></title>
    <url>%2F2017%2F11%2F21%2F%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6%EF%BC%9A%E7%90%86%E8%A7%A3%E5%9B%BE%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[Abstract: 机器学习中我们希望从数据中挖掘隐含信息或模型，若将图中的结点作为随机变量，连接作为相关性关系，那么我们就能构造出图模型，并期望解决这一问题。而构造这样的概率图模型需要一定的图论知识。本文就总结了图论的基本概念、以及与ML的关系。 图论：以图为研究对象，描述某些事物间的特定关系。由结点与边组成，G = {V，E}。有向边与无向边。有向图与无向图。 树型结构：树是图的一种；从根节点开始，并能和其他结点相连接的图；有向性 ML X 图论：决策树；概率图模型 //马上要考离散了，紧张复习ing… 把离散中的图论与ML结合起来复习，感觉还行 ~ 图论 Graph Theory关键图： 路：开路，回路，真路，链，闭链 连通图，连通子图，分图 欧拉图，哈密顿图 树，生成树，最小生成树，有向树 二部图 平面图 机器学习与图论ML: 从数据中挖掘隐含信息与模型 将图中的结点作为随机变量，边（连接）作为相关性关系，则可构造出图模型。 概率图模型（概率论+图论）是实现这一任务的重要手段 决策树：可表示为给定特征条件下类的条件概率分布。 结点：由表示特征的内部结点和表示类的叶节点构成 决策树的学习包括特征的选择、决策树的生成和决策树的剪枝 树型：包含于图论 图论 数学的分支 以图为研究对象，研究顶点和边组成的图形 图数据结构或树型算法：来源于图论 Loosey–goosey图非线性结构： 最基础特征：数据不遵循特有顺序（至少无明显数值关系），类似数组和链表 树 树型结构：从根节点开始，并能和其他结点相连接； 树由一组规则定义而成：即一个根结点可能连接或不连接到其他结点，但最终所有叶结点或内部结点都能追溯到这个特定的位置。 一些树有更多的特定规则，如二叉搜索树，该树在任意时间内每个结点都只和两个子结点相连 机器学习常用的决策树：可以看成是 IF-THEN 规则的集合；即由决策树的根结点到叶结点的每一条路径构建一条规则，路径上内部结点的特征对应着规则的条件，而叶结点的类对应着规则的结论。 树的有向性：一棵树只能朝一个方向传播，即树型是由有向边（directed edge）构成的；每棵树都是由根节点开始，向下往子节点或叶节点传播；每条路径唯一，且路径上所有子节点有且仅有一个父节点；故树型结构一定不会存在循环结构或链路 图不存在根节点、叶节点、单向边等概念，图中结点可连接多个子节点和多个父节点，路径可有向或无向 有向图（directed graphs）和无向图（undirected graphs） 图的基本原则：有且至少有1个单结点（才能被看做是图啊） 结点和结点之间的连接并没有确切的规则，边（有时候也称为链接）能以任何方式连接结点 边的类型（大多数情况下：有向/无向）是图之间最大、最明显的区别之一。 有向边：规定两结点间只有单一方向，origin——&gt;destination 无向边：两结点间路径双向互通，未固定起始结点和目标结点 有向图：所有边都是有向边；无向图：所有边都是无向边 图（如何定义图？）图：一种正式表征网络的结构，基本上是所有互连对象的集合；结点无层次结构 图与函数的一致性： 函数：二维坐标轴上分布的有序对（x，y）集合 图：（x，y）=（点vertices，边edges）/（v，e）；图的正式数学定义即为：G=（V，E） 有序对（V，E）：由2组对象组成，一组结点，一组边 下图是无向图G={8，12} 下图是有向图G={3，3} 图论的应用1.网络是巨大的图结构 互联网——边、终端——结点、用户——在图中切换（如点击URL来回浏览） 以网页间的切换为例：有向边——只能从一个网页转到另一个；无向边——可在两网页间来回切换 2.社交网络是图结构 微信：大型无向图 微信本身是一个庞大的社交网络图 结点：用户；边：用户间的沟通联系（好友关系） 无向性（理解为信息沟通的双向性）：两个用户间的关系是双向的，可互相传递信息，无起始结点和目标结点的概念 微博：大型有向图 有向性（信息传播的单向性）：用户发微博时博文会在同时间点由用户向粉丝发送，这一过程有方向且不可逆 概率图模型ML:从数据中挖掘隐含信息与模型 将图中的结点作为随机变量，边（连接）作为相关性关系，则可构造出图模型。 概率图模型（概率论+图论）是实现这一任务的重要手段 概率图模型 图论定义：概率图模型是一个包含结点与边的图；结点分为2类：隐含结点和观测结点；边分2类：有向边与无向边 概率论定义：概率图模型是一个概率分布，图中的结点对应于随机变量，边对应于随机变量的相关性关系 给定一个实际问题，我们通常会观测到一些数据，并且希望能够挖掘出隐含在数据中的知识。那么怎样才能使用概率图模型从数据中挖掘这些隐藏知识呢？ 构建一个图：用观测结点表示观测到的数据，用隐含结点表示潜在的知识，用边来描述知识与数据的相互关系，最后获得一个概率分布。给定概率分布之后，通过进行两个任务获取知识：即推断 (给定观测结点，推断隐含结点的后验分布）和学习 (学习概率分布的参数）。 基本图模型的2个类别 贝叶斯网络（Bayesian Network） 马尔科夫随机场(Markov Random Field) 二者的主要区别：采用不同类型的图来表达变量间的关系；贝叶斯网络采用有向无环图（Directed Acyclic Graph）来表达因果关系；马尔可夫随机场则采用无向图 (Undirected Graph) 来表达变量间的相互作用； 图论——详细概念梳理图论是一种表示 “多对多” 的关系 图是由顶点和边组成的：(可以无边，但至少包含一个顶点) 一组顶点：通常用 V(vertex) 表示顶点集合 一组边：通常用 E(edge) 表示边的集合 图可以分为有向图和无向图，在图中： (v, w) 表示无向边，即 v 和 w 是互通的 表示有向边，该边始于 v，终于 w 图可以分为有权图和无权图： 有权图：每条边具有一定的权重(weight)，通常是一个数字 无权图：每条边均没有权重，也可以理解为权为 1 图又可以分为连通图和非连通图： 连通图：所有的点都有路径相连 非连通图：存在某两个点没有路径相连 图中的顶点有度的概念： 度(Degree)：所有与它连接点的个数之和 入度(Indegree)：存在于有向图中，所有接入该点的边数之和 出度(Outdegree)：存在于有向图中，所有接出该点的边数之和 Extensional ResourcesDifference between Trees and Graphs What’s the difference between the data structure Tree and Graph? Applications of Graph Theory In Computer Science: An Overview Graph Traversal Data structures: Introduction to graphs（视频） ReferenceWiki.图论 想了解概率图模型？你要先理解图论的基本定义与形式]]></content>
      <categories>
        <category>Math</category>
        <category>Discrete Mathematics</category>
      </categories>
      <tags>
        <tag>Math</tag>
        <tag>Discrete Mathematics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[离散数学：格与布尔代数]]></title>
    <url>%2F2017%2F11%2F21%2F%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6%EF%BC%9A%E6%A0%BC%E4%B8%8E%E5%B8%83%E5%B0%94%E4%BB%A3%E6%95%B0%2F</url>
    <content type="text"><![CDATA[Abstract：逻辑代数实质是符号逻辑，布尔代数即逻辑代数，核心是类的演算。偏序关系是格的先修知识。当偏序集里的所有子集都有最大下界和最小上界时，称为格。其中有补分配格称为布尔代数（有补，分配，有界）。 布尔代数初导逻辑代数实质是符号逻辑，德摩根与布尔算是逻辑代数的创始人，布尔代数即逻辑代数。 德摩根定律：德摩根定律：一个组(aggregate)的反面(contrary)是各个组的反面的复合(compound)；一个复合的反面是各成分的反面的组合。 1-(x+y)=(1-x)(1-y)1-xy=(1-x)+(1-y)布尔代数外延逻辑extensional logic：类(class)的逻辑 用小写字母表示类或集合，用大写字母表述个体元素 1:万有类，0：空类或零类 xy：x，y中共有元素的集合；x + y：所有元素的集合；1 - x：x的补集；x - y：由不是y的x组成的类；xy = x：x包含在y中(x的外延小于y)；等号：表示两个类的同一性 矛盾律：$x（1+x）= 0$，即A不能既是A又是B xy=yxxx=xx+y=y+xx(u+v)=xu+xv排中律：$x+(1-x)=1$，即任何东西不是A就是非A 1x=x0x=0布尔：延伸—命题逻辑类的演算可以解释为命题的演算: 若x与y不是类而是命题，则xy是x与y的同时肯定；而x+y是x或y或两者的肯定；x=1表示命题x为真，x=0表示命题x为假，1-x表示x的否定。 格与布尔代数偏序关系（偏序是格的先修知识）偏序：设R为非空集合A上的关系，若R是自反的，反对称的，可传递的，则称R为A上的偏序关系，简称偏序。记作$≤$。如{1,2,4,8}是偏序关系。 常见的偏序关系： 整数集合R上的小于等于、大于等于关系 幂集（所有子集）中的包含关系 正整数的整除和整倍数关系 注：一个偏序的逆关系也是偏序。 偏序集：，即集合A与A上的偏序关系 覆盖：是一个偏序集，如果对任何x, y ∈ A, 有x≤y且x≠y, 不存在其他的元素z∈A, 能够让x≤z且z小于等于y即$x\leq y\wedge x\ne y\wedge \left( x\leq z\leq y\Rightarrow x=z\vee z=y \right)$成立，则称元素y覆盖x。 如：{1，2，4，8}中，8覆盖4，但8不能覆盖2（因为存在4∈A, 使x∈4且4≤8） 哈斯图：表示偏序关系的图。以小圈表示元素，如果有x, y∈A, x≤y且x≠y, 则把x的小圈画在y的小圈下面。而且如果y能够覆盖x的话，便在中间连上一条线（这条线的方向默认是从下往上的，所以不需要标注方向）。如： 偏序的重要概念偏序与格的概念联系非常紧密 1.设是一个偏序集合，且有Q⊆，y∈Q。 最小元： 对任意x（x∈Q）有y≤x，则y为Q的最小元，通常记作0 最大元： 对任意x（x∈Q）有x≤y，则y为Q的最大元，通常记作1 极小元： 对任意x（x∈Q）且x≤y，有x=y成立，则y为Q的极小元 极大元： 对任意x（x∈Q）且y≤x，有x=y成立，则y为Q的极大元 2.设是一个偏序集合，并有Q⊆P，y∈P 上界：对任意x（x∈Q）有x≤y，则y为Q的上界，不一定唯一，注意y只是集合中的一个元素 下界：对任意x（x∈Q）有y≤x，则y为Q的下界，不一定唯一，注意y只是集合中的一个元素 最小上界：因为上界不一定唯一，那么Q中所有的上界中最小元即为最小上界 最大下界：因为下界不一定唯一，那么Q中所有的下界中最大元即为最大下界 注意：若某子集有N个“貌似最大下界”，且它们不在一条偏序路径上，则它们不具有可比性，因此没有最大下界。 如上图，A = {2,3}无最大下界！最小上界是6. 格与布尔代数的概念1.格的第一种概念：（用偏序理解）设是一个偏序集，对P中任意元素x和y，{x，y}组成的集合都有最大下界和最小上界，则称为格。 即，所有子集都有最大下界和最小上界。attention:判断格里的子集都是二元的，{x，y}， 不能是{x, y, z}.判断是否为格：当发现找不到两个元素, 使他们没有最大上界和最小上界, 则为格. 反例：下图中{b,c}无最大下界，因为d，e不具有可比性。 格的二元运算符号： x∧y: x与y最大下界（向上开口就是最大嘛） x∨y: x与y最小上界（向下开口就是最小嘛） 格的二元运算满足的运算定律： 交换律，结合律，等幂律，吸收率，无分配律 如果对该格有 a∧(b∨c) = (a∧c)∨(a∧b), 即满足分配律的话, 就是分配格。 2.格的第二种概念：（用代数系统理解）概念：对具有两个二元计算的代数系统，如果和+满足交换律，结合律，吸收率（但是不一定要对+满足分配率），那么这个代数系统构成一个格。 子格： 设构成格, L是S的非空子集, 如果L关于格中的运算∨和∧仍然能够构成格, 那么L就是S的子格。完全格：格的每个非空子集均有上下确界，则成为完全格。 有界格：若格有最小元0和最小元1(不记得了的话往上翻), 则成为有界格, 记作, 完全格必然有最小元和最大元。 补元：有界格中, 对任意元素a∈L, 若存在b∈L, 并且a∧b = 0, a∨b = 1, 则称b是a的补元，补元不一定唯一。 有补格：有界格中，每一个元素都有补元，则称为有补格。 布尔代数： 定义1：如果一个格是有补分配格，那么称为布尔代数。 定义2：设是含有两个二元运算的代数系统, +和满足交换律, 分配率, 同一律, 补元律, 那么就是一个布尔代数。同一律即a 1 = a , b + 0 = b。 补元律即a * a’ = 0, b + b’ = 1。 布尔代数——有补分配格：满足以下3个条件的格 满足分配律 有最小元和最大元 每个元素都有补元 Reference格与布尔代数]]></content>
      <categories>
        <category>Math</category>
        <category>Discrete Mathematics</category>
      </categories>
      <tags>
        <tag>Math</tag>
        <tag>Discrete Mathematics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Machine Vision vs Computer Vision]]></title>
    <url>%2F2017%2F11%2F21%2FMachine-Vision-vs-Computer-Vision%2F</url>
    <content type="text"><![CDATA[Abstract：Machine Vision与Computer Vision同属于AI，外延不同但互相交叉；MV对象是物，CV对象是人，你把摄像头对着人就是CV，对着车间就是MV。本文解释了MV与CV的概念和区别。 Machine Vision vs Computer VisionMachine Vision：更多注重广义图像信号（激光，摄像头）与自动坏控制（生产线）方面的应用 偏重计算机视觉技术工程化，能自动获取和分析特定图像，以控制相应行为 功能：物体定位，特征检测，缺陷判断，目标识别，技术和运动跟踪 Computer Vision：更多注重（2D, 3D）图像信号本身的研究以及和图像相关的交叉学科研究（医学图像分析，地图导航） 采用图像处理、模式识别、人工智能技术相结合的手段，着重对一副或多幅图像的计算机分析 分析：对目标物体的识别，确定目标物体的位置和姿态，对三维景物进行符号描述和解释 主要区别： CV为MV提供图像和景物分析的理论和算法基础；MV为CV的实现提供传感器模型、系统构造和实现手段 CV：主要是对质的分析，如分类识别（图片里是什么，xx是什么）；MV：主要是对量的分析，如视觉测量一个零件的直径 应用场景：MV对象是物，CV对象是人；你把摄像头对着人就是CV，对着车间就是MV。 外延区别：MV与CV同属于AI，外延不同但互相交叉 Reference机器视觉与计算机视觉的区别？]]></content>
      <categories>
        <category>Artificial Intelligence</category>
        <category>Machine Vision &amp; Computer Vision</category>
      </categories>
      <tags>
        <tag>Machine Vision &amp; Computer Vision</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[摄像机，光电转换原理，CCD感光器件]]></title>
    <url>%2F2017%2F11%2F21%2F%E6%91%84%E5%83%8F%E6%9C%BA%EF%BC%8C%E5%85%89%E7%94%B5%E8%BD%AC%E6%8D%A2%E5%8E%9F%E7%90%86%EF%BC%8CCCD%E6%84%9F%E5%85%89%E5%99%A8%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[Abstract：光电转换是摄像机的成像原理，摄像机有镜头、成像件(CCD 或 CMOS)、采样编码这3个主要部分。其中CCD和CMOS都是将光信号转换成电信号的感光器件。本文简要解释了光电转换原理、CCD原理、彩色成像原理。 //完成于摄像与编辑技术课 雾 ~ 摄像机光—&gt;电传感器 摄录一体机 基本构造：镜头，光电转换系统，录像系统，附件（话筒，录像器） 光电系统转换原理摄像的技术本质：光电转换 光电转换：实质是通过光照的强弱，最终影响电信号的大小 以电视机为例：把组成图像的各个像素亮度按一定顺序一个个转换成相应电信号，并依次传送出去；在接收端的荧光屏上再按同样的顺序，将各个电信号在对应的位置上转变成具有相应亮度的像素。 扫描：将组成一幅图像的像素，按顺序转换成电信号以及将电信号依次转换成图像的过程。 传送特点：一条信道，顺序传送 123graph LR A[像素亮度]--&gt;|按顺序转换|B[电信号]B--&gt;|按同样顺序转换|A CCD（Charge-coupled Device/感光耦合组件）及其原理WHAT：一种集成电路，上有许多排列整齐的电容，能感应光线，并将视频转变成数字信号。经由外部电路的控制，每个小电容能将其所带的电荷转给它相邻的电容。 Application：广泛应用在数字摄影、天文学，尤其是光学遥测技术（photometry）、光学与频谱望远镜，和高速摄影技术如幸运成像。 原理：在一个用于感光的CCD中，有一个光敏区域（硅的外延层），和一个由移位寄存器制成的传感区域（狭义上的CCD）。 图像通过透镜投影在一列电容上（光敏区域），导致每一个电容都积累一定的电荷，而电荷的数量则正比于该处的入射光强。用于线扫描相机的一维电容阵列，每次可以扫描一单层的电容；而用于摄像机和一般相机的二维电容阵列，则可以扫描投射在焦平面上的图像。一旦电容阵列曝光，一个控制回路将会使每个电容把自己的电荷传给相邻的下一个电容（传感区域）。而阵列中最后一个电容里的电荷，则将传给一个电荷放大器，并被转化为电压信号。通过重复这个过程，控制回路可以把整个阵列中的电荷转化为一系列的电压信号。 在数字电路中，会将这些信号采样、数字化，通常会存储起来；而在模拟电路中，会将它们处理成一个连续的模拟信号（例如把电荷放大器的输出信号输给一个低通滤波器）。 CCD成像与CMOS成像摄像机有3个主要部分：镜头、成像件(CCD 或 CMOS)、采样编码。 CCD成像 CMOS（Complementary Metal-OxideSemiconductor/互补性氧化金属半导体） CCD成像与CMOS成像的共同点： 原理：都是将光信号转换成电信号的感光器件 表面都是由像点(photosite)组成的矩阵；像点是成像器件的基本单位,当光照到像点上时，像点就能将光转换成电信号；摄像机收集全部像点产生的信号，处理之后生成画面 描述像点数量：水平数量 X 垂直数量 它们的像点只能感受光的强度，不能感受光的色彩 意味着：CCD和CMOS拍到的都是黑白画面，就像黑白胶卷那样 彩色画面的实现： 多片CCD实现彩色画：3CCD：3个相同CCD分别接受红绿蓝三束光）棱镜分光的方式实现分别获取红绿蓝信息 单片CCD实现彩色画：将拜尔滤镜安装在CCD上。每四个像素形成一个单元，一个负责过滤红色、一个过滤蓝色，两个过滤绿色（因为人眼对绿色比较敏感）。结果每个像素都接收到感光信号，但色彩分辨率不如感光分辨率。 像素、像点、显点 像点：CCD的成像单位 像素：画面单位；像素越小，单位面积上的像素数目越多，图像越清晰 显点：显示设备的单位 帧：如25帧/s，100帧播4s ReferenceWiki.CCD感光耦合组件 理解摄像机]]></content>
      <categories>
        <category>Artificial Intelligence</category>
        <category>Machine Vision &amp; Computer Vision</category>
      </categories>
      <tags>
        <tag>Machine Vision &amp; Computer Vision</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AI PM对技术的理解力分析]]></title>
    <url>%2F2017%2F11%2F09%2FAI-PM%E5%AF%B9%E6%8A%80%E6%9C%AF%E7%9A%84%E7%90%86%E8%A7%A3%E5%8A%9B%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[Abstract：厘清基本概念—&gt; 了解技术边界-瓶颈-机会—&gt;引导技术流程-架构-方向，是AI PM对技术理解力的成长路径。懂技术最好，但最关键的还是输出PM的核心价值：定位用户、场景、痛点，找最小产品闭环去验证。懂技术是帮助我们更好设计产品的能力。能力的锻炼最好还是在项目中锻炼，不要纯粹学习知识，易忘。 三个level123graph TDA[厘清基本概念]--&gt;B[了解技术边界-瓶颈-机会]B--&gt;C[引导技术流程-架构-方向] 了解技术边界-瓶颈-机会 能实现什么、不能实现什么、需要多少成本去实现、当前瓶颈是什么、行业最前沿在研究什么、有什么特别有意义/有趣的新突破进展、短期/长期的突破点机会在哪里等等 如果不够了解技术边界，很可能在评审产品方案时，你一句话就让AI研发知道你“并不懂/还是外行”，内心也许会小小的鄙视一下 对技术需要了解到什么程度 因人而异，不必勉强 有技术背景的PM会更受重视 关键是输出PM的核心价值：定位用户、场景、痛点，找最小产品闭环去验证 提升技术理解力，最快最高效的方式，还是实战；通过知识学习，是很慢的，而且很容易跟实际脱节，到头发现白学了；所以最好能在工作中学习]]></content>
      <categories>
        <category>Artificial Intelligence</category>
        <category>AI PM</category>
      </categories>
      <tags>
        <tag>AI PM</tag>
        <tag>Product Manager</tag>
        <tag>PM Ability</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AI PM VS 互联网PM,AI VS 互联网]]></title>
    <url>%2F2017%2F11%2F09%2FAI-PM-VS-%E4%BA%92%E8%81%94%E7%BD%91PM-AI-VS-%E4%BA%92%E8%81%94%E7%BD%91%2F</url>
    <content type="text"><![CDATA[Abstract：传统互联网时代PM的大量涌现是由于流量方式的改变；而AI时代将带来交互方式的改变。本文将从几个点阐述AI PM与互联网PM的区别。 流量方式的改变（流量的场景、定义与价值）—&gt;PM的批量化涌现 AI—&gt;带来交互方式的本质改变 流量方式：点—&gt;线—&gt;面，人获取信息的方式在不断变化 百度开发DUerOS的原因：数据（流量）资源枯竭；开发智能终端—&gt;（每个智能终端连接一个人）机器获取的信息规模变大 AI：人才 + 场景 + 数据 + 算法 AI VS 互联网 互联网是线性结构，而AI是非结构化，不好对信息进行归类。 线性结构时有序元素的集合，元素间一一对应；非线性结构中的各元素不保持在一个线性序列，元素关系比较复杂。 比如拍电影的叙事手法，单线叙事是线性，双线或多线叙事诗非线性。 终端+ 存储空间 +算力 + 领域专业性：把AI比作人的神经元，需要通过终端来进行声光电的信息采集 + 更大的存储空间 + 更强大的计算能力 + 领域专家设计数据模型（把知识灌输到机器中，用可量化的数据模型进行总结，投入算法，再通过机器获取新的经验） AI PM VS 互联网 PM 都需要考虑产品 + 用户 +场景 互联网时代，用户对已知生态圈抱有熟悉感；AI PM要考虑如何设计产品才能让用户很快地适应新的生态圈——新场景有很深的可挖掘性 AI PM的工作复杂度高于互联网 PM： 比如。产品经理的工作产出是一个拳头，那么互联网产品经理的第一步MVP，是做一个目标拳头的mini版，是一个小的、明确的目标，并且可被拆分、倒推、可控。而AI产品经理的第一步MVP，是做一个握拳70%的拳头，还没有完全握紧，需要一点点的聚拢，是一个大概的雏形，从目标、效果、方案等等，都有各种不确定性。 比如从“数据”这个角度来说，从收集（TTS，3个月）、分析（看大量聊天对话数据，才能自己提炼规则feature）、应用（产品早期，数据的价值甚至大过技术模型算法）到测试（产品需求、TE测试、用户使用，数据集都是不一样的，越来越不可控）等等，每个环节都有很大不同。]]></content>
      <categories>
        <category>Artificial Intelligence</category>
        <category>AI PM</category>
      </categories>
      <tags>
        <tag>AI PM</tag>
        <tag>Product Manager</tag>
        <tag>PM Ability</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线性结构 VS 非线性结构]]></title>
    <url>%2F2017%2F11%2F09%2F%E7%BA%BF%E6%80%A7%E7%BB%93%E6%9E%84-VS-%E9%9D%9E%E7%BA%BF%E6%80%A7%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[Abstract:线性结构时有序元素的集合，元素间一一对应。非线性结构中的各元素不保持在一个线性序列，元素关系比较复杂。 线性结构：有序数据元素的集合，数据元素间是一一对应的线性关系，除第一个和最后一个元素外其他所有数据元素都是首尾相连的。如线性表，栈，队列，双队列，数组，串。 非线性结构：各元素不保持在一个线性序列中的结构，每个数据元素可能与0个或N个其他数据元素发生联系。根据联系不同，可分为层次结构和群结构。如二维数组，多维数组，广义表，树（二叉树），图。 如多维数组是由多个一维数组组成的，所以不再是线性结构。 相对应于线性结构，非线性结构的逻辑特征是一个结点元素可能对应多个直接前驱和多个后继。 解决问题的效率与算法的简便程度 + 空间利用率 + 数据的组织方式有关。 评价一个算法的好坏的标准：空间复杂度S(n)：占用存储单元的长度；时间复杂度T(n)：耗费时间的长度，时间与空间都与n有关。]]></content>
      <categories>
        <category>Computer Science</category>
        <category>Data Structure</category>
      </categories>
      <tags>
        <tag>Computer Science</tag>
        <tag>Data Structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[思考|新技术如何助力中国广电行业的发展]]></title>
    <url>%2F2017%2F11%2F08%2F%E6%80%9D%E8%80%83-%E6%96%B0%E6%8A%80%E6%9C%AF%E5%A6%82%E4%BD%95%E5%8A%A9%E5%8A%9B%E4%B8%AD%E5%9B%BD%E5%B9%BF%E7%94%B5%E8%A1%8C%E4%B8%9A%E7%9A%84%E5%8F%91%E5%B1%95-1%2F</url>
    <content type="text"><![CDATA[Abstract：近十几年，广播电视行业发展现状低迷，观众不断流失，人才严重缺失。基于这种现状，变革是肯定的，问题是怎样变革？个人想从从广电作品的完整生命周期的7个环节来考虑，思考新技术如何助力广电的发展。//其实这也是某课的课堂展示作业 w ~ 注：由于本人犯懒，所以下面的文字只是对上图的部分解释，不全。 策划 + 存储1.建立用户大数据平台，深入分析用户的群体分布特征和多样化个性化需求，以用户数据、用户画像作为节目创新和服务创新的重要参考，以做到精准生产 2.通过整合新闻资源和节目内容，整合第三方版权资源和内容，构建出一个统一的媒资数据库，服务于广电媒体的内容生产、产品加工、多平台分发和版权经营 采编 + 制作1.节目制播分离已成常态化。优秀的制作公司根据电视台的特征与节目特色，对节目进行创新，也将更多创意引进电视台内部，形成内容输出的良性循环。同时，制作公司与电视台合力对节目进行包装和推广 2.扩展内容采集和制作的技术 发布1.构建统一多业务支撑的广电网：基础设施层面；因为广电网络是影响广电发展的重要因素，不同业务使用的网络是不同的。如直播所需要的组播网、点播所需要的IP网络、视频会议所需要的专网。构建有线、无线、物联网相结合的广电网络，可支撑多种业务。 2.基于物联网技术提供智慧家庭服务，物联网技术使得电视保住其在家庭中的统治地位。 3.多终端 家庭网关作为用户端网络设备，拥有强大的网络接入功能。家庭网关实现了广电广域网与家庭局域网的融合。 移动端-融合创新的重要渠道；“央视新闻”、“北京时间”、“触电”、“看看新闻Knews”；湖南卫视的“芒果直播”、广东广电的“荔枝直播；短视频也是近两年很火的内容形态 广电发展VR直播的优势：一方面沉浸式体验；另一方面，VR的传播需要占用很大的带宽，而广电刚好具备高带宽的广播通道，因此广电在VR视频行业中有巨大的先天优势。 电视应用ai技术：解放双手，智能语音交互；构建个性化推荐系统去实现自动分类统计用户点播习惯与收视偏好；同时实现多平台的广电市场广告及信息精准推送； 安全管控构建立体化的信息安全体系：通过统一的监管平台实现对内容、业务、平台、网络、终端全方位的监测,并通过对监测数据的多维分析研判+ 智能引擎提升主动预警和辅助决策能力。 运营构建统一的运营支撑体系：做到精细客户管理，灵活的资源调度，精准营销。 Reference中国2016广电行业发展报告]]></content>
      <categories>
        <category>Communication Science</category>
        <category>Medium Study</category>
      </categories>
      <tags>
        <tag>Medium Study</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最全的喜马拉雅FM产品分析报告]]></title>
    <url>%2F2017%2F11%2F08%2F%E6%9C%80%E5%85%A8%E7%9A%84%E5%96%9C%E9%A9%AC%E6%8B%89%E9%9B%85FM%E4%BA%A7%E5%93%81%E5%88%86%E6%9E%90%E6%8A%A5%E5%91%8A%2F</url>
    <content type="text"><![CDATA[Abstract：本文是对喜马拉雅FM的全方位产品分析，分为产品分析部分和内容分析部分。涉及到其商业模式、内容生产模式、文字内容创意特色、个性化推荐机制背后的原理、发展战略等的分析。//其实是一次课堂展示的作业 ~ w 产品部分产品视频 创始人采访短剪辑 产品介绍1.产品定位喜马拉雅FM：一款为音频生产者提供内容生产渠道的，通过平台的各种展现形式把“PGC+UGC”内容传递给用户的，国内最大的，在线移动音频分享平台(移动电台)。用语音形式做知识与内容“大而全”的极致的声音媒体。 2.产品发展史 2012年8月：组建，致力于在线音频分享平台的建设与运营，成为音频领域的YouTube 移动互联网的风口：移动互联网野蛮生长时期，音频是唯一的伴随式媒体;内容创业风口：内容为王的时代;创始团队的老练和多年创业积累的资源:创始人经历（余建军，喜马拉雅FM联合创始人兼联席CEO）——连续创业者，2个虚拟现实项目和1个3D街景项目，在喜马拉雅FM前创业10年.2012年再度出发：围绕移动互联网重新定位——选取音频为切入点，致力于成为音频领域的YouTube。 同时期诞生的产品：微信，今日头条，快手等。 2013.3：APP上线；半年达成千万用户目标 2014.5：突破5000万用户大关，成为国内最大的在线音频分享平台 2015.9：用户量达2亿 2016.12.3：打造国内首个知识付费节——123知识节，24h销售破5000万 2017.10：3.5亿用户，500万名主播，2000多种付费节目，市场占有率73%，人均收听的时长128分钟 3.市场和用户需求用户需求两类用户：听众 + 主播 移动电台App，通过连接主播和听众、满足了两者的需求。 1.听众：便捷低成本地获取音频内容 用户痛点：当双眼被占用的时候，希望解放双手和双眼，自由阅听。 深挖-用户心理：在无法看的时候以听的方式学习，降低内心的焦虑感。 再深挖-内容创业抓住的根本矛盾：有限的生命与无限的资讯矛盾。人的需求没有变化，变化的只是工具和技术。 如：往返于地铁，公交或驾车中的人们；处于运动；，吃饭，洗漱，干家务，睡觉前或入睡时等碎片或休息的时间；当然除了这些还有育儿需求等。 2.主播：便捷低成本地获取听众需求；高效获取收入和名气 高效制作内容；高效上传+管理内容；高效分发内容；高效变现；高效建立粉丝圈主播工作台：UGC、PGC生产的载体，满足平台UGC、PGC主播在喜马拉雅平台上生产、运营的需求。 目标用户目标听众：上班族（职场人士），汽车用户，学生，家庭主妇，分类频道目标听众，不方便收看视频者，对文本信息疲劳者。 目标主播：内容生产者（如文字创作者有扩展平台的需求），专业的电台人，节目人，行业名人，讲书人等。 4.竞品分析竞品对象荔枝FM，蜻蜓FM 竞品市场分析 差异化分析1.目标群体 喜马拉雅FM：用户群体极为广泛，学生党、上班族、有车一族都是它的主要用户群 荔枝FM：小众化（由于内容类型小众化）；年轻、女性用户为主，白领 蜻蜓FM：对音频质量有更高要求的用户（由于PGC）；男性占大多数 2.内容生产模式 喜马拉雅FM：PUGC(PGC + UGC)；加V过滤系统； PUGC模式：1.PGC：(1)拉拢头部KOL;(2)全方位的主播培养机制：专注孵化腰部、尾部KOL,期间孵化了2000位知识网红以及超过10000节付费课程，为主播建立了“录音平台 – 主播学习指导（喜马拉雅大学） – 数据收益分析 – 发展方向”闭环的需求满足。2.UGC：推出全民朗读功能，鼓励用户主动录制音频上传。 用户自制是自媒体时代的利器，它最大限度地给予个体创作的自由。喜马拉雅吸引大量专业人士参与到内容生产中，同时喜马拉雅的平台门槛很低，每一个用户注册后都可录制、上传节目，这吸引了大量用户，极大地提高了普通用户的创作积极性和参与度。 加V过滤系统:在主播的确立上，喜马拉雅建立了一套加v系统，当用户的节目质量达到一定的质和量之后，可申请加V，这对于喜马拉雅平台内容本身的质量建设而言，这套加V系统起到了过滤和沉淀的作用。整体而言，喜马拉雅的内容质量相对荔枝，高了不少。 荔枝FM：纯粹UGC 主打的是“人人都是主播”的概念，主播权限开放给全用户，主播门槛低，每一个人都能够在其web端的主播入口申请开专辑，在其手机客户端录制音频节目上传。 蜻蜓FM:PGC 除直播外，大部分内容都是来自于与传统电台和一些机构团体的直接合作。这使得蜻蜓在三家中，是一个纯粹度比较高的声音聚合类的pgc平台。但直播对用户来说是一种偏向被动接受的形式，减少了用户主动选择的机会；虽有点播但比不上荔枝、喜马拉雅。 3.内容类型 喜马拉雅FM：不被风格化，大平台，海纳百川(全品类覆盖)；内容细分程度高 喜马拉雅为用户提供了海量音频内容，其中也不乏小众和冷门的优质内容，满足不同用户的口味和需求，使每个用户都能找到自己感兴趣的内容。 荔枝FM：风格标签最强烈（文艺，精致，界面复古，小清新，情感电台）；内容的整体质量上却是很糟（因为纯粹UGC）；情感、治愈内容为主 蜻蜓FM:主打直播，有点播 为了体现其直播的概念，甚至你一打开他的网站，声音便随之而来，真的像收音机。 5.社交功能强弱 喜马拉雅FM：全面的社交性，包括关注、评论、点赞、转采和圈子等 本平台社交：植入社交媒体中的虚拟社区功能:当用户在进入喜马拉雅平台之后，在“新鲜事”版块看到自己好友圈好友的全部动态。通过好友的转载内容，用户可在自己相对封闭的好友圈中获得其他用户好友圈的内容。这种分享节目的模式类似于传统广播中听友口碑相传自己喜爱节目的模式，但是在直观性和时效性上都明显优于传统广播。 多平台并进：除了在喜马拉雅用户之间的节目分享方式外，喜马拉雅也建立了与微信平台、新浪微博平台、腾讯微博平台、人人网等多种社交网络平台的交互关系。喜马拉雅用户可将任何一个喜欢的节目分享到微信朋友圈、新浪微博、腾讯微博和人人网上，增强了节目的传播效果。 荔枝FM：只能点赞和转发和社区发帖 蜻蜓FM：封闭，只有关注和一个信息流的实时聊天功能 平台的社交功能：加强UGC主播与听众之间的联系；UGC主播通过互动交流来获取用户需求以改进内容生产；兴趣相投的听众聚集在一起也可提高平台粘性。 5.商业模式 商业盈利模式： 1.广告营收 部分音频节目有广告声音，APP中的游戏中心功能中也有游戏宣传。 优化广告投放方式:喜马拉雅 FM 尝试了通过软性的方式为商家提供广告方案，努力将其与音频节目定位、内容、风格相结合，避免因为广告的介入而影响收听体验。此外，喜马拉雅 FM 在上线之初便把粉丝互动、社群运营放在重要位置，使黏度高的主播、用户都成为了喜马拉雅 FM 的代言者与传播者。在形成一定体量的粉丝、社群规模之后，便具备了基于社交机制开展多元化运营的条件，成为盈利模式创新的重要基点。 2.版权分销 喜马拉雅FM可以将自己的专有内容、原创内容分销给其他公司。 3.订阅服务 喜马拉雅FM一个月的会员开通费用是25元，会员享有多种特权，包括每天获赠一本有声付费书、去除广告声音、购买专辑9.5折等。 4.粉丝经济 喜马拉雅FM上有大量音频主播，用户也可以增送主播礼物。 5.硬件增值 APP中有商城功能，商城中主要有耳机、音响、喜马拉雅随车听。AI音箱：体现喜马拉雅FM在智能家居领域布局的野心。 ++注：这里需要配一张喜马拉雅的硬件图（如其最新开发的AI音响“小雅”）++。 6.付费专辑 如123知识节、66会员日、典型付费专辑、喜马拉雅FM与混沌研习社共同打造的付费音频课程《30分钟口袋商学院》。对123知识节展开讲一下：国内第一个知识付费节，当天24小时销售额突破5000万。 内容部分1.整体内容特色和风格 海纳百川，不被风格化：大平台定位，不愿在其品牌上被贴上一个标签，做的是大平台，什么样的内容，什么样的风格均有。（淘宝） 人格化：有背书 干货多：有内容 生活化、轻松化和泛娱乐化（自媒体时代的受众需求） 2.文字创意特色分析流程根据用户接触喜马拉雅FM的先后逻辑来分析其文字创意特色： 产品slogan——推送和小广告——专栏名称——专栏简介——音频标题——音频本身内容 产品slogan和广告语slogan： 4亿用户的选择，海量音频任性听: 国内领先音频分享平台，随时随地，听我想听 听，见真知 动口不动手，想听啥都有 没有一本书的有声图书馆 堵车无聊，听喜马拉雅；家务枯燥，听喜马拉雅；晚上失眠，听喜马拉雅 文字创意特点分析： 聚焦于产品的核心特点：聚焦于“听”这个字眼，强调喜马拉雅FM内容的丰富性与高度的伴随性,朗朗上口 摆用户量，利用用户从众心理 4亿用户的选择//国内领先 将己方角度扭转至用户视角 我们对世界的认知都是基于“自我”（self-concept）构建的，可想而知，如果我们想让用户像溜滑梯般轻松的解读文案，那我们就必须将“自我视角”语言转化成“用户视角”语言。也是基于同样的理由，对于大部分人，撰写文字时的直觉习惯也是基于“自我视角”出发的，这样表达出来的内容在用户看来，往往是很难引起他的“注意”。 例子：来自泰国的乳胶枕，果冻般的质感（自我视角）——一夜沉睡，找回婴儿般的肌肤（那我们从用户视角看，睡眠不好可能会导致哪些问题？身体疲乏、精神不集中、加速衰老等等。所以，如果产品的核心目标用户是女士，我们可以从用户最在乎的问题——“影响外貌”入手思考文案） 任性//随时随地，听我想听//想听啥都有//有声图书馆//用户场景 场景化：暗示行为，告诉用户什么时候该使用这款产品，然后再暗示提醒你赶紧使用 暗示产品的使用行为，其实就是给给产品定位。定位的好处是什么呢？这就像有了一个标签，有了细分。能让顾客快速在万千商品中找到你，就像图书馆的图书编码一样。 六个核桃：用脑时刻，喝六个核桃。//吃完喝完嚼益达//今年过节不收礼，收礼就收脑白金。 用「比较」来凸显产品,而非量词、数字 比较——找到“锚定物”作为参照，然后凸显自身产品的优势。没有一本书的有声图书馆（以图书馆为锚定物） 贴标签 一旦告诉我们某个新事物的类别，对“类别”的特征记忆就能迅速的降低大脑的认知门槛，同时产生初步的“预期”。 有声图书馆。 推送和小广告 平台广告：直播 + 精品 + 分类 形式：图文搭配 文：大号字体的文字清晰传达广告目的（不超过15字） 如“我要上首页”，“喜马拉雅直播绿色公约——从我做起” 图：宣传对象——主播，书封面，其他对象 ++注：这里需要配一张主页的轮播图的截图。++ 文字创意特点： 名人吸睛法,以权威吸引人 凭借知名人士与推荐音频的关系，在音频推荐上刻意表现该名人与音频的相关程度，以达到吸引特定用户的目的。主讲人的名气本身就是吸引用户的一大特色。如于晓非金刚经导读，窦靖童星球会客厅。 站在用户角度 让用户感觉更亲切，更有代入感，以此吸引用户点击。如图3“天冷了，听这些歌暖暖吧”，图7“科学家爸爸讲故事：……” 适当夸张，引发读者的点击欲。 “火爆开抢”，“那些插曲只应天上有”，“有人颠覆世界”。 蹭热点大法。 精品课程本身就具有一定的吸引力，而这样的精品推送再融入当下的时事热点，使其传播效果成倍增加，手段十分高明。如《致剁手族：双十一如何不花冤枉钱》 直指利益 比如精品音频推荐几乎很少细节描绘，而直截了当地告诉用户“这个音频讲的是什么”，“点进去你能得到什么”，很真诚，很干货。 专栏名称文字创意特点： 专栏性质 + 主题 + 人物名 清华名师肖星的财务课;李开复：十堂人工智能课;王玥波说聊斋 数字型标题，引发用户兴趣 如：十秒钟学会说英语，3招速成英语发音等，靠数字吸引眼球； 用户视角 如：我知你心：你的故事我来解读；年糕妈妈：宝宝怎么带更聪明(告诉用户这款音频栏目到底有什么用)；小学问：如何在职场快速获得认可；一个人用英语去旅行； 场景化 暗示用户去养成这个习惯，如每天一个成语故事，每天听见吴晓波 音频简介 关于音频简介：一般决定了是否听众愿意进一步了解此音频。 引经据典 引用大众熟悉的概念，而又对其有一些创新，从而引发听众兴趣。真实三国，不需要演义;一个吃人的世界，狂人总行狂妄事;大内密谈，一个努力脱离低级趣味的电台;一日之计在于晨，新闻早餐不能少 将该音频主播能力或成就为噱头，暗示听众音频内容的质量上乘 看从5美元到一亿美元投资奇迹 采用诙谐有趣的语言，引起用户兴趣 君臣智斗，爆笑开演;小眼看世界，说学逗唱我最萌; 洞察人性，打情感牌，戳痛点 别以为你情能自禁;有些爱，止于唇齿 音频标题 抓特定群体的痛点，提问句 能不能问你几个问题;唱起毕业歌我们挥别了什么？ 这些校园民谣你都会唱吗?你们怎么好意思向粉丝要账?你真希望她过得比你好？ 要不要逃离北上广？ 工作中遇到讨厌的上司怎么办？避孕套是怎么来的？你是看上了人，还是看上了钱？工作应该怎么选，要高新还是凭喜好？感觉人生道路选错了，坚持还是放弃？大学打工到底有没有用 ？ 露大腿不露底裤 他为了吃去到这里，却被风景美哭.谢谢你们，给我的感动.三大奸臣干过的那些你不知道的事。原来各省的名字是这样来的。自带热搜体质，人人艳羡的高福利国家。 语气诙谐,口语化，趣味性强 说好一起胖到老，你却偷偷瘦三斤;我们都是路人甲，但我们不是炮灰；不要！二郎神怎么会是太监！ 放开我，我要和麻辣火锅结婚！洗澡的时候我们都是戏精。 音频本身的内容 用BGM渲染气氛 大多是抒情缓慢的歌曲，不会喧宾夺主，但又能起到很好的渲染气氛的作用。如《灰色轨迹》——Beyond，《涛声依旧》——辛晓琪，《红玫瑰》——陈奕迅，《心肝宝贝》——刘德华。 语速语气语调跟节目类型有关 人文历史类语速轻缓、语气柔和，资讯快餐类语速较快，新闻播报类语速偏快，语调基本上没有波动。 内容的时间安排上详略得当 如《新闻早餐》：十几秒的开场白之后，是一分钟左右的头条播报，之后的每条新闻不超过20秒，最后是十几秒的结束语。 话题新鲜，语言风格诙谐幽默(经常在解说时卖萌耍贱开玩笑) 如《“大力”史》：《古人是如何玩“自媒体”的？》《谁是中国史上最牛的伯乐？》《“piapiapia”——看看历史上哪些名人被“打脸”》等。语言表达和用词很活泼幽默，比如在“士别三日当刮目相待”这个典故里，把孙权称作吕蒙的老板，用孙权的口吻喊吕蒙“蒙蒙”。 如《李峙FM》中解说《红玫瑰》的“得不到的永远在骚动，被偏爱的都有恃无恐”时，提到用表情包表达这句话就是“一边的人嘤嘤嘤，另一边的人hiahiahia”。/提到《围城》“城里的人想出去，城外的想进来”时，调侃说“不如住在城乡集合部，进去方便，出来也方便”。/提到许秋怡在专辑封面上的一张照片，因为拍摄得不是很好，显得脸大，被李峙调侃成“也是巴掌脸，只不过是两个三个巴掌”。 3.案例创新根据以上总结的内容创意方法，自己设计了一个音频专栏： 女生睡前卧谈会 4.内容智能推荐和分发机制 每个产品都有衡量自己体验的关键指标，而对于视频/音频这类的产品来讲，最能代表的体验核心指标就是「活跃用户日收听时长」，它代表了这款产品到底占用了用户多少时间，多少的忠诚度。 喜马拉雅FM的用户使用时长长期领先对手，但在2015Q4阶段仍实现明显提升（日均收听时长达103min）。这一增长其上线的“猜你喜欢”功能密不可分，而它的背后则是一整套基于大数据的个性化推荐系统。 1.猜你喜欢 喜马拉雅FM是音频界最早启用智能推荐系统的平台，基于用户兴趣图谱进行个性化推荐。 2.用户兴趣图谱 用户操作行为记录 用户的每一次点击和搜索，以及其他各种行为，都会被记录下来产生数据，然后再基于年龄、性别、地域、职业等维度建立用户兴趣图谱。 对平台的意义:个性化内容推荐、广告投放、商品推送，被主播及运营团队用于节目内容的质量判定，同时平台也可对数量庞大的主播进行优劣筛选，构建主播自动孵化体系。 对用户的意义：尊重用户个性；降低用户使用成本（相比文字，音频需要试听），用户迅速找到自己想听的节目；伴随性媒体，实现不同场景的精准推荐（睡前，起床，堵车）；主播对听众进行精准定位；总的来说是大幅提升了平台的撮合效率。 3.推荐的本质 搜索：用户主动寻找内容 推荐：系统主动推荐给用户(从人工筛选到机器算法推荐) 推荐和搜索本质有相似的地方。搜索满足用户从海量数据中迅速找到自己感兴趣内容的需求，属于用户主动获取。推荐则是系统从海量数据中根据获取到的用户数据，猜测用户感兴趣的内容并推荐给用户，属于系统推荐给用户。本质上都是为了在这个信息过载的时代，帮助用户找到自己感兴趣的东西。 4.推荐系统 5.推荐算法 基于内容：基于内容的推荐算法，是将item的名称、简介等进行分词处理后，提取出TF-IDF值较大的词作为特征词，在此基础上构建item相关的特征向量，再根据余弦相似度来计算相关性，构建相似度矩阵。 如你看过《36氪商业情报局》，则系统会关联数据库中《36氪商业情报局》的信息。系统会推荐36氪的其他专栏，如《36氪创业谍报》。如果这个音频系统的数据被很好地分类，那么推荐系统也会给用户推荐这个分类下的其他作品。如果《36氪商业情报局》被归为IT科技作品，那么可能会推荐其他IT科技作品，比如《虎嗅商业评论》等。 基于内容的推荐好处在于易于理解，但是坏处是推荐方式比较依赖于完整的内容知识库的建立。如果内容格式化比较差，那么基于内容的推荐就无法实行。同时如果用户留下的数据比较少，则推荐效果很差，因为无法扩展。 基于用户的协同过滤（user-based CF）：通过用户的历史数据来构建“用户相似矩阵”和“产品相似矩阵”来对用户进行相关item的推荐，以达到精准满足用户喜好的目的。 比如亚马逊等电商网站上的“买过XXX的人也买了XXX”就是一种协同过滤算法的应用。 协同过滤（（Collaborative Filtering））大致可分为两类：一类是基于邻域的推荐、一类是基于模型的推荐；邻域方法是使用用户对已有item的喜爱程度来推测用户对新item的喜爱程度。与之相反，基于模型的方法是使用历史行为数据，基于学习出的预测模型，预测对新项的喜爱程度。通常的方式是使用机器学习算法，找出用户与项的相互作用模型，从而找出数据中的特定模式。 基于内容的协同过滤（item-based CF） 基于内容的协同过滤会分析系统已有数据，并结合用户表现的数据，对该指定用户对此信息的喜好程度预测。通过用户对不同内容的评分来评测内容之间的相似性，基于内容之间的相似性做出推荐；最典型的例子是著名的“啤酒加尿布”，就是通过分析知道啤酒和尿布经常被美国爸爸们一起购买，于是在尿布边上推荐啤酒，增加了啤酒销量。 基于标签：关于tag和分类，基本上是互联网有信息架构以来就有的经典设计结构。内容有标签，用户也会因为用户行为被打上标签。通过标签去关联内容。 标签查找的方法这里有很大可以发挥的空间，比如，通过知识库进行处理，或者语义分析处理。而对于一些设计之初就有标签概念的网站， 就比较容易，比如豆瓣和知乎。对于知乎而言，公共编辑的标签是天然的标签内容，对于知乎的用户而言，浏览回答关注等行为则是天然的用户标签素材。 流行度推荐算法:对item使用某种形式的流行度度量，例如最多的下载次数或购买量，然后向新用户推荐这些受欢迎的item。就和我们平时经常看到的热门商品、热门推荐类似。 混合推荐算法：混合推荐算法很好理解，就是将其他算法推荐的结果赋予不同的权重，然后将最后的综合结果进行推荐的方法。 举例来说，比如上述已经提到了三种方式，协同过滤算法中的基于用户和基于item的协同过滤推荐，和基于内容的推荐算法；而混合推荐算法中是将这三种推荐结果赋予不同的权重，如：基于用户的协同过滤的权重为40%，基于item的协同过滤的权重为30%，基于内容的过滤技术的权重为30%，然后综合计算得到最终的推荐结果。 猜测：混合推荐算法（基于内容 + 基于用户的协同过滤 + 基于内容的协同过滤 + 场景） 6.机器 vs 人工算法非万能，需要人不断调整修正算法模型，以及人工对算法结果进行有效干预。 123456789101112//以淘宝的个性化推荐遍布全站的前后为例：使用前的人工工作：- 需要大量的运营、文案编辑去填写补充布局（后被替代）使用后的人工工作：- 运营：双十一怎么运营，什么营销测流；怎么更好的利用个性化推荐引擎，抓住用户眼球- 模型设计：大量人工的标注和边界策略以保证个性化推荐引擎的有效性；通过人对业务的理解不断修正模型；人对业务的理解形成的外围策略，构成了模型的边界.总结：算法非万能，需要人不断调整修正算法模型，以及人工对算法结果进行有效干预。 5.未来发展规划1.构建完整的音频生态圈 音频生态上游：提高IP能力。在版权方面，继续加大版权签约力度和平台建设；内容制作方面，加快完善有声自媒体孵化体系，围绕内容创业者建设内容生产体系，并促进商业变现让平台主播获得更多收益。 音频生态中游：提升大数据技术。收集更多维度数据，建立用户兴趣图谱，用于内容质量判定、主播自动孵化体系，以及个性化内容推荐、广告投放、商品推送等各种用途。 音频生态下游：继续开拓硬件分发，建立手机+车+智能硬件的音频分发平台。加大与科大讯飞等智能语音技术公司的合作力度；完善开放平台，目前已与几百家硬件公司有合作关系，未来数量将进一步增加。 车联网：1、与整车厂合作，预装进车，如宝马BMW互联驾驶。2、自建硬件，发布车载声音盒子“随车听”。3、智歌车机，以后装车机的预装形式来达成音频内容的用户到达。 2.致胜场景化：家庭有声图书馆（打造全方位语音智能家居） （用户通过语音控制实现智能家居、智能穿戴的音频播放，实现音频的时刻相伴）]]></content>
      <categories>
        <category>Product Manager</category>
        <category>Internet &amp; Product</category>
      </categories>
      <tags>
        <tag>Internet &amp; Product</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[终端是什么]]></title>
    <url>%2F2017%2F11%2F07%2F%E7%BB%88%E7%AB%AF%E6%98%AF%E4%BB%80%E4%B9%88%2F</url>
    <content type="text"><![CDATA[Abstract：终端是人与机器交互的接口。 终端的解释Terminal：unix/linux中称使人与机器可交互的接口为终端。有两个基本功能：向主机输入信息和向外部输出信息。一般通过线路连接到主机。 12计算机 = 主机 + 终端终端 = 输入设备 + 输出设备 类比人：人类的终端是人类与外部世界进行信息交流的接口。输入设备包括感知器官，即眼睛、耳朵、嘴巴、鼻子等。输出设备包括嘴巴(发出声音)、四肢(改变外部世界)等。 Console：控制台，用于管理主机。 Reference什么是终端]]></content>
      <categories>
        <category>Computer Science</category>
      </categories>
      <tags>
        <tag>Computer Science</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELIT-NLP研究平台测试]]></title>
    <url>%2F2017%2F11%2F07%2FELIT-NLP%E7%A0%94%E7%A9%B6%E5%B9%B3%E5%8F%B0%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[Abstract：ELIT是一个新开放的NLP研究平台，目前提供情感分析工具，使用CNN模型。小小地测试了下这个平台，目前功能还少，不过以后估计会增加吧。 介绍AWS与埃默里大学合作开发的基于云的NLP研究平台ELIT 端到端的NLP pipeline 提供网页API将平台独立，任何人可通过网络连接请求在其上大规模计算 功能 ELIT SENTIMENT VISUALIZATIONELIT SENTIMENT VISUALIZATION CNN模型进行情感分析： 12345graph TDA[create matrix通过叠加每个词的词向量创建输入矩阵]--&gt;B[input matrix into convolution and pooling把输入矩阵送入卷积层和池化层]B--&gt;C[match the output with attention matrix将输出与attention矩阵匹配，测量输入文档中每个n-gram的强度]C--&gt;D[couple the output back to softmax layer模型将attention输出反馈给softmax层]D--&gt;E[softmax layer judge the rate of positiveness and negativeness and neutrality判断文本中词语正负中的概率] CNN-Sentiment Analysis 功能：单词化—&gt;分割—&gt;情感分析。 操作步骤ELIT SENTIMENT VISUALIZATION ELIT tutorial 1.输入文本 2.analyze（输入文本被发送到所选的运行NLP pipeline的ELIT服务器） 3.文本被用情绪编码标注，红-消极，绿-中立，蓝-积极；不同透明度（opacity）代表情感含义的强烈程度；可将强度等级可视化，圆圈越大，情感越强烈;如： 测试结果图： 注：测试发现情感标注效果并不算好，机器仍不能理解语义，只是做单纯的分词和按规则、语法标注。 python获取输出结果1234567891011import requestsr = requests.post(&apos;https://elit.cloud/public/decode/&apos;, data=&#123; &apos;text&apos;: &apos;I went to the cinema with my boyfriend last night. The movie is good, even better if the actor could perform better.&apos;, &apos;input_format&apos;: &apos;raw&apos;, &apos;tokenize&apos;: &apos;1&apos;, &apos;segment&apos;: &apos;1&apos;, &apos;sentiment&apos;: &apos;mov, twit&apos;&#125;) print(r.text) 以网络请求方式调用网页API，结果以json格式输出。 解码框架 解码请求通过弹性负载平衡器（Elastic Load Balancer）确保其可伸缩性。 ReferenceELIT SENTIMENT VISUALIZATION ELIT tutorial 项目介绍 代码地址 AWS官方博客介绍地址]]></content>
      <categories>
        <category>Artificial Intelligence</category>
        <category>Natural Language Processing</category>
      </categories>
      <tags>
        <tag>Artificial Intelligence</tag>
        <tag>Sentiment Analysis</tag>
        <tag>Natural Language Processing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阿里AI鲁班技术原理详解]]></title>
    <url>%2F2017%2F11%2F07%2F%E9%98%BF%E9%87%8CAI%E9%B2%81%E7%8F%AD%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Abstract:阿里双十一祭出AI设计师鲁班大人，具有=p6设计水平。本文重点介绍鲁班的技术原理、设计师与鲁班的合作、以及对设计师职业未来的思考。//做专业领域的超能AI(监督学习)，一定需要专业的资深人士设计数据模型；要想AI进化，需要不断喂给AI标注的数据（把设计数据化）且评估输出结果。数据量级非常重要！//三大模块：风格学习 + 行动器 + 评估网络。//关键部分：领域研究 + 数据链路 +算法框架。 介绍阿里鲁班AI设计师担任双11的banner设计，数量高达4亿张，平均每秒完成8000张。 原理123graph TDA[领域研究]--&gt;B[数据链路]B--&gt;C[算法框架] 领域研究：领域专家设计数据模型，即找到该领域专家深入研究该领域的经验知识，构建一套机器可以学习的数据模型。视觉设计专家把设计问题抽象成“风格-手法-模板-元素”这样一套数据模型，即把多年视觉设计经验变成机器可学习的“数据”。 数据链路：定义好数据模型——&gt; 抓取和标注数据—&gt; 对数据集进行分类和管理 在这个过程汇总，如果处理数据给算法训练的更新频次，用什么数据去验证模型，如何评估模型效果，离线模型与在线数据在产品端如何打通？这一系列的数据问题就需要一套清晰的数据链路设计。 算法框架：算法框架由算法科学家来制定，数据和算法的关系就像汽油和发动机，两者密不可分。产品设计师需要与算法讨论，把业务场景和数据问题输入给算法。 三大核心模块 1.风格学习模块（规划+元素）： 对大量设计素材的数据集进行结构化标注——&gt; 输入深度序列规划网络—&gt;输出空间+视觉的设计框架 标注：让机器理解该幅设计有哪些元素组成，比如它的商品主体，花的背景，蒙版；定义设计手法和风格，手法指这些元素为什么可以这么构成，风格指当这些元素构成之后，它从美学或者视觉角度看是一个什么感受，让机器知道它是用什么组成。 训练：准备设计的原始文件，比如一系列花朵和设计方法，输入到深度学习网络中。该网络具备一定记忆功能，可以记住设计步骤中复杂的过程。经过这层神经网络学习之后，会得到一个设计框架。从技术上理解，它是一堆空间特征和视觉特征构成的模型。从设计师的视角来看，它相当于设计师脑里在做一组设计之前那个大概的框架印象。 分类器操作：提前收集版权图库，以及自己造设计元素的方式，输入到元素分类器中。分类器会把这些元素分布到各个类型里，比如背景、主体、修饰，也会完成图片库的提取。 2.行动器：批量输入元素，由元素分类器进行学习，按照视觉特征和类型分类 行动器的作用：根据需求选取设计框架，并从元素中心选取元素，遍历状态空间，规划出多个最优生成路径，完成图片设计； 强化学习：行动器会在不断试错中更聪明、更智能 3.评估网络：对输出产品评分;智能生成的结果经过评估网络，对结果进行打分并反馈给神经网络 工作原理：输入大量的设计图片和评分数据，经过训练后，让机器学会判断设计的好坏。 技术挑战1.缺少标注数据：今天所有的人工智能都基于大规模结构化标注数据，设计这件事情连数据都没有完成在线化，更别说标准化、结构化的数据。 2.设计的不确定性：设计是个很不确定的东西，设计需求把握和结果评估都存在人类主观意识。比如你无法给机器输入“高端大气的海报”这样的指令。 3.无先例可循 注：鲁班做的AI是可控的视觉生成。可控，指的是根据商业的需求、业务的需求，智能地进行控制；视觉生成，则表明鲁班解决的是视觉从无到有的问题。 设计师的未来鲁班：P6设计师水平 p4有被替代的风险 训机师的出现： 为鲁班的进化提供规模更大、更丰富的数据，并且对于很多风格相关的事情实现“结构化数据”的转换。现在的阿里设计师，变成要去学习鲁班系统，学习如何训练机器，同时在美学方面做把控。 鲁班的基础是来源于设计师的设计模板素材和元素素材，因此会有两个设计师角色每天去训练鲁班，一个负责帮助鲁班完成最新的风格学习（风格学习），让鲁班不断进化，不断掌握更好的设计技巧。另一个的角色则是对鲁班设计出来的成果进行评估（评估网络），告诉鲁班什么样的设计才是最好的。 设计师的核心职责，在于把设计变成数据化。 鲁班从0到P6，也是设计师+算法工程师的合作成果 Reference详解阿里海报设计AI“鲁班”]]></content>
      <categories>
        <category>Artificial Intelligence</category>
        <category>AI Application</category>
      </categories>
      <tags>
        <tag>Artificial Intelligence</tag>
        <tag>AI Application</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TTS语音合成技术综述]]></title>
    <url>%2F2017%2F11%2F06%2FTTS%E8%AF%AD%E9%9F%B3%E5%90%88%E6%88%90%E6%8A%80%E6%9C%AF%E7%BB%BC%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[Abstract:TTS是文字转语音。实现原理是从文本中攫取足量信息用于语音合成，再生成波形。主要方法有：拼接法，参数法，混合法和神经网络端到端学习。 核心概念ASR(Automatic Speech Recognition):语音转文字； TTS(Text-T0-Speech)：文字转声音，如siri. TTS: 实现人机语音交互，建立一个有听和讲能力的交互系统所必需的关键技术;是将计算机自己产生的、或外部输入的文字信息转变为可以听得懂的、流利的口语输出的技术。 实现方法输入是文本，输出是波形(waveform)。 1.从文本中攫取足量信息用于语音合成：一套语言学标注系统，分词——文本转换为单词串成的句子——给句子标注音素级别（上一个音素／下一个音素）、音节级别（单词的第几个音节）、单词级别（词性／在句子中的位置）等对语音合成有帮助的信息。 2.生成波形：有2种思路；一种是拼接法（做语音库，再从库中找合适的speech unit拼接）；另一种是做语音库，用统计模型学习到每个音到底怎么发的，再根据学出的特征复原。 语音库:是大量文本和其对应音频的 pairs。为实现更精细的语音合成，需要用语音学标注系统自动标注一遍文本，再用类似语音识别的工具得到音素和音频时间上的区分，即得语音库里的每个音素在音频的起止时间(即音素本身的waveform)以及对应的语言学标注。 TTS语音合成技术的主流方法流程解析： A.拼接法定义：用语言学标注系统跑一遍输入文本，得到一串语言学标注。然后从中选取所需的基本单位拼接而成(单位最好在语言学和声学特征上都类似)。单位可以是音节、音素等。为追求合成语音的连贯性，也常使用双音子从一个音素的中央到下一个音素的中央）作为单位。 音节：音位组合构成的最小的语音结构单位。如汉语中一个汉字一般一个音节，每个音节由声母、韵母两个部分组成[1] 。汉语普通话中的无调音节（不做音调区分）共有400个音节。在英语中一个元音音素（音素不是字母；a e i o u共五个)可构成一个音节，一个元音音素和一个或几个辅音音素结合也可以构成一个音节。 音素：phoneme，是语音中的最小的单位，分为元音、辅音两大类。如汉语音节 ā（啊）只有一个音素，ài（爱）有两个音素，dāi（呆）有三个音素等。英语国际音标共有48个音素，其中元音音素20个、辅音音素28个。 严格来说没有声码器部分；或者说其声码器就是直接选取原声片段信息。 优点：语音质量较高，听起来比较自然。 缺点： 数据库要求太大，需要保存大量原音信息（一般需要几十个小时的成品语料）。企业级商用的话，需要至少5万句，费用成本在几百万元。 若库里音素切分出错、语言学标注出错，则最后合成的语音发音也会出错。 B.参数法1)定义：文本抽象成语音学特征，再根据统计模型学习出从语音学特征到其声学特征的对应关系，再从预测出的声学特征还原成waveform的过程。//或者说，根据统计模型来产生每时每刻的语音参数（包括基频、共振峰频率等），然后把这些参数转化为波形。主要分为3个模块：前端处理、建模和声码器。 核心：是个预测问题，即根据学习得的东西预测出声学特征，然后还原成波形；目前主流是用神经网络来预测。 统计模型学习：用统计模型学习每个音到底怎样发，再根据学出的特征复原声音。 基频:指一个复音中基音的频率。在构成一个复音的若干个音中， 基音的频率最低， 强度最大。 基频的高低决定一个音的高低。 平常所谓语音的频率， 就是指基音的频率共振峰频率:元音和响辅音声谱包络曲线上的峰巅位置。共振峰的本义是指声腔的共鸣频率。 前端：解析文本，决定每个字的发音是什么，这句话用什么样的语气语调，用什么样的节奏来读，哪些地方是需要强调的重点，等等。常见的语气相关的数据描述包含但不限于下面这些：韵律边界，重音，边界调，甚至情感。 声码器：作用是复现声音信号，难在重现声音细节，且让人听不出各种杂音、沉闷、机械感等。目前常见的声码器都是对声音信号本身作各种理论模型及简化假设，而对细节的描述近似于忽略。 声码器是目前主流语音合成系统的一个核心竞争力，因为最终声音好坏与之有直接关系。 注：拼接法和参数法，都有前端模块，拼接和参数的区别主要是后端声学建模方法的区别。 2)优点：数据库要求相对较小。 若只需出声（做demo），大概500句即可，但效果不行； 通用TTS一般需要至少5000句，6个小时（一般录制800句话，需要1个小时）； 从前期的准备、找人、找录音场地、录制、数据筛选、标注，最终成为“可以用的数据”，可能至少需要3个月。 个性化TTS：大多用“参数”法 3)缺点：质量比拼接法差一些（主要由于输出的是声码器合成的声音，而声码器忽略对细节的描述，有损失）。 deepmind提出的wavenet：前端无改进，无声码器，相当于把建模和声码器二合一，通过建模手段直接输出声信号。其质量提高的原因在于：直接对语音样本进行预测，不依赖任何发音理论模型，使最后出来的音质细节十分丰富，基本达到与原始语音类似的音质水准。 今年开始火起来的end-to-end的TTS建模方法，加上wavenent的声码器思想，才是未来TTS的发展方向。 C.混合语音合成解决方案融合参数法和拼接法二者的长处，用基于参数的语音合成系统预测声学上最匹配的音素后，再从库里把它找出来。 D.神经网络端到端学习 1.用神经网络直接学习文本端到声学特征端的对应关系（即省去语言学标注输入文本这一步） 2.用神经网络直接学习语言学标注端到帧级别的waveform端的对应关系（即省去学习已标注文本的语言学和声学特征的对应关系，以及声码器复现声音信号的步骤），如wavenet E.未来：是否能实现直接从纯文本端到帧级别的waveform的对应关系？ 瓶颈：若直接从字素到音频，模型可能无法自动修正错误拼写问题，故目前字素-音素-音频是必须的。 More Questions1.语料库怎么筛选文本才能让语音合成系统效果最好？2.语料库标注质量怎么保证？3.选哪种语音单位作为合成的最小单位？4.语言学标注系统怎么实现？5.到底要标注几层语言学标注才算足量？6.对于拼接法，从库里找匹配的音素时，怎样算和目标音素语音学上匹配？怎样才算和目标音素声学上匹配？最后搜索的时候，怎么定义这两方面都最相似？7.对于参数法，得到语言学标注后，到底需要哪些特征向量？特征向量怎么最优表示？同理，到底需要哪些声学特征向量？怎么最优表示？怎么获得静态声学特征的同时也捕捉动态声学特征？训练预测模型的时候，用什么神经网络结构比较好？用什么声码器？ Baidu DeepVoiceDeepVoice：实时语音合成神经网络系统(Real-Time Neural Text-to-Speech for Production) 基于传统TTS流程，DeepVoice采用深度神经网络和更简单的词性取代原有的转换方法，则系统可兼容所有数据集、语音文件甚至从未涉猎的领域(最大优势是能实时转换，比wavenet快400倍)；系统由5部分组成： 用于定位音素边界的分割模型：提出CTC损失（connectionist temporal classification）实现音素边界检测 用于字素转音素的转换模型 判断音素能持续多久的预测模型 基频预测模型 音频合成模型 现状：需要借助一个音素模型和音频合成组件，未来希望能实现end-to-end语音合成，无需复杂合成流程和依赖手工设计特征的输入或预训练文本。 TTS的目标1.可理解性（intelligibility）：音频的清晰程度，特别是听者能在多大程度上提取出原有的信息； 2.自然感（naturalness）：追求的是与可理解性相对的层面，即听者听懂句意的程度、全句的风格一致性，还有地域或语言层面的差异程度等。 TTS的评判标准1.主观测试 A.MOS(Mean Opinion Scores):专家级评测；1-5分，5分最佳。 B.ABX：普通用户评测，让用户来视听两TTS系统，对比优劣。 2.客观测试 A.对合成系统产生的声学参数进行评估，一般是计算欧式距离等(RMSE，LSD) B.对合成系统工程上的测试：实时率(合成耗时/语音时长)，首包响应时间(用户发出请求到用户感知到的第一包到达时间)。 当前技术边界1.通用TTS 用户预期不高，可满足商业化需求，如滴滴，高德，智能音箱，机器人等 用户预期高，则很难满足（由于机械感，不能很自然地模拟人声） 2.个性化TTS 效果没通用TTS好 3.情感TTS 业界情感合成增多（大概是由于数据变多） 学术界有理论储备，但没怎么做好，由于情感TTS很依赖于情感意图识别、情感特征挖掘、情感数据和情感声学技术等，而NLP在这些方面进展缓慢 瓶颈和机会1.基础技术 end-to-end的TTS建模方法，加上wavenent的声码器思想。 如何评价谷歌下一代语音合成系统WaveNet？ 百度的Deep Voice团队（在美国硅谷的AI Lab），实时语音合成神经网络系统（Real-Time Neural Text-to-Speech for Production）据说比 WaveNet 要快 400 倍。 2.数据缺乏 一方面，特别是个性化TTS，需要数据量更大。比如默认男孩声音，要转成女孩，就比较难。另一方面，数据的获取（制作）成本和周期，也是各家在初期的竞争着力点。 3.人才匮乏：不仅没法跟NLP、CV等热门AI人才比，就算跟同样不算热门的ASR比，TTS的人才都还要少一些。 4.产品化难度： A.由于技术限制，现阶段不可能有非常完美的TTS效果，所以尽量选择用户预期不苛刻的场景，或者在产品体验设计时，管理好用户预期（比如打车软件，郭德纲/林志玲的声音，差不多就行） B. 选择“参数法”还是“拼接法”，和公司的技术储备、成本、以及产品目标相关。在垂直领域，现有的TTS技术（参数或者拼接）都可以针对产品做得很好。现在行业还没有太好的效果，很大原因是因为产品经理还没有深入介入，有很多细节的坑要踩~未来一定会有惊艳的产品出现。 C. 体验细节设计，和一般互联网产品很不同，比如： 文案设计，非常重要；因为在语音交互场景，不能太长，用户没耐心和时间听完的。 可以加入背景音乐，掩盖杂音等细节瑕疵。 特殊场景，还有有特别的需求，比如远场TTS，和戴耳机场景，还会区别。 中英文混合TTS。比如用户想播首英语歌曲，困难在于：所有中文的发音当中，中文和英文合拍念出来是很难的，为什么呢？因为往往录音的人。录中文是一批人，录英文又是一批人。两种语言结合起来，再用机器学习学出来，声音就会变得非常怪。小雅音箱找到了一个能够和中文发音很像的女孩子，录了很多英语的音。 5.商业化压力 如果要有足够的市场竞争力，至少需要12个月的时间，2~6人团队，几百万资金投入（1个GPU一年十万，支持并发只有几十个）。并且，大公司的先发优势巨大，小公司必须切细分场景。 我个人认为，个性化TTS、情感TTS会在各细分场景得到更大的应用，比如知识付费、明星IP、智能硬件、机器人等。 商业应用：实现人机语音交互提供语音服务的设备：导航，为视觉障碍者提供语音辅助，智能客服，智能助理 等等 Ouestions：如何使语音合成的声音更自然？Q：人工智能语音在说中文时的语气感觉上还比较机械，怎样使人工智能语音的语气更自然一些？ A：把语音合成目标分为2级，第一级是发音清晰，第二级是语气自然与否。 目前主流TTS都是数据驱动，抑扬顿挫的感觉必须从录音数据中学习，要对语气进行建模，从单纯发音要求到自然要求，对录音数据量的要求一定变高，一定要学习更多数据。 语音合成：由简到繁。TTS：输入文本，输出语音，这是一对多的产生式问题（同一段文本有多种表达方式，涉及到上下文、说话对象、情绪、场景等） 数据训练（更自然需要训练更多数据）：训练时需要先对数据进行更加细致的描述；数据标注要把发音中的不同语气现象描述出来，然后再给模型训练算法进行学习。常见的语气相关的数据描述包含：韵律边界、重音、边界调、情感等。 语音合成时需预测：从文字中把数据描述信息预测出来。告诉AI用哪种表达方式读，然后才用模型生成对应语音；预测不准确常是合成不自然现象的首要原因。 扩充式积累数据，自然进化：数据积累很慢，故常让录音的语气逐步扩充。先选择产品最通用的语气情感，再逐步放开限定范围，增加相应变化。 end-to-end TTS：将数据描述的学习过程直接从未标注数据中心学习到（非监督学习）；一旦放松对数据的依赖，数据量的增加会非常快，合成性能也能有本质性的提升。 母语比外语更难提高语气自然度：复杂语言，词语句子的歧义性，话中有话等。 ReferenceAI技术通识系列（6）——语音合成TTS 如何评价谷歌下一代语音合成系统WaveNet？ 语音合成 TTS (Text-To-Speech) 的原理是什么？ Paul Taylor-Text-to-speech Synthesis]]></content>
      <categories>
        <category>Artificial Intelligence</category>
        <category>Speech Recognition and Synthesis</category>
      </categories>
      <tags>
        <tag>Artificial Intelligence</tag>
        <tag>Speech Recognition and Synthesis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[教育+ AI 目前行业应用]]></title>
    <url>%2F2017%2F10%2F31%2F%E6%95%99%E8%82%B2-AI-%E7%9B%AE%E5%89%8D%E8%A1%8C%E4%B8%9A%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Abstract：现阶段对民众来说，使用AI非强需求，但尝鲜AI是强需求。教育领域AI可应用的方向有如自动作业批改，个性化学习，智能辅导，互动学习，仿真教育，教学反馈和评测等。自适应学习应该是教育者最想实现的应用，但目前由于教育体制的固化以及AI自适应学习引擎的效果没有对学生的学习效果产生颠覆性的变化，故短期内不大可能有大发展。 自适应学习（因材施教）自适应学习：因材施教，个性化AI教学：根据学生的不同需求推荐不同的学习方案 短期内不大可能有好效果：应试教育短期内难以改变 目前教育系统本质是应试教育，考核标准是知识点的掌握数量和质量，且传统教育知识更新速度非常慢；因此若学生最终需掌握的知识数量太少，通过应教就能起到较好效果，则短期内AI不会发挥颠覆性的优势；而如果没有颠覆性的优势，是无法替代用户习惯和行业解决方案的。 核心理论：知识空间 知识空间：本质是双曲线（负曲率，能容纳无穷多结点）知识存在层级结构；子领域会随着举例快速分割（隔行如隔山）；知识快速膨胀（任何教学方案都可能在几年内过时）。 自适应学习引擎：类似大脑，以先验知识和学生大数据为基础，通过后台算法为学生制定最个性化和高效的学习方案，推荐最有价值的知识点、教程和习题。关键在于机器学习在推送背后的应用。 将自适应学习用到K12：乂学教育，KNEWTON，declara，kidaptive 乂学教育:专注于K12领域智能个性化辅导的智适应教育,与斯坦福研究中心（SRI）成立人工智能联合实验室，成功开发了国内第一个拥有完整自主知识产权、以高级算法为核心的自适应学习引擎。 自适应学习与MOOC的区别：MOOC用互联网方案解决课程的可获得性（随时随地）和可支付性（边际成本为0），但未解决互动性；而自适应教育采用自适应学习系统和名师教学内容，通过线上与线下有机结合的教学模式和培养学习社区，创造高度互动的学习环境，最大限度提升学习效果。 用户需求使用AI非强需求，但尝鲜AI是强需求。 教育 + AI的应用方向1.PR —&gt; 提高付费转化率 PR价值：打造品牌知名度，如AlphaGo使Deepmind路人皆知 提高付费转化率：使家长知道A公司用到AI技术，觉得A公司很高大上、不一般，从而更愿付费 2.感知层AI技术的应用： 语音评测：学生语音—&gt;机器评测，如流利说，一起作业，讯飞等语言学习类产品 动作纠正类的技能教育：如体育、武术、舞蹈 3.智力类细分领域的教育系统： 如跟AlphaGo学下围棋 4.发现需求场景，寻找新的商机 如现在中小学用微信群“做作业”很普遍，学生朗读课文发到大群，老师彻夜听；其实这是低效的方式，是否可开发出机器评测朗读质量的服务以给老师减负呢？ 5.实体或虚拟AI助教 6.作文批改：NLP 行业切入点 自动作业批改 个性化学习 智能辅导 互动学习 仿真教育 教学反馈和评测 智能招生、课堂和课后 My Puzzles 自适应学习引擎的内部原理？ 它如何能解决MOOC所无法解决的交互性？它的解决方案是MOOC所无法实现的吗？ 教育+AI还有哪些应用场景？（从生活中发现？） Reference教育行业应用AI的风险和短期机会]]></content>
      <categories>
        <category>Artificial Intelligence</category>
        <category>AI Application</category>
      </categories>
      <tags>
        <tag>Artificial Intelligence</tag>
        <tag>AI Application</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实际业务场景中的监督学习流程以及PM的工作]]></title>
    <url>%2F2017%2F10%2F31%2F%E5%AE%9E%E9%99%85%E4%B8%9A%E5%8A%A1%E5%9C%BA%E6%99%AF%E4%B8%AD%E7%9A%84%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%B5%81%E7%A8%8B%E4%BB%A5%E5%8F%8APM%E7%9A%84%E5%B7%A5%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[Abstract：只要是监督学习，必然需要做数据处理，流程为：数据标注——&gt;模型训练—&gt;模型测试—&gt;PM评估。其中数据标注是第一步。数据的质量会直接影响到模型的质量。PM需要提供具体的产品需求（如指标设定，分类规则）给算法员做模型训练。测试员需用测试集对模型测试，并反馈各项指标达成情况给PM，PM再评估其是否满足上线需求。PM在整个流程中起流程控制、质量评估、设定模型边界等作用。 监督学习与数据分类监督学习必走的流程： 数据分为两类： 被标记过的数据 未被标记过的数据 用有标记的数据去训练模型，即监督学习。监督学习需要不断用标注过的数据去训练模型，不断调整模型参数，得到指标值更高的模型。 数据标注数据标注的重要性：数据的质量会直接影响到模型的质量.。 数据标注任务的角色： 1234graph TDA[管理员人员管理+发放数据任务+统计工资]--&gt;B[标注员-标记数据]B--&gt;C[审核员-审核被标记数据的质量]C--&gt;D[投入模型训练] 数据标记流程： 1234graph TDA[任务分配-人工分批发放or抢单式]--&gt;B[标记程序设计-需要考虑到如何提升效率如快捷键边标记边存等功能的设置]B--&gt;C[进度跟踪-跟踪工作进度可以ddl淘汰人]C--&gt;D[质量跟踪-可审核标注员的正确率和审核通过率以评质量] 模型训练这部分基本由算法人员负责，PM可向其交代需注意的事项和给出具体的需求和指标，如希望算法精确度在95%以上。 举个栗子：一个识别水果的产品对黄瓜的识别效果不理想。经分析发现是因为黄瓜和丝瓜长得很相似。则为达到提高识别精度(+5%)的目标,解决办法有： 补充黄瓜的数据：包括正例(xx应被识别为黄瓜)和负例(xx不应被识别为黄瓜) 优化已标注的数据：修改以往的错误标注 模型测试测试员将未被训练的数据（预留的测试集）在新的模型下做测试。 PS：最好有后台设计，以实现自动化测试。 衡量模型优劣的指标： 1.通用指标： 精确率Precision = 真阳性的数量/预测值为阳性的数量 = 真阳性的数量/(真阳性的数量+假阳性的数量) 召回率Recall = 真阳性的数量/实际阳性的数量 = 真阳性的数量/(真阳性的数量+假阴性的数量) 还是以黄瓜和丝瓜为例：假设训练样本总数为100个，真阳性数量（正确识别为黄瓜）为90个，假阳性数量（错误识别为黄瓜）的样本数为95个，则： precision = 90/95; recall = 90/98. 3.2节详细阐述了precision和recall 模型的效果，需要在这两个指标之间达到一个平衡。一高一低或一低一高都不好。 2.因地制宜：测试还需关注不同领域不同类别相应的指标，如表情识别（喜怒哀乐恐惊中）各个情绪分类的指标不同。 测试反馈： 反馈什么：指标达成结果 意义：反馈给算法员做模型改进 + 反馈给PM以评估是否满足产品（上线）需求 产品评估评估对象：模型是否满足上线需求。 方法：反复验证模型效果，每次记录好指标数据的对比。 假设本次模型主要是为了优化领域内其中一类的指标，在关注目的的同时，产品还需同时注意检测其他类别的效果，以免漏洞产生。 PM制定模型边界PM工作：流程控制，质量评估，针对分类问题设定模型边界（直接影响模型是否能满足市场需求） 制定分类规则：需要非常细节地提出分类需求和设定分类规则。 例如，目的是希望模型能够识别红色，那产品需要详细描述“红色”包含的颜色，暗红色算红色吗？紫红色算红色吗？紫红色算是红色还是紫色？这些非常细节的规则都需要产品设定。 分类粗细对细分类下的数据量和数据归类有影响：如果分类细，那么针对某一类的数据就会少。如果分类大，那么一些有歧义的数据就会被放进该分类，也会影响模型效果。分类问题和策略问题道理是一样的，都需要产品对需求了解得非常深刻。 Q &amp; A1.数据标注、训练和测试过程中，经常遇到的问题？ 影响因素：数据标注的规范够清晰，对规则的界定从一而终 注意数据标注的一致性 分类性质的工作可从简到繁 1.标注规则可从二分法开始；规则设定由简到繁，带疑虑的数据打上记号先放着。 2.放弃低频问题的规则，有歧义或交叉的数据根据新规则标注。如“你说你会干什么？”可能是询问，可能是嫌弃，带有歧义，不能归到询问类去，需要将其剔除训练集。 多类规则同时进行的标注工作需要把每类规则定的足够细致。 2.设定模型的主要衡量指标有哪些方法？ 在已有模型基础上，根据具体业务和产品需求来优化模型，调配模型（数据公式）参数。 3.半监督学习？ 监督学习的人工和时间成本都最高，最好只在重要和求精的任务上使用。 半监督学习：结合已标注的数据和大量未标注的数据，在节约时间和准确率上效果不错，一般用于训练较大型的基础模型，如分类和相似度。 Reference零互联网工作经验想做AI产品经理怎么办？不如从数据标注工作入门]]></content>
      <categories>
        <category>Artificial Intelligence</category>
        <category>AI PM</category>
      </categories>
      <tags>
        <tag>AI PM</tag>
        <tag>Artificial Intelligence</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习时代下PM的价值]]></title>
    <url>%2F2017%2F10%2F30%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%97%B6%E4%BB%A3%E4%B8%8BPM%E7%9A%84%E4%BB%B7%E5%80%BC%2F</url>
    <content type="text"><![CDATA[Abstract：机器学习看似能自动决策，但其实不是真正的自动决策。原因在于单目标的决策会带来一些风险；而由于外部环境和规则的多变性，机器不可能把所有变量和数据纳入考虑，此时需要懂得系统逻辑的人去调整系统的发展 使用ML前：人作决策，计算机只负责计算。 使用ML后：机器做计算也做决策。 机器真的是自动决策吗？ 不是，在整个过程中计算机基本都是基于统计方法作分析和决策。本质上高度依赖于整个机器学习过程中系统设定的目标。 @ 举个栗子：商品的个性化推荐系统 预设目标：当天销售量 输入数据：历史销售数据，用户行为数据 当给机器预设的目标为销售量时，模型会根据历史销售数据和用户行为数据推荐给最有可能会用户选择的商品（此即基于历史分析的预测）。 但问题在于：最核心的目标只是短期的销售量吗？其实电商的运营目标可能有很多个（用户惊喜度，内容重复度等），而算法若为单目标最优则意味着完全不考虑其他目标，有很大可能牺牲其他重要利益。 如若国家发展的唯一目标是单一GDP，则环境质量、道德水平等就不在考虑范围内。 PM的价值： 给出系统一个准确的目标：必须是复合目标，多方位考虑 在懂得整体系统逻辑的基础上去把握产品发展方向。 因为无论系统如何发展，总会有一些数据无法被纳入到系统中，此时需要一个懂得系统逻辑的人去调整系统的发展。 Reference: 机器学习时代，产品经理未来的价值在哪里]]></content>
      <categories>
        <category>Product Manager</category>
        <category>Technical Logic &amp; Knowledge</category>
      </categories>
      <tags>
        <tag>Product Manager</tag>
        <tag>Technical Logic &amp; Knowledge</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[了解客户端架构]]></title>
    <url>%2F2017%2F10%2F30%2F%E4%BA%86%E8%A7%A3%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[Abstract：在PM逻辑层里，对客户端架构的理解属于基础产品逻辑。客户端&lt;——&gt;API&lt;——&gt;服务端&lt;——&gt;数据层。 APP客户端架构解释 客户端页面：最好只负责展现信息（即API给的数据），不要太多多余的逻辑处理。客户端页面被访问的时候，一些非固定的元素，需要去请求API。 只展示API给的数据的优点：扩展性高，即当界面展示信息的需求需要调整时，可以只改API，而不用改客户端。如果把界面信息展示的逻辑写死在客户端上，则需要发新版本才能修改。 API：客户端的数据可能来自各个业务线，API请求各个业务线的接口，各个业务线的服务端需要将数据组织成APP需要的格式返回给API。 业务线的服务端：数据来自于基础数据库，需要根据基础数据库的变化进行更新。 举例：个人知乎专栏首页信息架构分析 最顶部：返回按钮，分享按钮；专栏信息（名称，Logo，关注量，简介） 卡片流：头像，昵称，文章图片，文章标题，文章导语，文章赞同数，文章评论数，文章发布时间。 由客户端展示的信息分析，可能请求了2个API接口： 1.专栏基本信息接口； 2.卡片流接口； API为了能够返回如上信息，会去请求对应的服务接口（可能是通用接口，有专栏的所有基础信息，超出需要展示的信息的范畴，则API需要根据客户端的应用场景进行处理）；然后服务端会去向数据层请求基础数据，把基础数据返回给API，API再对信息进行处理（数据格式+数据数量）。 服务端的数据在基础数据有更新的时候会根据一定规则进行更新. 为什么要了解客户端架构 前瞻性地预测功能的后续发展方向，避免实现方式过于死板，导致突发的运营功能扩展需要发版解决 需求最重要：懂技术知识来被避免被骂SB的作用有限，毕竟程序员骂PM的大多数情况是“这个傻逼又改需求”，而不是“这个傻逼一点技术都不懂”。 Reference产品架构基础知识]]></content>
      <categories>
        <category>Product Manager</category>
        <category>Technical Logic &amp; Knowledge</category>
      </categories>
      <tags>
        <tag>Product Manager</tag>
        <tag>Technical Logic &amp; Knowledge</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[若有完美，必有谎言|产品策略模型设计的关键点]]></title>
    <url>%2F2017%2F10%2F30%2F%E8%8B%A5%E6%9C%89%E5%AE%8C%E7%BE%8E%EF%BC%8C%E5%BF%85%E6%9C%89%E8%B0%8E%E8%A8%80-%E4%BA%A7%E5%93%81%E7%AD%96%E7%95%A5%E6%A8%A1%E5%9E%8B%E8%AE%BE%E8%AE%A1%E7%9A%84%E5%85%B3%E9%94%AE%E7%82%B9%2F</url>
    <content type="text"><![CDATA[Abstract：产品策略模型是一个黑箱，包含输入输出和计算过程。模型设计过程中需要注意：模型不分高下分是否合适；处理高信息量的能力；需要衡量算法迭代的指标；以置信区间定计算的精度，不要让精度过犹不及；考虑多因素的涌现，和增强对失控的参数控制；人对业务的理解构成模型的边界。而最重要的是先想明白产品遇到了什么样的情境，再思考需要使用产品策略模型来解决什么问题（因时制宜）。 1.模型不分高下，只有是否合适 如推荐系统一定要用协同过滤或者机器学习”，“搜索系统一定带语义分析模糊匹配”，“排序系统 一定要用edgerank”这些都是先入为主的高下划定，实际的模型选择需要考虑实际业务场景和用户需求去选择最合适的，而不一定是理论上最好的。产品策略模型设计是一个复杂且需要在实践中不断打磨改进的过程。有时候一番计算得到的参数，不如二分法试出来的参数好。很多看似没有严格数学证明和依据的简单策略，却能起到好的作用。 2.信息量！处理高信息量的能力！ 模型 = “黑箱”，只有输入和输出两端。输入的信息经计算再输出分析结果。在模型分析能力相同的情况下，输入的信息量越大，分析结果效果越好。所以能承载的信息量是模型的关键因素。处理的信息量不一定越大越好，但随着模型的发展，能处理更多信息的模型最终效果一定很好。 3.指标：算法迭代需要目标值，且这个目标值需要在算法模型迭代过程中，随模型变化而变化，以确定迭代方向是否正确。 如AlphaGo的目标是获胜;搜索的目标是准确率(precise)、召回率(reward),或人工评估的nDGG值。通用测试方法：对数据需要划出测试集，用来验证算法的效果。比如知乎推荐分析每个人数据预测出了用户可能会喜欢的内容，可以在第二天使用用户真实的行为数据和之前的预测进行对比，来确认效果。指标的意义：1.衡量算法优劣和促使算法迭代；2.化抽象为具象：让不明白算法原理的人能在黑箱上看到一个表盘，知道每次迭代后算法有多大改进。 4.置信区间和精度：以置信区间定计算的精度，超过值本身置信区间的精度是没多大意义的（不要让精度过犹不及）。 算法结果够用就行，片面强调计算的准确性是没有必要的。比如要计算一个数据精度本身在+-1范围波动的值，而理论计算得到一个1.8的值本身就够用了，如果画费更大的力气计算出1.834，其实就是无效的。比如要预测未来的销售或者需求变化这些本身波动比较大的数据，花大量的经历计算一个足够精确的值本身就没有太大意义，因为太精确就超过了值本身的置信区间。精度一定要和置信区间匹配，否则就是浪费。 5.考虑多因素的涌现，和增强对失控的参数控制：好算法不会因输入参数的扰动或数据中的极值而极大影响输出结果的准确性。而现实情况下异常数据和参数扰动无法避免，需要考虑多个因素的系统，提高抗风险能力。最好是多个因素组合后涌现出结果，而不能依赖很多参数有一个非常精确的值才能起作用。 一个想要完全精确控制算法参数起作用的模型，很有可能就是调整了一个bad case，又引起了一堆别的bad case。 6.模型的边界，是人对业务的理解：模型非万能，不可能考虑所有信息入参。因此需要人对结果做有效地干预，并依据业务经验调整算法模型。模型一方面淘汰简单机械重复性工作，一方面又对模型设计者、产品运营者提出了更高的要求。 12345678910111213//以淘宝的个性化推荐遍布全站的前后为例：使用前的人工工作：- 需要大量的运营、文案编辑去填写补充布局（后被替代）使用后的人工工作：- 运营：双十一怎么运营，什么营销测流；怎么更好的利用个性化推荐引擎，抓住用户眼球- 模型设计：大量人工的标注和边界策略以保证个性化推荐引擎的有效性；通过人对业务的理解不断修正模型；人对业务的理解形成的外围策略，构成了模型的边界.总结：算法非万能，需要人不断调整修正算法模型，以及人工对算法结果进行有效干预。 // 若有完美，必有谎言.Processing… Reference 产品策略模型方法论]]></content>
      <categories>
        <category>Artificial Intelligence</category>
        <category>AI PM</category>
      </categories>
      <tags>
        <tag>AI PM</tag>
        <tag>Product Manager</tag>
        <tag>Product Pattern Design</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AI PM的人文素养和精神境界的评判标准？]]></title>
    <url>%2F2017%2F10%2F30%2FAI-PM%E7%9A%84%E4%BA%BA%E6%96%87%E7%B4%A0%E5%85%BB%E5%92%8C%E7%B2%BE%E7%A5%9E%E5%A2%83%E7%95%8C%E7%9A%84%E8%AF%84%E5%88%A4%E6%A0%87%E5%87%86%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[Abstract：感性认知——独到的insight——化抽象认知为具象产品设计。// actually，仁者见仁智者见智。 普通人 近距离接触是否让人感觉舒服/清净 如何看出人的潜力? 初地菩萨到十地菩萨的境界与神通差异 AI PM — 是否有自己的感性认知（关乎价值观）？ 如微软小冰与百度度秘，初始定位：有趣 vs 有用。 有了自己的感性认知后，能否有独到的insight？ 有了独到的insight后，是否能将宏观、抽象的认知，细化落地到具体的产品调性和体验设计？]]></content>
      <categories>
        <category>Artificial Intelligence</category>
        <category>AI PM</category>
      </categories>
      <tags>
        <tag>AI PM</tag>
        <tag>Product Manager</tag>
        <tag>PM Ability</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Paper Analysis|基于E-CNN的情绪归因]]></title>
    <url>%2F2017%2F10%2F30%2FPaper-Analysis-%E5%9F%BA%E4%BA%8EE-CNN%E7%9A%84%E6%83%85%E7%BB%AA%E5%BD%92%E5%9B%A0%2F</url>
    <content type="text"><![CDATA[Abstract：情绪归因的意义在于抽取出出发情绪产生的原因信息，从而有助于针对性对策。看了篇paper，本文介绍下里面提出的基于E-CNN神经网络的情绪原因识别方法：通过词向量、卷积、池化等操作充分融合句子的语义信息，利用多个CNN集成降低数据不平衡性对情绪原因识别的影响，克服传统情绪原因识别方法的繁琐规则制定、特征抽取、特征空间降维等过程。 背景张志华：提出基于RNN对长文本进行情感语义建模，解决了长文本的情感分类问题。 Yaqi Wang：利用RNN对句子建模，实现微博文本数据的多标签情绪分类。 采用CNN的句子建模能获得最好的分类性能。 why情绪归因：我们关注的重点不只是公众情绪是什么，而是公众为什么会从同情演变为愤怒的情绪。情绪归因是针对文本中出现的被描述者的情绪，抽取出触发被描述者情绪产生的原因信息。 关键：找出情绪原因的特征，定位其特征位置。 情绪归因的研究进展 Ying Chen：通过对标注语料的分析，发现超过80%的情绪原因信息位于出现情绪的核心子句的前后两个子句中（即上下文信息）。 Sophia Yat Mei Lee：建立了情感归因的语料库，且根据标注的语料库建立了相应规则，用于情绪原因句子的识别。 Alena Neviarouskaya：通过句法、语法和骨子额相结合的方法，分析“乐”的8种情绪原因的语言现象，以此推测一段文斌的情绪类别和原因。 Lin Gui：通过建立25条规则来进行文本情绪原因的预测，也用分类方法预测。 基于E-CNN的情绪归因的研究流程研究目的：精准定位情绪原因子句 流程：建库——标注情绪子句——标注情绪原因子句——归因预测——判定预测正误 详细流程：&gt; 构建情绪归因语料库——标注表达情绪的核心子句——在核心句的前后子句标注情绪原因子句——从核心句的前后每个子句中抽取候选原因事件——通过分类训练器最后判定抽取的候选原因事件是否真的为情绪核心子句的原因事件 E-CNN情绪原因识别模型 情绪归因识别过程：文本——通过E-CNN模型得到每个子句的一个概率值——选概率最大的候选原因子句作为每组示例的情绪原因子句。 Q：为什么将多原因子句分开训练再集成到一起？ A：由于原因子句与非原因子句分别构成多个较平衡的数据集，故分别在各个新数据集上进行对应的CNN训练，然后将训练好的多个CNN集成组合构成E-CNN模型（E-CNN是由多个CNN集成的）；最后在E-CNN上进行情绪原因子句的分类用于识别包含情绪原因信息的子句。]]></content>
      <categories>
        <category>Artificial Intelligence</category>
        <category>Natural Language Processing</category>
      </categories>
      <tags>
        <tag>Artificial Intelligence</tag>
        <tag>Sentiment Analysis</tag>
        <tag>Natural Language Processing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何看出人的潜力？]]></title>
    <url>%2F2017%2F10%2F30%2F%E5%A6%82%E4%BD%95%E7%9C%8B%E5%87%BA%E4%BA%BA%E7%9A%84%E6%BD%9C%E5%8A%9B%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[Abstract:仁者见仁智者见智。潜力是可以被观察出来的。 一个人有巨大潜力的十大征兆 看待钱的态度：不会被短期的缺钱蒙蔽双眼 从小细节看气场：如，集体照时，身量矮小却敢于站在中间 看对低阶层人群的态度：不傲慢，尊重 看心力：能承担本该他人承担的责任和委屈 看外界对ta的反馈：无人诋毁，被广泛认同 不贪：色，钱，欲 愿意相信前人智慧结晶：如儒释道经典 误区：把责任和决定权交给“信”的对象，一旦结果不好，就会说“信”的对象错了，从不认为是自己的问题。 不散乱：思维严谨有条理 看眼神、面相和感觉 看行动：是否做出了一些异于常人的、持续性的、非功利性的行动]]></content>
      <categories>
        <category>Product Manager</category>
        <category>PM Ability</category>
      </categories>
      <tags>
        <tag>Product Manager</tag>
        <tag>PM Ability</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[熵是什么？]]></title>
    <url>%2F2017%2F10%2F30%2F%E7%86%B5%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[Abstract：熵是什么？宇宙的能量总和是个常数，总的熵是不断增加的。每当能量从一种状态转化到另一种状态时，我们会“得到一定的惩罚”，此惩罚即熵，是无效的能量。 熵是什么？宇宙的能量总和是个常数，总的熵是不断增加的。 宇宙能量综合守恒：我们既不能创造，也不能消灭能量。宇宙中的能量总和一开始便是固定的，而且永远不会改变。而能量虽然既不能被创造又不能被消灭，但它可以从一种形式转化为另一种形式。 我们不能创造能量：一个人、一栋大楼、一张木桌、一台电脑，都体现了从一种形式转化成为另一种形式的能量。它们的出现并不是因为能量被创造，而是由于能量被从另一种形式转换成这种形式。 惩罚即熵：每当能量从一种状态转化到另一种状态时，我们会“得到一定的惩罚”。这个惩罚就是我们损失了能在将来用于做某种功的一定能量,即熵。熵是不能再被转化做功的能量的总和的测定单位。熵是某一系统中存在的一定单位的无效能量。 不能再被转化做功：一旦被转化，就不能再做功。 由于熵一旦产生就不能再被转化做功，所以总的熵是不断增加的。 Reference熵，一种新的世界观知乎问题：怎样以通俗易懂的方式向他人解释熵]]></content>
      <categories>
        <category>Computer Science</category>
      </categories>
      <tags>
        <tag>Computer Science</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AlphaGo Zero论文分析与模型可移植性的思考]]></title>
    <url>%2F2017%2F10%2F29%2FAlphaGo-Zero%E8%AE%BA%E6%96%87%E5%88%86%E6%9E%90%E4%B8%8E%E6%A8%A1%E5%9E%8B%E5%8F%AF%E7%A7%BB%E6%A4%8D%E6%80%A7%E7%9A%84%E6%80%9D%E8%80%83%2F</url>
    <content type="text"><![CDATA[Abstract：从用棋谱到扔棋谱，阿元狗完败阿法狗显示强化学习（∈非监督学习）不依赖人的经验也可以通过自学做的很好。其意义在于做AI应用不再需要人工去标注大量样本（非监督学习之区别于监督学习），通过摆脱对人类经验和辅助的依赖，类似的深度强化学习算法或许能更容易地被广泛应用到其他人类缺乏了解或是缺乏大量标注数据的领域。本文分析了Zero与旧版AlphaGo相比的改进，以及思考该模型是否具有可移植性？文末提出了自己的几个疑问，待解答。 AlphaGo VS AlphaGo Zero Deepmind:”从空白状态学起，在无任何人类输入的条件下，它能够迅速自学围棋，并以100:0的战绩击败“前辈”。AlphaGo Zero采用了新的reinforcement learning（强化学习的算法），并给该算法带了新的发展。” 阿法狗：基于海量数据样本训练而获得分类预测的能力。 阿法元AlphaGo Zero：在没有任何训练样本的前提下，通过完全的自学，在极具挑战的领域，达到超人的境地。不再被人类认知所局限，而能够发现新知识，发展新策略。 技术：强化学习（reinforcement learning） 训练过程和效果区别：阿法元只需要在4个TPU上，花三天时间，自己左右互搏490万棋局。而它的哥哥阿法狗，需要在48个TPU上，花几个月的时间，学习三千万棋局，才打败人类。 AlphaGo Zero降低了训练复杂度，摆脱了对人类标注样本(人类历史棋局)的依赖，让深度学习用于复杂决策更加方便可行。最有趣的是证明了人类经验由于样本空间大小的限制，往往都收敛于局部最优而不自知（或无法发现），而机器学习可以突破这个限制。 AI学习人类下法，而人类的下棋数据将算法导向了局部最优(local optima)了；放弃学习人类下法而使用完全随机的初始下法，训练过程也一直趋于收敛，未出现难以收敛的现象。 AlphaGo Zero如何实现无师自通？ AlphaGo采用传统增强学习技术+深度神经网络DNN完成搭建。其基于深度学习的增强学习方法按照使用的网络模型数量可以分为两类: 1.一类使用一个DNN”端到端”地完成全部决策过程(比如DQN)，这类方法比较轻便，对于离散动作决策更适用;2.另一类使用多个DNN分别学习子策略网络（policy）和胜率值网络（value）(比如之前战胜李世石的AlphaGoGo)，这类方法比较复杂，对于各种决策更通用。 DNN缺点：DNN的一个缺点日益明显: 训练过程需要消耗大量人类标注样本，而这对于小样本应用领域(比如医疗图像处理)是不可能办到的。 AlphaGo Zero如何实现无师自通？ 1.将策略网络和价值网络合并，组成一个可以同时输出策略p和价值v的新策略-价值网络：AlphaGo Zero采用类似DQN的一个DNN网络实现决策过程，并利用这个DNN同时输出输出policy和value，然后利用一个蒙特卡罗搜索树完成当前步骤选择。 Q: 为什么变分开训练为同时输出？A：policy与value网络相当于共用之前大部分的特征提取层，输出阶段的最后几层结构仍相互独立。作用：节省训练时间 + 混合的policy与value网络也许能适应更多种不同情况。为什么之前要分开学习和输出：当时暂时做不到。 2.训练过程从完全随机开始：只用随机落子作为初始训练样本，且省去快速走子（需要输入大量人类先验知识），从而解除对人工标注样本的依赖。 Q：为什么可解除依赖？A：其特征提取层采用了20或40个残差模块，每个模块包含2个卷积层。与之前采用的12层左右的卷积层相比，残差模块的运用使网络深度获得了很大的提升。AlphaGo Zero不再需要人工提取的特征应该也是由于更深的网络能更有效地直接从棋盘上提取特征。这两点结构上的改进对棋力的提升贡献大致相等。 改CNN为残差网络结构：DNN网络结构上吸收最新进展，采用ResNet网络中的Residual结构作为基础模块. ResNet使用的Residual结构比GoogLeNet使用的Inception结构在达到相同预测精度条件下的运行速度更快。 Q：为什么旧版AlphaGo没有做这些改进？A：分拆策略，价值网络，快速走子，是旧版AlphaGo暂时的妥协，而在新版终于得以解决。旧版没有做这些改进，应该是当时还做不到吧。 Zero的意义 通用人工智能：Google的目的是造出通用人工智能，而通用人工智能是不需要专业知识的。Zero从零学起而达的成就意味着离通用人工智能进了一步。 不必样本：从应用角度，以后可能不再需要耗费人工去为AI的产品做大量的前期准备工作。 没有充足样本：通过摆脱对人类经验和辅助的依赖，类似的深度强化学习算法或许能更容易地被广泛应用到其他人类缺乏了解或是缺乏大量标注数据的领域。 分析：Zero的模型是否具有可移植性？1.事实（Zero利用强化学习击败哥哥） 算法拥有了在一张棋盘上，自我进化出近乎理论上限的能力。 2.提问（是否能以同样的思路迅速攻克其他领域？即模型是否有可移植性？） 3.答疑（不能，因为外部环境的规则是不稳定的变幻莫测的，受很多外部噪声干扰） 围棋规则三千年不变，而其他领域（诸如交通，投资等）没有这样一张亘古不变的棋盘让算法进行计算。 4.解决方法（造一个优秀的外部环境使之尽可能符合业务需求且简单,在模拟环境中对算法和策略迭代） 外部环境复杂，不意味着增强学习算法无法应用。因为我们可以造一个“棋盘”，即模拟出来一个优秀的外部环境去训练算法和优化策略。 5.问题（缺少“棋盘设计者”，需要既深度了解业务，又深度了解算法的人，如AlphaGo的缔造者之一黄博士是围棋高手+算法大神） Questions 如何让AI在其他领域（规则不稳定，噪声多）也能实现从零开始地自我进化？可能有什么解决方法？ “造棋盘”真的可以解决上述问题吗？ 如何理解棋盘设计者这一“概念”？]]></content>
      <categories>
        <category>Artificial Intelligence</category>
        <category>Reinforcement Learning</category>
      </categories>
      <tags>
        <tag>Artificial Intelligence</tag>
        <tag>Reinforcement Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PM的逻辑——详细阐述]]></title>
    <url>%2F2017%2F10%2F29%2FPM%E7%9A%84%E9%80%BB%E8%BE%91%E2%80%94%E2%80%94%E8%AF%A6%E7%BB%86%E9%98%90%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[Abstract:我们总在说PM的核心能力之一是产品逻辑，但少有人能具体地解释产品逻辑到底是什么。本文将从what is logistic,how to evaluate,how to improve三方面阐述。 What is Logistic of PM?1.逻辑的范畴：PM的逻辑不应设置任何外延，只要是能提升产品效果的逻辑，PM都应去了解，且在此基础上优化产品的设计。 什么是外延 逻辑可能有：系统设计，信息架构，搜索推荐，用户体系，运营，客户端通讯机制，算法等。 2.PM逻辑分类 基础产品逻辑（入门级）：前端交互逻辑，信息展示逻辑，产品商业逻辑等等 数理逻辑：对统计学有基础的了解，基础的数据敏感性，从数据层层深挖定位到问题的能力；知道先验概率，置信度，归因方法等基础的统计学概念。 批判性思维逻辑：在表述方案和评估方案的时候，能够有明确的逻辑思维，知道什么是事实，什么是假设，什么是结论；知道事实是否充分，假设的依据是否可靠，结论的逻辑链是否通顺。 业务逻辑：深入了解所做的业务的业务逻辑； 电商系统:了解商品库和购物流程的机制、了解商品选品的思路和方法。社区:了解社区的运营思路和用户行为逻辑。 快速了解业务逻辑能力，是一个产品经理是否优秀的关键。 系统逻辑：对系统要有深刻的理解。 系统可能是：公司，供应链，产业，后台系统。 业务逻辑是了解业务是什么，系统逻辑这是理解为什么这么做。 做搜索推荐系统:了解算法的基本思路和局限性。做广告系统：了解广告系统的策略原理。 How to evaluate基础产品逻辑：限定场景的产品设计 数理逻辑：小的统计数据的分析case 思维逻辑：做产品分析时的分析方法 业务逻辑和系统逻辑：聊之前的产品经历和对考核者目前产品的看法。 HOW to Improve1.广泛阅读专业书 心理学、历史、法律、政治、经济、算法、数学、设计、新闻等要深度阅读专业书籍（有完整的逻辑链），而不是看公众号，听逻辑思维，听讲书。 2.多做挑战性业务 需要机遇 敢于走出舒适区 好的产品经理是失败堆出来的，就像好的狙击手是子弹喂出来的一样。敢于接受不同的项目，一旦接受也要拼尽全力达成效果。 3.不设边界 只有不设边界的产品经理才能快速成长，才能不断提升自己的能力，而在这个过程中，最核心的就是自己逻辑能力的提升。 Reference什么是产品经理的逻辑？怎么评估？怎么提高？]]></content>
      <categories>
        <category>Product Manager</category>
        <category>Technical Logic &amp; Knowledge</category>
      </categories>
      <tags>
        <tag>Product Manager</tag>
        <tag>Technical Logic &amp; Knowledge</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[逻辑学X集合论|内涵与外延，概念的外延间的关系]]></title>
    <url>%2F2017%2F10%2F29%2F%E9%80%BB%E8%BE%91%E5%AD%A6X%E9%9B%86%E5%90%88%E8%AE%BA-%E5%86%85%E6%B6%B5%E4%B8%8E%E5%A4%96%E5%BB%B6%EF%BC%8C%E6%A6%82%E5%BF%B5%E7%9A%84%E5%A4%96%E5%BB%B6%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB%2F</url>
    <content type="text"><![CDATA[Abstract：一个词项同时具有内涵和外延，分别指规定对象本质属性的含义和具有这些本质属性的对象的集合。涉及到逻辑学和集合论。 内涵与外延的区别（逻辑学）一个词项具有内涵和外延 内涵( intension,connotation)：含义（说明对象的本质属性） 外延(extension,denotation)：具有具体的对象组成的类/集合，这些对象都具有内涵所反映的本质属性 概念的外延之间的关系（集合论）根据概念的外延之间是否重合，可以把概念分成相容关系和不相容关系两大类。 用欧拉图表示： 相容关系：有各种形式的交集（等同，包含，真包含，交叉） 不相容关系：完全没有任何交集（无同属，矛盾反对） 形象举例词项：素数集 内涵：规定集合元素的性质，其元素全为素数 外延：集合的具体对象，{2,3,5,…} Reference逻辑学知识-概念外延间的关系]]></content>
      <categories>
        <category>Math</category>
        <category>Discrete Mathematics</category>
      </categories>
      <tags>
        <tag>Discrete Mathematics</tag>
        <tag>Logic</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[众包模式|对科大讯飞“方言保护计划”的分析]]></title>
    <url>%2F2017%2F10%2F29%2F%E4%BC%97%E5%8C%85%E6%A8%A1%E5%BC%8F-%E5%AF%B9%E7%A7%91%E5%A4%A7%E8%AE%AF%E9%A3%9E%E2%80%9C%E6%96%B9%E8%A8%80%E4%BF%9D%E6%8A%A4%E8%AE%A1%E5%88%92%E2%80%9D%E7%9A%84%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[Abstract：最近科大讯飞联合多方力量推出一项“方言保护计划”，本文对活动实质、设计等方面做了一些分析。 科大讯飞：全球领先的语音识别技术;出品讯飞输入法等应用，衍生的行业应用很多。 新增功能：方言识别 目的：差异化竞争，提高竞争壁垒，给其他进入语音识别领域的企业增加了成本。 创意落地活动：方言保护计划（共建中国方言库） 活动策划的用户痛点：保护和传承本区域的方言 语言是文化的载体。方言寄托着对家乡的归属感。 活动目的：实质是以“众包模式”获取大量的方言训练数据 众包是一种分布式的问题解决和生产模式,将工作先分配给很多参与者再合成为最终结果，是一种特定的获取资源的模式。个人或组织利用大量的网络用户来获取需要的数据、服务和想法。比如ImageNet就是以众包模式将海量图片标注的工作分发给全球各地的“志愿者”，以群体力量完成庞大的工程。 活动策划（闭环：立意+玩法+推广+数据获取+引流） 立意高：（观念倡导）宣传倡导保护方言文化的观念，使全国各地民众产生共鸣 玩法设计新颖有趣： 1.核心功能：录制乡音 + 寻找乡音2.活跃度：积分规则（证书） + 排行（奖励） + 奖励 自身带有强传播性：获赞可增加积分刷排行；高立意带来的非利益驱动力。 一级传播：本平台 + 合作伙伴(24家；特点：用户量大，功能不同针对不同用户群体，涵盖范围广；生活服务类APP)二次传播：引导用户分享（颁发专属编号证书，利用用户的成就感和炫耀心理引导分享） 推广策略：本平台 + 合作伙伴(24家；特点：用户量大，功能不同针对不同用户群体，涵盖范围广；生活服务类APP) 数据获取：系统提供词汇语句 —— 不同用户（性别，方言种类）录制方言 —— 增加系统识别库 流量留存—引流：将用户从H5导流到公众号 总结：以众包模式获取数据集、建立数据库是一种很好的方式。尤其是策划设计创意活动，以游戏化众包去攫取大众的注意力为自己的目的服务。 Reference: 科大讯飞“方言保护计划”H5 科大讯飞官网]]></content>
      <categories>
        <category>Artificial Intelligence</category>
        <category>AI Application</category>
      </categories>
      <tags>
        <tag>Artificial Intelligence</tag>
        <tag>AI Application</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdown X LeTex|如何愉快地书写数学公式？]]></title>
    <url>%2F2017%2F10%2F27%2FMarkdown-X-LeTex-%E5%A6%82%E4%BD%95%E6%84%89%E5%BF%AB%E5%9C%B0%E4%B9%A6%E5%86%99%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[Abstract：如何在有道云笔记里愉快地用markdown写数学公式呢？本文告诉你懒人的秘诀 （w 学习基础的LaTex语法1.插入公式：用`$ 包住表达式 1D = \left \&#123; T_1,w_1;T_2,w_2;...;T_n,w_n \right \&#125; D = \left \{ T_1,w_1;T_2,w_2;...;T_n,w_n \right \}2.空格:用 \qquad,\quad,a\b 12345`$ C_&#123;1&#125; \qquad &#123;C_2&#125; $``$ C_&#123;1&#125; \quad &#123;C_2&#125; $``$ C_&#123;1&#125; \ &#123;C_2&#125; $` 效果如下： C_{1} \qquad {C_2}C_{1} \quad {C_2}C_{1} \ {C_2}3.下标：使用符号_ 12`$C_&#123;1&#125; + &#123;C_2&#125;$` 效果如下： C_{1} + {C_2}4.上标：使用符号^ 12`$c_&#123;1&#125;^&#123;2&#125;=a^&#123;2&#125;+b^&#123;2&#125;$` 效果如下： c_{1}^{2}=a^{2}+b^{2}5.希腊字母： 12`$\lambda,\xi,\pi,\mu,\Phi,\Omega,\alpha, \beta, \gamma,\Gamma, \Delta$` 效果如下： \lambda,\xi,\pi,\mu,\Phi,\Omega,\alpha, \beta, \gamma,\Gamma, \Delta6.值比较符：\eq，\geq，\neq 12`$e^&#123;x^2&#125; \neq &#123;e^x&#125;^2$` 效果如下： e^{x^2} \neq {e^x}^27.平方根：使用\sqrt或 \surd 123`$\sqrt&#123;x+y&#125;$``$\sqrt[3]&#123;x^&#123;2&#125;+y&#125;$``$\surd[x^2 + y^2] $` 效果如下： \sqrt{x+y}\sqrt[3]{x^{2}+y}\surd[x^2 + y^2]更多基础LeTex语法见此文： LaTex 编辑公式 快速上手 学会基础的LeTex语法即可愉快地在mardown里书写数学公式辣~ 不过，有的同学（譬如我）比较懒，不想记繁复的LaTex语法，那么有2件利器值得推荐~ （让我们免于记语法的烦恼） 不想记LaTex语法就靠它们1.在线LeTex数学公式编辑器 特点在于提供了许多可视化语法工具，懒人必备~ 2.在线手写LeTex数学公式识别器 特点在于自动识别用户手写的公式，然后输出经LaTex语法转换过的数学公式。 不过…用鼠标或触摸板画公式很慢 w… Hexo——安装MathJax 插件怎样让静态博客支持LaTex语法，很好地渲染数学公式呢？ 有些 Hexo 主题自帶 MathJax。 如果沒有的话，可以从下列指令安裝： 1npm install hexo-math --save 参考教程： Hexo博客中插入数学公式 在 Hexo 中完美使用 Mathjax 输出数学公式 Reference一份其实很短的 LaTeX 入门文档 LaTex 编辑公式 快速上手 LaTex数学公式大全 markdown输入数学公式 Hexo博客中插入数学公式 在 Hexo 中完美使用 Mathjax 输出数学公式]]></content>
      <categories>
        <category>Math</category>
      </categories>
      <tags>
        <tag>Math</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[危机公关|如何化解企业危机？]]></title>
    <url>%2F2017%2F10%2F25%2F%E5%8D%B1%E6%9C%BA%E5%85%AC%E5%85%B3-%E5%A6%82%E4%BD%95%E5%8C%96%E8%A7%A3%E4%BC%81%E4%B8%9A%E5%8D%B1%E6%9C%BA%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[Abstract:危机公关是公关领域的一大热门，很多人会误以为危机公关是替企业掩盖事实，但其实危机公关的核心目的并不是改变事实的真相，而只是改变人们对事件的看法。本文介绍了危机公关的5S原则和解决方案。 危机公关以事实、价值二分法为分析工具，所有关乎人的危机皆是事实危机与价值危机的聚合体。 危机公关并不改变事实的真相，只是改变人们对事件的看法。 基于事实的基础上，引导媒体报道、公众玉昆与相关事态的良性发展，决不能规避产品本身的严重问题。 危机公关的5S原则 1.系统运行 以冷对热、以静制动：危机会使人处于焦燥或恐惧之中。所以企业高层应以“冷”对“热”、以“静”制“动”，镇定自若，以减轻企业员工的心理压力。 统一观点，稳住阵脚：在企业内部迅速统一观点，对危机有清醒认识，从而稳住阵脚，万众一心，同仇敌忾。 组建班子,专项负责：一般情况下，危机公关小组的组成由企业的公关部成员和企业涉及危机的高层领导直接组成。这样，一方面是高效率的保证，另一方面是对外口径一致的保证，使公众对企业处理危机的诚意感到可以信赖。 果断决策，迅速实施：由于危机瞬息万变，在危机决策时效性要求和信息匮乏条件下，任何模糊的决策都会产生严重的后果。所以必须最大限度地集中决策使用资源，迅速做出决策，系统部署，付诸实施。 合纵连横，借助外力：当危机来临，应充分和政府部分、行业协会、同行企业及新闻媒体充分配合，联手对付危机，在众人拾柴火焰高的同时，增强公信力、影响力。 循序渐进，标本兼治：要真正彻底地消除危机，需要在控制事态后，及时准确地找到危机的症结，对症下药，谋求治“本”。如果仅仅停留在治标阶段,就会前功尽弃，甚至引发新的危机。 2.真诚沟通：诚意、诚恳、诚实 3.速度第一 在危机出现的最初12-24小时内，消息会象病毒一样，以裂变方式高速传播。 4.承担责任 危机发生后，公众会关心两方面的问题：一方面是利益的问题，利益是公众关注的焦点，因此无论谁是谁非，企业应该承担责任。即使受害者在事故发生中有一定责任，企业也不应首先追究其责任，否则会各执已见，加深矛盾，引起公众的反感，不利于问题的解决。另一方面是感情问题，公众很在意企业是否在意自已的感受，因此企业应该站在受害者的立场上表示同情和安慰，并通过新闻媒介向公众致歉，解决深层次的心理、情感关系问题，从而赢得公众的理解和信任。 5.权威证实 请重量级的第三者在前台说话，使消费者解除对自已的警戒心理，重获信任。 危机公关方案 事件回顾 核心诉求 处理基调 内部高层统一思想 与政府主管部门沟通 与股东/债权人沟通 撰写声明 与首发媒体沟通 与重要媒体沟通 与当事人沟通 解决实际问题 正面新闻发布 负面新闻拦截 负面新闻优化 品牌信任重建计划 没事就是本事，摆平就是水平。]]></content>
      <categories>
        <category>Communication Science</category>
        <category>Public Relationship</category>
      </categories>
      <tags>
        <tag>Public Relationship</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[案例|蓝彪（BlueFocus）活动策划书]]></title>
    <url>%2F2017%2F10%2F25%2F%E6%A1%88%E4%BE%8B-%E8%93%9D%E5%BD%AA%EF%BC%88BlueFocus%EF%BC%89%E6%B4%BB%E5%8A%A8%E7%AD%96%E5%88%92%E4%B9%A6%2F</url>
    <content type="text"><![CDATA[Abstract:蓝彪是中国最大的公关公司。本文告诉你如何以系统严密的思路做一份漂亮的公关活动提案。 案例|蓝彪（BlueFocus）活动策划书 分2大部分：活动策划 + 传播策划 1.活动策划 背景思考：社会背景（缺乏勇敢的“正能量”；焦躁，低欲望社会），企业背景（挖掘企业发展史中的勇敢点） 活动目的：中心，受众，目标 受众关注点：参与方（合作伙伴，行业领导，媒体记者，与会嘉宾等）； 整合传播策略；核心词，造概念，而非大段冗余的话 策略解析 项目概况：活动主题，活动时间，地点，嘉宾；活动环节（A—&gt;B—&gt;C） 做策划提案必须要设计美工。 整体活动环境布置：现场布置 + 产品展示布置（做出模拟效果图） 活动流程：（时间，环节，描述，地点） 活动分N个主题/篇章。 流程细节 场地，主持人推荐 2.传播策划 传播目标：定位；知名度，忠实度，品牌认同；受众关注； 传播策略 主题诠释 传播主题分析 传播规划（Roadmap）：引爆期——持续期 新闻稿件示意 …]]></content>
      <categories>
        <category>Communication Science</category>
        <category>Public Relationship</category>
      </categories>
      <tags>
        <tag>Public Relationship</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库系统概论|基础篇—Chapter1 绪论]]></title>
    <url>%2F2017%2F10%2F25%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%AE%BA-%E5%9F%BA%E7%A1%80%E7%AF%87%E2%80%94Chapter1-%E7%BB%AA%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[Abstract：基础篇介绍数据库系统的基本概念和基础知识。第一章绪论，介绍数据库的额基本概念，数据模型的组成要素和常用数据模型，数据库系统的三季末世界观和其主要组成部分。数据库是数据管理的有效技术。越来越多的应用领域运用数据库技术来存储和处理信息资源。B端的企业管理、电子商务等，C端的网上购物、订票、网银存取款等，都是数据库的应用。 Chapter1:数据库系统概述 四个基本概念cont1.数据data 数据库中存储的基本对象。 是对现实世界中客观事物的符号表示，是描述事物的符号记录。 计算机中的数据：能输入计算机，并能为其所处理的符号序列 种类：数值，文本text，图形graph，图像image，音频audio，视频video等 特点：数据与其语义（数据的含义）不可分。 数据库 DB 定义：长期存储在计算机内、有组织的、可共享的大量数据集合。 存放数据的仓库，仓库位于计算机存储设备上，且按一定的格式存放。 特征： 数据按一定的数据模型组织、描述和储存 可为各种用户共享 冗余度（redundancy）较小 数据独立性（data independence）较高 易扩展（scalability） 3.数据库管理系统DBMS 数据定义功能 DBMS提供数据定义语言（Data Definition Language，DDL）以便用户对数据库中的数据对象的组成与结构进行定义。 数据组织、存储和管理 DBMS要分类组织、存储和管理各种数据，包括数据字典，用户数据，数据的存放路径等。 如何实现数据之间的联系：确定以何种文件结构和存取方式在存储级上组织这些数据 多种存取方式提高存取效率：索引查找，hash查找，顺序查找 数据操纵功能 DBMS提供数据操纵语言（Data Manipulation Language，DML）以便用户操纵数据，实现对数据库的基本操作：增删改查 数据库的事务管理和运行管理 数据库由DBMS统一管理和控制 数据库的建立和维护功能（实用程序） 包括：初始数据的输入、转换功能，数据库存储、恢复功能，数据库的重组织功能和性能监视、分析功能 其他功能 DBMS与其他软件系统的通信功能 数据转换 异构数据库的互访和互操 4.数据库系统DBS 定义：在计算机系统中引入数据库后的系统构成。简称数据库。 构成： 数据库 DBMS及开发工具 应用系统 数据库管理员DBA：负责DBS的建立，使用和维护工作 用户End User 1234567graph LR用户--&gt;应用系统 应用系统--&gt;应用开发工具 应用开发工具--&gt;数据库管理系统 数据库管理系统--&gt;操作系统 操作系统--&gt;数据库数据库管理员--&gt;数据库管理系统 数据管理技术的产生和发展需求：数据处理和管理 数据处理：数据收集，存储，加工，传播 数据管理：数据分类，组织，编码，存储 经历三阶段 人工管理 文件系统 数据库系统阶段 出现需求：数据量大，关系复杂，共享性要求强（多应用，多语言共享数据） 数据库系统的特点1.数据结构化 整体数据结构化（DBS与文件系统的本质区别） 用数据模型描述数据结构，无需程序定义和解释 数据可以变长 数据的最小存取单位是数据项 从单应用到多应用，从无结构/部分结构化到整体结构化 共享性高 数据共享可大大减少数据冗余，节省存储 数据独立性高 物理独立性：用户的应用程序与存储在磁盘上数据库中的数据是相互独立的 逻辑独立性：用户的应用程序与数据库的逻辑结构是相互独立的；一方改变，另一方不变 由DBMS的二级映像功能来保证 ps：数据与程序的独立把数据的定义从程序中分离出去，从而简化应用程序的编制和维护修改。 数据由DBMS统一管理和控制 安全性控制：规定用户只能对某些数据以某些方式处理，防止数据因不合法使用而被破坏 完整性控制 并发控制：对多用户的并发操作进行控制 数据库恢复 数据模型两大类数据模型1.数据模型的概念 数据及数据间联系的表示形式（对现实世界特征的模拟，抽象和表示） DBS的核心和基础 2.应满足3方面要求 较真实地表示现实世界 易为人所理解 便于计算机实现 3.分2个不同层次 123graph LR现实世界--&gt;信息世界 信息世界--&gt;计算机世界 ==概念模型-E-R模型== 按用户观点对信息进行建模，主要用于DBS设计 E-R方法：实体——联系方法 数据库设计的第一阶段 逻辑模型和物理模型 逻辑模型：从计算机实现的观点来对数据建模，如层次模型（树状结构），网状，关系模型（集合、表格结构），面向对象数据模型，对象关系数据模型，半结构化等，主要用于DBMS的实现 物理模型：对数据最底层的抽象，描述数据在系统内部的表示和存取方法 1234graph LR现实世界--&gt;认识抽象 认识抽象--&gt;概念模型 概念模型--&gt;逻辑模型 数据模型的组成要素 数据结构 数据操作 完整性约束条件（integrity constraints）：数据的完整性约束是一组完整性规则。完整性规则是给定的数据模型中数据及其联系所具有的制约和遗存规则，用以限定符合数据模型的数据库状态以及状态的变化，以保证数据及的正确、有效和相容。 概念模型（cont）信息世界的几个基本概念 实体entity：客观存在 属性attribute：实体具有的特性 实体型entity type：实体+属性=实体型 学生（学号，姓名，性别，所在院系） 码key：唯一标识实体的属性集；如学号是学生实体的码 实体集entity set：同类型实体的集合。如全体学生是一个实体集 联系relationship：实体之间的联系=不同实体集之间的联系 实体型间的联系（A—&gt;B 的映射关系） 一对一（1：1） 一对多（1：N） 多对多（M:N） 联系的表示方法 实体：矩形框； 联系本身：菱形；联系名+无向边+联系类型；联系本身也是实体型，也有属性 属性：椭圆框 注：设计要避免冗余（浪费存储空间；带来数据的不一致性，一旦数据变化就需要大量修改）如酒和厂家应分为2个不同实体，即2张表。 两个实体间的多对多的联系 —&gt; 三个实体间的两个“一对多联系 层次模型树结构 层次数据模型的数据操作：增删查改 通俗的讲层次模型就是一颗树，这棵树需要满足以下两个条件： 有且只有一个节点没有双亲节点，这个节点称为根节点 根以外的其他节点有且只有一个双亲节点。 优缺点：数据结构比较简单清晰，查询效率高，提供了良好的完整性支持，但是并不能适用于现实世界的联系，有时能使用，但是很笨拙。 网状模型典型代表：DBTG系统，亦称为CODASYL系统。 满足条件： 允许一个以上的结点无双亲 一个节点可以有多于一个的双亲 较层次模型更宽松的规则，更具有普遍性。 优缺点： 优点：能够更为直接描述现实世界，良好的性能，存取效率较高。 缺点：结构复杂，网状模型的DDL、DML复杂，并且需嵌入某一种高级语言中。有一定的应用程序负担 ==关系模型== 关系：一个关系对应通常说的一张表。 元组：一行 属性：一列 码：也称为码键，表中的某个属性组，唯一确定一个元组。 域：域是一组具有相同数据类型的值的集合。 分量：元组中的一个属性值。 关系模式：对关系的描述，一般表示为 关系名（属性1，属性2，。。。。属性n） 数据操作：集合操作；操作对象和结果都是关系（即为若干元组的集合） 存储结构：文件形式存储； 优缺点：建立在严格的数学概念上，概念单一，数据结构简单、清晰，用户易懂易用，开发方便。 典型关系数据库系统 关系模型要求关系必须是规范化的，关系的每一个分量必须是一个不可分的数据项。 数据库系统结构==三级模式+二级映像+数据独立性==型”和“值”的概念 型type：对某一类数据的结构和数学的说明 值value：型的一个具体赋值。 模式 模式是数据库中全体数据的逻辑结构和特征描述，模式的一个具体值称为模式的一个实例。模式是相对稳定的，而实例是相对变动的。 模式：也称逻辑模式/概念模式，是数据库中全体数据的逻辑结构和特征的描述，是所有用户的公共数据视图。具有特点：全体性，逻辑性，特征性； 一个DB只有一个模式。 模式的地位：中间层，最核心。 数据库系统的三级模式结构：外模式、模式和内模式三级构成。 外模式 外模式：也称子模式（模式的一个子集）或用户模式（面向应用程序/用户的），它是数据库用户能够看见和使用的局部数据的逻辑结构和特征的描述，是数据库用户的数据视图，是与某一应用有关的数据的逻辑表示。 外模式：介于模式与应用之间 模式与外模式的关系：一对多（多个外模式间可以有交集，可重叠） 反映：不同用户对数据保密、应用需求、看待数据的方式等的要求 如数字校园DBS，面向后勤集团的模式必须跟面向教务的分开，不然后勤都可以改学生的成绩了。 内模式 内模式（存储模式、物理模式）：它是数据物理结构和存储方式的描述，是数据在数据库内部的组织方式。 一个数据库只有一个模式，也只有一个内模式。 三级模式（数据的三个抽象级别） 数据库系统的三级模式结构：外模式、模式和内模式三级构成。 模式描述数据的全局逻辑结构，而外模式描述数据的局部逻辑结构。 定义外模式到模式的对应关系 优点：把数据的具体组织留给DBMS管理，使用户能逻辑地、抽象地处理数据，而不必关心数据在计算机中的具体表示方式与存储方式（说到底就是数据的独立性！）。 数据库的二级映象功能 出现原因：为了能在系统内部实现这三个抽象层次的联系和转换，DBMS在这三级模式之间提供了两层映像：外模式/模式映像和模式/内模式映像。 功能1-保证数据的逻辑独立性：当模式改变时，DBA只需修改有关的外模式\模式映像，使外模式保持不变。由于应用程序是根据外模式编写的，故应用程序也不必修改。（改映像不改模式） 功能2-保证数据的物理独立性：当DBS的存储结构改变时，DBA只需修改有关的外模式\内模式映像，使模式保持不变，从而应用程序也不必改变（改映像不改模式） 模式／内模式映象：当数据库的存储结构改变时，由数据库管理员对模式／内模式映象作相应改变，可以使模式保持不变，从而应用程序也不必改变，保证了数据与程序的物理独立性，简称数据的物理独立性。 采用三级模式+二级映像——&gt;DBS具有数据独立性 独立性：数据的定义和描述可以从应用程序中分离出去 数据的存取由DBMS管理，从而简化了应用程序的编制，大大减少了应用程序的维护和修改 数据的逻辑独立性是指应用程序对数据全局逻辑结构的依赖程度。数据逻辑独立性高是指当数据库系统的数据全局逻辑结构改变时，它们对应的应用程序不需要改变仍可以正常运行。 DBS具有数据独立性的原因 1.因为数据库管理系统能够提供数据的物理结构与逻辑结构之间的映像或转换功能。这种数据映像功能使得应用程序可以根据数据的逻辑结构进行设计，并且一旦数据的存储结构发生变化，系统可以通过修改其映像来适应变化。所以数据物理结构的变化不会影响到应用程序的正确执行。 2.数据库系统能够提供数据的全局逻辑结构和局部逻辑结构之间的映像和转换功能。 3.这种数据映像功能使得数据库可以按数据全局逻辑结构设计，而应用程序可以按数据局部逻辑结构进行设计。这样，当全局逻辑结构中的部分数据结构改变时，即使那些与变化相关的数据局部逻辑结构受到了影响，也可以通过修改与全局逻辑结构的映像而减小其受影响的程度，使数据局部逻辑结构基本上保持不变。 123graph LR外模式--&gt;模式模式--&gt;内模式 DBS外部的体系结构1.单用户结构的DBS 2.主从式结构的DBS 3.客户\服务器（C/S）结构的DBS 权限、可获取的系统资源都比B/S丰富 4.浏览器/应用服务器/数据库服务器结构 客户端 服务器]]></content>
      <categories>
        <category>Computer Science</category>
        <category>Database System</category>
      </categories>
      <tags>
        <tag>Computer Science</tag>
        <tag>Database System</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《批判性思维原理和方法》阅读笔记]]></title>
    <url>%2F2017%2F10%2F25%2F%E3%80%8A%E6%89%B9%E5%88%A4%E6%80%A7%E6%80%9D%E7%BB%B4%E5%8E%9F%E7%90%86%E5%92%8C%E6%96%B9%E6%B3%95%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[摘要：本书是大学哲学批判性思维的经典教材。批判性思维是一种通过一定的评判标准来分析问题的思维方式，从而引导人避免盲从。本书讲了思维与逻辑的形式，如何进行严密的论证和推理，常见的逻辑推理误区，如何挖掘隐含假设，等等。 Chapter1大众的盲从困境：鱼龙混杂的洪流中分辨和选择的困境。 问题：曾经有用的标准不再可靠，权威的言论不再权威（“砖家”） 现象：网络或传统媒体上某个言论经常会因其一边倒的情绪化浪潮。大众在浪潮中浮沉、推波助澜。但其实只要运用批判性思维的最基本原则就可以很容易怀疑言论消息的真实性和准确性。而现实是大部分人没能避免盲从。 批判性思维重要作用：引导人冲破盲从，避免被忽悠。 三大来源：理由的虚假性，推理的不充足性，论证的单一性。 以批判、怀疑的态度去看待事物和言论，质疑合理性 独立思考判断是不是独立思考：看论者是否能说出前提和结论间的具体关系和推理。不以特定的立场、结论来衡量是否独立思考或盲从。 真正的独立思考： 独立思考不在于受到什么影响，不在于和什么观点相同和不同，在于哪里和为什么相同和不同。独立于已有知识体系和思想的纯粹的思考是不存在的，人不是受到这种思潮的影响就是受到那种观念的推动。真正的独立思考是开放心灵，公正考察所有已知的事实和不同观点。 独立思考：包括自我反思，把别人的观点当镜子，以便发现和突破自己的成见。 “在科学的问题上，一千个人的权威抵不上一个人的谦卑的推理。”——伽利略 Chapter2 批判性思维写基础、规范和途径真理的存在性 实在主义:真理独立于意识存在 相对主义：真理不独立于意识存在 虚无主义：真理不存在，没有认识它的必要性 批判性思维者应具备的特质 理智的谦虚Intellectual Humanility；每个人的理智能力是有限度的，我们天生有有自我中心倾向，我们对事物的看法在很多情况下会产生偏见、假象、受立场限制，并不能完全认识真相；不去表现得比实际知道得更多，避免夸耀、自负、自满、伪装。 理智的勇气Intellectual Courage：理智接受真理，即使不喜欢；理智跑去谬误，即使被公认。 理智的自主性Intellectual Autonomy：不盲从权威 理智的换位思维Intellectual Empathy：设身处地从别人的立场看事情 理智的诚实Intellectual Intergrity：坦率承认不足和错误 理智的坚持Intellectual Perseverance 相信理性Confidence in Reason 心灵公正Farimindenes “维生素C可以防止感冒，因为最新报道说它可以阻碍病毒生长” 论证的假定指论证中的一些可能没有明确表达出来的前提，但我们要去证明隐含前提的合理性。若不合理，则论证也不合理。 论证的标准 清晰性clarity：语言清晰，不具有模糊性和歧义性 准确性accuracy：事实证据和理由正确 精确性precision 相关性relevance 重要性significance：证据和理由对主题、中心问题的相关程度，以及推理的支持程度 充足性sufficiency 深度depth 广度breadth：考察角度的全面性 逻辑logic 公正性fairness 批判性思维的路线图发现和质问基础假设——检查事实的准确性和逻辑一致性——说明背景和具体情况的重要性——想象和开创替代选择 问题1.问题 准确的说，什么是这里的中心议题 我全部同意还是部分同意它的断言，为什么 它的断言实际上奠定在某种假设上吗？假设合理吗？ 它的断言是否只在某些条件下有效，如果是，那是什么条件？ 我需要限定或解释某些用在断言中的词吗 什么样的理由支持我采取这样的立场 什么例子可以用来说明这些理由 .考虑反驳 别人会用什么理由反驳我的立场 我怎样承认或反驳他们的观点 .分析步骤 理解主题论点 分析论证结构：用树状图分析 澄清观念意义：必须理解论证中的关键词句、概念 审查理由质量 评价推理关系：从前提到结论的推理关系 挖掘隐含假设 考察替代论证 综合组织论证：综合前面所想，对论证做出整体的评判、修正。 Chapter3 分析的思考：辨别和分析论证6大认知能力：阐明，分析，评价，推理，解释，自省 论证标准化图尔敏模型 数据、理由 ——————限定————断言—————————————-|————-|————————————保证——辩驳————————————— |————————————支撑 论证的基本过程是：资料（D）和必要条件（B）共同构成了理由（W），在接受了例外（E）的反驳之后，经过限定（Q），使结论得以成立。 好的论证由6部分组成 数据data：用来论证的事实证据和理由（小前提） 断言claim：即结论，要被证明的陈述、主题和观点 保证warrant：用来连接证据和结论之间的普遍性原则（大前提/隐含假设） 支撑backing：用于支撑保证（大前提）的陈述、理由，不直接支持结论，而是支持保证 辩驳rebuttals：对已知反例、例外的考虑、反驳和说明 限定qualifiers：对保证、结论的范围和强度进行限定的修饰词，常是因有了对反例的考虑，从而对结论限定。 理类型 概括：从事实样本推导出更大范围的真实性 类比：从相似性推导出相似性 标志：广泛共识 因果 权威：引用权威 原则：普遍性原则 eg.”孕妇不要喝酒”&gt;理由：孕妇喝一点酒都有可能对胎儿产生危险限定：怀孕后期保证：孕妇对胎儿健康的关切更胜于自己的享受辩驳：并非总是有影响支撑：生命更重要结论：孕妇不要喝酒 Chapter4 澄清概念：清洗和细致的思考论证中的语言意义问题 模糊性 模棱两可：词汇，指称，句法 意义的歪曲 同义反复和空洞 模糊性和空洞 含混的精确性 学究和晦涩 过分、有负载的语言：情绪化或预设的语言 必要条件和充分条件的混淆 充分条件：只要X存在，Y就存在；必要条件：X必须存在，才可以有Y。EG.“如果下雨，地就会湿”——X是Y的充分但不是必要，因为X一定可以推导出Y，但不是只有X才能推导出Y。 论证的语言意义守则定义 定义过宽 定义过窄 既过宽又过窄 证的语言意义守则 澄清概念——具体和明确 避免套话和八股 避免繁琐和晦涩 避免预设的情绪化的语言 词义一一对应 Chapter5 真实的思考：什么是好理由理由的性质问题真理问题形式逻辑中。一个完满论证，必须同时满足2个基本条件： 前提真，或至少可以接受 推理有效 input—process—output :前两个环节都有可能发生错误，需要明确错误发生在哪里，是起源还是过程。 虚假的来源 有意造谣，说真话者其实是说谎者 人的本能和认识的局限 对讨论的问题无知或误解对其所见所闻做出错误的解释虽然了解事实但无意的/有意的进行自我欺骗，或做出粗枝大叶或不准确的表达 社会生活的复杂性和人认知能力的局限性 ，无意欺骗但因为各种因素，我们认为真的事实，其实包含错误。 偏见根深蒂固，你永远叫不醒一个装睡的人。 理由的特征理由：可接受的事实和观念 理由的经验来源：个人经验和公共经验 经验证据的可靠性观察是构造和推理眼见不一定是事实； 记忆不是录像带，人会本能地将事件和他们过去的经验拼接起来。 1.观察是推理和拼凑的过程人是带有色眼镜看世界的，这不可避免。我们在生活，家庭和教育环境中形成了各种各样的观念、信仰、习惯和认知模式。认知偏见，盲点。 2.正常的观察是要出错的观察一个不能运用已有经验来推理的人，= 二百五。 3.观察是知识的运用和判断 4.观察是偏向的心理活动我们只能看到我们想看到的东西。 我们的意图、爱好、倾向、信念、生活模式、理论等，支配我们的视野、构造我们看到的结果。 人们更愿意接受自己喜欢的事实，符合自己或党派的观点的证据是真实且重要的。不符合的是不真实的，或可忽略的小节问题。人们容易相信谣言和片面的不实之词，因为它们能舒服地和已有的观念、偏向、爱好吻合。 鉴定公共经验的性质证据来源的资格来源的信誉、权威性、直接性（第一手资料，避免多次传播），公正性 影响信息来源可靠性的因素：偏见，利益，认识的局限性 偏见：观念，爱好，信誉，虚荣，羞耻，忠诚，报复，野心，逃避 新闻报道从来就是受倾向支配的，虽然它们无一例外地自称中立，多元，真实。多元只能靠读者自己读不同报纸的方法获得。许多公正表达了全部立场的媒体最多是做了些更精致的包装。观念和利益因素管制着所有媒体。 报纸的倾向的来源： 记者自己的偏见、兴趣和观念，他不报道自己不喜欢、不能说清的东西 来自他公司的利益要求：信息是商品，媒体是商业，第一宗旨是报道大众喜欢看的，以提高利润 如明星的八怪一定多与矿工的挣扎故事。 观念、文化等方面的偏见，记者不会报道大众没有兴趣的文化消息，或已有意识形态不能解释的东西 对于一般人，他的世界观等于他每天从媒体上看到的东西。而媒体报道的倾向性很可能导致民众对世界的认知的偏颇。喜欢看什么，媒体就给他看什么，投其所好，继续赚钱。比如，媒体从不会报道飞机安全降落，而是飞机坠毁。如此导致很多民众认为飞机是不安全的，但实际上只是事故的发生几率被媒体无限放大了而已。 客观性的最好保证：多方面的独立来源 关于信息可靠性的判断准则： 证据来源的信誉 证据来源的资格 证据来源的公正 证据是相关的吗 该证据和其他的观察、常识和知识一致吗 证据获取的背景和条件是怎样的 什么是描述，什么是解释:从花哨的文字中找到事实，并牢记这只是记者看到的部分事实 Chapter6：充足的思考：推理（1）一个合适的推理在于前提充分支持结论，首先前提和结论要相关。 证据的相关性，充足性，有效性 相关性：前提和结论必须相关（资格） 充足性：前提对结论构成了足够的支持 有效性 证据的相关性 如运动员因气闷而中途退赛和证明污染对运动员影响的论证是无关的。因为气闷的引发因素有很多，不一定是因为污染。 无关谬误，支持不充分谬误 相关是相对的:不能以反驳对手个人的方式来反驳其观点 如母亲要女儿不要多喝酒，他对身体有害。女儿反驳说“你说这话不奇怪吗？你在聚会时总是多喝酒还抽烟”这种反驳是无关论证。母亲自己喝醉不等于过量饮酒对身体没有坏处。 前提不应该依赖于结论。（为前提的辩护必须独立于结论） 如“上帝存在，因为圣经是这么说的。” 推理的充足性归纳推理：从个别、过去的例子推论到普遍、未来。 演绎推理：从一般到个别。 归纳推理可导致新知识，但结论大于前提，可能错误；演绎推理不会导致新知识，但胜在准确（当前提正确时，结论正确）。 诱导推理：各种证据综合在一起，为结论提供很强的但不是决定性的支持。 如证人说被告开了枪 + 子弹从被告抢中射出 + 有人听到被告说要搞死受害人 != 被告一定是施害者 回溯推理：根据已观察到的现象，结合经验，提出一个关于这个现象的原因或规律的假说。可信与否，在于它是否是一个最佳解释。 实践推理：包含目的前提 + 手段前提 评价论证的三步骤： 找出论证，将其标准化，最好用图来展示其结构 区分论证的推理类型 根据推理类型来运用相应标准评价论证 检查有效性：寻找反例 检查推理的形式1.肯定前项的推理 如果P，则Q 。P。所以，Q。 上面是肯定前项。 2.否定后项推理 如果P，则Q。非Q。所以，非P。 3.选言推理 P或者Q。非P。所以， Q。 老师要么在教室，要么在图书馆。既然他不在教室，那就肯定在图书馆。 4.假言推理 如果P，则Q。如果Q，则R。所以，如果P，则R。 几个典型的无效推理形式1.否定前项谬误：不能通过否定充分条件的前项来否定其后项。 如果P，则Q。非P。所以，非Q。 上面是错误的。P只是Q出现的一个原因。 2.肯定后项谬误 如果P，则Q。Q。所以，P。 错误原因：P不是Q出现的唯一原因。 归纳推理1.简单枚举归纳 2.统计归纳 检查方面： 前提（样本）例子和结论的相关性 样本的代表性，选取的随机性 样本的数量 调查问题的明确下，计算、解释的准确性 类比推理检查： 事物间相似性的真实性 相似性的数量、种类和重要性 差异性的数量，种类和重要性 相似性和差异性与结论中的性质的关系 相似性和不相似性的比较、权衡 Chapter7 最好的思考：推理（2） 理智不能感觉，感官不能思考，知识只能产生于它们的联合。（康德） 因果推理：原因如何导致结果？关于因果关系（：原因导致结果的关系）的推理： 用推理来得到因果认识 依据因果认识来推理 用充分和必要条件表达因果关系 P是Q的充分条件：有P则Q，但P不一定是唯一产生Q的原因 P是Q的必要条件：没有P则没有Q，但有P不一定产生Q P是Q产生的一个因素，非充分非必要 原因总是条件的集合 强调非单一原因决定论 (p+q+w+d+)—&gt;Q 逻辑误区：错把关联当因果 小布什：在我任期内，曾有连续52个月的就业增长。如果小布什坚持把就业增长算在自己头上，那么他也得承受战后最严重的经济危机的责任。 把偶然的相关现象推断为因果关系，被表面的联系迷惑。“因为跟着他发生，所以由他发生”。 科学中的推理问题——假说——检验——证伪——新问题 证伪（否定后项的推理）：如果H，则P。非P。所以，非H。 一种现象可以有多种解释。 决策行动——状态（与不同可能行动的结果有关的可能状态）——结果（每个行动-状态相结合所导出的结果）——价值（用来排列各个结果的重要性和可取程度的标准） 决策原则 最佳行动原则：结果总和的预期值最高的行动 满意行动：最差结果都令人基本满意 赌博：选择那个有一个结果是最高值的行动 保险：选择最差结果值比别的行动的最差结果都高的行动 中间原则（理性主义）：平均值比别的行动结果的平均值都高的行动 Chapter8:深入的思考：挖掘隐含假设和基础没有假设就没有证明，在任何论证里，必须审查假设。 预设假定 隐含前提 支撑假设 （谋杀是不可容忍的）。堕胎是谋杀。所以，堕胎是不可容忍的。 如何寻找隐含假设 寻找连接前提和结论的关系 如果持续干旱，土地就会沙漠化。 中国西北地区持续干旱）所以，中国西北地区的沙漠扩大是必然的。 符合逻辑的有效性 如果禁止烟草公司做广告，他们就省下了这笔钱。 有了钱，为了和别的公司竞争，它们会降价）所以，禁止烟草公司做广告就导致吸烟的增加。 想象和排除反例 补充隐含假设的准则 目的：使论证有效、完善 忠实原意 宽容原则：不要构造不符合论证原意的隐含假设 强弱合适：使用最弱的、可使论证有效地前提 保持性：不会使原来的前提多余 可检验性和可信性：隐含假设可被检验，且没有被证伪]]></content>
      <categories>
        <category>Product Manager</category>
        <category>PM Ability</category>
      </categories>
      <tags>
        <tag>PM Ability</tag>
        <tag>Reading Notes</tag>
        <tag>Critical Thinking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论3个区别:文 vs 工，传播学 vs 传统文科，PM vs 程序员]]></title>
    <url>%2F2017%2F10%2F23%2F%E8%AE%BA3%E4%B8%AA%E5%8C%BA%E5%88%AB-%E6%96%87-vs-%E5%B7%A5%EF%BC%8C%E4%BC%A0%E6%92%AD%E5%AD%A6-vs-%E4%BC%A0%E7%BB%9F%E6%96%87%E7%A7%91%EF%BC%8CPM-vs-%E7%A8%8B%E5%BA%8F%E5%91%98%2F</url>
    <content type="text"><![CDATA[Abstract：身在一个文工交叉的专业，说实话，挺迷的。跟文科比，拼不过写作；跟工科比，拼不过技术。作为PM，有时也会对核心竞争力有所质疑。可当理清楚各个主体之间的区别后，方向就渐渐明晰了。什么该深入，什么可以不必深入，自己心里开始有数。 文工科的思维方式区别： 工科：工具理性思维逻辑，有什么问题和用什么技术解决它； 文科：功能需求思维逻辑，人需要什么。 传统文科与传播学的区别： 关键在于形式和内容的区别（哲学概念） 传统文科：内容创新创意采写和编辑； 传播学：形式创新创意 PM与程序员的区别： PM干什么：创造新的工具；挖掘形式的创新、组合 PM：思考解决什么问题，怎么解决； 程序员：如何实现。]]></content>
      <categories>
        <category>Product Manager</category>
        <category>PM Ability</category>
      </categories>
      <tags>
        <tag>Product Manager</tag>
        <tag>PM Ability</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[广告创意与策划课程笔记]]></title>
    <url>%2F2017%2F10%2F23%2F%E5%B9%BF%E5%91%8A%E5%88%9B%E6%84%8F%E4%B8%8E%E7%AD%96%E5%88%92%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Abstract：广告创意是指通过独特的技术手法或巧妙的广告创作脚本，更突出体现产品特性和品牌内涵，并以此促进产品销售。强调运用创造性思维为消费者提供超越预期的感受。广告创意的关键在于清晰地传递品牌价值 + 创造前所未有的新奇元素以吸引消费者关注。 //本门课程是梨花君老师上的，听听广告创意课也可以培养下创造性思维，和抽象提取产品核心概念等能力。 典型案例引入 江小白 百雀羚 广告语的设计要符合对象的身份，并考虑到受众的喜好和接受度。 如在电台播放华科的广告，不能太过网络用语化，要正式官方些。在短时间内通过介绍最能引起大众认同感的点来达到广告效果。如：一流的学科，一流的平台，微信之父张小龙，etc. 广告创意创意的互动性：tell &lt; show &lt; involve 看得见：性感/美感；摸得到：快感；想得到：感动。 农夫山泉广告：纪录片风格，细节刻画 创造需求，引起注意 星巴克：专属礼品社交平台，打通线上线下社交与消费体验 123456体育品牌：- 最看重运动员：鼓舞人心的人格魅力；奋勇拼搏的体育精神- 耐克：活出你的伟大- I&apos;m WHY- 孙杨361度宣传广告：创意点在于打破了运动员代言体育品牌的固有形象（奋勇拼搏），策划了运动员轻狂的形象 广告创意的流程（从上至下） 抽象信息/产品事实、数据 诉求概念，创意 具象的视觉符号（场景设计） 媒体 受众 创意—&gt;改变人既有的行为模式;打造新的体验 广告公司评选创意的方式 民主表决 以简报内容为筛选目标，是否满足客户需求 由创意总监决定 创意筛选标准 是否屡次见效 是否受团队成员喜欢 让目标用户、竞争对手看 测试可理解度，是否在几秒内能看懂 品牌属性是否强 广告创意的二要素 是否能传达品牌价值 是否包含前所未见的新奇元素，能引起大家注意 广告配音 以声渲染氛围 如国窖1573：低沉浑厚的声音让人感觉喝的不是酒，是文化 案例分析：可口可乐“传递分享精神”：可口可乐特有的红白色+波浪绸带+两只手+一个可乐瓶 创意清单 目标：要传达什么讯息？——分享的可乐精神 策略：如何思考创意？——感性夸大，运用产品符号表达特殊含义 架构：创意的结构如何运作？——新旧元素结合 检索：还可以找哪些相关的创意元素？——手握可乐瓶，产品独特色彩等 观点：采取什么视角看待事情？——目标群体 时程：哪个时段做什么事？——手拉手传递可乐 五感：其他感官怎么感受创意？——视觉，触觉 媒介：用哪些媒体呈现最好？——户外广告 执行：哪种形式适合表现内容？——色彩，图形 创意执行力：各方面臻于完美 灾难性的广告题材：运用得当，可给消费者留下很深的印象。 广告为什么要有创意 吸引消费者注意 加强记忆，保持兴趣 促进消费者行动 讲好课的2大定律：有意义 + 有意思 吸引消费者注意人对信息：偏颇吸收 人的注意力是如何产生的？ 注意的中枢机制是神经过程的诱导定律。 理性与非理性 影响消费者注意的因素 消费需求：合适的消费对象，周期性，精神和无知的双重需求 经济能力：消费者的消费能力，广告定价 消费者的意志活动：广告要克服消费者的障碍意识 双十一最打动消费者的广告语：错过今天，再等一年！ 如何引起消费者注意 指向：确定一个独特的与消费者利益相关的消费导向 集中：对不想管的信息影响加以抑制，保证对象鲜明清晰地展现出来]]></content>
      <categories>
        <category>Communication Science</category>
        <category>Advertising and Marketing</category>
      </categories>
      <tags>
        <tag>Communication Science</tag>
        <tag>Advertising and Marketing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[新闻主体的多情境写作技巧]]></title>
    <url>%2F2017%2F10%2F23%2F%E6%96%B0%E9%97%BB%E4%B8%BB%E4%BD%93%E7%9A%84%E5%A4%9A%E6%83%85%E5%A2%83%E5%86%99%E4%BD%9C%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[Abstract：传统新闻主体写作有四大结构，倒金字塔、华尔街日报、时间顺序和跳笔。它们在各自特定的场景下发挥着作用。 倒金字塔结构 按事实的重要程度从前往后写 无需为结构苦恼，只用按重要性程度先后写即可 快写，快编，快读 多用于事件性新闻 改良：因果倒叙（果-正文-因） 果：选择果要接近事件核心 一文只围绕一果：否则会让读者觉得混乱 缺点： 只适宜突发事件严肃写作，但对富有人情味和故事性强新闻不适合 缺乏文采，没有生气，不能体现风格个性 路透社1963年11月22日达拉斯电，报道肯尼迪遇刺的一则消息后来成为倒金字塔结构的范例：肯尼迪总统今天在这里遭到刺客枪击身亡。总统与夫人同乘一辆车中，刺客开3枪，命中总统头部。总统被紧急送往医院，并经输血，但不久身死。官方消息称，总统下午1点逝世。副总统约翰逊将继任总统。 12345678910111213141516171819标题：刘邦出席宴会时险遭刺杀（汉元年十二月，法通社灞上电）刘邦在项羽举办的鸿门宴会上险遭刺杀，已返回灞上军营。（开头开宗明义，直接把新闻最核心的点出。）此次宴会旨在解决项羽与刘邦关于关中占领问题的争端。在宴会期间，项羽的堂弟项庄要求舞剑助兴，借此靠近刘邦，但被及时阻止。（杀手身份、行刺手段以及结果，属于次要新闻点）刺杀事件发生后，刘邦很快离开席位，宣称去上厕所，但他没有再次出现在宴会场合，而是与樊哙、夏侯婴、靳强、纪信等四人经郦山、芷阳小道返回灞上军营。（针对”返回军营“做进一步解说，至此鸿门宴的核心新闻已经报道完整。）据信，阻止项庄刺杀的人，是项羽的叔父项伯。他当时也同时起身舞剑，阻挡住了项庄。项庄被迫退后，但宴会并未因此而中断。（针对刺杀一词做详细解释，解释为什么是“险遭”。）刘邦离开以后，他的幕僚张良向项羽和范增做出了解释，声明刘邦已经喝醉，并进献了白璧一双、玉斗一双表达歉意。（对刘邦如何离开鸿门的详细解释。至此，对核心新闻的细节补充也完整了。）有宴会出席者指出，在此刺杀事件发生前，项羽的重要幕僚范增曾数次举起自己的玉玦。在宴会结束后，他还用剑将张良进献的玉斗击碎。（补充性材料。）目前鸿门和灞上的军队没有异动，但据信刘邦的左司马曹无伤已被处决。（事件发生后的后续动向，不分析，只罗列事实。）在刺杀事件发生后不久，刘邦的一名部属樊哙曾闯入宴会现场，痛斥项羽。但项羽赞扬了樊哙的举动，并赏赐他一块生猪肉。樊哙未经烹煮即行食用。如果没有他的出现，刘邦的事业恐怕会遭遇失败。刘邦离开时，他也随之护送离开。（对宴会细节的描述，但这条信息与核心信息关联少，重要性比较低，所以放在后面说）唯一仍留在现场的张良和项伯关系良好，曾经救过后者的性命，两人来往十分密切。（这里仍在罗列事实，但记者已经通过事实罗列在表达自己的观点。） 华尔街日报结构DEE结构：D-description，E—explanation，E-Evaluation 优点:人性化开头（细节描写，一定要抓住某些关键细节，加以细化），生动有趣，故事引人入胜 “华尔街日报体”在结构上一般由四部分组成： 第一部分，人性化的开头，即与新闻主题有关的人物故事； 第二部分，过渡，即从人物与新闻主题的交叉点切入，将真正的新闻内容推到读者眼前； 第三部分，展开，即集中而有层次地阐述新闻主题； 第四部分，回归人物，即重新将人物引入新闻，交代此人与新闻主题的深层关系。也可总结为DEE：Description描写、Explanation 解释、 Evaluation评价。 12345678910111213141516171819202122232425262728293031323334标题：鸿门宴，一次几乎爆发的政治危机（汉元年十二月，墨通社灞上电）樊哙闯进鸿门宴会的时候，他的双手紧握着短剑和盾牌，头发直竖，眼角几乎要裂开。守门的持戟卫士试图要阻止他，但失败了。这位年轻时在沛县从事狗类屠宰业的的平民，此时正面对着天下最有权势的贵族项羽，高声发出严厉的指责。整个宴会现场鸦雀无声。（开头从小人物的视角切入，设置悬疑。）樊哙本来是在门口担任警卫工作，没有资格出席。但他突然收到同僚张良的提醒，自己的主君沛公在鸿门宴会上差点遭遇了一次刺杀，而刺杀者项庄显然得到了项羽的默许。他情急之下不顾自己的低微身份，未经许可闯入宴会，试图履行自己的职责。樊哙和所有沛公幕僚都很清楚，这次危机并不只是沛公一个人的危机，而是樊哙、张良这些部属乃至整支沛军的危机。如果任由项羽阵营充满敌意的态度发酵，将会对天下局势产生不可逆转的深刻影响。（从樊哙个人的遭遇过渡到政治层面的危机）在这次宴会不到一个月之前，反秦联军在关中取得了一次辉煌的胜利。刘邦军团攻克函谷关，占领了秦的首都咸阳。但这次胜利引起了项羽的不满。他当时刚刚打破秦军对赵国的围困，却被告知刘邦已经进入关中。项羽认为自己受到了侮辱。在此前的一次政治会议上，楚怀王与将领们约定，先入关中者为王——这被认为是项、刘之间发生争执的重要动机之一。“我兄长对刘邦的这一无耻举动非常愤怒，这个卑贱的小官吏窃取了不属于他的贵族荣誉。”项羽的堂弟项庄说，后来正是他主导了鸿门宴上的刺杀。刘邦的左司马曹无伤向项羽写信，指称刘邦试图在关中称王，这成为激怒项羽的最直接因素。这次政治纠纷随即演变成了军事摩擦。项羽调动了四十万军队驻屯在鸿门，而刘邦的军队数量只有十万。虽然两军暂时并非发生冲突，但局势一触即发。刘邦阵营的一位军事观察家郦食其表示：“如果两军开战，刘邦军队将没有任何胜算。” 但同时他也指出，这对于反秦的整体战略是个非常沉重的打击。项羽阵营也有人持同样的意见。“与其对军队实施打击，不如直接对刘邦采取必要手段。项王不能接受反秦联军失去十万名勇士，但他并不介意只失去一个厚颜无耻的同僚。” 范增如此说道。他是项羽的高级顾问，在决策圈拥有很大的影响力。当然，并非每一个人都和范增持同样意见。项羽的叔父项伯利用和张良——后者是刘邦的军事顾问——的亲密私人关系，成功说服了项羽在鸿门召开一次会谈，以求尽快消弭双方的分歧。在发出的公开声明中，项羽表示愿意尽最大的诚意与刘邦当面沟通，并以已故将领项燕的名义起誓保证他的安全。刘邦很快做出了回应，说他将亲自前往鸿门赴宴，为自己之前鲁莽的军事行动道歉。沛公的这一回应让爱好和平的人们如释重负，范增也是。（这一部分从樊哙扩展到天下大局，简要概括了双方起冲突的大背景，然后又将视角缩小集中在鸿门，转到中心问题。要尽可能地直接引用多方发言，不要简单概括成“他们纷纷表示”，或者”大家都认为“，要细致到每一个人的立场表达，这些人一般都经过精心挑选，可以代表一个阶层或者一个团体的普遍意见，也可以当事人身份表达重要信息。）在鸿门宴开始后，刘邦很快就发觉自己陷入了危机。据一位不愿意透露姓名的侍者宣称，他清楚地看到范增先后三次举起玉玦，这应该是某种危险的暗示。但项羽并没有做出任何回应，显然他还没在现实利益和名誉之间做出任何抉择。范增很快找到项庄，项庄起身表示要临时增加一项娱乐活动。他舞剑助兴，并慢慢靠近刘邦。促成这次会谈的项伯这时也拿起剑来，借舞剑之名阻挡了项庄的攻势。“我当时没想那么多，和谈必须进行下去。何况如果沛公被杀的话，那么张良也会遭到连累，他是我最好的朋友。”项伯在事后如此表示。舞剑结束以后，刘邦安然无恙地留在自己座位上，暂时。张良认为危机仍旧没有解除，他立刻通知再守护在门外的樊哙。樊哙采取了非同寻常的做法，他直接闯进宴会大厅，斥责项羽背离贵族道义。项羽面对斥责显得很愧疚，他赏赐了生猪肉和酒给樊哙，并称赞他是勇者。樊哙趁机再度阐明刘邦的立场：我方无意占据咸阳，更无意称王。目前的军事调动，只是一种保护性的临时占领，可以随时解除。”这是一个最鲁莽的人做出的最明智的举动。“张良事后评价。数刻之后，刘邦起身去厕所，他再也没有回来过。据信，刘邦是在樊哙、夏侯婴、靳强、纪信等四人的护送下，经郦山、芷阳小道返回灞上军营。而张良确信刘邦离开之后，才向项羽表示刘邦喝醉了，已经早早返回，并进献了白璧一双、玉斗一双表达歉意。耐人寻味的是，项羽居然收下了这件礼品。范增是如此地失望，以至于拔出剑将玉斗砸碎。“啊，真是一个缺乏头脑的蠢货。将来夺取天下的，必然是沛公。”有人听到范增言辞激烈地批评道。刘邦回到灞上的第一件事，就是处决了曹无伤。处决是在一个深夜进行的，没有棺材，也没有墓地。（叙述新闻核心事件，多人物，多视角，按时间顺序。）鸿门宴结束后，刘邦履行了自己的承诺，将军队撤出了关中，避免了一次惨痛的失败。项羽立刻占领了咸阳，并指使军队进行了数次劫掠。项羽错失了一次打击潜在竞争对手的机会，但他至少如愿以偿地得到了咸阳。刘邦虽然被迫撤离，但至少保住了有生力量和良好的声望。在秦朝灭亡后的政治版图中，他仍旧可以发挥显著作用。当然，刘邦的占领并非一无所获。不只一名目击者报告，在刘邦军队撤离时，用几十辆大车运走了地图、政府档案等物资，这为这名野心勃勃的平民领主做了一个意味深长的注脚。”我们只是出于学术目的才搜集的，反正项羽将军没兴趣。”萧何断然否认了这个猜想。樊哙在鸿门宴后的工作暂时没有任何变动，但在关键时刻的突出表现，让这位勇者的职业生涯一片光明。当被问起未来五年的职业规划时，这位勇者谦逊地引用了陈胜的一句话：“王侯将相宁有种。”（做简单的总结和分析，并回到开头故事） 时间顺序结构优点：更接近事物的原始形态，符合读者认知状态 跳笔结构优点： 打破时间和空间限制 快速写作，简洁明快 关键在于场景或人物的切换 缺点：只适合短新闻]]></content>
      <categories>
        <category>Communication Science</category>
        <category>Advertising and Marketing</category>
      </categories>
      <tags>
        <tag>Communication Science</tag>
        <tag>Advertising and Marketing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[特殊网络新闻写作]]></title>
    <url>%2F2017%2F10%2F23%2F%E7%89%B9%E6%AE%8A%E7%BD%91%E7%BB%9C%E6%96%B0%E9%97%BB%E5%86%99%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[Abstract：继报刊、广播、电视之后，网络媒体凭借其多媒体、互动性、易于检索、海量信息、迅速传播等特性，已迅速成为新的主流媒体。网络媒体时代的到来，使数字信息的传播速率得到了极大的提高。传播介质的不同使网络新闻的采写有着与传统新闻相比更为特殊的要求。网络媒体在新闻宣传工作中要想继续求得更大的发展,就必须了解今天的受众,了解今天的网络新闻采写规律,摒弃陈旧的新闻采写模式,为受众提供更好的新闻作品。网络新闻写作要在内容 + 形式上创新。 1.滚动新闻指最新的新闻滚动播报（按时间先后顺序，自上而下或自左至右滚动） 全时性发布,时效性高 更新频率高，信息量大 如门户网站：固定板块的滚动新闻；腾讯滚动新闻 2.网络图片新闻表现形式 单幅，多幅，拼图，图集 静态单图，多图切换，360度全景图 纯图，图+文字合成，Flash动漫图，3D图 专业高清图 分类 新闻照片 漫画：通常带讽刺性，夸张，幽默 图示：统计图表，示意图，地图等 图饰：版面的装饰 中国图片新闻网 3.网络视频新闻4.嘉宾访谈5.博客新闻大众创造新闻，UGC； 6.微博新闻7.网络评论 网络新闻的形式创新发展史受互联网的技术的影响（如上网速度的不断提高） Reference滚动新闻介绍与编写技巧 网络图片新闻的编辑特色]]></content>
      <categories>
        <category>Communication Science</category>
        <category>Advertising and Marketing</category>
      </categories>
      <tags>
        <tag>Communication Science</tag>
        <tag>Advertising and Marketing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LOGO设计思考流程和设计需求文档（附案例）]]></title>
    <url>%2F2017%2F10%2F23%2FLOGO%E8%AE%BE%E8%AE%A1%E6%80%9D%E8%80%83%E6%B5%81%E7%A8%8B%E5%92%8C%E8%AE%BE%E8%AE%A1%E9%9C%80%E6%B1%82%E6%96%87%E6%A1%A3%EF%BC%88%E9%99%84%E6%A1%88%E4%BE%8B%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Abstract：Logo是一款产品不可或缺的部分，通常情况下是用户接触产品的第一眼。能不能在第一眼给用户留下较深的印象和具备辨识度，很重要。本文具体阐述PM怎样进行logo的设计思考，以及如何与设计师进行设计需求的对接。附有最近做的Unique AI组LOGO设计需求文档。 Logo的重要性成功的LOGO可以让用户在短时间内判断你是谁，做什么的，而且区别于竞品的同时，传递自己的品牌理念，让用户达成共识，并进行广泛传播。 Logo设计流程提取关键词与设计师沟通关键词. 关键词可以从产品属性，核心理念，应用场景，用户群体，情感传递等方面提取。 找3-4个最能体现产品品位的，接下来的设计就围绕着它们进行。 脑暴logo形式 产品名字的首字母／一个字； 能够准确地表达APP的应用属性及其核心业务，更简单实用，易于推广。 合理变形；融入产品想要传达的理念进行加工，才是设计的精髓。 AirbnbLOGO有四层含义。1.这是一个字母A，代表了Airbnb；2.这像是一个人张开双手，代表了人；3.这像是一个标记地理位置的符号，代表地点；4.这是一个倒过来的爱心，代表了爱。 产品全称； 简单粗暴，加深用户对产品名称的记忆，不需要对抽象符号二次加工。 体现产品核心功能的图形／符号； 以体现产品核心功能的图形为LOGO主要元素，优点是：用户第一次使用时，通过图形能够预判这个产品是干嘛的。 产品名称的形象。 印象笔记为什么用大象：大象永远不会忘记。妙啊！ 确认配色颜色对于一个LOGO来说也相当重要，它承载着针对用户群体的情感传递，以及品牌认知。在LOGO配色环节要围绕着前期确认的设计关键词进行，会使自己的设计更有说服力。而不是根据个人喜好去定义LOGO的颜色。 可以多看看色彩心理学相关书籍，结合自己对产品的理解和大众对颜色的普遍认知，从而找到最合适的配色方案。 打磨细节 LOGO巧妙的细节处理，会让人对产品提升好感度。 在LOGO设计中，常用的协助精确图形轮廓和细节的工具，是黄金比例。 流程总结 先了解产品背景，提取出关键词； 尽可能的脑爆不同形式的LOGO，筛选最巧妙的符合产品定位的方案； 根据产品属性及用户群体，定义配色； 打磨LOGO细节。 采用这种流程设计LOGO，能够使你从比较广的维度去思考，慢慢聚焦到细节去设计，从而产出一款比较有说服力的设计方案。 如何写一份LOGO设计需求文档 PM可以用一份简洁的设计需求文档向设计师清晰阐释你的设计需求。 0.设计目的 1.预算和工期 2.目标用户 2.设计需求 设计关键词 设计风格 设计寓意 形象载体 什么是”不想要”的 3.设计范围 4.参考材料 如何与设计师沟通 带点子过去 用草稿交流 用参考沟通 拿纸笔记录 带共识回来 案例：UNIQUE-AI组 Logo设计需求文档1.设计目的：在具体场景下的组别标识需求 AI作为近年开始热门的科学，缺少如安卓、ios、web等具有的全球统一的标识；用网上下载的图太low了，有失联创水准 具体场景：群头像；全组技术博客Logo；团队组别介绍等 2.工期 第八周周六中午前 3.设计需求 设计关键词：以AI为核心；大脑结构；机器；人脑与机器的结合； 设计风格：科技感；简洁； 参考颜色：蓝；黄；绿，红（色系） 形象载体：字母（AI或A或UNIQUE AI）；AI相关的形象（机器，人脑） 4.Logo结构：Logo部分 + 文字部分（Unique AI/UNIQUE AI） 4.参考材料（见压缩包）]]></content>
      <categories>
        <category>Product Manager</category>
        <category>Design</category>
      </categories>
      <tags>
        <tag>UI Design</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB|基本了解]]></title>
    <url>%2F2017%2F10%2F22%2FMongoDB-%E5%9F%BA%E6%9C%AC%E4%BA%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Abstract：MongoDB是文档型NoSQL数据库，文档以由JSON字符串组成的文档形式保存，通过key-value键值对表示数据。关于数据库类型的选择：脱离具体业务场景的选择问题都是伪命题。MongoDB在特定的使用场景下的优势明显。 定位 通用数据库 流行度远不及MySQL，但潜力很大 特点 文档型（Document store）的NoSQL数据库，数据以文档（对应关系型数据库的记录，本文有时候会混用）的形式在MongoDB中保存，文档实际上就是一个个JSON字符串（使用JSON的好处是非常直观，通过一系列的Key-Value键值对来表示数据，符合我们的阅读习惯）。 在主流的计算机语言如Java、Python中对JSON都有很好的支持，数据从MongoDB中读取出来后，可无需转换直接使用。 Key-Value键值对支持丰富的数据结构，Value可以是普通的整型、字符串，可以是数组，也可以是嵌套的子文档，使用嵌套的好处是在MongoDB中仅需一次简单的查询就能够获取到你所需的数据。 选择什么数据库？ 脱离具体业务场景的选择问题都是伪命题。 选择MongoDB的场景 无需要跨文档或跨表的事务及复杂的join查询支持 敏捷迭代的业务，需求变动频繁，数据模型无法确定 存储的数据格式灵活，不固定，或属于半结构化数据 业务并发访问量大，需数千的QPS TB级以上的海量数据存储，且数据量不断增加 要求存储的数据持久化、不丢失 需要99.999%的数据高可用性 需要大量的地理位置查询、文本查询]]></content>
      <categories>
        <category>Computer Science</category>
        <category>Database System</category>
      </categories>
      <tags>
        <tag>Computer Science</tag>
        <tag>Database System</tag>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB|安装和运行]]></title>
    <url>%2F2017%2F10%2F22%2FMongoDB-%E5%AE%89%E8%A3%85%E5%92%8C%E8%BF%90%E8%A1%8C%2F</url>
    <content type="text"><![CDATA[Abstract：Mac上MongoDB的安装和运行。 安装用homebrew安装 123brew updatebrew install mongodbbrew install mongodb --with-openssl 运行1.create the data repository：1sudo mkdir -p /data/db 2.run1234567891011//Run without specifying pathsmongod//Specify the path of the mongod&lt;path to binary&gt;/mongod//Specify the path of the data directorymongod --dbpath &lt;path to data directory&gt; 3.stop MongoDB press Control+C in the terminal where the mongod instance is running.]]></content>
      <categories>
        <category>Computer Science</category>
        <category>Database System</category>
      </categories>
      <tags>
        <tag>Computer Science</tag>
        <tag>Database System</tag>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[团队技术博客设计]]></title>
    <url>%2F2017%2F10%2F22%2F%E5%9B%A2%E9%98%9F%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2%E8%AE%BE%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[Abstract：有个集体技术博客挺好的。唔。 博客作用 学习记录 团队事务记录 对外展示 博客框架 Ghost hexo WordPress 团队技术博客例子按首页最大特点来划分： 成员： 大搜车团队 文章： 淘宝前端 淘宝 腾讯云社区 时间： 美团 目录型： limboy 分类目录： objccn Unique AI组博客设计 Menu（首页，分类，标签，归档，关于，招新）; 正文流（标题，摘要，时间，标签，访问量，作者名+头像）； 热门标签（10个）； 成员列表； 友链；]]></content>
      <tags>
        <tag>blog/website</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何用sublime编译和运行C和C++程序文件？]]></title>
    <url>%2F2017%2F10%2F20%2F%E5%A6%82%E4%BD%95%E7%94%A8sublime%E7%BC%96%E8%AF%91%E5%92%8C%E8%BF%90%E8%A1%8CC%E5%92%8CC-%E7%A8%8B%E5%BA%8F%E6%96%87%E4%BB%B6%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[Abstract:sublime是一款颜值很高超简洁的代码编辑器。但不原生支持C语言文件编译，本文提供了用sublime编译和运行C和C++程序文件的配置和操作步骤。 问题如何用sublime编译和运行C和C++程序文件？ 解决步骤0.安装gcc（用homebrew安装） 123brew tap homebrew/versionsbrew install gcc49 1.点击 Tools &gt; Build System &gt; New Build System…新建自己的配置文件，以后选择自己的这个 Build System 来编译 2.修改配置文件内容为： 123456789101112131415//配置C++文件：C++.sublime-build&#123; &quot;shell_cmd&quot;: &quot;g++ -o \&quot;$&#123;file_path&#125;/$&#123;file_base_name&#125;\&quot; \&quot;$&#123;file&#125;\&quot;&quot;, &quot;file_regex&quot;: &quot;^(..[^:]*):([0-9]+):?([0-9]+)?:? (.*)$&quot;, &quot;working_dir&quot;: &quot;$&#123;file_path&#125;&quot;, &quot;selector&quot;: &quot;source.c++&quot;, &quot;variants&quot;: [ &#123; &quot;name&quot;: &quot;Run&quot;, &quot;shell_cmd&quot;: &quot;g++ -o \&quot;$&#123;file_path&#125;/$&#123;file_base_name&#125;\&quot; \&quot;$&#123;file&#125;\&quot; &amp;&amp; open \&quot;$&#123;file_path&#125;/$&#123;file_base_name&#125;\&quot;&quot; &#125; ]&#125; 123456789101112131415//配置C文件：C.sublime-build&#123; &quot;shell_cmd&quot;: &quot;gcc -o \&quot;$&#123;file_path&#125;/$&#123;file_base_name&#125;\&quot; \&quot;$&#123;file&#125;\&quot;&quot;, &quot;file_regex&quot;: &quot;^(..[^:]*):([0-9]+):?([0-9]+)?:? (.*)$&quot;, &quot;working_dir&quot;: &quot;$&#123;file_path&#125;&quot;, &quot;selector&quot;: &quot;source.c&quot;, &quot;variants&quot;: [ &#123; &quot;name&quot;: &quot;Run&quot;, &quot;shell_cmd&quot;: &quot;gcc -o \&quot;$&#123;file_path&#125;/$&#123;file_base_name&#125;\&quot; \&quot;$&#123;file&#125;\&quot; &amp;&amp; open \&quot;$&#123;file_path&#125;/$&#123;file_base_name&#125;\&quot;&quot; &#125; ]&#125; 3.写C/C++程序 4.cmd + b 编译；cmd + shift + b 运行 5.终端运行：gcc 文件名]]></content>
      <categories>
        <category>Computer Science</category>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>Computer Science</tag>
        <tag>C/C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库可视化软件|MangoDB Compass初探]]></title>
    <url>%2F2017%2F10%2F19%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8F%AF%E8%A7%86%E5%8C%96%E8%BD%AF%E4%BB%B6-MangoDB-Compass%E5%88%9D%E6%8E%A2%2F</url>
    <content type="text"><![CDATA[Abstract:MongoDB Compass是一款数据库可视化软件。用户连接到自己的数据库，然后通过这个图形界面管理工具来可视化数据库。无需命令行即可对DB进行增删查改。其作用体现在模式发现、数据发现和查询的可视化构建。 what数据库可视化软件（连接到自己的数据库，然后通过这个图形界面管理工具来可视化数据库） 无需命令行即可对DB进行增删查改。 作用模式发现 在集合模式中展示字段的数据类型 展示文档之间不同数据类型的字段中，不同数据类型所占的百分比分布。 将缺失值的比例显示为“undefined” 数据发现 以直方图的形式展示一个集合中的数据频率和分布。 查询的可视化构建 自定义查询：Compass中的图表是全方位交互式的。点击一个图表值或者块将会自动在接口中构建一个匹配选定范围的MongoDB查询。 Reference MongoDB使用手册 MongoDB Atlas Download]]></content>
      <categories>
        <category>Computer Science</category>
        <category>Database System</category>
      </categories>
      <tags>
        <tag>Computer Science</tag>
        <tag>Database System</tag>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AlphaGo Zero完败AlphaGo：强化学习突破人类局限]]></title>
    <url>%2F2017%2F10%2F19%2FAlphaGo-Zero%E5%AE%8C%E8%B4%A5AlphaGo%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AA%81%E7%A0%B4%E4%BA%BA%E7%B1%BB%E5%B1%80%E9%99%90%2F</url>
    <content type="text"><![CDATA[Abstract：从用棋谱到扔棋谱，阿元狗完败阿法狗显示强化学习（∈非监督学习）不依赖人的经验也可以通过自学做的很好。其意义在于做AI应用不再需要人工去标注大量样本（非监督学习之区别于监督学习），通过摆脱对人类经验和辅助的依赖，类似的深度强化学习算法或许能更容易地被广泛应用到其他人类缺乏了解或是缺乏大量标注数据的领域。 // 今天票圈被《人工智能从0到1, 无师自通完爆阿法狗100-0》刷屏，Zero的确比他哥哥强悍很多，应该算是强化学习应用的标志性成绩吧。(w AlphaGo VS AlphaGo Zero阿法狗：基于海量数据样本训练而获得分类预测的能力。 阿法元AlphaGo Zero：在没有任何训练样本的前提下，通过完全的自学，在极具挑战的领域，达到超人的境地。不再被人类认知所局限，而能够发现新知识，发展新策略。 技术：强化学习（reinforcement learning） 训练过程和效果区别：阿法元只需要在4个TPU上，花三天时间，自己左右互搏490万棋局。而它的哥哥阿法狗，需要在48个TPU上，花几个月的时间，学习三千万棋局，才打败人类。 AlphaGo Zero降低了训练复杂度，摆脱了对人类标注样本(人类历史棋局)的依赖，让深度学习用于复杂决策更加方便可行。最有趣的是证明了人类经验由于样本空间大小的限制，往往都收敛于局部最优而不自知（或无法发现），而机器学习可以突破这个限制。 AI学习人类下法，而人类的下棋数据将算法导向了局部最优(local optima)了；放弃学习人类下法而使用完全随机的初始下法，训练过程也一直趋于收敛，未出现难以收敛的现象。 AlphaGo Zero如何实现无师自通？1.AlphaGo: AlphaGo采用传统增强学习技术+深度神经网络DNN完成搭建。其基于深度学习的增强学习方法按照使用的网络模型数量可以分为两类: 一类使用一个DNN”端到端”地完成全部决策过程(比如DQN)，这类方法比较轻便，对于离散动作决策更适用; 另一类使用多个DNN分别学习子策略网络（policy）和胜率值网络（value）(比如之前战胜李世石的AlphaGoGo)，这类方法比较复杂，对于各种决策更通用。 DNN缺点：DNN的一个缺点日益明显: 训练过程需要消耗大量人类标注样本，而这对于小样本应用领域(比如医疗图像处理)是不可能办到的。 2.AlphaGo Zero: AlphaGo Zero综合二者长处，采用类似DQN的一个DNN网络实现决策过程，并利用这个DNN同时输出输出policy和value，然后利用一个蒙特卡罗搜索树完成当前步骤选择。 为什么变分开训练为同时输出：policy与value网络相当于共用之前大部分的特征提取层，输出阶段的最后几层结构仍相互独立。训练的损失函数也同时包含了policy和value两部分。这样的显然能够节省训练时间，更重要的是混合的policy与value网络也许能适应更多种不同情况。 训练过程从完全随机开始：解除对人工标注样本的依赖。 为什么可解除依赖：其特征提取层采用了20或40个残差模块，每个模块包含2个卷积层。与之前采用的12层左右的卷积层相比，残差模块的运用使网络深度获得了很大的提升。AlphaGo Zero不再需要人工提取的特征应该也是由于更深的网络能更有效地直接从棋盘上提取特征。这两点结构上的改进对棋力的提升贡献大致相等。 DNN网络结构上吸收最新进展，采用ResNet网络中的Residual结构作为基础模块：ResNet使用的Residual结构比GoogLeNet使用的Inception结构在达到相同预测精度条件下的运行速度更快。 意义 不必样本：从应用角度，以后可能不再需要耗费人工去为AI的产品做大量的前期准备工作。 没有充足样本：通过摆脱对人类经验和辅助的依赖，类似的深度强化学习算法或许能更容易地被广泛应用到其他人类缺乏了解或是缺乏大量标注数据的领域。]]></content>
      <categories>
        <category>Artificial Intelligence</category>
        <category>Reinforcement Learning</category>
      </categories>
      <tags>
        <tag>Artificial Intelligence</tag>
        <tag>Reinforcement Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[新媒体如何写出吸引眼球的创意新闻？]]></title>
    <url>%2F2017%2F10%2F18%2F%E6%96%B0%E5%AA%92%E4%BD%93%E5%A6%82%E4%BD%95%E5%86%99%E5%87%BA%E5%90%B8%E5%BC%95%E7%9C%BC%E7%90%83%E7%9A%84%E5%88%9B%E6%84%8F%E6%96%B0%E9%97%BB%EF%BC%9F-1%2F</url>
    <content type="text"><![CDATA[Abstract:在新闻实地采写受限的情况下，新媒体要想获取流量，惟创意能奇袭。那么如何写出吸引眼球的创意新闻？选取特殊的新闻角度是关键。核心目的是迎合受众需求。除了新闻角度，还可以在新闻表现形式和内容结构上创新。 新闻创意写作需要具备的思维 多向思维：从问题的多侧面、多角度、多层次、多渠道、多切入点进行推测、想象 发散思维：关联思考，联想 动态思维：根据事物不断变化的情势，相应地改变思维的切入点、方向和程序，在变化中不断认识运动的事物 换位思维 可创新方面 新闻角度（标准答案永远不止一个） 新闻表现形式 新闻内容结构 如何选取新闻角度（5大问） 你的新闻角度能引起人的兴趣吗？ 是不是离读者最近的？ 是不是读者最易接受的？ 是不是最有人情味的？ 是不是切中要害、最能说明问题的？ 核心思想：迎合受众需求。 最新案例创作（喜迎十九大新闻创作）Q：如何在没有采写十九大现场的权限的情况下，写出喜迎十九大的创意新闻，攫取大量读者眼球？ 新闻角度： 从网络评论挖掘：看网友如何喜看十九大 全国各地人民喜迎十九大众生爆笑百态（：贴近受众生活） 喜迎十九大，大学生最关注的是啥？ “习大大：我最牵挂的是困难群众”——原来总书记一直惦记着我，感动！“2020年全部脱贫，必须完成硬任务”——再过两年半我就不是穷人了，好激动！ 朋友圈：看你的朋友们都在如何喜迎十九大？ 看各地十九大人民“红” 喜迎十九大，100热词翻译 … 创意新闻的其他方式——表现形式 场景视频H5：2017两会王小丫的朋友圈 长图新闻 视频新闻 数据新闻 AR新闻 创意新闻的其他方式——内容结构 跳笔：围绕同一主题的场景或人物切换]]></content>
      <categories>
        <category>Communication Science</category>
        <category>Advertising and Marketing</category>
      </categories>
      <tags>
        <tag>Communication Science</tag>
        <tag>Advertising and Marketing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阿里如何定义新零售及其方法论]]></title>
    <url>%2F2017%2F10%2F16%2F%E9%98%BF%E9%87%8C%E5%A6%82%E4%BD%95%E5%AE%9A%E4%B9%89%E6%96%B0%E9%9B%B6%E5%94%AE%E5%8F%8A%E5%85%B6%E6%96%B9%E6%B3%95%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[Abstract：定义新零售为以消费者体验为中心的数据驱动的泛零售形态。其核心是数据驱动和CGC。新零售诞生的原因在于市场和消费者的需求升级，和商业技术基础设施的进步。新零售的知识框架包括前台、中台和后台。前台：场景，消费者，商品；中台：C2B生成模式，营销，市场，流通链条；后台：基础设施，技术，3D/4D打印，数字化，人工智能，物联网etc。中国市场通过新零售可实现零售领域的跨越式发展。 新零售定义：以消费者体验为中心的数据驱动的泛零售形态。 三大特征: 以心为本：掌握数据就是掌握消费者需求（从数据洞察需求） 零售二重性：二维思考下的理想零售 零售物种大爆发：孵化多元零售新形态与新物种 零售的本质： 无时无刻为消费者提供超出期望的内容 数据与商业逻辑的深度结合，实现消费方式逆向牵引生产变革 新零售的产生人类零售演进史: 1870s 百货商场 ——1930s超级市场——1950s便利店、专卖店、购物中心——1990s电子商务——2010移动购物 传统零售：技术引领生产变革，生产变革引导消费方式变革 新零售：消费方式逆向牵引生产变革 诞生的三大原因 新商业基础设施：大数据，云计算，移动互联网，端；智慧物流，互联网金融； 消费者：数字化程度高，认知全方位，购物全渠道；质量诉求逐渐取代价格诉求；高品质、高科技、个性化；中国消费升级引领全球消费增长 行业：全球实体零售放缓，需要新动力；多元零售形态出现；中国零售缺乏顶级品牌 第二次信息革命：2017——全球3.0 数字化全球共享平台 人工智能、物联网 新零售方法论（知识框架）前台：场景，消费者，商品商品：标准化，个性化专业功能，定向折扣，更高性价比的产品组合，更高颜值，更高品质 ，商品服务属性 ，无缝融合的不同场景 随时待命的服务 ，贴心的个性化服务 ，方便灵活的体验和交付，参与感，文化认同，价值认同 中台：C2B生成模式，营销，市场，流通链条重构人货场：从“货-场-人”到“人-货-场” 传统：经验供货 + 分渠道场景（线上线下渠道割裂）+ 模糊的消费者（观察颗粒粗糙） 新：数字化消费者（可清晰辨识和服务，网状互联）+ 按需智能供货（按需组合产品，最优供应链，智能制造）+ 无处不在的消费场景（线上线下随时随地，零售即体验服务） 新市场：基于数字经济的统一市场（线上线下统一） 基于地域、营业时间的传统商业逻辑被打破 任何场景下的任何两个主体形成可瞬时达成交易的 新流通链：新零售服务商重塑高校流通链（生产-分配-供应-消费） 新生产：数字化生产，数字化转型咨询，智能制造 新金融：供应链新金融 新供应链：智能物流，数字化供应链，电商服务商 新门店经营：数字化服务培训，门店数字化陈列 新生产模式：C2B催生高效企业 后台：基础设施，技术，3D/4D打印，数字化，人工智能，物联网etc技术基础： 3D：以数字模型文件为基础，运用粉末状金属或塑料等可粘合材料，通过逐层打印方式构造物体的技术 4D：用能自动变形的材料来构造物体。关键是记忆合金。 AR/VR :新设备及显示技术。AR是在常规视觉或取景器外叠加虚拟图形信息层。VR将成为类似PC的网络入口。 传感器：通过传感设备，按约定协议将任何物品通过物联网域名建立连接，进行信息交换和通信的网络概念。即万物互联。包括射频识别REID，红外感应器，GPS，激光扫描器等。 应用： 自动结账 布局优化 客户追踪 个性化促销 库存优化 人工智能（数据，算力，算法） CV：CV，手势识别，视频内容自动识别 NLP：语音识别 情景感知计算，推荐引擎协作式过滤器，机器实翻，虚拟个人助理，智能机器人 跨越式发展 类比跳过现金直接到移动支付 从地产模式跨越到新零售（欧美，日还处于先进零售） Reference阿里研究院新零售研究报告 2017.3]]></content>
      <categories>
        <category>Product Manager</category>
        <category>Internet &amp; Product</category>
      </categories>
      <tags>
        <tag>Internet &amp; Product</tag>
        <tag>New Retail</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据分析|Python提取微博用户信息和画像]]></title>
    <url>%2F2017%2F10%2F15%2F%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-Python%E6%8F%90%E5%8F%96%E5%BE%AE%E5%8D%9A%E7%94%A8%E6%88%B7%E4%BF%A1%E6%81%AF%E5%92%8C%E7%94%BB%E5%83%8F%2F</url>
    <content type="text"><![CDATA[Abstract：网络上数据大多是非结构性数据，需要对其进行预处理以便进行下一步的文本分析。本文用python提取微博用户的数据，将用户的名称、ID、性别、标签、地点等提取出来。 提取微博用户信息代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546 print &quot;开始获取粉丝 ID&quot; fansurl = &quot;https://api.weibo.com/2/&#123;0&#125;.json?access_token=&#123;1&#125;&amp;&#123;2&#125;=&#123;3&#125;&amp;&#123;4&#125;=&#123;5&#125;&amp;&#123;6&#125;=&#123;7&#125;&quot;.format(fansmethod ,access_token,&apos;screen_name&apos;,&apos;大都会&apos;,&apos;count&apos;,2000,&apos;cursor&apos;,1) fansurlfile = urllib2.urlopen(fansurl) fansuid = fansurlfile.read() fansidlist = eval(fansuid)[&apos;ids&apos;] print &quot;获取粉丝 ID 成功&quot; false = False null = None true = True print &quot;开始写入粉丝用户信息&quot; outputfile = open(&quot;wbuserprofile.xml&quot;,&quot;w&quot;) for id in fansidlist: outputfile.write(&quot;&lt;user&gt;\n\t&lt;id&gt;&quot;+str(id)+&quot;&lt;/id&gt;\n&quot;) print &quot;写入 uid&#123;&#125;&quot;.format(id) try: profile = urllib2.urlopen(&quot;https://api.weibo.com/2/&#123;0&#125;.json?access_token=&#123;1&#125;&amp;&#123;2&#125;=&#123;3&#125;&amp;&#123;4&#125;=&#123;5&#125;&amp;&#123;6&#125;=&#123;7&#125;&quot;.format(userprofmethod,access_token,&quot;uid&quot;,id,&apos;&apos;,&apos;&apos;,&apos;&apos;,&apos;&apos;)).read() outputfile.writelines(&quot;\t&lt;screen_name&gt;&quot;+eval(profile)[&quot;screen_name&quot;]+&quot;&lt;/screen_name&gt;\n&quot;) print &quot;写入 uid&#123;&#125; 微博名称完成&quot;.format(id) outputfile.writelines(&quot;\t&lt;location&gt;&quot;+eval(profile)[&quot;location&quot;]+&quot;&lt;/location&gt;\n&quot;) print &quot;写入 uid&#123;&#125; 微博地点完成&quot;.format(id) outputfile.writelines(&quot;\t&lt;gender&gt;&quot;+eval(profile)[&quot;gender&quot;]+&quot;&lt;/gander&gt;\n&quot;) print &quot;写入 uid&#123;&#125; 微博性别完成&quot;.format(id) tags = urllib2.urlopen(&quot;https://api.weibo.com/2/&#123;0&#125;.json?access_token=&#123;1&#125;&amp;&#123;2&#125;=&#123;3&#125;&amp;&#123;4&#125;=&#123;5&#125;&amp;&#123;6&#125;=&#123;7&#125;&quot;.format(tagsmethod,access_token,&quot;uid&quot;,id,&apos;&apos;,&apos;&apos;,&apos;&apos;,&apos;&apos;)).read() print &quot;开始写入 uid&#123;&#125; 标签&quot;.format(id) outputfile.writelines(&quot;\t&lt;tags&gt;\n&quot;) for tag in eval(tags): for key in tag.keys(): if key.isdigit(): outputfile.writelines(&quot;\t&quot;+tag[key]+&quot;\n&quot;) outputfile.writelines(&quot;\t&lt;/tags&gt;\n&quot;) print &quot;完成写入 uid&#123;&#125; 标签&quot;.format(id) except: print &quot;连接出错无法写入 , 跳过！&quot; outputfile.write(&quot;&lt;/user&gt;&quot;) print &quot;sleep 60 mins&quot; time.sleep(3600) continue outputfile.write(&quot;&lt;/user&gt;&quot;) outputfile.write(&quot;\n&quot;) outputfile.close() print &quot;完成粉丝用户信息&quot; print &quot;文件写入结束&quot; 提取微博用户肖像12345678910111213141516171819202122232425262728293031323334353637383940414243444546 &lt;user&gt; &lt;id&gt;2863185903&lt;/id&gt; &lt;screen_name&gt; 花开有季 xn&lt;/screen_name&gt; &lt;location&gt; 北京 延庆县 &lt;/location&gt; &lt;gender&gt;f&lt;/gander&gt; &lt;tags&gt; 星座命理娱乐 &lt;/tags&gt; &lt;/user&gt; &lt;user&gt; &lt;id&gt;1246347253&lt;/id&gt; &lt;screen_name&gt; 杰里 - 商 &lt;/screen_name&gt; &lt;location&gt; 北京 房山区 &lt;/location&gt; &lt;gender&gt;m&lt;/gander&gt; &lt;tags&gt; 搞笑幽默 &lt;/tags&gt; &lt;/user&gt; &lt;user&gt; &lt;id&gt;3265394820&lt;/id&gt; &lt;screen_name&gt; 小荷相公丶 &lt;/screen_name&gt; &lt;location&gt; 重庆 &lt;/location&gt; &lt;gender&gt;f&lt;/gander&gt; &lt;tags&gt; &lt;/tags&gt; &lt;/user&gt; &lt;user&gt; &lt;id&gt;2036066523&lt;/id&gt; &lt;screen_name&gt; 青春的 Dalin&lt;/screen_name&gt; &lt;location&gt; 江西 南昌 &lt;/location&gt; &lt;gender&gt;f&lt;/gander&gt; &lt;tags&gt; &lt;/tags&gt; &lt;/user&gt; &lt;user&gt; &lt;id&gt;2013144111&lt;/id&gt; &lt;screen_name&gt; 势必拿下会计证 _ 微微猫 &lt;/screen_name&gt; &lt;location&gt; 北京 宣武区 &lt;/location&gt; &lt;gender&gt;f&lt;/gander&gt; &lt;tags&gt; 旅游 WE 90 后 &lt;/tags&gt; &lt;/user&gt;]]></content>
      <categories>
        <category>Computer Science</category>
        <category>Data Mining and Analysis</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Data Mining and Analysis</tag>
        <tag>Public Opinion Analysis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AI如何在新零售中应用]]></title>
    <url>%2F2017%2F10%2F15%2FAI%E5%A6%82%E4%BD%95%E5%9C%A8%E6%96%B0%E9%9B%B6%E5%94%AE%E4%B8%AD%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Abstract: 新零售是以消费者体验为中心的，数据驱动的泛零售的形态。应用方向有增强现实、智慧门店、机器人、可穿戴设备等。探究AI如何在新零售（无人零售）领域的应用，是一大话题。目前最明显的应用是CV技术之于无人零售。 新零售是以消费者体验为中心的，数据驱动的泛零售的形态。 体验，数据（信息），泛零售 人工智能角度：怎样能得到更多有用的数据 数据的角度：应用的场景很多，百货公司、购物中心，便利店，甚至直播，视频、电子商务。 场景角度：买东西的本质，无非是人和商品。我们做的事情就是去理解人，理解物，然后把人跟物联系起来，让用户更好和更快地找到他满意的商品，把商品能够更快更好的送到用户的手里。 信息的角度：要得到关于人的信息或者关于物的信息，特别在很多线下的场景当中，计算机视觉或者用相机是非常好的方法，识别人、物、动作，高信息量感知。 人是用眼睛感知世界，所以这个世界实际上是为了我们的眼睛而设计的，从红绿灯也好，标志牌也好，很多商品的包装，很多时候都是为了适应人眼，计算机视觉就是利用了这个世界设计的规律，然后试图用同样的途径来得到更多的信息。问题：必须有光照，遮挡（因为我们用的是可见光，波长很短，无法绕过前面的遮挡物）和精度不够。 MAP：平均精度均值 室内定位：wifi，蓝牙，超声波，视觉（最精准） 应用方向：增强现实、智慧门店、机器人、可穿戴设备 增强现实：现实世界跟虚拟世界的叠加。如虚拟家具，可以拿 PAD 看家里，可以把家具放到具体位置看是否合适。三维定位、三维建模，渲染。 智能门店：相机网络（跟踪人，分析人流，看停留的时间，看有人有没有拿东西，看了多久，有没有放回去，监控货架） 可穿戴设备：记录生活，识别环境，识别人，识别动作状态，提供信息，个人助手]]></content>
      <categories>
        <category>Product Manager</category>
        <category>Internet &amp; Product</category>
      </categories>
      <tags>
        <tag>Internet &amp; Product</tag>
        <tag>New Retail</tag>
        <tag>AI Thoughts</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对新零售的理解以及如何达到极致坪效]]></title>
    <url>%2F2017%2F10%2F14%2F%E5%AF%B9%E6%96%B0%E9%9B%B6%E5%94%AE%E7%9A%84%E7%90%86%E8%A7%A3%E4%BB%A5%E5%8F%8A%E5%A6%82%E4%BD%95%E8%BE%BE%E5%88%B0%E6%9E%81%E8%87%B4%E5%9D%AA%E6%95%88%2F</url>
    <content type="text"><![CDATA[Abstract：对电商新零售的理解，以及线下专卖店如何达到极致坪效。两个关键公式：1.零售 = 流量 转化率 客单价 * 复购率；2.坪效 =（流量x转化率x客单价x复购率）/ 面积。 电商缺点 损失体验性 损失即得性 对新零售的理解零售 = 流量 转化率 客单价 * 复购率 流量，就是有多少人进店。在线下，这常常叫做人流、客流。人流量大的地方，叫旺铺。 转化率，就是进店的那么多人中，最终有多少人买了东西。在线下，这常常叫做成交率。 客单价，客单价，就是如何在单次购买更多的东西。买得越多，越有价值。 复购率，就是这个客人走了，下次还会来吗？在线下，这常常叫回头客。 衡量业绩的指标：坪效——每平方米的年销售额 店铺面积几乎决定运营成本。均摊到每平方米店铺面积上的销售额，才真正体现一家店的零售能力。 坪效 =（流量x转化率x客单价x复购率）/ 面积。 新零售的关键：极致的坪效 如何极致坪效流量 选址对标快时尚：用户重合度高（追求高性价比） 低频变高频：集聚几十几百个低频分类信息即造就高频 转化率 爆品战略：极致单品（专注于极致单品打造） 大数据选品：根据线上积累的互联网数据精选畅销品售卖 客单价 提高连带率：就是买了一样东西，顺便多买几样。技术上的关联性、协同性，甚至仅仅是颜值上的一致性，都会提高连带率。 增加体验感：线下体验 复购率 强化品牌认知 打通全渠道：线下往线上引流 小米新零售的商业逻辑：靠小费赚钱线下商品以成本价出售，获取用户和品牌忠实度；通过用户购买小米的其他服务和交纳会员费来盈利。 雷军：哪天小米要破产了，靠众筹都可以救活小米。 以上盈利模式待验证，个人有以下几个疑问： 1.用户是否真的会出于对小米品牌的信赖和喜欢而长期交纳会员费？ 2.通过增值服务获得的收益真的足够吗？ 3.小米之家还有哪些商业盈利的方式？]]></content>
      <categories>
        <category>Product Manager</category>
        <category>Internet &amp; Product</category>
      </categories>
      <tags>
        <tag>Internet &amp; Product</tag>
        <tag>New Retail</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[社交网络分析中的部分概念解释]]></title>
    <url>%2F2017%2F10%2F13%2F%E7%A4%BE%E4%BA%A4%E7%BD%91%E7%BB%9C%E5%88%86%E6%9E%90%E4%B8%AD%E7%9A%84%E9%83%A8%E5%88%86%E6%A6%82%E5%BF%B5%E8%A7%A3%E9%87%8A%2F</url>
    <content type="text"><![CDATA[Abstract：介绍社交网络分析中的网络密度、层、集群、结构洞。 网络密度 （Network Density）一个网络的密度是在一个给定的网络中的关系（边）的数目在网络中的节点之间的可能的关系的总数的比率 公式： 可观测到的实际联系/全部潜在的联系＝关系密度 一个完全连通的网络密度为1 而下面的网络举例显示密度为0.83 如图，可见关系数为(5),可能关系数（6），所以关系密度 5/6 密度测度与跟踪，如信息扩散（eg.思想，谣言，疾病传播etc）的现象有关。假定，在紧密连接的网络中，信息传播的速度更快，达到更广泛的节点集。网络密度越大，越有可能被认为是一个有凝聚力的社区（即社会支持和有效传播的来源）。 层（Degree）指特定行动者对网络其他成员持有的关系数量和类型。 主要分： 单方向性关系 （directed relationship） 对称关系 （symmetric relationship） In-Degree： Out-Degree: 表明节点1和2之间的往复运动 基本上是一个衡量领带强度，这是同样相关的分析整个网络 （Weight Degree ） 测量权重可以：互动频率、交换项目数、个体对关系强度的感知 结构洞（structural holes）结构空洞的思想描述了网络密度的相反，即缺乏连接。 结构孔归因于节点，否则连接密集的网络部分，是分成重要的连接节点。连接网络部分的这些节点被称为“Brokers”。 弱联系的概念与结构空洞的概念密切相关 类似于结构孔，弱关系在网络中嵌入较少。 尽管如此，他们承担重要的职能： 他们促进信息流之间的集群（即从遥远的部分网络） 弱联系有助于整合社会系统，否则将支离破碎和语无伦次 集群（clustering）计算和识别网络中的集群，特别是大型网络可能很麻烦 预定义的算法有助于识别集群 Force Atlas 和 Force Altas 2：最常用的一种算法来确定在大型网络社区（在Gephi为例） 使用的算法的优势：没有现有的知识图理论需要可视化和分析集群网络 缺点：他们的准确性是高度依赖于我们正在分析的网络类型。]]></content>
      <categories>
        <category>Computer Science</category>
        <category>Social Network Analysis</category>
      </categories>
      <tags>
        <tag>Computer Science</tag>
        <tag>Communication Science</tag>
        <tag>Social Network Analysis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Coursera Machine Learning|Week8:Cluster & PCA]]></title>
    <url>%2F2017%2F10%2F11%2FCoursera-Machine-Learning-Week8-Cluster-PCA%2F</url>
    <content type="text"><![CDATA[Abstract:本文讲了聚类和降维两种无监督学习方法。聚类是对无标记数据分析结构并分类的划分算法。降维是减少数据集的特征变量，一般通过PCA算法实现，有数据压缩和可视化数据的作用。 聚类clusterAbstract：聚类是一种无监督学习方法，我们要从无标记的数据中学习，聚类是对无标记数据分析结构并分类的划分算法。几个典型应用实例是市场分析、社交网络分析、组织运算集群和星系数据分析等。K-Means（K均值）算法是目前应用最广泛的聚类算法，通过随机选取K个点作为聚类中心，反腐进行簇分配和移动聚类中心的方式来实现分类。K均值优化目标函数将帮助我们找到更好的簇，并且避免局部最优解。K均值算法的原理就是最小化代价函数J的过程。随机初始化聚类中心：初始化K均值很多次，并运行K均值方法很多次，通过多次尝试来保证我们最终能得到一个足够好的结果，一个尽可能局部或全局最优的结果。关于聚类数量的选取：肘部法则可帮助确定聚类数量，但不能保证总是表现的好；通过下游（应用目的）来决定聚类数量。大部分时候聚类数目是人工输入决定。选择聚类数目的更好方法是去问一下你运行K-均值聚类是为了什么目的？然后想一想聚类的数目是多少才适合你运行K-均值聚类的后续目的。 无监督学习算法与监督学习算法对比监督学习：将有标记的数据分类。如：这是一组附有标记的训练数据集，我们想要找出一个决策边界，来将两者分开。并针对一组标记的训练数据提出一个适当的假设函数来拟合训练样本，以便更好地预测未来。 无监督学习：面对的是一组无标记的训练数据，数据之间不具任何关联的标记。要求算法对无标记数据分析结构并分类。其中一种可能的结构是所有的数据大致地划分成两个类（或组），这种划分的算法称为聚类算法。 聚类聚类是对无标记数据分析结构并分类的划分算法。 应用实例： 图1是细分市场，将所有用户划分至不同的细分市场组，以便于营销或服务。 图2是社交分析体系，比如在社交网络中观察一群人，看他们和谁有电子邮件来往，或者查找一群相互有联系的人。 图3是用聚类来组织运算集群或组织数据中心，因为，如果你知道在集群中，哪些计算机的数据中心倾向于一起工作，你可以用它重新组织你的资源，网络的布局，以及数据中心和通信。 图4是使用聚类算法来试图理解星系的形成，和其中的天文细节。 K-Means算法在聚类问题中，我们有未加标签的数据。我们希望有一个算法能够自动的把这些数据分成有紧密关系的子集，或是簇。K均值 (K-means)算法是现在最为广泛使用的聚类方法。 如上图，有一些没加标签的数据，随机选择2个点，为聚类中心。2个点即聚成2个类。 K均值是一个迭代方法，它要做成2件事情： 簇分配 移动聚类中心 K-Means 第一步：簇分配遍历样本，根据相对距离来分成两类：在K均值算法的每次循环中，第一步是要进行簇分配。这就是说，我要遍历所有的样本（就是图上所有的绿色的点），对数据集中的所有点，依据他们更接近红色这个中心、还是蓝色这个中心，进行染色。染色后： K-Means 第二步：移动聚类中心移动2个聚类中心到和它颜色相同的那堆点的均值处：找出所有红色的点，计算出它们的均值位置，然后我们就把红色点的聚类中心移动到这里。对蓝色的点也同样计算平均位置，然后移动蓝色聚类中心到该平均位置处。 K-Means 第三步：重复执行上面两步然后我们就会进入下一个簇分配。我们重新检查所有没有标签的样本，依据它离红色中心还是蓝色中心更近一些，重新将它染成红色或是蓝色。然后我们再次移动聚类中心。计算蓝色点的均值，以及红色点的均值，然后移动两个聚类中心。然后再做一遍簇分配和移动聚类中心操作。实际上，如果你从这一步开始，一直迭代下去，聚类中心是不会变的；并且 那些点的颜色也不会变。在这时，我们就能说K均值方法已经收敛了。 K-Means的规范化描述K均值算法接受2个输入： 第一个是参数K，表示你想从数据中聚类出簇的个数 第二个输入参数是训练集｛x(1),x(2),…,x(m)｝ 因为这是非监督学习，我们的数据集中不需要yy，同时在非监督学习的 K均值算法里，我们约定x(i)x(i)是一个nn维向量，这就是“训练样本是nn维而不是n+1n+1维”的原因（按照惯例，排除x0=1x0=1这一项）。 K均值算法： 1.随机初始化K个聚类中心，记作μ1，μ2一直到μK。 2.K均值内部循环执行以下步骤： 簇分配 首先对于每个训练样本，我们用变量c(i)表示K个聚类中心中最接近x(i)的那个中心的下标（具体的类别），这就是簇分配。 大写的K表示所有聚类中心的个数，小写的k则表示某个聚类中心的下标。 在所有K个中心中，找到一个k使得xi到μk的距离是x(i)到所有的聚类中心的距离中最小的那个，这就是计算ci的方法。 移动聚类中心：对于每个聚类中心：kk从1循环到KK，将μkμk赋值为这个簇的均值。 某一个聚类中心，比如说是μ2μ2被分配了一些训练样本：1,5,6,10 这个表明c(1)=2, c(5)=2，c(6)=2, c(10)=2。如果我们从簇分配那一步得到了这些结果，这表明，样本1,5,6,10被分配给了聚类中心2；然后在移动聚类中心这一步中，我们计算出这四个的平均值，即计算x(1)+x(5)+x(6)+x(10)，然后计算它们的平均值。这时μ2就是一个n维的向量，因为x(1),x(5)，x(6),x(10) 都是n维的向量。这样聚类中心μ2的移动就结束了。 异常情况目的是让μk移动到分配给它的那些点的均值处，那么如果存在一个没有点分配给它的聚类中心，那怎么办? 直接移除那个聚类中心，最终将会得到K−1个簇，而不是K个簇。 如果就是需要K个簇，尽管存在没有点分配给它的聚类中心，那就重新随机找一个聚类中心。 K均值算法的另一常见应用：应对没有很好分开的簇(non-separated clusters)。 指的是没有很好地隔离开的K个簇，可使用K均值算法进行聚类，如： 上图是一个关于T恤大小的应用例子（根据人们身高体重的样本来分成三类，以做出三种不同大小的T恤）： 假设你是T恤制造商，你找到了一些人，想把T恤卖给他们，然后你搜集了一些这些人的身高和体重的数据。我猜，身高体重更重要一些。然后你可能收集到了上图中一些关于人们身高和体重的样本，然后你想确定一下T恤的大小。 假设我们要设计三种不同大小的t恤：小号、中号、和大号，那么小号应该是多大的?中号呢?大号呢? 尽管这些数据原本看起来并没有三个分开的簇，但是从某种程度上讲，K均值仍然能将数据分成几个类。你能做的就是看这第一群人，然后查看他们的身高和体重，试着去设计对这群人来说比较合身的小号衣服；以及设计一个中号的衣服；设计一个大号的衣服。 这就是一种市场细分的例子。当你用K均值方法将你的市场分为三个不同的部分，你就能够区别对待你三类不同的顾客群体，从而更好的适应他们不同的需求。就像大、中、小，三种不同大小的衣服那样。 优化目标优化目标：在大多数我们已经学到的监督学习算法中(例如线性回归，逻辑回归，以及更多的算法）都有一个优化目标函数，即需要通过算法进行最小化的代价函数。 K均值也有这样一个优化目标函数（或者说是代价函数）。 了解和使用这个K均值的优化目标函数有两方面的目的： 帮助我们调试学习算法，确保K均值算法是在正确运行中。 帮助我们找到更好的簇，并且避免局部最优解。 当K均值正在运行时，我们将对两组变量进行跟踪： 1.c(i) c(i)=表示K个聚类中心中最接近x(i)的那个中心的索引（即当前样本x(i)所归为的那个簇的索引） 2.μk: μk=表示第k个簇的聚类中心（μk∈Rn） K均值中我们用大写K来表示簇的总数，用小写k来表示聚类中心的序号;k∈｛1,2,…,K｝ 还有另一个符号，我们用μc(i)：μ(i)c=表示x(i)所属的那个簇的聚类中心 假如说x(i)被划为了第5个簇，这就是说x(i)被分配到了第5个簇，也就是c(i)=5。因此μc(i)=μ5。所以这里的μc(i)就是第5个簇的聚类中心。而也正是我的样本x(i)所属的第5个簇有了这样的符号表示，现在我们就能写出K均值聚类算法的优化目标了。 K均值的代价函数K均值算法需要最小化的代价函数：J(c(1),…,c(m),μ1,…,μK)=1/m∑i=1/m||x(i)−μ(i)c||^2 代价函数J的参数c(1),…,c(m)以及μ1,…,μK，随着算法的执行过程，这些参数将不断变化。 函数的右边给出了优化目标，即每个样本x(i)到它所属的聚类中心距离的平方值。x(i)就是训练样本的位置，μc(i)是x(i)样本所属的聚类中心的位置。 如上图，x(i)样本被划分到了μ5这个聚类中心，那么||x(i)−μ(i)c||2这个距离的平方，也就是在求样本点x(i)到μ5之间的距离的平方。 K均值的目标就是要最小化代价函数：minc(1),…,c(m),μ1,…,μKJ(c(1),…,c(m),μ1,…,μK) 总结：K均值算法的原理就是最小化代价函数JJ的过程。我们也可以用这个原理，来调试我们的学习算法，保证我们对K均值算法的实现过程是正确的。 随机初始化Abstract：初始化K均值聚类方法，如何避开局部最优来构建K均值聚类方法。 随机初始化聚类中心μ1,μ2,…,μK∈R。 1.确保K&lt;m当运行K均值方法时，你需要有一个聚类中心数值KK，KK值要比训练样本的数量m小，即K&lt;m。 2.随机初始化 随机挑选K个训练样本，然后我要做的是设定μ1,…,μk让它们等于这K个样本。 举例： 随机挑选几个样本，如图中2点作为聚类中心。 此时的这个例子看起来划分的相当不错，但是有时候我可能不会那么幸运。也许我最后会挑选到下面这样的两个点作为聚类中心： 通过对上面两种初始化情况的对比，你可能会猜到K均值算法在它们两种情况下，会得到不同的结果。这取决于聚类簇的随机初始化方法。K均值方法最后可能得到不同的结果，尤其是如果K均值方法落在局部最优的时候。 如果你运行K均值方法，如果它最后得到一个局部最优，这可能是真正的全局最优，你可能会得到这样的聚类结果： 但随机初始化K均值方法也可能会卡在不同的局部最优上面： 对于左边的这种情况，相当于将左下方和上方的样本分为了一类，将右下方的样本分了两类。对于右边的这种情况，相当于将下面的样本整体的分为了一类，将上方的样本分为了两类。 如果你担心K均值方法会遇到上面这种局部最优的问题，并且你想提高K均值方法找到最有可能的聚类的几率的话，我们能做的就是尝试多次随机的初始化，而不是仅仅初始化一次K均值方法就希望它会得到很好的结果。 我们能做的是：初始化K均值很多次，并运行K均值方法很多次，通过多次尝试来保证我们最终能得到一个足够好的结果。一个尽可能局部或全局最优的结果。 选择簇的数量即确定聚类数目，即如何选择K值。 决定聚类数目最常用的方法：通过看可视化的图，或看聚类算法的输出结果，或其他一些东西来手动决定聚类数目。 大部分情况下，对于数据集中有多少个聚类中心通常是模糊的。实际上，它的真实类别数量的确模糊，并无正确答案，此即无监督学习的特点。 如图，有人看到K=4，有人觉得K=2. 肘部法则 (Elbow Method)即通过不断改变K值并求出对应代价函数J值，得出一组对应数据，然后画出曲线图，发现曲线像肘部，其中折线转折处很像一个肘点，肘点即我们可选择的K值。 步骤： 1.我们用K值为1来运行K-均值聚类算法。这就意味着所有的数据都会分到一个类里。然后计算代价函数（或者说计算畸变）J，在图中标出； 2.然后我们选用两个聚类来运行K-均值聚类算法。可能用了多个随机的初始中心，也可能没用。那么有两个聚类的话，我们很可能得到一个较小的畸变值，再在图中标出； 3.然后用三个聚类来运行K-均值聚类。你很有可能得到更小的畸变值，标出； 4.之后再让聚类数目等于4、5来运行K-均值聚类，最后我们就能得到一条曲线，它展示了随着聚类数量的增多，畸变值是如何下降的。我们可能会得到一条这样的曲线： 5.在这里，你会发现这样一种模式：K从1变化到2、再从2到3时，畸变值迅速下降；然后在3的时候，到达一个肘点。此后畸变值就下降得非常慢。这样看起来，也许使用3个类是聚类数目的正确选择。这是因为那个点是曲线的肘点。就是说畸变值快速地下降，直到K=3这个点，在这之后就下降得非常慢，那么我们就选K=3。 肘部法则局限性：有时得到的曲线没有清晰的肘点，则用此法选择K值很困难。 通过下游来决定聚类数量why：通常人们使用K-均值聚类算法是为了某些后面的用途，或者说某种下游的目的。而要求得一些聚类也许你会用K-均值聚类算法来做市场分割。例如我们之前谈论的T恤尺寸的例子，也许你会用K-均值聚类来让电脑的聚类变得更好，或者可能为了某些别的目的学习聚类，等等。如果那个后续下游的目的（比如市场分割）能给你一个评估标准，那么通常来说决定聚类数量的 更好的办法是，看不同的聚类数量能为后续下游的目的提供多好的结果。（即依据你的应用目的来确定K值） 举例：我需要3种尺寸 or 5种？ 情景：选择K=3。我可能有小号、中号、大号三类T恤；K=5，那么我可能有特小号、小号、中号、大号和特大号尺寸的T恤。所以，你可能有3种、4种或者5种T恤尺寸。 K=5时，结果可能如图： 分析：商业盈利角度思考几种尺寸最有利于盈利？我是需要更多的T恤尺寸 来更好地满足我的顾客？还是说我需要更少的T恤尺寸，我制造的T恤尺码就更少，我就可以将它们更便宜地卖给顾客？因此T恤的销售业务的观点 可能会提供给你一个决定采用3个类还是5个类的方法。 降维 (dimensionality reduction)Abstract：第二种无监督学习问题为降维。我们使用降维的一个主要原因是数据压缩（压缩冗余的特征变量）。数据压缩不仅通过压缩数据使得数据占用更少的计算机内存和硬盘空间，它还能给算法提速。降维步骤：检查带标签的训练数据集并提取出输入数据，然后应用PCA降维，得到降维厚的数据集，再把已降维的数据集输入到学习算法中得出假设函数，并把降维后的数据作为输入带入，做出预测。PCA：寻找一组k维向量(一条直线、或者平面、或者诸如此类等等)对数据进行投影，来最小化正交投影误差。 动机主成分分析（PCA）：可数据压缩以加快学习算法，并通过降维可视化复杂数据集。 动机1：数据压缩数据压缩： 通过压缩数据使得数据占用更少的计算机内存和硬盘空间 给算法提速。 降维举例： 假设我们有一个有很多很多特征变量的数据集，如上图。假设我们不知道这两个特征变量。其中x1是某个物体的长度，以厘米为单位；另一个x2是它以英寸为单位的长度。所以这是一个非常冗余的数据，与其用两个特征变量x1和x2，它们都是测量到的长度，或许我们应该把这个数据降到一维，只用一个长度的数据。 冗余特征变量的存在：如果你有上百或者上千的特征变量，很容易就会忘记你到底有什么特征变量，而且有时候可能有几个不同的工程师团队。一队工程师可能给你200个特征变量，第二队工程师可能再给你300个特征变量，然后第三队工程师给你500个特征变量。所以你一共有1000个特征变量，这样就很难搞清哪个队给了你什么特征变量。实际上得到这样冗余的特征变量并不难。 所以如果以厘米计的长度被取整到最近的厘米整数，以英寸计的长度被取整到最近的英寸整数。这就是为什么这些样本没有完美地在一条直线上。就是因为取整所造成的误差。 这种情况下，如果我们可以把数据降到一维而不是二维，就可以减少冗余。 降维：减少数据集的特征变量，特征变量数量=维数。 二维降到一维 如图，二维降到一维指：希望找到一条线，基本所有数据都映射到这条线上，以便直接测量这条线上每个样本的位置，称降维后得到的新特征为z1. 实现方式：用一个一维向量（一个数字）z1来表示每个训练样本的位置，这是对原始训练样本的近似。 优点：减少一半的内存需求，加快算法。 三维降到二维 在更典型的降维例子中，我们可能有1000维的数据，我们可能想降低到100维，但是因为这里能可视化的展示数据的维度是有限制的，所以以三维到二维为例。 有一个如上图的数据集，有一个样本x(i)的集合，x(i)是一个三维实数的点，所以我的样本是三维的：x(i)∈R^3 实现方法：把所有的数据，都投影到一个二维的平面内。为了表示一个点在平面上的位置，我们需要两个数来表示平面上一个点的位置。这两个数可能叫做z1和z2。即我们可用一个二维向量z（z1，z2）来表示每一个训练样本。 z(i)∈R^2 如图，3D绘图来重现上面的整个过程。左边是原始数据集，中间是投影到2D的数据集，右边是以z1和z2为坐标轴的2D数据集。因为大部分数据差不多可能都落在某个2D平面上，或者说距离某个2D平面不远。所以我们可以把它们投影到2D平面上。 动机2:可视化数据 对于大多数的机器学习应用，它真的可以帮助我们来开发高效的学习算法，但前提是我们能更好地理解数据。降维就是数据可视化的一种方法。 举例： 如图，假设我们已收集了大量有关全世界不同国家的统计数据集。第一个特征x1是国家的国内生产总值；第二个特征x2是一个百分比，表示人均占有的GDP；第三个特征x3是人类发展指数；第四个特征x4x4是预期寿命；…直到x50。 在这里我们有大量的国家的数据，对于每个国家有50个特征。我们有这样的众多国家的数据集，为了使得我们能更好地来理解数据，我们需要对数据进行可视化展示。这里我们有50个特征，但绘制一幅50维度的图是异常困难的，因此我们需要对数据进行降维，然后再可视化。 实现方式：使用特征向量x(i)来表示每个国家。x(i)有着50个维度。我们需要对这50个特征降维之后，我们可以用另一种方式来代表x(i)：使用一个二维的向量zz来代替之前50维的x。 z(i)∈R^2 我们用z1和z2这两个数来总结50个维度的数据，我们可以使用这两个数来绘制出这些国家的二维图，使用这样的方法尝试去理解二维空间下不同国家在不同特征的差异会变得更容易。 降维处理：z1表示象征国家整体情况的特征向量，如”国家总面积”、”国家总体经济水平”；z2表示象征人均情况的特征向量，如“人均GDP”，”人均幸福感”。 降维处理后的维度图如上图。右侧的点，象征着国家整体经济比较好的国家；上方的点，象征着人均经济比较好、人均幸福感较高、人均寿命较长…的国家。 主成分分析法(Principal Componet Analysis, PCA）Abstract：PCA是降维问题中目前最流行的算法。 首先开始讨论PCA问题的公式描述，也就是说，我们用公式准确地精确地描述：我们想让PCA来做什么。 PCA的执行过程2D -&gt; 1D 情景：有一个二维数据集。 目的：二维降一维。 方法：找到一条直线将数据投影到直线上，而且各样本点到直线的最短距离的平方和必须最小。 PCA：寻找一个低维的面(在这个例子中，其实是一条直线）数据投射在上面，使得这些蓝色小线段的平方和达到最小值。这些蓝色线段的长度被叫做投影误差。【PCA所做的就是寻找一个投影平面，对数据进行投影，使得这个能够最小化。】 具体阐述PCA：寻找一个向量u(i)，该向量属于nn维空间中的向量（在这个例子中是二维的），我们将寻找一个对数据进行投影的方向，使得投影误差能够最小（在这个例子里，我们把PCA寻找到这个向量记做u(1)。所以当我把数据投影到这条向量所在的直线上时，最后我将得到非常小的重建误差。 注：无论PCA给出的是这个u(1)u(1)是正还是负都没关系。因为无论给的是正的还是负的u(1)u(1)它对应的直线都是同一条，也就是我将投影的方向。 attention：PCA被应用前，通常需先进行均值归一化和特征规范化，使得特征x1和x2均值为0，数值在可比较的范围之内。 PCA的执行过程3D -&gt; 2DPCA：寻找一组k维向量(一条直线、或者平面、或者诸如此类等等)对数据进行投影，来最小化正交投影误差。 如图，寻找两个向量u(1)和u(2)，一起定义一个二维平面，并将数据投影到这个二维平面。 PCA和线性回归的关系PCA与线性回归看起来相似，但其实不同。1.线性回归是最小化点与直线之间的平方误差，而PCA是最小化点与直线的最短距离(直角距离)的平方和误差。2.线性回归有一个特别变量y作为即将预测的值，而PCA没有特殊变量要预测。 如上图1为线性回归，图2为PCA。 PCA算法的实现过程数据预处理1.均值归一化(mean normalization) 首先应该计算出每个特征的均值μ，然后我们用x−μ来替换掉x。这样就使得所有特征的均值为0。 举例： 如果x1表示房子的面积，x2表示房屋的卧室数量，然后我们可以把每个特征进行缩放，使其处于同一可比的范围内。 首先计算出每个特征的均值：μj=(1/m)∑(m,i=1)xj(i) 然后每个样本值对应的特征减去其对应的均值：x(i)j←x(i)j−μj 将所有的特征替换为这种形式的结果。这样就保证了所有特征的均值为0。 2.特征缩放(feature scaling) 将每个特征的取值范围都划定在同一范围内，因此对于均值化处理之后的特征值x(i)j−μjxj(i)−μj，我们还需要做进一步处理：x(i)j ← (xj(i)−μj)/sj. 这里sj表示特征j度量范围，即该特征的最大值减去最小值。 PCA算法PCA是在试图找到一个低维的子空间，然后把原数据投影到子空间上，并且最小化平方投影误差的值（投影误差的平方和，即下图中蓝色线段长度的平方和）。 那么如何计算这个子空间？ 1.假如说我们想要把数据从nn维降低到kk维，我们首先要做的是计算出下面这个协方差矩阵(通常用∑来表示)： ∑=(1/m)∑(n,i=1)(x(i))*(x(i))^T 这个希腊符号∑∑和求和符号重复了,勿混淆。 2.计算出这个协方差矩阵后，假如我们把它存为Octave中的一个名为Sigma的变量，我们需要做的是计算出Sigma矩阵的特征向量(eigenvectors)。 在Octave中，你可以使用如下命令来实现这一功能： [U,S,V] = svd(Sigma); 注：svd函数输出3个矩阵USV，我们真正需要的事U矩阵（n*n矩阵），因为U 矩阵的列元素就是我们需要的u(1)，u(2)等等 如果我们想将数据的维度从n降低到k的话，我们只需要提取前k列向量。这样我们就得到了u(1)到u(k)，也就是我们用来投影数据的k个方向。 取出U矩阵的前k列得到一个新的，由u(1)到u(k)组成的矩阵Ureduce： （将上图中的n改为k即为Ureduce矩阵,n*k维） 然后我们用这个Ureduce来对我的数据进行降维。我们定义：z = Ureducex，z∈R^k；其中Ureduce是nk维矩阵，xx 是n×1的矩阵，因此z是k×1的矩阵。 总结PCA全过程： 1.均值归一化 ：保证所有的特征量都是均值为0的。 2.特征缩放 3.计算出这个协方差Sigma矩阵 4.应用svd函数来计算出U S V矩阵 [U,S,V] = svd(Sigma); 5.取出U矩阵的前k列元素组成新的Ureduce矩阵 Ureduce = U(:,1:k); 6.z = Ureduce`*x 给出我们从原来的特征xx变成降维后的zz的过程 （三行代码实现PCA） 应用PCA如何还原数据？ 假设有一个已经被压缩过的z(i)它有100个维度，怎样使它回到其最初的表示x(i)也就是压缩前的1000维的数据呢？ 如图，在PCA算法中，我们有下面这些样本。我们让这些样本投影在一维平面z1上，并且明确地指定其位置。那么给出一个一维实数点z我们能否，让zz重新变成原来的二维实数点x呢？即做到：z∈R → x∈R^2. 已知：z=UTreduce*x 若想还原，则方程变为：xapprox=Ureduce*z 检查维度，在这里Ureduce是一个n×k矩阵，z就是一个k×1维向量。将它们相乘得到的就是n×1维。故xapprox是一个n维向量 同时根据PCA的意图，投影的平方误差不能很大。也就是说xapprox将会与最开始用来导出z的原始x很接近。用图表示出来就是这样： 这就是用低维度的特征数据z还原到未被压缩的特征数据的过程。我们找到一个与原始数据xx近似的approx。我们也称这一过程为原始数据的重构(reconstruction)。 选择主成分的数量k把n维特征变量降维到k维特征变量。这个数字k是PCA算法的一个参数。这个数字k也被称作主成分的数量。 PCA所做的是尽量最小化平均平方映射误差 (Average Squared Projection Error) 。 (1/m)∑(m,i=1)||xi−x(i)approx||^2 即最小化xx和其在低维表面上的映射点之间的距离的平方。这就是平均平方映射误差。 定义数据的总变差(Total Variation)： (1/m)∑(m,1=m)||x(i)||^2 数据的总变差 (Total Variation) 是这些样本的长度的平方的均值。它的意思是 “平均来看，我的训练样本距离零向量（原点）多远？”。 当我们去选择k值的时候，我们通过平均平方映射误差除以数据的总变差来表示数据的变化有多大。我们想要这个比值能够小于1%： 1m∑mi=1||x(i)−x(i)approx||21m∑mi=1||x(i)||2≤0.01 [(1/m)∑(m,i=1)||xi−x(i)approx||^2]/[(1/m)∑(m,1=m)||x(i)||^2] &lt;= 0.01 数字0.01是人们经常用的一个值，另一个常用的值是0.05。如果选择了0.05，就意味着95%的差异性被保留了。从95到99是人们最为常用的取值范围。 你可能会发现，对于许多数据集，即使保留了99%的差异性，可以大幅地降低数据的维度。因为大部分现实中的数据，许多特征变量都是高度相关的。所以实际上大量压缩数据是可能的，而且仍然会保留99%或95%的差异性。 具体实现算法1.从1开始，依次递增k的值，尝试检查差异性是否达到预设值。 例如： 尝试k=1时的PCA。 计算出Ureduce，z(1)，z(2)，…，z(m)，x(1)approx，…，x(m)approx 检查是否满足： [(1/m)∑(m,i=1)||xi−x(i)approx||^2]/[(1/m)∑(m,1=m)||x(i)||^2] &lt;= 0.01 如果满足条件，我们就用k=1；但如果不满足，那么我们接下来尝试k=2，然后我们要重新走一遍这整个过程。 以此类推一直试到上面不等式成立为止。 这种方式非常低效。每次尝试使用新的kk值带入计算时，整个计算过程都需要重新执行一遍. 2.一种更快的算法当你调用svd来计算PCA时，你会得到三个矩阵[U,S,V]: [U,S,V]=svd(Sigma) 除了之前提到的U矩阵之外，当你对协方差的矩阵Sigma调用svd时，我没还会得到中间的这个S矩阵。S矩阵是一个n×n的对角矩阵，它只有在对角线上的元素不为0，其余的元素都是0。并且显而易见，它是一个方阵： 实际上对于一个给定的k值，可以通过这个SS矩阵方便的计算出差异性那一项的值： 例如，假设差异性要满足小于0.01，那么可以得出： 即： ∑(k,i=1) Sii/∑(n,i=1)Sii ≥ 0.99 可以从1开始，慢慢增大kk的值，来计算上面这个不等式，直到满足为止即可（得到满足上面不等式的最小kk值）。 总结：只需要调用一次svd函数，通过svd给出的S矩阵你就可以通过依次增加k值的方式来求解 步骤： 1.对协方差矩阵Sigma调用一次svd： [U,S,V] = svd(Sigma) 2.使用下面的不等式求得满足条件的最小k值： ∑(k,i=1) Sii/∑(n,i=1)Sii ≥ 0.99 即使你想要手动挑选kk值，如果你想要向别人解释你实现的PCA的性能具体如何，那么一个好方法就是算出这个值：∑(k,i=1) Sii/∑(n,i=1)Sii。它会告诉你百分之多少的差异性被保留了下来。 PCA应用场景总结1.数据压缩 减少内存或者磁盘空间的使用 提升学习算法的效率（k值的选择是关键） 2.数据可视化 将数据降维到二/三维度进行可视化展示 通过PCA来提高学习算法的速度举例说明，假如你正在用机器学习来处理图片数据。假设每张输入的图片尺寸是100×100100×100的，那么对于每张图片来说，都有10000个像素点。假设样本x(i)是包含了10000像素强度值的特征向量，即： x(i)∈R 那么对于我们的样本数据集来说： (x(1),y(1)),(x(2),y(2)),…,(x(m),y(m)) 每个样本中，对应的x(i)都是10000维的特征向量。 可想而知，这么高维度的数据带入到逻辑回归、神经网络、支持向量机或者任何别的算法中，学习算法运行的都会很慢。 幸运的是，通过使用PCA，我们能够降低数据的维数，从而使得算法能够更加高效地运行。这就是PCA提高算法运算效率的原理。 降维步骤首先我们需要检查带标签的训练数据集，并提取出输入数据。我们只需要提取出x并暂时把y放在一边。这一步我们会得到一组无标签的训练集： x(1),x(2),…,x(m)∈R10000 从x(1)到x(m)，每个样本都是10000维的数据。然后我们应用PCA降维，我们会得到一个降维后的1000维的数据集： z(1),z(2),…,z(m)∈R1000 这样我们就得到了一个新的训练集： (z(1),y(1)),(z(2),y(2)),…,(z(m),y(m)) 现在，我可以将这个已经降维的数据集输入到学习算法中，来得出假设函数，并把降维后的数据作为输入带入，做出预测。 以逻辑回归为例： 逻辑回归中，我们得到的假设函数如下： hθ(z)=1/[1+e^(−θTz)] 我们将z向量作为输入带入，并得出一个预测值。 最后，如果你有一个新的样本x，那么你所要做的是将你的测试样本x通过同样的PCA降维之后，你会得到这个样本所对应的z。然后将这个z值带入到这个假设函数中进行预测。 PCA的错误使用—使用它来避免过拟合有一个值得提醒的频繁被误用的PCA应用场景，那就是使用它来避免过拟合。 具体原因是将高维度数据降维处理后，相较于原先的数据，会更不容易出现过拟合的现象。例如我们将10000维的数据降到了1000维，那么降维后的1000维数据相较于降维前的10000维数据更不容易产生过拟合。 因此有人认为PCA是一种避免过拟合的方法，需要强调一下，为了解决过拟合问题而使用PCA是不适合的. 如果你比较担心过拟合问题，那么你应该使用正则化方法，而不是使用PCA来对数据进行降维。 PCA会丢失信息：如果你仔细想想PCA的工作原理，你会发现它并不需要使用数据的标签，你只需要设定好输入数据x(i)，同时使用这个方法来寻找更低维度的数据近似，在这个过程中，PCA实际上已经把某些信息舍弃掉了。 两个建议1.真的需要PCA吗？ 2.不要一开始就带入PCA:先使用原始数据x(i)看看效果]]></content>
      <categories>
        <category>Artificial Intelligence</category>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Artificial Intelligence</tag>
        <tag>Machine Learning</tag>
        <tag>Coursera ML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[“井底之蛙”：论媒体报道的倾向性对民众的影响]]></title>
    <url>%2F2017%2F10%2F10%2F%E2%80%9C%E4%BA%95%E5%BA%95%E4%B9%8B%E8%9B%99%E2%80%9D%EF%BC%9A%E8%AE%BA%E5%AA%92%E4%BD%93%E6%8A%A5%E9%81%93%E7%9A%84%E5%80%BE%E5%90%91%E6%80%A7%E5%AF%B9%E6%B0%91%E4%BC%97%E7%9A%84%E5%BD%B1%E5%93%8D%2F</url>
    <content type="text"><![CDATA[Abstract：我们观念上认为新闻报道是公正中立多元的，但实际上媒体从来都是受倾向的支配。观念，利益等因素管制着媒体的喉咙。而媒体报道 的倾向性会导致民众视野的狭窄化，其同时也影响民众世界观的塑造。现如今以今日头条为代表的机器推荐算法更是有局限用户视野的倾向，那么如何解决？ 新闻报道从来就是受倾向支配的，虽然它们无一例外地自称中立，多元，真实。多元只能靠读者自己读不同报纸的方法获得。许多公正表达了全部立场的媒体最多是做了些更精致的包装。观念和利益因素管制着所有媒体。 报纸的倾向的来源有三种： 记者自己的偏见、兴趣和观念，他不报道自己不喜欢、不能说清的东西 来自他公司的利益要求：信息是商品，媒体是商业，第一宗旨是报道大众喜欢看的，以提高利润 如明星的八怪一定多与矿工的挣扎故事。 观念、文化等方面的偏见，记者不会报道大众没有兴趣的文化消息，或已有意识形态不能解释的东西 对于一般人，他的世界观等于他每天从媒体上看到的东西。而媒体报道的倾向性很可能导致民众对世界的认知的偏颇。喜欢看什么，媒体就给他看什么，投其所好，继续赚钱。 如，媒体从来都是报道飞机坠毁而非安全降落。如此导致很多民众认为飞机是不安全的，但实际上只是事故的发生几率被媒体无限放大了而已。 就像井底之蛙，因为只看得到那片天，我们便以为那就是世界。 反观如今机器推荐算法被广泛应用在新媒体内容平台上，根据用户的阅读偏好向用户推荐内容，这又是否会导致用户的阅读面和视野的狭窄化呢？而当我们意识到这个问题，又可以如何通过改进机器推荐算法来缓解或避免这种现象的出现？下一次分析这个话题。]]></content>
      <categories>
        <category>Communication Science</category>
      </categories>
      <tags>
        <tag>Communication Science</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Coursera Machine Learning|Week7:SVM & Kernels]]></title>
    <url>%2F2017%2F10%2F08%2FCoursera-Machine-Learning-Week7-SVM-Kernels%2F</url>
    <content type="text"><![CDATA[Abstract：第一部分讲SVM支持向量机的原理和推导，以及其为何可产生大间距分类。第二部分讲如何通过核函数达到改造支持向量机以构造复杂的非线性分类器的目的。第三部分将如何使用SVM。当遇到机器学习问题时，算法确实很重要，但是通常更重要的是：你有多少数据，你有多熟练，是否擅长做误差分析和调试学习算法，想出如何设计新的特征变量，想出如何设计新的特征变量，以及找出应该输入给学习算法的其它特征变量等方面。通常这些方面会比你使用逻辑回归还是SVM更加重要。 大间距分类Abstract：相较于逻辑回归，支持向量机SVM在学习复杂的非线性方程时提供更为清晰强大的解决方式。SVM的推导是在逻辑回归的基础上改进。在对样本的分类效果上，支持向量机比逻辑回归的要求更高——要求大间距。即SVM始终在努力用最大间距去分类样本。 优化目标监督学习中许多学习算法的性能都非常类似，因此重要的不是你该选择使用学习算法A还是学习算法B，而更重要的是应用这些算法时所创建的大量数据。 支持向量机SVM(Support Vector Machine)：广泛应用于工业界和学术界。与逻辑回归和神经网络相比，支持向量机或者简称SVM在学习复杂的非线性方程时提供了一种更为清晰，更加强大的方式。 支持向量机引入逻辑回归的假设函数形式：hθ(x) = 1/[1+e^(−θTx)] S型激励函数： 我们需要逻辑回归做什么？ 如果有一个样本为y=1，那么我们希望假设函数h(x)≈1，即θT&gt;&gt;0。你不难发现，此时逻辑回归的输出将趋近于1。 如果有另一个样本为y=0，那么我们希望假设函数h(x)≈0，即θT&lt;&lt;0。此时逻辑回归的输出将趋近于0。 进一步观察逻辑回归的代价函数，会发现样本(x,y)都会为总代价函数增加这样一项： 在逻辑回归中，这一项表示一个训练样本所对应的表达式。 现在考虑y = 1和y = 0的2种情况： 1.y=1的情况下（即θTx&gt;&gt;0） 对于： 由于(1-y) = 0,故我们只考虑前半部分： 若画出代价函数关于z的图，则有下图： 当z增大时(即θ^(Tx)增大时)，z对应的值会变得非常小，对整个代价函数而言，影响也非常小。 现在开始建立支持向量机，从代价函数−y*[log(1/1+e−θTx]开始修改： 画出一个非常接近于逻辑回归函数的折线，其由z = 1的一点的两条线段组成。 2.y = 0的情况下（即θTx&lt;&lt;0） 对于 由于y = 0，故只考虑后半部分： 画出代价函数关于z的图： 建立支持向量机： 目的是在y = 0的前提下使用新的代价函数。 然后给上面两种情况得出的2个方程命名： 第一个为cost1(z)，第二个为cost0(z)，下标分别指函数中对应的y = 1 or 0的情况。 构建支持向量机1.替换逻辑回归函数逻辑回归中的代价函数J(theta): 做法：将逻辑回归中的代价函数J(theta)里的(−loghθ(x(i)))和((−log(1-hθ(x(i)))分别替换为cost1(z)和cost0(z)，即cost1(θ^Tx(i))和cost0(θ^Tx(i)),故对支持向量机的最小化代价函数问题，代价函数的形式为： 2.去除多余的常数项1/m现在按照支持向量机的惯例，我们去除1/m这一项，因为为常数项，即使去掉我们也可以得出相同的θ最优值： 3.正则化项系数的处理在逻辑回归的目标函数中，我们有两项表达式： 来自于训练样本的代价函数:1/m[∑i=1my(i)(−loghθ(x(i)))+(1−y(i))((−log(1−hθ(x(i)))))] 正则化项：λ/2(∑j=1n) θj^2 我们需要使用正则化项来平衡代价函数，即：A+λB 其中，A相当于上面的第一项，B相当于第二项。 通过修改不同的正则化参数λ来达到优化目的，就可以使训练样本拟合得更好。 对于支持向量机，按照惯例我们将使用一个不同的参数来替换这里使用的λ来实现权衡这两项的目的。这个参数我们称为C。同时将优化目标改为: CA + B 在逻辑回归中，如果给λ一个很大的值，那么就意味着给与B了一个很大的权重，而在支持向量机中，就相当于对C设定了一个非常小的值，这样一来就相当于对B给了比A更大的权重。 实质：使用参数C来替换λ来控制A和B的权衡关系（A+λB ——&gt; CA + B ） 因此，即得到在支持向量机中的我们的整个优化目标函数： 注:有别于逻辑回归的一点，对于支持向量机假设函数的形式如下：hθ(x)=0 if θTx&lt;0;hθ(x)=0 if θTx&lt;0;而不是逻辑回归中的S型曲线：hθ(x)=1/(1+e^−x) 大间距的直觉Abstract：有时会将支持向量机看做是大间距分类器。 支持向量机模型的代价函数： 1.如果有一个正样本，即y=1时，那么代价函数cost1(z)的图像如下： 只有在z≥1(即θTx≥1)时(不仅仅是≥0)，代价函数ost1(z)的值才等于0。 2.如果你有一个负样本，即y=0时，那么代价函数cost0(z)的图像如下： 只有在z≤−1(即θTx≤−1)时(不仅仅是&lt;0)，代价函数cost0(z)的值才等于0。 注：以上是支持向量机的一个有趣的性质。 安全距离因子在对样本的分类效果上，支持向量机比逻辑回归的要求更高——要求大间距。 逻辑回归： 如果你有一个正样本，即y=1的情况下，我们仅仅需要θTx≥0； 如果你有一个负样本，即y=0的情况下，我们仅仅需要θTx&lt;0； 就能将该样本恰当的分类了。 支持向量机：不仅仅要求θTx≥0或θTx&lt;0，而且要求θTx比0大很多，或小很多。比如这里要求θTx≥1以及θTx≤−1。相当于在支持向量机中嵌入一个额外的安全距离因子。 接下来看这个因子会导致什么结果： 将代价函数中的常数项C设置成一个非常大的值，比如100000或者其他非常大的数，然后再来观察支持向量机会给出什么结果。 此时，我们希望找到一个使第一项为0的最优解。第一项为： 当输入一个正样本y(i)=1时，我们想令上面这一项为0，对于代价函数cost1(z)我们需要使得θTx(i)≥1。 当输入一个负样本y(i)=0时，我们想令上面这一项为0，对于代价函数cost1(z)我们需要使得θTx(i)&lt;=-1。 选择参数使第一项为0，因此这个函数的第一项为0，因此是：minθC0 + 1/2(∑j=1n)θj^2. C0的结果是0，因此可以删掉，所以最终得到的结果是： minθ12∑j=1nθ2j 其中： 若y(i)=1时，则θTx(i)≥1 即得一个非常有趣的决策边界。 SVM决策边界：线性分割案例 如上图，这个数据集是线性可分的（即存在一条直线把正负样本分开）。存在很多直线能把正负样本分开，如绿线和红线。支持向量机会选择黑线，因其看起来更稳健，即黑线拥有相对于训练数据更大的最短间距（margin）。因其一直努力用一个最大间距来分离样本，故器有时又被称为大间距分类器。而红线和绿线因离训练样本很近，故分类效果会比黑线差。 大间距分类器中的异常值Abstract：支持向量机中的异常数据处理。 异常值出现 当C值非常大时，仅一个异常值就会将我们的决策边界旋转很大角度，这样是不明智的，但支持向量机确实会这样处理。而若我们适当减小C值，最终还是会得到黑色决策边界。 若数据线性不可分，如： 支持向量机也可恰当分开。 注： C的作用其实等同于1/λ，λλ就是我们之前用到的正则化参数。在支持向量机中，CC不是很大的时候，可以对包含异常数据、以及线性不可分的数据有比较好的处理效果。 支持向量机所做的事情，其实就是在极小化参数向量θθ范数的平方（或者说是长度的平方）。 核函数 Kernel Function（Kernels）Abstract：如何通过核函数达到改造支持向量机以构造复杂的非线性分类器的目的，以及如何在实际中应用这些想法，如如何处理向量机中的偏差方差折中。关于核函数，我们通过标记点和相似性函数来定义新的特征变量从而训练复杂的非线性分类器。核函数实际上是通过双次复合定义特征变量来得到的函数。 核函数 IAbstract：讲解如何通过标记点，以及核函数，来训练出非常复杂的非线性判别边界的方法。 如上图，如若有这样一个训练集，并希望拟合一个非线性的判别边界来区别正负样本，则判别边界可能如图。该决策边界由类似下面的多项式构成： 如果θ0+θ1x1+θ2x2+θ3x1x2+θ4x21+θ5x22+…≥0，则预测hθ(x)=1； 如果θ0+θ1x1+θ2x2+θ3x1x2+θ4x21+θ5x22+…&lt;0，则预测hθ(x)=0； 若把假设函数改写成一下形式：θ0+θ1f1+θ2f2+θ3f3+θ4f4+θ5f5+…，则有：f1=x1;f2=x2;f3=x1x2;f4=x1^2;f5=x2^2; 若使用高阶项作为特征变量，运算量会非常大，因为有太多高阶项需要被计算。那么，我们是否有不同的选择，或者是更好的选择来构造特征变量，以用来嵌入到假设函数中呢？ 用核函数构造新特征1.定义3个特征变量(但是对于实际问题而言，我们可以定义非常多的特征变量），将这三个点标记为l(1)，l(2)，l(3) 2.定义新特征变量：f1 = similarity(x,l(1))； similarity(x,l(1))是一种相似度度量，度量样本x与第一个标记l(1)的相似度。相似度公式：f1 = similarity(x,l(1)) = exp(−||x−l(1)||/(2*σ^2)),exp是自然常数e为底的指数函数,||w||是表示向量w的长度,||x−l(1)||是向量的欧式距离。 3.依次写出f1,f2,f3:f1 = similarity(x,l(1)) = exp(−||x−l(1)||/(2σ^2))f2 = similarity(x,l(2)) = exp(−||x−l(2)||/(2σ^2))f3 = similarity(x,l(3)) = exp(−||x−l(3)||/(2*σ^2)) 注：similarite(x,l)函数，即核函数(Kernels)/高斯核函数.核函数我们通常不写作similarity(x,l(i)，而是写作：k(x,l(i)). 核函数可以做什么？f1 = similarity(x,l(1)) = exp(−||x−l(1)||22σ2) = exp[−∑nj=1(xj−l(1)j)22σ2] l(1)是之前在图中选取的几个点之中的一个，上面是x和l(1)之间的核函数。 其中||x−l(1)||^2这一项可以表示成各个x向量到l向量的距离求和的形式：∑nj=1(xj−l(1)j)^2（这里我们依然忽略了截距的影响，即令x0=1）。 假设，如果x≈l(1)，即x与其中一个标记点非常接近，那么这个欧氏距离||x−l(1)||就会接近0，因此：f1≈exp(−0^2/2σ^2)≈1. 相反，若x离l(1)很远，则有:f1 ≈ exp(−(large number)^2/2σ^2) ≈ 0. 这些特征变量的作用是度量xx到标记l(1)的相似度的，并且如果x离l非常接近，那么特征变量f就接近1；如果x离标记l(1)非常远，那么特征变量ff就接近于0。 则三个标记点l(1)，l(2)，l(3)每一个标记点会定义一个新的特征变量。 f1 = similarity(x,l(1)) = exp(−||x−l(1)||/(2σ^2))f2 = similarity(x,l(2)) = exp(−||x−l(2)||/(2σ^2))f3 = similarity(x,l(3)) = exp(−||x−l(3)||/(2*σ^2)) 即：给出一个训练样本x，我们就能基于我们之前给出的标记点l(1)，l(2)，l(3)来计算出三个新的特征变量f1，f2，f3。（双次复合特征变量） 深入理解核函数x对f的值的影响假设我们有两个特征x1和x2，假设我们第一个标记点是l(1)：l(1) = [3，5]T 假设σ^2=1,若画出：f1 = similarity(x,l(1)) = exp(−||x−l(1)||/(2*σ^2)) 结果： 其中，左图纵轴为f1，水平为x1和x2；右图为左侧图的等高线图。 当x=(3,5)的时候，f1=1，因为它在最大值的位置上。所以如果x往旁边移动，离这个点越远，那么从图中可以看到f1的值就越接近0。 σ^2对f的值的影响σ^2是高斯核函数的参数，改变它会得到略微不同的结果。 对比σ^2=1,σ^2=0.5,σ^2=3的情况: 发现：函数形状相似，只是σ^2=0.5相较于σ^2=1凸起的宽度变窄了，等值线图也收缩了一些；σ^2=3相较于σ^2=1凸起的宽度变宽了，等值线也扩张了一些。 所以，如果我们将σ^2设为0.5时，特征变量f1下降到0的速度也会相应变快；如果我们将σ^2设为3时，特征变量f1下降到0的速度也会相应变慢。 获取预测函数Abstract：在了解了特征变量的定义的基础上，我们来看看能得到什么样的预测函数。 给定一个训练样本x，我们要计算出三个特征变量f1，f2，f3 若θ0+θ1f1+θ2f2+θ3f3≥0 ，则预测函数的预测值为1，即y=1。 对于这个特定的例子而言，假设我们已经找到了一个学习算法，并且假设我已经得到了这些参数的值，比如：θ0=−0.5，θ1=1，θ2=1，θ3=0 假设我有一个训练样本x，计算预测函数的结果： 训练样本x接近于l(1),则f1接近于1：f1 ≈ 1 x离l(2)，l(3)都很远，故f2就接近于0，f3也接近于0：f2≈0，f3≈0； 带入上面的公式可得：θ0+θ1f1+θ2f2+θ3f3=θ0+θ1·1+θ2·0+θ3·0=−0.5+1=0.5 ≥ 0 故预测结果为1 同理，选择另一个训练样本x，带入计算，发现f1f2f3都接近于0，故得到θ0+θ1f1+θ2f2+θ3f3=−0.5&lt;0,故预测的y值为0. 对于接近l(1)和l(2)的点，我们的预测值是1，对于远离l(1)和l(2)的点，我们最后预测的结果等于0。 最后得到的预测函数的判别边界为： 在这个红色的判别边界里面，预测的y值等于1；以外预测的y值等于0。 总结：以上是如何通过标记点以及核函数，来训练出非常复杂的非线性判别边界的方法。 核函数 IIAbstract：介绍如何在实际中应用这些想法，如怎么处理支持向量机中的偏差方差折中。 如何选取标记点landmark 选择标记点，如l(1),l(2),l(3) 这些点使我们能够定义相似度函数，也称核函数。此例中我们的相似度函数为高斯核函数。 上例中，我们的标记点是手动选取的，但在复杂度额学习问题中，我们需要更多的标记点。如何选取标记点？ 方法：直接将训练样本作为标记点。如图可得m个标记点，每一个标记点的位置，都与每一个样本点的位置精确对应。特征函数基本上是在描述每一个样本距离样本集中其他样本的距离。 步骤解析： 1.给定m个训练样本：(x(1),y(1)),(x(2),y(2)),…,(x(m),y(m))； 2.选取与m个训练样本精确一致的位置作为我的标记点：l(1)=x(1),l(2)=x(2),…,l(m)=x(m). 3.当输入样本x（样本x可以属于训练集，也可以属于交叉验证集，也可以属于测试集），我们可以计算这些特征，即：f1=similarity(x,l(1))，f2=similarity(x,l(2)) 4.最终我们能得到一个特征向量，我们将特征向量记为f：f = [f1,f2,f3…fm]T 5.如果我们需要的话，可以添加额外的特征f0,值始终为1：f = [f0, f1,f2,f3…fm]T f0作用类似于截距x0. 如果已得到参数θ并且想对样本x做出预测，我们先要计算特征向量f，f是m+1维的特征向量（这里有m是因为我们有m个训练样本，因此就有m个标记点）。 我们在θTf≥0时，预测y=1 这里θTf=θ0f0+θ1f1+…+θmfm 以上是当已知参数θ时，怎么做出预测的过程 那么，怎样得到参数θ？ 在使用SVM学习算法的时候，具体来说就是要求解这个最小化问题： 需要求出能使这个式子取最小值的参数θ。注意，这里我们把之前的x(i)换成了f(i)。 通过解决这个最小化问题，我们就能得到支持向量机的参数。 最后一个对于这个优化问题的细节是：我们有n=m个特征。有效的特征数量应该等于f的维数，所以n其实就等于m。 以上就是支持向量机的学习算法。 注： 将核函数用于逻辑回归上时，会变得非常的慢 我们并不需要知道怎么去写一个软件，来最小化代价函数。能找到很好的成熟的软件来做这些，就像不建议自己写矩阵求逆函数，或者平方根函数的道理一样。这些软件包已经包含了那些数值优化技巧，所以我们不必担心这些东西。 SVM参数1.C(=1/λ) λ是逻辑回归算法中的正则化参数，所以C对应着我们之前在逻辑回归问题中的λ，这意味着: 较小的λ对应较大的C，这就意味着有可能得到一个低偏差但高方差的模型。 较大的λ对应较小的C，这就意味着有可能得到一个高偏差但低方差的模型。 所以使用较大的C值模型，为高方差，更倾向于过拟合；而使用较小的C值的模型，为高偏差，更倾向于欠拟合。 2.σ^2 如果σ^2越大，那么高斯核函数倾向于变得越平滑，由于函数平滑且变化的比较平缓，这会给你的模型带来较高的偏差和较低的方差，由于高斯核函数变得平缓，就更倾向于得到一个随着输入x变化得缓慢的模型； 反之如果σ^2越小，那么高斯核函数会变化的很剧烈，在这种情况下，最终得到的模型会是低偏差和高方差： 以上即利用核函数的支持向量机算法。 使用SVMAbstract：对于SVM，我们需要知道如何使用SVM安装包。SVM核函数的选择很关键，有多种核函数供选择（因时制宜）。至于逻辑回归和SVM的使用在不同场景下有不同的选择。而当遇到机器学习问题时，算法确实很重要，但是通常更重要的是：你有多少数据，你有多熟练，是否擅长做误差分析和调试学习算法，想出如何设计新的特征变量，想出如何设计新的特征变量，以及找出应该输入给学习算法的其它特征变量等方面。通常这些方面会比你使用逻辑回归还是SVM更加重要。 支持向量机是一个特定的优化问题，但是不建议自己去手动实现这一算法来求解参数θ。就像如今只有很少的人，或者说根本没有人会考虑自己写代码，来实现对矩阵求逆，或求一个数的平方根等。我们只需要调用库函数来实现这些功能即可。强烈建议使用一个高度优化的软件库，如liblinear和libsvm，而不是尝试自己去实现它。 尽管不需要自己去实现SVM，但也需要做以下几件事： 选择参数C 选择核函数（相似度函数） 核函数的选择线性核函数（无核函数）即不用任何核函数。 即对于预测结果y=1，满足θTx≥0。 例如这种情况下当θ0+θ1x1+…+θnxn≥0时，预测y=1。 使用情境：当特征数量n很大，但数据量m很小时，由于数据量不足，在这种情况下如果使用其他核函数，可能会过拟合，因此，此时线性核函数是一个合理的选择。 高斯核函数fi = exp(−||x−l(i)||^2/2σ^2) 需要选择一个σ。 如果σ^2很大，那么可能得到一个较高偏差较低方差的分类器。 如果σ^2很小，那么可能得到一个较低偏差较高方差的分类器。 使用情境:若原来的特征变量x是n维的，而且n很小，样本数量m很大时，高斯核函数会是一个不错的选择。 如何使用高斯核函数在很多SVM的软件包中，如果你需要使用SVM时，你需要提供一个核函数。 具体地说，如果你决定使用高斯核函数，那么你需要做的就是根据你所用的SVM软件包，来提供一个用于计算核函数的特定特征的方法，如图。然后它将自动地生成所有特征变量。 注意：如果你有大小很不一样的特征变量，在使用高斯核函数之前，对它们进行归一化是很重要的。为了让SVM更好的工作，我们需要对特征变量进行归一化处理。这将会保证SVM能够同等地关注到所有不同的特征变量。 选择其他核函数多项式核函数（Polynomial kernel） 字符串核函数（String kernel） 如果你的输入数据是文本字符串，或者其他类型的字符串，有时会用到这个核函数。 卡方核函数（chi-square kernel） 直方图交叉核函数（histogram intersection kernel）… 可以用它们来度量不同对象之间的相似性。 例如，你在做一些文本分类问题，在这个问题中，输入变量xx是一个字符串，我们想要通过字符串核函数来找到两个字符串间的相似度 两个细节多类分类 怎样让SVM输出下面这种各个类别间合适的判定边界呢？ 1.大部分SVM软件包已经内置了多类分类的函数了，因此，如果你用的是这种软件包，你可以直接使用内置函数。 2.另一种方式就是使用一对多(one-vs-all)方法。这个我们在讲逻辑回归的时候讨论过，所以，你要做的就是要训练KK个SVM，每一个SVM把一个类同其他类区分开。这种操作会给你KK个参数向量： θ(1),θ(2),…,θ(K) 最后，这就与我们在逻辑回归中用到的一对多方法一样，选取使得(θ(i))Tx最大的类ii即可。 其实大多数软件包都已经内置了多类分类的函数，因此你不必重新造轮子。 逻辑回归 vs SVM关于逻辑回归和SVM，什么时候用哪个？ 假设n是特征变量的个数，m是训练样本数： -如果n(相对于m)大很多时，使用逻辑回归，或者使用无核函数的SVM（线性核函数）。比如你有一个文本分类的问题，特征数量n=10000，而且如果你的训练集大小为m=10，在这个问题中，你有10000个特征变量，对应10000个词，但是你只有10个训练样本。这种情况下就比较适合使用逻辑回归或者线性核函数的SVM了。-如果n较小，m是中等大小，（例如n为1到1000之间的值，mm为10到10000之间的值）那么使用高斯核函数的SVM效果好一些。-如果n很小，m很大时（例如n=1000,m=100000+），那么高斯核函数的SVM运行起来会很慢，这种情况下，需要尝试手动地创建更多的特征变量，然后使用逻辑回归或者无核函数的SVM（线性核函数）。 逻辑回归和不带核函数的SVM它们都是非常相似的算法，他们会做相似的事情，并且表现也相似，但是根据你实现的具体情况，其中一个可能会比另一个更加有效。 但是SVM的威力会随着你用不同的核函数而发挥出来。 什么时候使用神经网络？一个设计的好的神经网络很可能非常有效,但其缺点是训练起来比较慢。但如果你有一个很好的SVM实现包，它就会运行的比较快，比神经网络快很多。 SVM的优化问题，实际上是一个凸优化问题。因此好的SVM优化软件包总是会找到全局最小值，或者接近它的值。对于SVM，你不需要担心局部最优。在实际应用中，局部最优对神经网络来说不是非常大，但是也不小。所以使用SVM，你不用考虑这部分问题。 总结当遇到机器学习问题时，算法确实很重要，但是通常更重要的是：你有多少数据，你有多熟练，是否擅长做误差分析和调试学习算法，想出如何设计新的特征变量，想出如何设计新的特征变量，以及找出应该输入给学习算法的其它特征变量等方面。通常这些方面会比你使用逻辑回归还是SVM更加重要。 但是SVM仍然被广泛认为是最强大的学习算法之一，最强大的学习算法之一，而且SVM在一个区间内是一个非常有效地学习复杂非线性函数的方法。因此，有了逻辑回归、神经网络、SVM这三个学习算法，我们已经具备了在广泛的应用里构建最前沿的机器学习系统的能力。]]></content>
      <categories>
        <category>Artificial Intelligence</category>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Artificial Intelligence</tag>
        <tag>Machine Learning</tag>
        <tag>Coursera ML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Coursera Machine Learning|Week6:Advice for Applying Machine Learning]]></title>
    <url>%2F2017%2F10%2F06%2FCoursera-Machine-Learning-Week6-Advice-for-Applying-Machine-Learning%2F</url>
    <content type="text"><![CDATA[Abstract：第一部分介绍如何用机器学习诊断法评价一个学习算法的可用性以及告知如何改进算法效果（面对机器学习问题，提高算法表现的方法）。第二部分讲怎样评价一个学习算法，以及如何用交叉验证法判断一个算法是偏差OR方差问题。第三部分讲如何操作偏斜数据。第四部分讲使用大数据集的必要性。 Evaluating a Learning Algorithm 评价一个学习算法Core Problem：假如我们正在开发一个机器学习系统，或想试着改进一个机器学习系统的性能，应如何决定接下来应该选择哪条路？ EG（预测房价）： 假设在预测房价的例子中，我们已完成可正则化线性回归，即最小化代价函数J的值。 假设我们通过训练训练集得到学习参数θ后，要将假设函数h(θ)放到一组新样本上测试，却发现测试新样本时预测出现巨大的误差，那么问题来了：如何改进这个算法？ 根本原因： 改进算法的方法通常有3种： 1.使用更多的训练样本；（但有时候不能解决问题） 2.尝试使用更少的特征集；（可尝试从众多特征集中精选一小部分来防止过拟合） 3.尝试使用更多的特征集；（也许目前的特征集对预测不是很有帮助，故可尝试获取更多有用的数据） 4.尝试增加多项式特征(x1^2,x2^2,x1,x2,etc.) 5.增大正则化参数λ； 6.减小正则化参数λ； 上面的方法耗时长且可能会白费功夫，下面介绍机器学学诊断法来评估算法性能。 机器学习诊断法(Machine learning diagnostic)引入作用： 深入了解某种算法是否有用 告诉你如何改进算法效果 Diagnostic:A test that you can run to gain insight what is/isn’t working with a learning algorithm, and gain guidance as to how best to improve its performance. 评估假设函数为什么要评估： 训练误差小不能说明假设是一个好的假设函数（可能过拟合），当推广到新训练集时可能不适用 如何判断一个假设函数过拟合？ 情况1：单特征变量 如上图，单特征变量时可画图，观察图形趋势与样本的拟合情况来判断是否过拟合。 情况2：多特征变量（评估假设函数的标准方法） 如上图，假设有一组数据组，将这10组数据三七分； 70%为训练集(Training set)：(x(1),y(1))，(x(2),y(2))，……，(x(m),y(m) m表示训练样本的总数 30%为测试集:(Test set):(x(1)test,y(1)test)(x(2)test,y(2)test)……(x(mtest)test,y(mtest)test) mtest 表示测试样本的总数，test表示这些样本是来自测试集。 注：若数据有某种规律则按7:3的比例取数据时应随机选取。 评估步骤详解线性回归中训练/测试流程： 1.对训练集学习，得到学习参数θ；（即最小化训练误差，即使用70%的数据训练得到的结果） 2.计算出测试误差；（将学习参数θ代入Jtest(θ)以计算测试误差） 上式实际上是计算测试机平方误差的平均值。 分类问题中训练和测试逻时 1.从训练数据中学习得到参数θ 2.用下面的式子计算测试数据的误差值 在分类问题中的测试误差Jtest(θ)——误分类率（0/1错分率）：表示预测到的正确或错误样本的情况。 比如说可以这样定义一次预测的误差： 当hθ≥0.5时y=0 或当hθ&lt;0.5时y=1 这两种情况下我们的假设都对样本进行另外误判，否则其他情况下假设值都能正确的对样本y进行分类。 然后我们就能应用错分率误差来定义测试误差，也就是： 以上我们介绍了一套标准技术来评价一个已经学习过的假设，接下来，我们要应用这些方法来帮助我们进行诸如特征选择一类的问题（比如多项式次数的选择，或者正则化参数的选择）。 多项式模型的选择以及训练集/验证集/测试集的划分Abstract：模型选择问题：确定对于某组数据最合适的多项式次数是几次，怎样选用正确的特征来构造学习算法。以及，如何将数据分为三组：也就是训练集、验证集和测试集，而不仅仅是前面提到的两组数据。 选择合适的模型解决过拟合问题Problem：充分拟合了的训练集误差通常不能准确预测出新样本。非常拟合训练集的参数不能推广到一般情况，或者说不能作为实际的泛化误差。 模型选择问题： 假设要选择最能拟合数据的多项式次数。d代表应选择的多项式次数（上图1-10）。具体地说，比如你想要选择一个模型，那就从这10个模型中，选择一个最适当的多项式次数，并且用这个模型进行估计，预测你的假设能否很好地推广到新的样本上。 那么你可以这样做： 1.对每一个模型求最小化训练误差，分别得到参数向量θ(1),θ(2),…,θ(10)； 2.对所有模型求出测试误差，分别为Jtest(θ(1)) 、Jtest(θ(2))、Jtest(θ(3))…Jtest(θ(10))； 3.比较，找出测试机误差最小的模型。 假设最终选择了五次多项式模型：Choose θ0+ … θ5x5. 正确评价某个假设函数的预测能力Problem：使用最能拟合测试机的参数d可能导致在拟合测试机结构时出现比实际泛化误差更完美的预测结果。而我们更关心的实际是对新样本的拟合效果，故如果我们用训练集来拟合参数θ0,θ1等参数时，拟合后的模型在作用于训练集上的效果是不能预测出我们将这个假设推广到新样板上时效果如何的。这些参数能很好拟合训练集但不一定能很好拟合新样本。（我们实际是用测试机来拟合参数d，但这不能保证假设函数在遇到新样本时的表现） 方式（将数据集分成 训练集、验证集、测试集3段）为了解决上述问题，在模型选择中，如果我们要评价某个假设，我们通常采用以下的方法： 1.给定某个数据集，我们要将其分为三段：训练集、交叉验证集（cross validation set）、测试集,典型分割比例为6：2：2。 m是训练集个数，mcv是交叉验证集个数，mtest是测试集个数。 2.得到 训练集/交叉验证集/测试集的误差： 3.使用交叉验证集来选择合适的模型，即通过使用训练集对每一个假设函数依次去求最小化的代价函数minJ(θ)，并求得对应的参数向量θ(d)。 4.是在交叉验证集中测试这些假设的表现，测出Jcv来看看这些假设在交叉验证集中表现如何。 5.选择交叉验证集误差最小的那个假设模型，假如这个模型是Jcv(θ(4))对应的那个模型，因此我们就选择这个四次多项式模型：θ0+θ1x1+…+θ4x4，拟合出最好的系数d=4。 注意：这个过程中，我们没有使用测试集进行拟合，即可回避测试集的嫌疑。则可光明正大使用测试集来估计所选模型的泛化误差。 6.可使用测试集来评价模型的表现。 总结：最佳评估方法是把数据分成 训练集、验证集、测试集三段。 偏差VS方差偏差VS方差Problem：若算法表现不理想，多半是出现2种情况：1.偏差大（欠拟合）；2.方差大（过拟合）； Core Problem：搞清楚到底是和偏差还是方差有关，或两个都有关。 Need to solve：弄清楚怎样评价一个学习算法，判断一个算法是偏差OR方差问题。 用实例解释： 如上图，若用2个很简单的假设来拟合数据，则会导致欠拟合；若用2个很复杂的假设来拟合，则会导致过拟合（拟合得过于完美）；而中间展示的中等复杂度的假设则对数据拟合得刚刚好，其对应的泛化误差也是三者中最小的。 方差：平方误差，即对训练集或验证集数据进行预测所产生的平均平方误差。 下面是示意图： 如上图，横坐标表示多项式次数，纵坐标代表训练误差。蓝色线代表训练集误差的变化情况，红色线代表验证集误差的变化情况。 可看到，随多项式次数增多，训练集误差呈下降趋势，验证机误差呈先降后升趋势。 如何分辨算法处于偏差还是方差？Problem：假设得出一个学习算法，其交叉验证误差或测试集误差都很大，我们应如何判断此时的学习算法正处于高偏差 OR 高方差的问题呢？ 如上图，左边圈表示偏差bias情况，右边圈表示方差variance情况。 偏差情况偏差情况下，测试集误差和验证集误差都很大，两者误差可能很接近。若是此情况，则说明算法正处于高偏差问题。如图： 方差情况若交叉验证集误差远大于训练集误差，这就预示着你的算法正处于高方差和过拟合的情况。如图： 正则化和偏差/方差Problem：偏差和方差之间是如何互相影响的，以及和算法的正则化之间的相互关系。 假设我们要对这样一个高阶多项式进行拟合： 为防止过拟合现象，需要使用一个正则化项（上式末位一项）来使参数值尽可能小。正则化项的求和范围照例取为j=1到j=m，而非j=0到j=m。 正则化参数λ对假设函数的影响分析三种情形： 情形1：正则化参数λ取一个比较大的值（比如λ的值取为10000甚至更大） 在这种情况下，所有这些参数θ1,θ2,θ3θ1,θ2,θ3等等，将被大大惩罚。其结果是这些参数的值将近似于等于0，并且假设模型h(x)h(x)的值将等于或者近似等于θ0θ0。因此我们最后得到的假设函数应该是这个样子的： 这对数据集来说，不是一个好假设。 情形2：λ值很小（比如λ的值等于0） 在这种情况下，如果我们要拟合一个高阶多项式的话，那么我们通常会处于过拟合（overfitting）的情况。 在拟合一个高阶多项式时，若未进行正则化或正则化程度微小，则通常会得到高方差/过拟合的结果。因为λ的值等于0，相当于没有正则化项（正则化程度微小），因此会对假设过拟合。 情形3：取一个中间大小的，既不大也不小的λ值时，我们才会得到一组合理的对数据刚好拟合的θ参数值。 那么问题来了：如何自动选择出一个最合适的正则化参数λ？ 【重申】我们的模型和学习参数、最优化目标： 假设在使用正则化的情形中，定义Jtrain(θ)Jtrain(θ)为另一种不同的形式，同样定义为最优化目标，但不使用正则化项。 当我们没有使用正则化时，我们定义的JTrain(θ)就是代价函数J(θ)。 当我们使用正则化，多出这个λ项时，就将训练误差Jtrain定义为训练集数据预测误差的平方求和。或者说是训练集的评价误差平方和，但不考虑正则化项。 同理,交叉验证集误差和测试集误差定义为对交叉验证集合测试集进行预测的平均误差平方和。 总结：我们对于训练误差Jtrain，Jcv，Jtest的定义，都是平均误差平方和。 选择一个正确的λ下面是自动选取正则化参数λ的方法： 1.选取一系列想要尝试的λ值，如从0.01,0.02,0.04开始，一直试下去，将步长设为2倍的速度增长，直到一个比较大的值。本例最终取值为10.24（实际上我们取的值是10，但已经非常接近了）。因此我们得到了12个不同的正则化参数λ，对应的12个不同的模型 2.选用第一个模型，λ = 0，最小化代价函数J(θ)，得到某个参数向量θ 使用θ(1)来表示第一个参数向量θ,然后再取第二个模型λ=0.01的模型，最小化代价方差，当然现在λ=0.01，那么会得到一个完全不同的参数向量θ，用θ(2)来表示。得到θ(3)对应于我的第三个模型，以此类推，一直到最后一个λ=10或λ=10.24的模型对应的θ(12)。 3.用交叉验证集评价这些假设参数θ；从第一个模型开始，然后是第二个模型，对每一个不同的正则化参数λ进行拟合，然后用交叉验证集来评价每一个模型。测出每一个参数θ在交叉验证集上的评价误差平方和，然后选取这12个模型中交叉验证集误差最小的那个模型作为最终选择。 对于本例而言，若最终选择了θ(5)，因为此时的交叉验证集误差最小，做完这些最后，如果想看该模型在测试集上的表现，可以用经过学习得到的模型θ(5)来测出它对测试集的预测效果是如何。重申：这里我们依然使用交叉验证集来拟合模型。 用以上方法即可用这部分测试集比较准确地估算出参数向量θ对新样本的泛化能力，这就是模型选择在选取正则化参数λ时的应用。 正则化参数λ对交叉验证集误差和训练集误差产生的影响Problem：当我们改变正则化参数λ的值时，交叉验证集误差和训练集误差会随之发生怎样的变化。 最初的代价函数J(θ)J(θ)是包含正则化项的，但在这里我们把训练误差和交叉验证集误差定义为不包括正则化项。 solution: 绘制出Jtrain(θ)和Jcv(θ)的曲线，表达的是随着增大正则化项参数λ，看假设在训练集上的是如何变化的，以及在交叉验证集上表现如何变化。 若λ值很小，即说明几乎没有使用正则化，因此有很大可能处于过拟合；而如果λ值取的很大的时候，则很有可能处于高偏差的情况。 λ在训练集上的变化若画出Jtrain(θ)和Jcv(θ)的曲线，会发现： 当λ的取值很小时，对训练集的拟合相对较好，因为没有使用正则化。 当λ的值很大时，将处于高偏差问题，不能对训练集很好地拟合，训练集误差Jtrain(θ)的值会趋于上升。 λ在交叉验证集上的变化 曲线右端，当λ值取很大时，则处于欠拟合/偏差问题，则此时交叉验证集误差将会很大，假设不能在交叉验证集上表现地比较好。 曲线左端，对应高方差问题，此时λ值取得很小很小，则会对数据过度拟合，所以交叉验证集误差也会很大。 这就是我们改变正则化参数λ值时，交叉验证集误差和训练集误差随之发生的变化。 学习曲线(Learning Curves)绘制学习曲线：判断某一个学习算法是否处于偏差方差问题，或者二者皆有。 1.先绘制出Jtrain(θ)（也就是训练集数据的平均误差平方和）或者Jcv(θ)（也就是交叉验证集数据的平均误差平方和） 2.绘制成一个关于参数m的函数，即一个关于训练集样本总数的函数 m一般都是一个常数，比如m=100，表示100组训练样本。但我要自己取一些m的值，比如说我取10，20，30或者40组训练集，然后绘制出训练集误差，以及交叉验证集误差。 假设我使用二次函数来拟合模型:hθ(x)=θ0+θ1x+θ2x2; 当只有一组训练样本，即m=1，假设函数对数据的拟合情况正如下图所示.由于只有一个训练样本，拟合的结果很明显会很好。对这一个训练样本拟合，其误差一定为0。 若有两组训练样本，二次函数也能很好地拟合： 若用三组训练样本，看起来依然能很好地用二次函数拟合： 当m=1,m=2或m=3时，对训练集数据进行预测，得到的训练集误差都将等于0（这里假设不使用正则化，当然如果使用正则化那么误差就稍大于0）。 总结：当训练样本容量m很小时，训练误差也会很小，因为若训练集很小，则很容易就能把训练集拟合到很好，甚至完全拟合。 而当m=4时，二次函数似乎也能对数据拟合得很好： m = 5时：效果差强人意 结论： 随训练集容量增大，平均训练误差逐渐增大，训练集误差对假设进行预测的误差平均值随着m的增大而增大。（如下图） 验证集误差和测试集误差都会随着训练集样本容量m的增加而减小，因为用的数据越多，能获得的泛化能力就越强，或者说对新样本的适应能力就越强。因此数据越多，越能拟合出合适的假设。 若把Jtrain(θ)J和Jcv(θ)绘制出来，就应该得到这样的曲线： 高偏差/高方差下的学习曲线Problem：当处于高偏差或者高方差的情况时，这些学习曲线会变成什么样。 高偏差下的学习曲线绘制出交叉验证集误差： 最左边表示训练集样本容量很小，比如只有一组样本，则表现当然很不好；当达到某一个容量值时，你就会找到那条最有可能拟合数据的那条直线，且此时即便你继续增大训练集的样本容量m，你基本上还是会得到一条差不多的直线。因此交叉验证集误差将会很快变为水平而不再变化。 同样，训练误差一开始也是很小，而在高偏差的情形中，你会发现训练集误差会逐渐增大，一直趋于接近交叉验证集误差，这是因为你的参数很少。但当m很大的时候，数据太多，此时训练集和交叉验证集的预测结果将会非常接近： 上图：学习算法处于高偏差情形时学习曲线的大致走向。 注：高偏差的情形反映出的问题是交叉验证集和训练集误差都很大，也就是说，你最终会得到一个值比较大的Jcv(θ)和Jtrain(θ) 结论： 如果一个学习算法有很大的偏差，那么当我们选用更多的训练样本时，我们发现交叉验证集误差的值不会表现出明显的下降，实际上是变为水平了。 如果学习算法正处于高偏差的情形，那么选用更多的训练集数据对于改善算法表现无益。 高方差下的学习曲线若训练集样本容量很小，如图： 若用很高阶次的多项式来拟合：hθ(x)=θ0+θ1x+…+θ100x100；假设我们使用一个很小的λ，那么很显然，我们会对这组数据拟合的非常好： 若训练集样本容量很小，训练集误差Jtrain(θ)将会很小，随着训练集样本容量的增加，可能这个假设函数任然会对数据或多或少有一点过拟合，但很明显此时要对数据很好地拟合显得更加困难和吃力了： 随着训练集样本容量的增大，我们会发现Jtrain(θ)的值会随之增大，因为当训练样本越来越多的时候，我们就越难将训练集数据拟合得很好，但总体来说，训练集误差还是很小： 交叉验证集误差： 在高方差的情形中，假设函数对数据过拟合，因此交叉验证集误差将会一直都很大，即便我们选择一个比较合适恰当的训练集样本数： 结论： 算法处于高方差情形最明显的一个特点是在训练集误差和交叉验证集误差之间以一段很大的差距gap。 高方差情形下使用更多的数量级对改进算法的表现事实上是有效果的。 总结：在改进算法时，画出学习曲线将有助于看出偏差或方差的问题。 重新审视决定下一步做什么例子：Debugging 一个学习算法：假设你用正则化线性回归来预测房价。但当你尝试在一组新的数据上使用你的假设函数时，你发现它出现了很大的无法容忍的误差。你接下来要怎么做来改进这个算法呢？ 可选方案： 通过使用更多的训练样本（对高方差问题有效） 尝试选用更少的特征集（对高方差问题有效） 尝试选用更多的特征集（对高偏差问题有效） 如果你需要增加更多的特征时，一般是由于你现有的假设函数太简单，因此我们才决定增加一些别的特征来让假设函数更好地拟合训练集。 尝试增加多项式特征的方法(x21,x22,x1x2,etc.)（对高偏差问题有效） 通过增大正则化参数λ（对高方差问题有效） 通过减小正则化参数λ（对高偏差问题有效） 和神经网络的联系当你在用一个相对比较简单的神经网络模型进行神经网络拟合的时候： 1.其中有一种选择是选择相对简单的隐藏层结构，比如说只有一个隐藏层，或者相对来讲比较少的隐藏层单元： 这种结构简单的神经网络参数就不会很多，很容易出现欠拟合。最大优势在于计算量较小。 2.与之相对的另一种情况是相对较大型的神经网络结构：要么隐藏层单元比较多，要么隐藏层单元比较多： 这种比较复杂的神经网络参数一般比较多，也更容易出现过拟合,计算量大。 总结： 若经常应用神经网络，特别是大型神经网络的话，就会发现越大型的网络性能越好，但如果发生了过拟合，可使用正则化的方法来修正过拟合。 使用一个大型的神经网络并使用正则化来修正过拟合问题，通常比使用一个小型的神经网络效果更好。但主要问题可能出现在计算量相对较大。 选择隐藏层的层数:默认使用一个隐藏层。但若想选多个隐藏层，可把数据分割为训练集、验证集和测试集，然后使用交叉验证的方法比较一个隐藏层的神经网络，然后试试两个三个隐藏层以此类推，看哪个神经网络在交叉验证集上表现得最理想，再对每一个模型都用交叉验证集数据进行测试，算出三种情况下（隐藏层分别为一层、两层、三层）的交叉验证集误差Jcv(θ)，然后选出你认为最好的神经网络结构。 操作偏斜数据Abstract：前面分析了误差分析和设定误差度量值的重要性。存在偏斜类问题，指训练集中正例和负例的比率非常接近于一个极端值的情况。此情况下我们需要一个合适的误差度量值来告诉我们分类模型的效果好坏。其中一种评估度量值是查准率（precision）和召回率（recall）。查准率是真阳性数量与预测为真阳性数量的比值，召回率是真阳性数量与实际阳性数量的比值。通过这两个度量值可避免被算法”欺骗”。但它仍存在一个问题：在高查准率低召回率情况 and 高召回率低查准率情况下，模型效果都不好，因此我们需要想办法排除这两种极端情况。我们需要权衡查准率和召回率，关键在于临界值的选取，即设定在可信度大于xx%的情况下才预测y = 1。通过变动临界值，可以控制权衡查准率和召回率。而选取合适的临界值的方式有F1Score，F1Score = 2*[PR/(P+R)]，如此即可排除上述两种极端情况。 偏斜类的错误度量Problem：误差分析和设定误差度量值的重要性。我们需要设置某个实数来评估学习算法并衡量它的表现。如果仅仅使用分类误差或分类精度，学习算法仍可能会出现偏移类问题，此时选择一个合适的误差度量值可以避免算法出现偏斜类问题。 情景举例：癌症分类问题。我们拥有内科病人的特征变量，想知道他们是否患有癌症。类似恶性和良性肿瘤的分类问题，是二分问题。 @ 偏斜类问题举例： 假设： 训练逻辑回归模型hθ(x)。(如果是癌症y=1，否则y=0)； 发现你在测试集上有1%的错误率。(99%的诊断是正确的)； 只有0.50%的病人患有癌症 我们训练逻辑回归模型，假设用测试集检验了这个模型，发现它只有1%的错误，说明我们99%会做出正确判断。看起来是不错的效果。但问题在于测试集中只有0.5%的患者真正得了癌症。此时若一个算法总是预测y = 0，那么其错误率 = 0.5% &lt; 1%, 甚至比我们之前得到的1%还要好。这种情况叫偏斜类问题，发生在正例和负例的比率非常接近于一个极端值，此例中正样本的数量与负样本的数量相比，非常非常少。（因为样本数量极大地向某一类结果倾斜，故称为偏斜类） function y = predictCancer(x) y = 0; %ignore x! return // 忽略了输入值X，它让y总是等于0，因此它总是预测没有人得癌症，那么这个算法实际上只有0.5%的错误率. 因为偏斜类问题的存在，我们不知道是否真的提升了分类模型的质量（y = 0 不是一个好的分类模型，但它会将误差降低到0.5%）。 所以，我们应该寻找一个不同的误差度量值来评估模型的效果。 查准率（precision）和召回率（recall）假设情景：一个二分问题，学习算法为每一个测试集中的实例做出预测，预测值也是等于0或1。 上图将样本基于实际情况与预测情况分类。 查准率（Precision）查准率：对所有预测患有癌症的病人，有多大比率的病人是真正患有癌症的。分母是预测为阳的数量，分子是真阳性的数量。查准率越高就越好.高查准率说明对于这类病人我们对预测他们得了癌症有很高的准确率。 真阳性的数量/预测值为阳性的数量 = 真阳性的数量/(真阳性的数量+假阳性的数量) 召回率（Recall）召回率：如果所有的在数据集中的病人（假设测试集中的病人，或者交叉验证集中的病人）确实得了癌症，有多大比率 我们正确预测他们得了癌症。分母是实际为阳性的值，分子是真阳性数量。召回率越高越好。 真阳性的数量/实际阳性的数量 = 真阳性的数量/(真阳性的数量+假阴性的数量) 通过计算查准率和召回率我们能更好的知道分类模型到底好不好。 排除了偏斜类情况：总是预测y = 0的算法，其真阳性数量为0，则查准率和召回率 = 0. 注：在查准率和召回率的定义中，我们总是习惯性地用y=1，因此如果我们试图检测某种很稀少的情况（比如癌症，我希望它是个很稀少的情况），查准率和召回率会被定义为y=1而不是y=0作为某种我们希望检测的出现较少的类。 查准率和召回率练习癌症分类例子： 假设我们用逻辑回归模型训练了数据，输出概率是在0-1之间的值：0≤hθ(x)≤1。设置临界值为0.5. 因此可以得到： 如果hθ(x)≥0.5，预测值为1 如果hθ(x)&lt;0.5，预测值为0 但我们希望只有在我们非常确信的情况下，才告诉这个人他得了癌症。因此需要修改算法。 高查准率低召回率情况若设置临界值为0.7，则表示：我们会在我们认为他有≥70%得癌症的概率情况下，告诉一个人他得了癌症。 若临界值设为0.9,则这个模型是高查准率，低召回率。 高召回率低查准率情况目的：避免漏掉患有癌症的病人，即避免假阴性。 方法：我们设置临界值很低，为0.3，表示：我们认为这些病人有&gt;30%的概率患有癌症，我们以更加保守的方式来告诉他们患有癌症，因此他们能够接受治疗。 弊端：高召回率，低查准率。 综上，我们需要权衡查准率和召回率。 临界值的选取 如图为改变临界值时，查准率与召回率的变化曲线。 曲线接近y轴时，为高查准率低召回率情况；接近x轴时，为高召回率低查准率情况。 注：查准率-召回率曲线可以是各种不同的形状，如： solu：我们需要一个标准来帮助权衡查准率与召回率。 选取临界值的方式：F1Score(F score)通过变动临界值，你可以控制权衡查准率和召回率。 假设有3个算法： Empty 查准率P 召回率R 平均值 算法1 0.5 0.4 0.45 算法2 0.7 0.1 0.4 算法3 0.02 1.0 0.51 结合查准率和召回率的方式之一: F1Score = 2*[PR/(P+R)] F score的定义会考虑一部分查准率和召回率的平均值，但是它会给查准率和召回率中较低的值更高的权重，因此你可以看到F score的分子是查准率和召回率的乘积，如果查准率等于0或者召回率等于0，F score也会等于0。因此它结合了查准率和召回率，对于一个较大的F score，查准率和召回率都必须较大。 F score给出了你所需要的有效方法，因为无论是查准率等于0，还是召回率等于0，它都会得到一个很低的F score。因此，如果要得到一个很高的F score，你的算法的查准率和召回率都要接近于1。具体地说，如果P=0P=0或者R=0R=0，你的F score也会等于0。 使用大数据集Abstract：虽然增大数据集只能在某些情况下起到改进算法的作用。但研究证明，在特征值x包含足够多的用来准确预测y的信息时，获取大量的数据是提高算法性能的好方法。在这一前提下，可以说：取得成功的人不是因为拥有最好算法，而是因为拥有最多数据。 上图是使用多种血虚算法到不同大小的训练数据集后得到的结果。横轴代表以百万为单位的数据集大小，即从0.1百万到1000百万（10亿）规模的数据集；纵轴代表精确度，范围是0到1。 表明： 大部分算法都具有相似的性能 随着训练数据集的增大，这些算法的准确性也都对应增强 若任意选择一个算法，如选择了一个”劣等的”算法，如果你给这个劣等算法更多的数据，那么从这些列子中看起来，它很有可能会其他算法更好，甚至会比”优等算法”更好。 结论：取得成功的人不是因为拥有最好算法，而是因为拥有最多数据（前提情况：在特征值x包含足够多的用来准确预测y的信息时，获取大量的数据是提高算法性能的好方法。） 举例1：混淆词分类问题（监督学习问题，分类：什么样的词在一个英文句子特定的位置才是合适的） For breakfast I ate __ eggs. 在这个句子中，空白处的可选词是： totwotoo 在这个例子中应该填入two。 分析：空白词附近的词就是我们需要捕捉的特征x，这些词中包含了大量的信息来告诉我空白处应该填写two而不是to或者too。实际上对于特征捕捉，哪怕是周围词语中的一个词，就能够给我足够的信息来确定出标签y是什么了。这就是通过一个有充足的信息的特征值x的来确定y的例子。 举例2：房屋价格预测问题 假设我们只知道房屋尺寸（而没有其他任何特征信息），则很难预测价格。因为影响价格的因素太多了。]]></content>
      <categories>
        <category>Artificial Intelligence</category>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Artificial Intelligence</tag>
        <tag>Machine Learning</tag>
        <tag>Coursera ML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PM需要了解的行业技术名词]]></title>
    <url>%2F2017%2F10%2F01%2FPM%E9%9C%80%E8%A6%81%E4%BA%86%E8%A7%A3%E7%9A%84%E8%A1%8C%E4%B8%9A%E6%8A%80%E6%9C%AF%E5%90%8D%E8%AF%8D%2F</url>
    <content type="text"><![CDATA[Abstract:这是一份来自互联网公司招聘网的职位清单。其涵盖了互联网行业中重要的技术名词。作为PM要对这些技术概念和原理有所了解。可以用平时零碎的时间慢慢加强技术理解，再选择深入的方向。但注意，对PM来说最重要的是痛点挖掘、提出solution和寻找产品关键路径. 后端开发：Java，Python，PHP，.NET，C#，C++，C，VB，Delphi，Perl，Ruby，Hadoop，Node.js,数据挖掘，自然语言处理，搜索算法，精准推荐，全栈工程师，GO ASP，Shell，后端开发 移动开发：HTML5，Android，ios，WP 前端开发：web前端，Flash，html5，js，U3D，COCOS2D-X， 测试：测试工程师，自动化测试，功能测试，性能测试，测试开发，游戏测试，白盒测试，灰盒测试，黑盒测试，手机测试，硬件测试，测试经理 运维：运维工程师，运维开发工程师，网络工程师，系统工程师，IT支持，IDC，CDN，F5，系统管理员，病毒分析，WEB安全，系统安全，运维经理 DBA：MySQL，SQLServer，Oracle，DB2，MongDB，ETL，Hive，数据仓库； 硬件开发：硬件，嵌入式，自动化，单片机，电路设计，驱动开发，系统集成，FPGA开发，DSP开发，ARM开发，PCB工艺，模具设计，热传导，材料工程师，精益工程师，射频工程师 企业软件：实施工程师，售前工程师，售后工程师，BI工程师 学习方法：利用零碎时间慢慢加强技术理解，再选择深入的方向. 产品思维：痛点、solution、产品关键路径.]]></content>
      <categories>
        <category>Product Manager</category>
        <category>Technical Logic &amp; Knowledge</category>
      </categories>
      <tags>
        <tag>Technical Logic &amp; Knowledge</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Interpret Technique to PM|Chapter11:Interpretation of some Techniques]]></title>
    <url>%2F2017%2F10%2F01%2FInterpret-Technique-to-PM-Chapter11-Interpretation-of-some-Techniques%2F</url>
    <content type="text"><![CDATA[Abstract：对一些技术概念和原理的解释。如搜索引擎，推荐算法，贝塞尔曲线，A/B 测试等。 百度和搜索引擎 爬虫：搜索引擎的关键构件；自动提取网页的程序； 爬虫的策略：1.选择：决定要下载的页面，2.重新访问：决定什么时候检查页面的更新变化，3.平衡礼貌：指出怎样避免站点超载，4.并行：指出怎么协同达到分布式抓取的效果； 无损音乐 采样：声音从自然界的模拟信号到数字信号的转换需要采样，即将声波上的每个点量化成一个值，存储在磁盘上；每秒采样的次数越多，声音越连贯，失真越小；单位时间的采样次数即采样频率，单位是赫兹Hz； CD标准：采样频率是44100Hz，16bit来存储每个采样到的声音；即每秒采样44100次，每次占16bit空间；（8bit-1byte；1024byte-1M） 推荐算法协同过滤算法 需要数据：推荐的东西+用户行为数据； 核心：物以类聚，人以群分；分为基于用户的协同过滤算法和基于物品的； 基于用户的：爱好相似的人喜欢的东西相似；余弦相似度公式（通过计算n维空间中两个向量的夹角余弦来表示相似度）； 基于物品的协同过滤：假设AB是相似的物品，则喜欢A的人也可能喜欢B； 基于内容的算法 在无用户行为数据情况下，面临的冷启动问题；基于内容即：事先把所有内容归个类，用户进行第一次消费后即把同类的内容推荐给用户； 简单粗暴，命中率低； 真正的推荐系统会综合运用各种算法； 贝塞尔曲线 原理：几个点控制一段曲线，以画出不同形状的曲线／图形； 思考的重要性 一个问题的思考路径：全面调查清楚当前现状（包括现象和规律）——是什么引起了当前的状况／改变——当前的状况或变化是一次性的还是市场或环境发生了变化？——应对当前情况的核心点在哪里？——针对核心点思考解法； 声音传文件 流程：手机确认要发送文件——首先将文件上传到服务器——手机接收服务器返回的短地址——手机将短地址编码成声音——接收的手机将声音解码成短地址——从地址下载文件；（声音表示的是文件所在的短地址，而非文件） AR公司 公司：Hololens，magic leap，Meta； 产品：目前依托于移动端承载，以后一定会脱离移动端独立存在； Google Loon 目标：实现全球任意地区都可接上互联网； 实现：利用热气球飞往天空的平流层，利用大气流动的算法使其移动到指定区域，从而实现互联网接入；（即一个漂浮在天空的基站） 热气球基站的构成：气球+基站（白天太阳能，晚上电池）； 云计算 大数据：对数据挖掘的包装，可创造新的商业价值； 云计算：Iaas（Infrastructure as a service基础设施即服务，即卖服务器），PaaS（Platform as a service平台即服务，即面向软件开发者提供一整套方便的开发和部署环境），SaaS（software as a service软件即服务）； 计算机时间 NTP：Network Time Protocol网络时间协议，同步时间，在网络上部署一些标准的时钟服务器，其他设备都通过NTP协议来计算自己的时钟与标准时钟的误差，再调整自己的时钟来弥补误差，使设备时间标准化； A／B测试 通过设定一个变量，对比几个版本之间在真实环境下的数据，来选出最佳方案； 技术角度：A/B测试不一定要打两个包、生成两个版本，变量不要写死在代码里，可以由服务器动态下发，由后台决定下发流量的多少、下发给谁、什么时候下发；（但可能技术基础不支持） AppAdhoc：A/B测试专业平台；支持各个端产品；只需后台设置要测试的变量和指标即可实时跟踪不同版本的数据表现；科学试验流量分割算法，保证不同版本用户都有相同代码；流量分配可后台自主调节； 抽奖代码 流程：获取一个系统生成的随机数——将随机数映射到名单里的某个人——被映射者即中奖者； 伪随机数：用函数计算出的随机数，当输入的参数相同时获得的随机数相同； 真随机数：通过对无规律的自然现象量化后得到，如热力学噪声、量子现象；代价高；random.org提供各种获取随机数序列的API； 视频直播 技术：HLS（http live streaming），RTSP，RTMP； 直播流程：采集视频源和音频源——对视频源H264编码／对音频源AAC编码——分割成TS文件——组装成M3U8文件——Http 传输以及部署到CDN； HLS原理：利用http服务器，以http的方式传输音视频文件，由于音视频文件分割得足够小，故下载也足够快，近似于实时直播； DLNA——平民化跨屏互联解决方案 DLNA：定义了逻辑上的功能模块（DMS数字媒体服务器，DMR数字媒体渲染器，DMC数字媒体控制器，DMP数字媒体播放器）和模块间的交流方式； IFTTT普通人也能写的小程序 if判断语句； 微信支付的业务流程 1.注册订单信息：用户确认订单信息—商户后台讲商户信息、订单明细、金额信息发送到微信支付后台-微信后台将信息缓存到后台服务器里+同时生成prepay_id——prepay_id返回到终端后商户APP会展示支付确认界面； 2.微信客户端展示订单：用户在商户app点击微信支付——吊起微信客户端+将prepay_id、商户信息传给微信客户端——客户端通过prepay-id、商户身份信息、微信账号信息向微信支付后台查询本次支付是否合法+检查客户端的支付权限——确认无误后微信后台同志客户端展示支付授权页面； 3.用户确认，完成交易：用户在授权页面点击确认并输入密码——交易完成； prepay-id标识微信支付行为的开始； 人工智能——机器学习及相关概念解析 模式识别：（模式——理解为特征）利用待处理数据（图像、文字、语音等）的特征，与模版匹配并输出匹配结果；如语音识别、文字识别等；实质是依赖于人类赋予的确定规则来对数据进行处理，不具备学习能力（要提高算法的成功率需要人肉调优）； 学习：个体由经验或练习引起的在能力或倾向方面的变化，也指变化的过程； 计算机程序的学习：在不修改算法的前提下，算法的正确性可通过经验（历史数据）和练习（一组有明确结果的数据）来逐步提高； 深度学习：参考人类大脑对数据的存储和处理方式建立计算模型，使算法具有自适应、自组织能力； 微信-查找附近的人 GeoHash：对每个坐标打上可逐级查找的地址标签；以半切蛋糕的方式分区，切的次数越多，GeoHash越长，小区范围越小；范围细化到小区时就不必再切，在同一个小区的人的GeoHash相同，匹配附近的人即匹配GeoHash相同的人； 模糊 目标：让每个像素点的细节不明显； 实现：将某个像素点周围的像素值根据一些权重加起来，然后取平均值作为新图中对应位置的像素值； 动画 绝大部分动画可通过基本动画的组合来完成：平移，旋转，缩放，Alpha（透明度变化）； 以图搜图 感知哈希算法：仿照人眼对图片相似的判断，只要轮廓、大体颜色一致，就认为相似； 流程：把图片缩小成8*8，人为降低匹配精度——把图片颜色信息转化成64阶灰度（目的是比较颜色的相对强弱，不关心颜色是什么）——计算所有像素的灰度的平均值并记录信息、大于均值的标1反之标0、形成一个64bit的二进制数即为这张图的hash值——两图对比hash值、64位里相同的越多越相似；]]></content>
      <categories>
        <category>Product Manager</category>
        <category>Technical Logic &amp; Knowledge</category>
      </categories>
      <tags>
        <tag>PM Communication</tag>
        <tag>Technical Logic &amp; Knowledge</tag>
        <tag>Reading Notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Interpret Technique to PM|Chapter10:Noun Interpretation]]></title>
    <url>%2F2017%2F10%2F01%2FInterpret-Technique-to-PM-Chapter10-Noun-Interpretation%2F</url>
    <content type="text"><![CDATA[Abstract：一些会经常碰到的技术名词的通俗解释。如抽象，封装，多态，SDK，API，架构，PAC，hook，哈希表等。 4D打印 3D打印：同时具备X／Y／Z三轴，能描述三维控件； 4D打印：多一个纬度：时间T；随时间变化，材料可随环境／温度进行变化（前提：材料已被编程好）；可能还需要10年预热期才能进入成熟期被应用； 抽象／封装／类／实例／对象 对事物进行抽象，从而封装为类，由类可以生成实例或对象； 封装的前提首先是抽象，抽象是对一系列拥有共同属性或行为的描述；（抽象对应的是具体，抽象后具体消失，共性出现） 抽象出的共性用来封装为类（class），类可以定义实例或对象；eg.小红（对象）打开化妆包（抽象），拿出红石榴水（具体），运用拖拉摇椅拽拍打七种方法（能力）进行脸部护理（效果／目的）； 封装／继承／多态是面向对象的三大特性； SDK，API，架构 SDK：software development kit，软件开发工具包，是一整套工具的集合；sdk里一般封装了许多api； API：application programming interface应用程序编程接口；可理解为一个函数，函数提供里某一样特殊能力；分公有API（系统以SDK形式暴露出来，对所有开发者可见）和私有API（系统内部使用或仅限于系统自带的一些应用程序使用，不对开发者开放）； 架构：软件系统的结构；就是考虑了一个软件系统的所有设计要素后，梳理清楚模块划分，以及模块间的关系，所形成的结构设计； 控件，组件，框架 控件：程序设计中最小粒度的可复用可编程的部件； 组件：一个组合功能的控件，功能比控件复杂； 框架：由很多控件和组件组装到一起且能在某一领域里完成一系列操作；如jQuery框架是对原生js的封装，提供更方便快捷的js操作； 二进制文件 文本文件：txt； 计算机硬盘上存储的饿文件都是二进制存储的，把文件转化为二进制，计算所占字符数，会发现： 二进制文件在数字上存储要比文本文件省空间，因为文本文件按字符存储，二进制按数据类型存储； 二进制文件的优点：省空间，写入速度快，可读性差，加密作用； 本地数据和云端数据 好的产品设计：本地默认写死一些频道（打底数据，在无网络情况下可展示，用于解决用户体验问题），展示了打底数据后发起云端请求，拉取云端运营数据，若拉取成功则将新的频道数据覆盖本地数据，保证客户端下次启动时展示上次成功拉取的数据；若拉取失败则继续展示本地数据； 哈希表 哈希：又称散列，设计一种映射关系f（key）=address，根据key来计算存储地址；可实现一次查找；f是存储过程中用来指引数据存储到什么位置的函数+将来查找这个位置的算法（哈希算法）；核心：哈希函数的设计； 哈希算法：可随意设计，如直接定址法，平方取中法，除数取余法；本质是通过固定的算法将存储元素变成一个数字来存储（key-value），查找时就通过计算其对应的数字一次查询； 哈希冲突：很多记录映射到一个位置；哈希冲突与哈希函数的设计正相关，随机性越大产生哈希冲突的可能性越小；解决方法：1.哈希再哈希，针对第一次哈希的结果再进行一次hash来减小冲突的概率；2.对原方法增加随机性和对冲突进行适当的有损化处理； hash表：一种数据结构，效率极高的查找方式； hook hook：在执行某操作之前优先处理一下，再决定后面的执行走向； eg.利用hook的技术：hook住程序的校验签名函数，假装校验通过以欺骗程序； 配置文件 作用：更改一些需要经常变化的设置（若写死在程序里就无法更改） 算法复杂度 衡量复杂度的相对标尺：1.时间复杂度，用大写的O标识，如O（n／n^2/n^3）；2.空间复杂度：每个算法对内存使用的相对衡量（空间消耗），如S（n／n^2）; eg.要计算n*n次，则复杂度为O（n^2）;长度为n的数组，空间复杂度为s（n），二位数组为s（n^2）； 算法优化：降低算法的复杂度，但优化不改变复杂度系数（复杂度是对算法最差情况的衡量）； 锁 无锁情况下的异步执行：操作异步性和时间重叠导致写入数据库的数据有误； 加锁：强行将并行执行变成顺序执行； 设计模式，架构，框架 设计模式：一种思想，关注的是写代码过程中遇到的非常具体的某一类问题《常见的23种设计模式》 框架：树的主干，自己写的代码是叶子； 架构：一种设计决策； 时间戳，MD5，GUID 时间戳（TimeStamp）：当某一事件发生时，立刻为他存储一个时间； MD5:信息摘要算法第五版；主要解决的问题：对一个文件或字符串生成一个唯一标识，保证一个字符串生成的MD5码是绝对唯一的； GUID：全局唯一标识符，128位的数字标识符； 栈 有好几层含义： 1.数据结构；标识一个先进先出的队列；push进，pop出； 2.表示由操作系统管理和分配的一些内存区域，用来存储程序中的变量和参数；栈溢出——内存空间被用完了； 3.栈信息：程序出错的打印信息； 全栈：精通前端／后台／终端／架构的工程师； OpenGL 图形编程：最基本的问题——如何用程序再显示器上绘制东西；有两种方式：1.软件绘制：CPU在内存里准备好了要绘制的东西然后交给显示器去显示，2.硬件加速：显卡在显存里准备好了要绘制的东西交给显示器去显示； OpenGl：利用显卡的硬件加速能力帮助绘制图形；700多个API（每个API代表一种能力），跨平台，支持各种语言；是一个状态机（不停做状态切换）； 引擎 引擎：一套能力的组合； PAC文件 PAC：proxy auto-config，自动配置代理，核心是一个函数「FindProxyForURL（url，host）」，输入一个url和域名，则返回一个或多个代理服务器的地址，或返回一个命令，告诉浏览器不使用代理访问该url； 渲染 流程：测量-排版-绘制； 展示熳、卡顿问题：渲染的不是很流畅； AR/VR技术 AR：增强现实，是对真实世界的信息扩展，虚实结合；关键技术：计算机视觉（对真实环境进行处理的前提是让电脑认识真实环境中的事物）； VR：提供沉浸感极强的虚拟世界；利用计算机三维建模（构建场景）和动作追踪技术（提供交互）作为系统输出和输入；三维建模完成虚拟世界的建设，VR设备集成的各种传感器则负责让用户与世界建立真实的连接，通过传感器的动作捕捉，将用户的一举一动作用到实时计算生成的三维虚拟世界中； 位图和矢量图 位图：用像素点阵来描述图片；失真；jpg，png，WEBP； 矢量图：由点、直线、多边形等几何图元（可由数值和方程式描述的）构成的图像；不失真；SVG； 接口 接口：提供一种别人可调用的能力的标准；一组能力的集合，抽象的，不包括具体实现（调用者在不需要知道具体实现的前提下即可获得结果）； 线程池，对象池，连接池 对象池：提供很多对象放到池子里，循环利用（创建对象是浪费时间的）； 连接池：同理（数据库连接是宝贵的）； 池化技术：充分保障系统效率的前提下，充分复用资源的方式； 关于兼容 向后兼容：对已发出去的老版本兼容；可适度做的，无法长期兼容未来的； 向前兼容：对未来还未做好的版本兼容；一定要做的，一定有解决方案的； 游戏引擎 游戏世界的基础 框架，定义并实现了游戏中的自然法则，包括光影效果、动画系统、物理系统、渲染系统等； 渐进式图片 渐进式：压缩方式是根据小波变换，先存储低频（轮廓）内容，再存储高频（细节）内容；展示方式：先显示图片轮廓，然后随下载数据的增多逐渐细化图片的各个细节，使图片分辨率逐渐提高，最终还原完整图像； 普通jpeg图片：从左至右、从上至下压缩； LiFi LiFi：light fidelity，利用可见光传输信息的技术；原理是将可见光的明暗变化进行编码，由接收端将光信号转化为数字信号； 优点（相对Wi-Fi）：高频宽（不用为降低信号干扰而频繁调整信道）；高速率（传输速率高）；]]></content>
      <categories>
        <category>Product Manager</category>
        <category>Technical Logic &amp; Knowledge</category>
      </categories>
      <tags>
        <tag>PM Communication</tag>
        <tag>Technical Logic &amp; Knowledge</tag>
        <tag>Reading Notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Interpret Technique to PM|Chapter9:Development Technology]]></title>
    <url>%2F2017%2F10%2F01%2FInterpret-Technique-to-PM-Chapter9-Development-Technology%2F</url>
    <content type="text"><![CDATA[Abstract：产品研发过程中的一些基础的开发技术知识。如越界，动画开发，并行计算，程序与数据结构,MVC模式，敏捷开发模式等。 越界 数组越界：由于线程执行顺序导致的异常； 数据范围越界：要存储的数字超过了其选用的数据类型所能表示的最大范围；一般开始定义时没问题，但掺杂各种运算处理后数据范围易出现问题； 深度学习 Google Tensorflow框架；Facebook Torch框架；UC Berkely Caffe框架； 程序设计的时空大法空间换时间 目的：运行流畅； 原理：将经常使用的资源或将来可能使用的资源加载到内存里（缓存／预读）； 时间换空间 原理：在空间资源有限的情况下，消耗更多的时间做加载或运算来减少内存的占用； 方式：1.用更多的运算代替存储；2.直接清理内存空间；3.从外存加载到内存； 目的：节省存储资源； 开发动画的工作量 位移动画：让控件在一段时间内不断改变位置而形成的动画效果；实现：创建一个translateanimation对象（位移动画的配置清单，包括：动画开始时控件的起始位置+结束时控件要到达的位置+动画持续时间）——让控件执行启动动画的命令——系统根据默认的插值算法，在动画的持续时间内，计算每一帧中控件的位置，然后绘制出来； 缩放动画：控件在一段时间内不断改变自身大小而形成的动画效果；实现：配置清单（动画开始时控件的缩放倍率+结束时的缩放倍率+持续时间+原点）——动画启动后的每一帧都会计算当前时间控件对应的缩放率； 渐隐渐现动画：在一定时间内持续改变控件的透明度形成的动画效果；实现：配置清单（动画开始时控件的透明度——结束时的透明度+持续时间）； 旋转动画：让控件在一段时间内围绕一个固定点旋转指定的角度；实现：配置清单（动画开始时控件的旋转角度+结束时的旋转角度+持续时间+原点）； 动画 帧Frame：动画过程中的某一张静止画面； 帧率：动画每秒播放的帧数；FPS为单位；30帧／s为连贯，60帧／s为流畅（每帧16毫秒）； 技术实现：绘制图形的命令（drawCircle／drawRect／drawText）——refresh（刷新屏幕）——scheduleNextFrame（定时器：告诉操作系统下一帧的时间）； ios／android都内置了几种常见动画类型（缩放／平移／旋转／渐变等），只需设置好动画时长／动画中要变化的东西，然后发出start命令即可； 有交互的动画：由用户手指操作触发刷新屏幕； 程序与数据结构 程序=算法+数据结构，本质是对数据进行处理； 构建数据模型：把可能用到的东西数据化； 数据结构：描述一些彼此有关联的数据的集合（帮助程序员处理批量数据的：插入修改删除查询CRUD）； 数组 特点：一下子可承很多数据，挨个排放，每个位置都有一个编号，数据通过编号访问； 插：后面所有元素要往后挪一格；改：喊编号即可；删：后面的往前挪一格；查：遍历数组； 链表 特点：数据非挨个存放，也无编号；每个数据都有一个指向下个数据位置的指针（指针：地址，指向下个元素的位置）；像链子一样把所有数据串起来； 插：只需改A元素的指针指向B，然后改B指针指向C；删：删-衔；改：改地址；查：遍历； 队列 特点：先进先出的数据结构，进——数据的插入，出——数据的删除；强制限制顺序，不允许插队； 栈 特点：先进后出； 栈溢出stackoverflow：app用的内存不小心超过系统限制，被系统封杀； 线程池：复用的艺术 服务器：用来接收并响应客户端的请求； 线程池：用生产者消费者模型来理解，即：生产者——客户端，生产网络请求，消费者——线程池里的线程，消费请求，消费过程即响应网络请求的过程；（当请求太多，而线程池里没有空闲线程时，则把请求放入队列中排队）； 复用思想：弄个池子，放一些线程，用则取，不用则放归； 二进制图片转字符 目的：直接在html里插入图片，节省网络请求（请求图片）； 如何实现转换：用base64编码转换（base64:64个可用字符，6bit表示一个字符）； 缺点：html文件会变大，适合编码小图片； 后台是干啥的（RPC框架） 后台技术栈的核心：RPC（remote procedure call远程过程调用）框架即基于调用远程服务方法封装的一套框架； 技术源于生活生产者和消费者模式 多线程：并发执行模式，主要目的——最大化利用各种资源（CPU，磁盘，显卡，网络带宽），提高程序整体性能；精髓在于定义了一个缓冲区； 流水线 面向对象的精髓：把所有东西抽象成对象，对象能响应命令； pipeline流水线技术：重复执行一项任务时可把它细分成很多小任务，让小任务们重叠执行，以提高整体的运行效率； 敏捷开发与瀑布式开发 敏捷开发：拥抱变化，快速响应； 加载等待的艺术 菊花：异步加载技术；一种简单的交互，为使界面交互更流畅自然； 同步加载的执行速度太慢：凡跟内存之后的存储设备（存储设备按运行速度排序：寄存器-高速缓冲器-内存-硬盘；内存前的设备可立刻加载，内存后的很慢）打交道的任务【IO密集型任务】都不适合同步加载； 菊花的使用场景：当用户跳转到新场景或加载新网页（必须执行一些io密集型任务才能让用户看到内容）时； 入场动画：动画是GPU密集型任务，后台执行CPU／IO密集型无影响；动画：一遍遍渲染界面（渲染：刷新界面，16毫秒／个）； 耦合与解耦 pm眼中的功能：一块一块的积木，可组合可拆分；开发眼中的功能：耦合，血脉相融，牵一发动全身； 避免耦合：A功能的代码不要写在B功能里，若二者需要交互，可通过接口／消息／引入框架，但不要交叉写； pm需提前提醒：若预见功能有未来独立化的趋势，需提前提醒开发避免耦合，以免增加后期代码耦合的工作量； 代码混淆 将代码写的晦涩难懂（如改成难懂的变量名），以增加人为分析的难度和时间成本； 软件架构中的三权分立模式：MVC模式 model：模型，主要用于表示一些数据结构和数据存取（数据层）； view：UI样式； controller：model和view的桥梁，负责处理数据和视图之间的逻辑关系（逻辑层）； 并行计算 并行计算：一个计算机或计算系统（如分布式系统）在同一时刻执行两个货两个以上的任务； 两种实现方式：时间上的并行，空间上的并行； 时间上的并行：流水线技术（极少用） CPU执行一条指令的流程：取指-译码-地址生成-取操作数-执行-写回； 空间并行 人多力量大，通过加硬件=资源来解决效率问题（如多核处理器）； 系统进行调度的单位：线程，进程； 多核操作系统：将相互独立的线程和进程分配到不同核心上，以达到并行计算的目的； 分布式计算、云计算：都是对并行计算的包装； 进程通信 操作系统：多进程并发执行，每个进程有自己的一亩三分地（以避免对其他进程产生影响，便于系统调度）； 通信方式：1.基于文件；2.基于管道（点对点通信）；3.基于信号／信息，信号量，套接字； 为什么有些bug不能改 开发施展各种奇技淫巧来解决各种奇葩问题导致最后代码只能看不能动； 用小bug隐藏大bug； 为什么程序会挂 开发在设计算法时考虑不周，没有对可能遇到的异常状态做处理； 操作系统有bug； 名词面向对象 面向过程：每次需求变更都需要重新定义函数； 面向对象：抛弃函数，把对象作为程序的基本单元；对象：对事物的抽象描述，可用数据+能力来描述；对象有数据、能力，还可接受命令； 面向对象编程：依次对不同对象发送命令； 特性：1.自己的事自己做；2.面向接口编程（对象把能力通过接口公布出来，自己则成为接口的实现者）； 多态 多态：多种状态，运行时根据函数返回的内容不同而执行不同的逻辑；好处：屏蔽类与类的差异，当需求扩充时可保证代码稳定； eg.快递公司是父类，各大具体的快递公司是子类，多态即：虽然每个子类对送快递实现的具体逻辑不同，但它们都完成同一类功能； 继承 继承者首先要是被继承者；被继承者（父类／基类）用一万行代码描述自己的功能特性，继承者（子类）只需额外用二十行代码声明自己独有的特性，即可拥有一万零二十行代码对应的所有功能； 类：对属性和功能集合的静态描述；实例：属性真正的载体和功能的执行者； 重构 重构：在保留现有功能的基础上重新梳理软件中的代码结构，让原本杂乱无章的代码重新具有可读性／结构性／可扩展性，增加软件开发的效率，优化程序的性能；重构范围可大可小，大到整个产品的各个模块，小到一个函数；eg.盖房子：加盖楼层需要考虑到原本房子的承重结构； Unity-VR unity引擎：看作是在openGL基础上，实现了很多特效的一堆代码集合；移动端；归根到底是对素材的编辑；可一键生成android／ios代码；]]></content>
      <categories>
        <category>Product Manager</category>
        <category>Technical Logic &amp; Knowledge</category>
      </categories>
      <tags>
        <tag>PM Communication</tag>
        <tag>Technical Logic &amp; Knowledge</tag>
        <tag>Reading Notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Interpret Technique to PM|Chapter8:Development Tools]]></title>
    <url>%2F2017%2F10%2F01%2FInterpret-Technique-to-PM-Chapter8-Development-Tools%2F</url>
    <content type="text"><![CDATA[Abstract：关于开发工具的一些基础知识。如编程语言，IDE，GitHub，SVN等。 编程语言 js：脚本语言； java：编写客户端程序、后台服务器程序、android app； Ruby：一门简介快速的编程语言，搭配Ruby on rails框架，可快速开发一些网站应用程序； PHP：脚本语言，用来编写动态网页；低门槛易上手； Python：脚本语言，写后台服务器程序／客户端程序／数据分析； CSS C++：面向对象（通过抽象和接口，使代码更为有序简单）；应用广泛，客户端、后台 android app； C#：基于.NET框架的面向对象的高级编程语言；微软出品； C：基础语言，什么都能干（机器学习／操作系统／人脸识别／硬件开发等） HTML：网页标记语言； Objective—C：C语言升级版；用于开发apple的OS X系统和ios； Github git：分布式版本控制库；github：为使用git作为版本控制的软件项目做代码托管； 核心功能：Fork &amp; Pull Request，用户可简单复制一个项目并增加自己的修改——将修改贡献回原项目——原作者简单处理即可发布修改版——修改者可获得开源贡献成就（贡献越多，能收获越多朋友关注） 博客功能：创建一个项目专门存放个人站点——博客框架——通过xxx.gitgub.io访问站点 IDE IDE：集成开发环境，代码编辑、程序编译+运行+调试； 版本管理器SVN 作用：协同撸代码； 重要功能：文件提交记录，文件修改比对；差异比对，版本递增发布；]]></content>
      <categories>
        <category>Product Manager</category>
        <category>Technical Logic &amp; Knowledge</category>
      </categories>
      <tags>
        <tag>PM Communication</tag>
        <tag>Technical Logic &amp; Knowledge</tag>
        <tag>Reading Notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Interpret Technique to PM|Chapter7:Secure]]></title>
    <url>%2F2017%2F10%2F01%2FInterpret-Technique-to-PM-Chapter7-Secure%2F</url>
    <content type="text"><![CDATA[Abstract:产品研发过程中的一些计算机安全方面的知识。如web安全，信息攻击，加密，信息安全等。 撞库 利用在a网站盗取的账户密码登b网站（大部分人喜欢在不同网站上用同一组账户密码） 防范：1.网站：从其他维度检验（如若不在常用城市登录，则发个短信进行二次验证）；2.用户：密码记忆工具or给自己定一个密码规则便于记忆； web安全SQL注入攻击 原因：开发者未对用户提交的内容进行过滤，导致恶意SQL语句的执行； 防范：设置过滤机制，如只允许使用字母和数字；（永远不要相信外界输入的数据）；XSS漏洞 XSS（cross site script）：跨站脚本攻击，即利用js产生的一个网站攻击另一个网站的安全问题； dom-based xss（基于dom的xss漏洞） stored xss：只要看过某网页的人的cookie都被发送到b网站； 根源：未对参数货一些可执行代码进行校验货过滤； 解决方法：进行校验（如手机号不是11位不让提交到服务器）或对可执行代码变普通文本废掉可执行能力；不对输入参数进行防护都会造成xss漏洞，可用工具扫描； CSRF攻击 CSRF：cross site request forgery，跨站点请求伪造；即a网站对用户建立信任关系后在b网站利用这种信任关系、跨站点向a发起一些伪造的用户操作请求以达到攻击的目的； 原因：服务器对浏览器过于信任，未区分请求是用户主动发出的还是模拟发出的； 防范：1.关键数据操作的请求使用post请求，限制get请求的数据；2.关键数据操作时加上验证码验证用户身份；3.Anti CSRF token：在真正表单面里隐藏一个随机的每次都变化的token，当用户提交表单时，将token提交到后台进行验证，若验证通过则执行操作（b拿不到a表单里的token）； 黑产系列ARP攻击 mac地址（网卡地址）：当网络上相邻的两台网络设备通信时必须明确知道对方的mac地址才能通信； 局域网中的主机可以利用ARP（地址解析协议）来获取局域网IP对应主机的mac地址； ARP协议的请求过程：主机A在局域网中发送ARP请求，内容为请主机B把mac地址告诉我——局域网中所有主机和网关都会收到这个消息，但只有主机B会回答这个消息（我是主机B，我的mac地址是xxx）——主机A获得B的mac地址并将其保存在自己的ARP缓存表里 被攻击的根源：请求过程有漏洞； ARP攻击能达到的目的：1.制造网络中断（how：使至少一方无法获得对方的正确mac地址，如攻击者主动回复A ARP消息）；2.ARP欺骗，中间人攻击（how：攻击者主动回复病将自己的mac地址放入应答消息里伪装成网关，则A发送的所有收发消息都被窥探）； ARP攻击的主要手段：让被攻击主机的ARP缓存表中加入攻击者期望的错误信息； android应用之二次打包 what：把Google play上的应用进行二次打包分发到国内市场，通过广告植入等手段牟利； 流程：下载应用——反编译——篡改代码——重新打包——国内分发； defend：1.校验签名（在应用的关键节点进行签名校验）；2.加固处理（使无法被反编译或即使反编译了也无法打包）； 年产百亿的DDoS攻击 原理：通过对服务器发送大量无用的请求，耗尽服务器资源（CPU，内存等）导致服务器无法正常运行，正常请求无法得到响应； 实现DDoS攻击需要：攻击者比被攻击者拥有更多的硬件资源（更好的CPU和更大的带宽），实现途径：1.通过流量平台租赁流量实现流量攻击；2.种植肉鸡，构建僵尸网络，利用云控指令对被攻击者发起攻击； 可怕之处：被攻击者无法正确地区分哪些是攻击者发来的请求，哪些是正常的请求； 攻击方式的切入点：利用TCP等协议栈或系统本身的缺陷达到最大化利用攻击资源的效果； TCP协议的三次握手：客户端向服务器发送一个SYN报文（我要跟你建立TCP连接）——服务器向客户端发送一个SYN+ACK报文（ACK是对第一次握手中SYN报文的确认：同意建立连接）——客户端向服务器发送一个ACK报文（确认第二次握手的SYN报文被成功接收）——TCP连接成功建立，服务器和客户端开始数据的传输； ROOT权限：裸奔模式 app遵循系统开发框架：开发框架定义了app获取系统能力的通道，app通过获取系统提供的能力来完成自己产品对应的功能；用户所做的每一个操作都要经过这个通道，由操作系统处理后完成；【通道是实现功能的必经之路】 风险：如果通道被挟持／篡改，我们的敏感信息就可能暴露「这种技术叫：动态注入」 动态注入：所有数据都会被某个「其他应用」窥视一遍，然后才会到达系统； root的风险：手机root+安装某具有动态注入功能的应用——结果：信息被窥探； 动态注入监控应用的实现过程：应用启动，A使用动态注入将自己的窥视代码注入应用进程中——窥视代码注入后会针对性拦截应用到系统的通道——应用使用通道时数据会先被A窥视，然后才会交给操作系统； 好的作用：通过获取用户的使用习惯来做针对性的体验优化； 诈骗短信和伪基站 原理：伪基站（核心原理：模拟正常基站的频率来吸引手机连接，然后对用户手机发送诈骗短信；GSM模式下手机与基站是单向认证，手机天生信赖所有基站） 基站：手机能接入网络的桥头堡，所有用户的通话和上网行为都要通过基站连接网络； 家庭wifi防蹭网指南 定期更换密码 使用加密效果更好的加密方式：加密方式（WEP／WPA/WPA2/WPASK/WPA2-PSK;WEP加密弱，WPA／WPA2需要额外的认证服务器，不适合家庭网络，WPAPSK／WPA2-PSK最适合加密强） 隐藏无限网络SSID：SSID是Wi-Fi名称，在路由器里关闭SSID广播即可防蹭，自连Wi-Fi时手动输入SSID即可； 关闭DHCP服务：控制IP地址的分配，手动给自己的设备配IP地址； 开启mac地址过滤：只允许自己的mac地址通过路由器； 设计windows病毒 伪装：提高用户主动启动病毒源文件的可能性； 释放主程序：感染计算机时病毒源文件需要将主程序释放到计算机的其他目录里（为了防止用户删除源文件导致病毒被清除）； 添加自启动：释放主程序的同时将主程序添加到注册表（windows系统里的“数据库”，存储着操作系统的一些关键设置，包括自启动设置）定义的启动项中； 深度隐藏：设置病毒主程序所在的文件夹为隐藏+修改注册表、屏蔽常规恢复文件可见性的操作； 自我复制：备份病毒的可执行程序——拷贝病毒的exe文件到各个磁盘的根目录+修改autorum.inf文件内容（打开磁盘=触发病毒运行）； 传播：通过windows提供的远程登录功能，利用管理员账户的弱口令来登陆到局域网中的其他计算机并拷贝病毒到计算机； 公共Wi-Fi 无密码wifi：无任何加密保护； 带WEP加密的wifi：可被窃听的； 带WPA加密的Wi-Fi：不易被窃听的； 钓鱼wifi 蜜罐wifi：下次自动连接； 风险：窃听重要信息，篡改信息； 加密 密钥：一堆参数，把参数与明文信息按一定算法混到一起，把原始信息整的像代码一样； 加密类型：对称加密（加解密双方的密钥相同），非对称加密（公私钥，不同，不可推导）； android系统权限管理 常用权限：网络访问；读取手机状态和身份；开机启动；修改货删除外置存储中的相机／内容； app调用相机权限的步骤：把需要的权限写在程序清单文件里——把文件打包到APK里——用户下载APK——安装时弹出一个权限说明——用户选择是否安装（选择安装=同意app调用权限）——app自调用 微信如何校验分享的合法性 应用获取分享功能的过程：在微信平台上注册自己的信息（应用名称／包名／签名的公钥信息：使微信有校验应用的能力）——签名校验信息无法通过则拒绝分享；]]></content>
      <categories>
        <category>Product Manager</category>
        <category>Technical Logic &amp; Knowledge</category>
      </categories>
      <tags>
        <tag>PM Communication</tag>
        <tag>Technical Logic &amp; Knowledge</tag>
        <tag>Reading Notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Interpret Technique to PM|Chapter6:Front End]]></title>
    <url>%2F2017%2F10%2F01%2FInterpret-Technique-to-PM-Chapter6-Front-End%2F</url>
    <content type="text"><![CDATA[Abstract：产品研发过程中的一些前端知识。如Ajax，PWA，H5,react,websocket,spider 等。 Ajax技术的妙用 Ajax：不用刷新网页来和后台交互获取数据，并应用于当前网页； DOM：document object model文档对象模型，定义了一个网页的大概结构； Ajax实现：发起网络请求（利用XmlHttpRequest对象发起）——网络请求返回若干个联想词的json串——网页中的javascript程序开始处理，进行json串的解析——将解析后的字符串插入网页的dom结构中——页面渲染，在当前页面展示一个下拉列表框并填入联想词； ajax是一项用于异步拉取数据并展示在当前页面的技术，适用于需要延迟加载数据和触发式加载数据； Google PWA PWA：progressive web apps：利用chrome的一系列基础能力整合成的一套技术框架，使web应用可像原生app一样在手机 上运行； PWA能力 快速加载（service worker：可控制缓存数据在本地，以便下次加载时直接使用本地数据） 添加到桌面+push消息（浏览器使用service worker时会开辟独立后台线程，通过线程可与服务器保持稳定通信，通过浏览器使用系统通知栏push消息）+安全（建立在https加密链接上）+响应式设计（继承网页响应式设计，从容适配机型） 使用本地硬件设备 H5 表单：是用户与服务器数据交互的用户界面，一切向服务器提交的数据都是由这几个简单标签所组成； h5应用程序缓存：application cache技术可扩容（离线存储技术，使明确页面中哪些静态资源可在第一次访问网页的同时缓存到本地，并在下次访问时向服务器询问本地资源是否需要更新），可用在web应用上；加快网页访问速度，节省带宽，提供离线功能； React Native 前端技术；用javascript即可同时编写前端，移动终端，后台应用程序；随时热更新； UA UA：useragent用户代理； web开发 javascript：用来控制html里的每个元素； URL url地址的组成：protocol，hostname，port，path，parameters，query； protocol：协议，定义数据如何封装／打包／解释的规则； hostname：主机名／域名；qq.com是一级域名，www.qq.com是二级域名； path：路径；即最终文件所在的路径和文件名；存储在本地货服务器上； parameters：参数／查询，目的是在url中戴上去一些本地的信息传给服务器； port：端口号，即从哪个端口访问资源； http://news.qq.com:8080/a/20160203/012323.htm?a=1&amp;c=3#p=1; websocket 实现实时更新用户界面数据：1.轮询：每隔几秒就重新向服务器发送一个请求（缺点：效率低，浪费不环保）；2.使用comet技术（发送一个更新请求出去后就一直占据端口，等待服务器响应，直到服务器有数据返回时菜断开链接；也不环保，浪费资源）；3.websocket：建立在TCP上的一种全双工协议，客户端可向服务器发送信息，服务器也可向客户端push信息； 跨域与同源 iframe是html中的一个标签；可指定一个随意的url地址；作用就是将一个URL地址嵌入到当前页面并展示出来； 跨域： 同源策略：协议／端口／host三个都一样的url即同源；同源即同域； 网页 1.生成dom 2.生成渲染树；（css样式描述） 3.测量／排版／绘制； 动态网页和静态网页：最核心的区别是后台是否有数据库的支撑，网页展示的内容是否需要因人而异地变化；静态网页一般不需要后台程序干预处理，直接由服务器返回，由于数据不需要更新故实现为静态网页后一般不需要维护；动态网页一般需要程序处理（asp，jsp，php，python，ruby等）并由数据库提供数据支撑； 互联网的黄金矿工：爬虫 爬虫原理：给爬虫几个原始的url——爬虫把这些链接的网页抓取回来——经过对网页的分析后得到两部分数据（1.网页的有效内容，用来建立搜索关键词的索引，2.网页中的url链接「作为下一轮爬虫抓取的目标网页」；） 挑战：从页面中解析出正确的url——很快的抓取速度——挑选最有价值的页面进行抓取的能力——适应不同的网站； 设置不想被抓取：在网站根目录下放一个tobot.txt文件，描述哪些页面可以被抓取，哪些不能； 网页内跳转的实现——锚点：网页元素可用id-xxx来定义位置（锚点）； 广告过滤机制：1.屏蔽广告的招数：不让广告被下载下来（找到广告对应的url，浏览器不拉取即可） or 即便下载下来也不让它被展示出来（找到网页藏身的标签，不展示标签即可）； AngularJS与双向绑定：1.bootstrap解决了快速构建一个网页框架和样式的问题；2.如何构建一个动态网页／webapp：用angularJS框架；3.双向绑定：在输入框输入同时在网页中实时输出trump（控件与数据双向通信）； 前端和后台的数据交互协议：1.移动端如何与后台交换数据并演示：用户刷新——客户端开始组织数据请求——数据请求传到服务器——服务器查看并准备数据——数据传回客户端——用户界面上显示数据；2.客户端与服务器传输数据的格式：PB（protocol buffer二进制数据传输），JSON（javascriptobject notation，轻量级数据传输格式，非二进制，用一堆中括号把数据组织起来）； bootstrap：创业公司只打磨mvp原型／写代码来成就感；轮子是无所不有的！ 响应式设计和viewport：网页的内容可根据屏幕的尺寸不同而自动适配；1024*768；viewport：视口，描述一块区域，浏览器可在这块区域上去排版／渲染；网页适配手机的方式：css的px与真实的px之间根据不同机型构建不同的换算关系； 自动秒杀脚本：1.脚本：一些命令的集合，不同脚本实现不同的功能；2.自动秒杀的障碍：不停刷新浏览器（js：定时器）——自动发起购买请求（模拟用户点击购买按钮或虚拟一个http请求，把需要的参数填进去，然后不停发往服务器）；3.document.getElementById(‘kw’).value=’给产品经理讲技术’; document.getElementById(‘su’).click();4.防自动化脚本：加验证码；5.实现流程：用js实现自动刷新-自动点击购买按钮-自动填写验证码； 网页与元升app的交互：安卓里是js2java实现网页与app的双向通信，app通过webview（即一个没有任何操作按钮的组件）来展示网页； js：解释性语言，java：编译性语言； python：高大上的脚本语言； json：javascript object notation从js里走出的数据格式；优点是人类能看懂，机器好解析，适合前端和后台数据传输； 盗链与反盗链：1.盗链：盗取别人的链接；（给别人网站增加了资源消耗）2.反盗链返回一个警告图片之类的； 浏览器缓存：浏览器回缓存他浏览过的资源（图片，脚本，网页等），若资源在保质期内，则下次同样的请求直接用缓存，过期后会带着资源上次的修改实践由服务器来判断是否失效； 数据类型：各种编程语言都有自己的基本数据类型，bool（布尔型），int（整形），float（浮点型），string（字符串）；二进制数据表示； 序列化：将数据对象转化成二进制串，反序列化是将二进制串还原为数据对象，能被序列化是数据对象能够持久化保存在硬盘中货网络中、不同平台间互相传输的先决条件；button不能被序列化；能被序列化解决了协议传输的问题，关于协议解析需要遵循通用的协议表示的规范（基本数据类型里扩展性最好的是字符串） 协议收发过程：发送方将协议制定的字段写成xml或json格式的字符串，序列化后传输，接收方反序列化还原出字符串，然后按照xml或json格式取出传过来的各个协议字段；（协议写成字符串——序列化——传输——接收——还原字符串——按格式取出协议字段） 正则表达式 用来匹配字符串（即自己先定义一套规则，然后根据这套规则去一个字符串里着哪些符合规则的字符串） 搜索需要精确查找，而正则可匹配一类东西； \b：单词的分界，表示只取单词；\w：表示单词和数字；\d:表示一个数字；「.」可匹配任意字符；「{num}」：数量限定符；「*」：表示任意数目；「+」：表示最少1个；「？」：表示最多一个； eg. \d「{18}」表示18位身份证号；\d\d\d\d\d\d\d\d表示匹配8位数的qq号； 应用：验证用户输入是否合法； 爬虫网页分析 爬虫：脚本程序；把命令写到脚本里让计算机自动执行； 微信公众号历史消息：一次加载10篇，典型的异步加载； html：以很多标签来描述网页，标签一般成对，标签里还可以套标签，表示层级关系；eg一个人叫html，有head／body，body上有hand，hand上有finger； 找到url：即网页源代码里的 抓包 如何实现自动下拉：1.让程序模拟人的操作；2.获知网页是如何下拉的，再不断伪造这个请求； 抓包：在客户端／浏览器和服务器之间设一个检查站，获知所有经过的数据的来源／目的地／数据内容； 网络通信：客户端／浏览连接到服务器——通过http协议向服务器发出拉取数据的请求——服务器收到请求做出应答吐出数据；【渠道：检查源代码——network】 network——相应操作的response里找到下面网页的URL 爬虫抓包的思路：打开历史消息——爬到最近10篇——伪装页面向服务器发起加载更多的请求——服务器吐出一个json格式的响应——解析json即得后10篇文章的url——重复几次得所有； json：数据格式；一些键值对； json解析 算法：告诉电脑该怎么做； json：javascript object notation轻量级的文本交换格式；用{key ：value}格式表达信息使易解析；key——数据名，value——值（字符串／数字／数据／数组／空值／真假／另一个jason数据）；字符串{『name』：」guoguo」};数字{「age」=18}；真假{「hansome」：true}；另一个json数据{「game」：{「name」：“clash”」}（只需要在值的位置再加一个大括号描述json数据即可）；一堆数据{「articles」：[{「title」：“xxxxxx”，」author：“果果”」}];(用中括号把一个个value扩起来即一堆数据)； 微信小程序 Js2Java：网页通过js接口调用微信的native能力；微信——将native能力封装成接口，开发者——做简单的前端页面开发+在服务器上部署服务+通过微信插件框架加载起来； 优化网页速度的7种方法 网页加载流程：打开网页——拉取html页面——浏览器解析html页面——根据页面内容拉取js／css和图片文件——根据文件渲染出页面； 影响网页展示速度的主要因素：不是网页本身，而是它依赖的一些其他文件；优化重心：资源的加载速度； 1.优化图片资源的格式和大小，在保证质量的前提下尽量使用高压缩率的图片格式（优先级：webp-jpeg-png-bmp），根据展示尺寸来拉取图片资源； 2.开启网络压缩：以gzip压缩方式传输数据可减少70～80%的流量； 3.使用浏览器缓存：同一站点下的不同页面往往会复用一部分资源文件，若设置成可缓存的，则在刷新或跳转时则无需从网络再拉取相关资源； 4.减少重定向请求：使用响应式设计，一个站点覆盖所有终端，避免重定向请求； 5.使用CDN存储静态资源：CDN是静态内容分发网络（在每个地区都有其服务器，就近从cdn拉取可快速获取资源），把不会变化的内容放在cdn上可提升资源下载速度； 6.减少DNS查询次数：浏览器解析域名需要时间，故尽可能将图片放在同一个域名下； 7.压缩css和js内容：将css和js里的大量空格／变量命名精简即减小文件大小，加快速度； 总结方法：减少请求数+减小资源大小+找最快的饿服务器 javascript js的运行环境：解释器——js引擎（浏览器都会内置js引擎） js应用：react native ；node.js； react react：前端UI组件库，前端框架； 面向对象的好处：能封装就封装；用js包装html，使html也有封装能力；可以少写很多代码； 自带虚拟dom树；]]></content>
      <categories>
        <category>Product Manager</category>
        <category>Technical Logic &amp; Knowledge</category>
      </categories>
      <tags>
        <tag>PM Communication</tag>
        <tag>Technical Logic &amp; Knowledge</tag>
        <tag>Reading Notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Interpret Technique to PM|Chapter5:Computer Network]]></title>
    <url>%2F2017%2F10%2F01%2FInterpret-Technique-to-PM-Chapter5-Computer-Network%2F</url>
    <content type="text"><![CDATA[Abstract：产品研发过程中的关于计算机网络的一些基础知识。如服务器，网关，协议，长连接等。 网络基础之协议栈 数据传输的原理：分解——传输——还原； 应用程序产生的数据经过五层协议栈模型自上而下逐层分解成可供物理介质传输的信号，当到达目的主机后再自下而上还原成应用程序数据； 分层的好处：每层可被独立设计，只要保证上下层借口一致； 协议栈模型 应用层（http，telent，FTP，TFTP）——传输层（TCP／UDP）——网络层（IP）——数据链路（Ethement，802.3，PPP）——物理层（借口和线缆）； 应用层：为应用程序提供数据传输的网络接口，如网络协议； 传输层：提供端到端的连接，即让A主机上的程序a找到B主机上的程序b，TCP／UDP协议，端口号的概念； 数据链路层：网卡的工作地，负责将数字信号转化为可供物理层传输的电信号／光信号； 物理层：信号传输的物理通道，如网线和配套的接口； 互联网一切皆下载 流的概念：从另外一个地方不间断的获取数据； 获取=下载，只不过应用程序对获取到的数据的使用方式不同：播放／展示／存储； Docker 解决应用程序的环境依赖／大规模部署／应用间的相互依赖问题：Docker——相当于集装箱； Google - GUIC协议传输快*基于UDP协议，增发校验数据包，再连无须确认，网连发生变化时无须重建连接； IP地址枯竭的后悔药-网络地址转换NAT 私有网络访问internet的方式需要用到NAT技术，有3种实现方式： 1.静态转换Static Nat：内外网地址一一对应； 2.动态转换dynamic nat：内外网的对应关系不固定，一般ISP会使用这种方式给接入用户提供访问外网的能力； 3.端口多路复用：让外网ip地址被多个内网ip同时共享； P2P终结者的原理 原理：欺骗网内所有主机，让大家把数据先发给他，然后它控制数据的流向／传输速度吗，达到控制局域网内网速的目的； 破解法：在网关上把客户端的ip和mac地址绑定，并在电脑上将网关的ip和mac地址绑定（这样arp就不能通过mac地址号称自己是网关了） ping和网关 ping：网络诊断工具（packet internet groper），检测网络是否连通；ping是TCP/IP协议族的一部分，原理是向IP地址发送一个数据包，若对方返回同样大小的数据包，则证明连通，且可测试时延； eg.在命令行里输入ping baidu.com检测百度的连通，输入ping 127.0.0.1检测本机TCP／IP协议设置是否准确；字节=32表示发送出去的数据包是32个字节，时间表示发包到收包的耗时，TTL表示这个包可存活的数量单位； 网关：网络的关卡，即两个网络之间的门；表示为IP地址； session session机制：解决协议无状态的问题，相当于给每个用户自动分配了一个身份，从而完成对用户身份的识别；指服务器记住用户的能力（通过cookie机制来辅助实现），客户端和服务端产生联系； 场景：购物车的商品记录； 端口 端口：终端留给外部的接口，是不同设备间通信的桥梁； 驱动：外部硬件与计算机之间交流时的翻译； 物理端口：如电脑的网孔，usb端口等； 虚拟端口：如网络数据的收发； 沟通的核心：软硬结合，分而治之； TCP/UDP TCP（transmission control protocol）传输控制协议，IP（internet protocol）因特网协议；TCP／IP是一个协议族，定义了整个互联网通信的基础，定义互联网如何连接／协商； TCP：需要对方确认的（即传输前需要进行三次握手，三次确认才能通信）；可靠带来效率下降（如时间大部分浪费在互相确认中，资源消耗多） UDP（user datagram protocol用户数据包协议)：粗暴，不管对方什么情况，直接发送，不需要确认过程；不可靠带来效率提升同时服务质量下降；udp不一定不可靠（如果可以通过校验算法保证数据质量的稳定），如QQ发送消息即用UDP； 稳定性优先的场景优先选TCP，如浏览器访问网页，如微信（要维护一条稳定的信息传输通道）； TCP为什么可靠：尽最大可能把数据包传送给目的主机+即便传不到也会反馈失败信息；保证可靠的手段：顺序编号（数据包拆小且一一编号）+确认机制）（ACK，向发送方反馈成功接收的信号）+超时重传（发送方每发送一个数据包都会为数据包装定时器，当定时器归0而发送方仍未接收到成功信号，就会重传，直至接收到信号或连接断开） UDP无确认机制和超时重传机制，发出去的就像断了线的风筝； HTTP／2新一代网络协议 http：hypertext transfer protocol超文本协议； http／2的进化点：多路复用（将多个http请求复用在一个TCP链路上，避免重复请求，提高效率）+二进制传输（可传输的东西变多，如加密后的数据／视频流）+使用HPACK压缩头部信息+ 服务器端push（服务器可直接将资源推给客户端）； 邮件协议（仅论客户端） 收邮件：1.POP（post office protocol邮局协议）：支持从邮件服务器上将用户邮件下载到本地；（可离线所有邮件）；2.IMAP：可先获取邮件标题和摘要+ （当想看正文时再拉取正文）+ 将邮件处理状态（标记已读，删除）同步给服务器； 发邮件：1.SMTP（simple mail transfer protocol），将邮件推送给服务器；2.exchange activesync：微软的同步协议，同步内容包括邮件／日程／联系人等； 反向代理 代理：客户端向外界发起请求不直接与目标服务器连接，而是经过代理服务器去连接外界目标服务器；（服务器隐藏客户端） 反向代理：服务器接受客户端的请求——将请求分发到被代理的服务器上——服务器处理完请求后将结果转发给客户端；（服务器隐藏服务器；接口人角色：接洽所有客户端请求并进行简单处理而后分发到服务器） 意义：负载均衡；减轻后端服务器压力；为后端服务器阻挡一些网络攻击； CDN内容分发网络 CDN（content delivery network）:专注静态资源的分发和访问；动作是分发；落定于网络； CDN作用：一个基于互联网数量巨大的服务器，专注于内容和资源的分发，方便用户快速访问，提升用户体验的内容网络；资源的分布式存放和备份的方法； 用户就近接入服务器：利用DNS判断用户位置； push数据包 车票（ip地址+端口号），高速列车（TCP／IP），ARP广播（获取下一个网关的mac地址，创建路由表，引文传输过程需要经过无数网关路由器的中转才能到达目的地） 长连接 连接：客户端通过TCP／IP协议从服务器上获取数据时需要一个连通服务器和客户端的连接，连接通过三次握手建立，四次握手释放； 短连接：每次获取数据都创建一个独占的连接； 长连接：一个能供多个请求多次传输数据并在数据传输后保活一段时间的连接； 长连接的使用场景：1.短时间内对同一个服务器的多次数据请求；2.实现push功能（基于长连接的全双工的通信能力，push系统的客户端需要利用最低频率的心跳保证长连接的存活时长）； 免费电话 将语音信息转化为能在互联网中传输的IP数据包——UDP协议传输； HTTPS技术 https（安全超文本传输协议）：http协议的主要缺点是明文传输（易被篡改）；https在http与TCP间加了层SSL协议（保障网络数据传输安全的协议，在传输层对http封装加密，并将数据交由TCP协议发送到网络上） 使用https服务器需要在受信任公司申请一套数字证书（即公私钥，进行非对称加密，公钥加密的数据需要私钥解密，私钥加密的数据需要公钥解密） https协议建立连接的过程：客户端发起https请求——服务器将公钥发送给客户端验证身份——客户端生成一个加密密钥，公钥加密后将蜜月传给服务器，服务器用私钥解密报文获得客户端密钥——服务器和客户端的数据传输都通过客户端密钥进行加解密； 就近接入 问题：跨运营商的网络访问慢； 解决思路：访同一运营商的网站； 方案：就近接入，即利用DNS服务找到离用户最近的机器，从而达到最短路径提供服务，理论上可找到这个公司的所有机房和IP从而达到流量调度的目的； 网络带宽 带宽：单位时间能通过链路的数据量； 快播与p2p 成功的原因：真实解决了痛点（免费视频消费需求+一整套的视频网站技术解决方案）； P2P：去中心化服务，无中心服务器，每个用户既是下载者又是下载服务提供者；（共享模式） socket 一套封装了TCP／IP的实现的API；实现和其他电脑的通信+作为进程间通信的一种方式； 搭服务器 安装Nginx 测试 REST representational state transfer表现层状态转化； REST是一种定义API的风格，URL只表示资源，而对资源的操作一律用http协议自带的动词来完成；]]></content>
      <categories>
        <category>Product Manager</category>
        <category>Technical Logic &amp; Knowledge</category>
      </categories>
      <tags>
        <tag>PM Communication</tag>
        <tag>Technical Logic &amp; Knowledge</tag>
        <tag>Reading Notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Interpret Technique to PM|Chapter4:Workplace Client Technology]]></title>
    <url>%2F2017%2F10%2F01%2FInterpret-Technique-to-PM-Chapter4-Workplace-Client-Technology%2F</url>
    <content type="text"><![CDATA[Abstract：一些客户端技术原理，如APK，传感器，热补丁，系统权限等等。 广告追踪 pc端追踪：cookie标识下收集用户的个人浏览习惯 移动端：GUID标识用户设备； APK是怎样炼成的 APK：安卓系统上的应用安装包；androidmanifest：包含软件的基本信息，包括版本号／协议，是程序间互相调用的基础；class.dex装载了所有代码；resource.arsc：设计资源，包括res：所有设计图，xml文件：描述动画效果和页面布局等；META-INF：签名信息（公钥私钥）； xposed 安卓系统上的一套hook框架：在不改变原app情况下修改app中函数的行为，使第三方开发者也可以对app做定制化的改动；可拦截并改变一个函数的输入返回值；可hook应用里的每个函数； 安卓掉电快：push 各个软件以自己的节奏唤醒cpu来实现其push操作； apple pay NFC技术：短距离高频无线通讯技术，允许电子设备间进行非接触式点对点数据传输；10cm内交换数据； 安全单元secure element：独立电子芯片，有自己的微处理器／存储／加密硬件，只有授权的应用才能访问安全单元的数据； touch id：指纹识别，指纹传感器将单个指纹转化为一组数学表达式，用于解锁一些机密信息，只对活体指纹识别； secure enclave：cpu里集成的安全执行环境，用于处理敏感信息，信息用特殊密钥加密传输； 流程： 12345678910111213绑卡（用户扫描银行卡或输入——apple将信息加密传输给银行——银行鉴定信息的准确性——（正确）——以银行卡与绑定的ios设备为一个组合生成唯一的设备id喝交易动态密钥——系统将其储存在安全单元+apple pay读取设备id和交易动态密钥）——支付（apple pay通过NFC与pos机通信，传入收款商家信息与金额——激活设备并请求用户指纹授权——指纹识别转化为数学表达式传入secure enclave——secure enclave对比已存储的数据——一致则认为是同一个用户，授权 通过——secure enclave通知安全单元生成一个只对当前这笔交易有效的token——通过nfc将设备id和交易token发给pos机——pos机将数据传给银行——银行通过设备id查询到最初绑定的卡号并对比token的有效性——（token有效）交易可行——资金处理+通知pos机交易成功） android 应用的续命大法 监听系统事件：利用android系统的广播机制（把系统发生的事件通知给需要知道此事件的应用）唤醒自己；缺点：有的rom会对广播限制； 守护进程：应用启动后创建守护进程，在本应用被杀死后复活它； 全家桶系列：不同应用相互唤醒； 手机传感器 磁场传感器：获得收集在xyz三个方向上的磁场强度，旋转手机直到只有一个方向的值不为0时，手机指向正南方；可根据三方向上的磁场强度不同，计算出手机在三维空间的具体朝向； 加速度传感器：返回手机在xyz三方向上的加速度值，若手机水平放置，则z值即当前的重力加速度G，通过判断G值不同推测用户是在南北极or赤道；应用：计步； 三轴陀螺仪：可获取手机在xyz三方向上的角加速度，主要用来检测手机的旋转方向；应用：接听电话； 近距离传感器：检测收集附近物体距离收集正面的距离（小型雷达），通过发送一些脉冲信号并检测返回事件来计算距离； 光线传感器：检测收集正面接收到的光照强度，从而对应改变屏幕亮度； 气压传感器：检测大气压强从而推测海拔高度；如计算爬了几层楼梯； 温度传感器：获取当前环境的温度； 热补丁技术（android端） Q：app版本发布但发现bug，如何补救？ 1.线上推送一段修补bug的代码代替原代码插在运行队列的前面，由于同名代码段不能同时运行，故bug代码永远无法运行； 2.dexposed：直接接管虚拟机的执行代码流程，执行任何我们想要的代码；限制：只能改自己的app，改别人的app需要root权限；且android5.0以上的系统不可用； 3.微信Tinker热更新：不修改系统加载程序的顺序，而是直接在客户端生成全新的程序文件加载； 超级APP诞生记账号登陆 注册和登陆后密码会经过哈希变换再发送给服务器；哈希：把一串数字变成谁也不认识的文字，且过程不可逆推； 服务器验证成功后不会记住你，但会返回给你一个票据代替密码（类比机票代替身份证） 记住密码!=客户端真的记住密码，而是保存票据把票据拿给服务器做验证； 定位 GPS定位原理：（核心是参照物，已知参照物的位置和自己与参照物间的距离，即可定位自己的位置）卫星不断广播自己的位置，打开GPS信号接收器收集至少4颗卫星发出的信号，用收到信号的事件乘以光速即可算出自己与每颗卫星间的距离，再加上卫星位置已知，即可确定自己的位置； 基站定位和Wi-Fi定位：参照物为手机连接的基站（查询基站位置）和路由器（查询路由器位置）； 版本更新——增量更新 增量更新：只更新发生改变的部分，而非下载完整安装包； 实现：将最新安装包与历史发布的所有版本进行差异对比，得到相应差异包（服务器脚本批量完成）——下发差异包（将当前版本信息发送给服务端，服务端判断后选择相应差异包下发）——合成新包（取出历史安装包，合成）——校验完整性（校验当前历史包的hash值／差异包hash值／合成后新包hash值是否都与预期匹配） 省流量：25%／100% 黑知识Google黑科技：instant app 从链接打开instant app，在native界面操作，用完即走；app+web； 点九图 利用draw9patch，在png图像四周点黑点，黑点平行对应的区域是可拉伸的，不对应的区域不可拉伸；原理：告诉程序这张图片上哪里（不）可拉伸，可拉伸的区域实际不是被拉伸，而是复制粘贴它本身来填充区域以达到拉伸的效果； 作用：解决图片拉伸后的失真问题； 应用国际化 要中文配置文件——译成英文——查显示效果（文字过长？图片上的文字怎么改？） android系统的65536 class.dex文件行数不可超过65536；但可有多个class文件，但apk安装时会对class.dex做优化却不能对其他class文件（只能在程序首次启动时优化）优化，导致应用首次启动耗时长甚至首次启动无响应； android系统权限 权限管理：用户选择是否授予程序权限； 网络访问权限，修改或删除外置存储中的内容，读取手机状态和身份（用户标识），查看wlan连接，控制振动，检索正在运行的应用（瞅瞅竞品的活跃程度），防止手机休眠（维持后台运行），大致位置（基于网络），开机启动，相机，在其他应用之上显示内容（覆盖），精确位置，安装快捷方式，录音，卸载快捷方式（悄悄将竞品的快捷方式删掉），读取联系人信息，停用屏幕锁定，发送短信（花用户的钱给自己发短信），读取短信； app的推送功能 实现一个推送系统：需要服务器和终端的配合； 方法：1.轮询（定期不断向服务器请求，此法低级），2.push（和后台维系一条通信通道，两端不定期互相通信以维持后台，但通道易被杀死）； ADB（android debug bridge） android调试桥接器： 图片缓存系统 分两级，一级缓存（内存缓存，存取速度更快，程序退出则数据消失，且多占内存，以空间换时间）+二级缓存（硬盘缓存，缓存容量更大，速度 慢，程序再启动后仍可用）； 刷图片的流程：先占用一级缓存空间——一级满则将部分图片（不经常被使用的）清理到二级缓存——二级满则删除部分图片； 微信与新闻app在列表滑动时图片显示的区别：滑动微信通讯录列表的主要目的是快速查找（若此时读取头像则会导致整个列表的华东掉帧卡顿，故舍弃滑动效果），滑动新闻app列表的目的是浏览新闻（慢速滑动，快速滑动的卡顿感被削弱）【同一操作行为背后的关键路径的不同导致技术方案的差异】 庖丁解牛安卓平台第三方市场的自动安装功能 利用root权限静默安装； 利用安卓辅助功能（accessilility，为残障人士／感知受限人士提供的）自动安装； android accesibility 一套接口，为特定人群开发定制的辅助功能，降低操作门槛，优化体验（基于用户对辅助功能的授权）； 功能：获取用户当前界面的UI元素，界面改变时系统会将ui界面的变化信息传输给辅助软件（信息：当前展示的是哪款应用的UI界面+当前ui的结构和所有控件的信息+所有控件可执行的操作+对制定控件进行模拟操作） 应用：自动安装app；微信抢红包神器+朗读控件文本内容（为听觉障碍人设计）+自动设置默认应用 抢红包神器的实现：监测微信会话列表，发现列表中出现包含【微信红包】字样的控件时对该控件发送指令+ 进入会话详情，对包含【微信红包】字样的控件发送点击指令+进入红包开启页面找到【开】字样的控件发送点击指令——红包到手； 网易新闻客户端的UI结构 检查：系统开启显示布局边界（指UI控件所占区域的大小） 主界面构成：标题栏+子导航栏（scrollview控件：外层为父控件，里层为子控件；实现上下左右滚动）+ 内容区域（listview：一批样式相同的题图和简介信息组成的父控件；特点：将用户滑出屏幕的子控件进行复用，绑定新的数据来展示新的内容，因为控件过多占用内存） 结构化数据：有强管理需求的（用数据库） hybrid app 通过开发者选项-显示布局边界来分辨UI控件；原生UI：由若干控件构成；H5:实际是webview控件，只有一个大红框； 1.何时使用原生UI：流畅性要求高的场景+UI样式相对固定不会频繁变化+交互复杂；2.何时使用h5界面：较强的动态运营需求（highly accesible）+UI样式复杂多变+交互简单+多平台复用 用显示布局边界法去分析app； 缓存的使用 全屏的loading动画：loading内容来自本地的loading数据，将服务器下发的数据存储到终端的行为即缓存； 自动的下拉刷新：数据是从服务器拉回的；当距上次刷新超过30min时，下次打开客户端就会触发这个刷新操作（若启动客户端前断开网络则不会触发）；刷新过程：将服务器的数据拉到本地+将新数据对应的内容更新到UI上+将新拉取的数据加入缓存中 加快首屏显示速度的方法：优先展示本地缓存，然后拉取网络数据（读缓存比拉服务器要快得多）； app卡顿 原因：1.过度绘制（开发者选项下界面蓝绿色多则未过度绘制，若红多则流畅度糟糕）； 微信授权登录 白先生（用户）授权馆长（第三方应用）使用自己存放在库房（微信服务器）的藏品（用户数据／关系链等），库房给了馆长一个蓝色令牌以在有效期内借还藏品，一个红色令牌以在有效期结束后到库房换新的蓝色令牌； 微信授权登录系统：基于OAUth2.0协议标准，让三方应用可在不知道用户登录名和密码的情况下访问用户再授权方服务器上的私密数据和资源； 授权流程：用户请求三方应用用微信号登录——三方应用使用appID向微信开放平台（客户端）发送登录请求——客户端加载授权页面并请求用户确认——用户点击确认按钮——微信拉起三方应用并传递临时授权码（code）给三方应用——三方应用使用临时授权码（code）／appID／AppSecret通过https协议向微信开放平台（服务器）请求access_token（有效期2h） + refresh_token（有效期30days）——服务器返回access_token + refresh_token； 临时授权码（code）+ appID +appsecret三者兼备才能获取授权； 获取安卓用户的app使用频率 目的：分析用户偏好，精准运营push； 限制：无法直接获取，需要系统开发者的系统签名； 方法：通过activity mananger提供的接口获取辅助信息； 接口：1.getRecentTasks接口：获取用户最近使用过的应用程序列表（按使用的先后顺序逆序排列；2.getRunningTasks接口：获取当前正在运行的应用列表； 实现的算法：1.每经过较长时间间隔获取一次最近使用列表，并对比相邻两次列表中应用的相对位置的改变／或出现新的应用，则标记新列表中位置前移或新出现的应用各使用过一次；2.每经过一个很短的时间间隔获取一次正在运行列表，对比相邻两次列表中的第一个应用有无变化，标记变化； 适配android屏幕 痛点：安卓设备碎片化严重（屏幕大小上万），需要通用度量单位； 诉求：在不同分辨率／屏幕密度的手机上，同样dp大小的UI元素看起来是一样大的（即视觉上看到的物理尺寸）； 像素（px），分辨率（共有像素总和），屏幕密度dpi（每英寸像素数）； dp（密度无关像素）：将px换算成dp；dp=（dpi／160）*px；屏幕密度越大，1dp可表示的px越多； （手机按密度分档次）【mdpi】密度为160dpi的屏幕：1dp=1px；【hdpi】320dpi：1dp=1.5px；【xhdpi】320dpi：1dp=2px；【xxhdpi】480dpi：1dp=3px；【xxxhdpi】1dp=4px； 设计师出图（适配不同屏幕）的方式：1.再所有目录下各放置一份不同大小（换算）的相同图片，让系统自动去寻找最合适的（设计工作量大；增加安装包体积）；2.选一个基准屏幕（一般是xhdpi：720p），把所有图放里面，让系统自己去缩放；（小的占内存，大的会失真）； 准则：1.尽量用dp；2.尽量用百分比和相对位置； 适配ios屏幕 使用点（point）作为与px的换算单位； 3GS：1point=1px；iphone4／5/6:1point=2px；iphone6 plus: 1point=3px； 图片名加@3x表示是三倍资源； 安卓手机实现计步 计步传感器（只支持android4.4以上）；加速度传感器（用户携带手机行走，垂直方向上的加速度会有规律性的变化） 系统应用删不掉 原因：应用存储在system/app里，可删除的放在data／app里； root权限可删； 关于图片失真 插值算法：邻近插值 &amp; 双线性插值；在原有像素值基础上插入或修改一些像素值，并尽可能保证原图的特征； 听歌识曲 关键：如何判断两个音频间的匹配； 以图搜图：对图片进行缩放、灰度处理，提取出64位的哈希值作为特征码，再去匹配；——识曲：找到歌曲的特征（乐纹）； 获取乐紋：把歌转化为单声道、低采样率的wav格式，即排除其他干扰只留歌曲的整体特征； 乐紋比对 乐紋获取流程：对数据库里的所有音乐提取乐紋（对每首：算出其频谱图—分成一段段，每段在频谱图上找几个特征点——特征点构成乐紋——乐紋以倒排索引形式存到数据库里） 匹配流程：上传录音-检索已有乐紋-检索数据库-考虑乐紋排列／时间等因素； android视图：view树 viewgroup——view+view+viewgroup（——view+view+view）；（理解为控件？） 安卓与ios的系统响应顺序 ios：touch-media-service-core 安卓：application-framework-library-Kernal架构； 影响：安卓手机抢红包慢； 为什么美颜app可以美颜 原理：使用图像处理里的几个滤镜算法； 灰度处理：把图片的RGB值全部改成150； 磨皮：用特殊的高斯模糊（双边滤波），即在保留边缘的前提下进行高斯模糊处理；（+ 肤色检测+人脸识别等技术配合） 视频直播美颜：实时对摄像头里采集到的视频画面应用滤镜； GPU而非CPU做图像处理； 反美颜软件：只是用了丑化照片的滤镜（加工过的信息只会越来越少，是不可逆的）； 电池续航长短的原因 续航长的关键：开源节流【增大电池容量+节约用电（运行时用最少的电量做最多的事+待机时用最少的电量不做事）】 后台任务：安卓——应用持续占领后台；ios——只有指定的几种类型的任务才能后台运行； 唤醒：安卓——cpu持续不断被唤醒； 应用市场如何为软件显示更新提示 流程：市场软件提取用户手机上所有已安装文件的包名和对应的版本号——通过http等网络协议将数据上报给服务器——服务器通过客户端上报的信息再当前app市场上架的apk里查找比对出相同包名且版本号高的apk——服务器将apk数据通过网络协议返给手机——市场软件获取服务器返回的数据后出现更新提示； 应用的生命周期 指应用自身运行的生命周期（创建-运行-消亡） oncreat，onstart,onresume,onpause,onstop,ondestroy; 如何优化客户端性能启动速度优化： 启动：用户从【打开】到【可操作】的过程； 优化的关键——数据分类：必须的（尽可能压缩），可延时加载的（降低其对UI操作的影响），不需要主动加载的； 操作和动画卡顿的优化 影响因素：过度绘制；过深的UI层级（系统传递指令的过程过长） 工具hierarchy viewer：检查绘制和应用程序的UI层级； 内存优化 内存泄漏：导致app卡顿甚至异常退出；原因是”有人“把内存借走了不还（强占内存）； 解决的关键：找到谁把内存借走了没还； 网络流量优化 问题：（android易出现）异常后台流量 解决的关键：抓包——分析抓包数据——找到可优化的点 合并请求：把同一时刻相同包头的多个数据包合并成一个发过去，省流量； 做按钮不容易 按钮三要素：形状，背景，状态；]]></content>
      <categories>
        <category>Product Manager</category>
        <category>Technical Logic &amp; Knowledge</category>
      </categories>
      <tags>
        <tag>PM Communication</tag>
        <tag>Technical Logic &amp; Knowledge</tag>
        <tag>Reading Notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Interpret Technique to PM|Chapter3:Hardware]]></title>
    <url>%2F2017%2F10%2F01%2FInterpret-Technique-to-PM-Chapter3-Hardware%2F</url>
    <content type="text"><![CDATA[Abstract：一些产品研发过程中的硬件基础知识。注：由于有道云笔记突然bug，所以丢失了很多内容。 CPU的区别 手机商家拿cpu的核心数造势，多个烂核可能不如一个优核； 影响cpu性能的主要因素——cpu架构；手机端：arm架构；pc端：x86 &amp; x64； CPU：控制器（管指令的运行调度）+存储器（存放指令）+运算器（运算指令）；cpu的重要能力是操作内存里的数据；如刷朋友圈本质也是数据的流动，以及网卡／显示器等外部设备也要与cpu交互，接受cpu的命令； 功能：执行指令，操作数据； 精简指令集CISC &amp; 复杂指令集RISC GPU（graphics processing unit）：快速高效处理图形；多核（上百上千），每核有独立的运算能力和内存空间 快速充电 增加充电器的功率 屏幕 LED：使用背光板，通过电流驱动液晶 OLED VR晕：刷新率+帧率*刷新率：输出设备每秒更新的次数，Hz帧率：显卡更新画面的次数 刷新率由硬件决定，帧率由软件决定； 存储设备传感器 光线，距离，加速度 蓝牙，BLE，NFC，ibeaconGPS定位不准无限充电原理*磁场产生电场]]></content>
      <categories>
        <category>Product Manager</category>
        <category>Technical Logic &amp; Knowledge</category>
      </categories>
      <tags>
        <tag>PM Communication</tag>
        <tag>Technical Logic &amp; Knowledge</tag>
        <tag>Reading Notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Interpret Technique to PM|Chapter2:Data Processing]]></title>
    <url>%2F2017%2F10%2F01%2FInterpret-Technique-to-PM-Chapter2-Data-Processing%2F</url>
    <content type="text"><![CDATA[Abstract：pm需要了解的产品研发过程中设计到的数据处理的基础知识。 数据处理 关系型数据库：mySQL，Oracle，RDBMS；问题：1.数据高负荷运转时出现性能瓶颈，数据库操作会更慢；2.扩展性：要给数据增加一个字需要更改大量表结构；3.恢复数据的速度慢； 非关系型数据库：NOSQL（Rediscovered，MongoDB，HBase） NOSQL：1.多台机器分担单一机器的高负荷，解决性能瓶颈；2.数据之间无依赖关系，可随时增删字段；3.结构简单，反应快；（高扩展性，分布式计算，低成本，架构灵活，半结构化数据）； 数据恢复原理 文件系统的设计——索引区+数据区； 存入数据的流程——在索引区添加索引+在数据区写入文件的完整内容； 删除文件的流程——索引信息被删且数据区里标记为无人使用+新文件存入时可能覆盖数据区的被删文件的位置； 非关系型数据库 关系型：=excel；字段指表头； 非关系型：key-value形式（数据库不关心value到底是什么，每个key对应的value项可不一致），查询无需SQL语句，查询：输入key，返回json字符串；优点：不束缚在字段的设计； eg.1334389483{“name”:”zhang”,”male”:”female”,”age”:”13”} key:手机号；value：一个个json字符串； 分布式计算基础：Mapreduce mapreduce：Google提出的分布式计算模型；即总分单，单归总；（把大量数据分解成独立单单元执行map，然后将结果归并在一起reduce） eg.分析2.6TB数据找出最热单10个人名；分布式方法：90台机器map（分别分析一个个单个文件得出key-value，key-人名，value-出现次数）+10台机器reduce（分析完的同key的结果丢到同一台机器里，什么key到什么机器要提前设好，最后获得最终的完全key-value，列出次数最多单前10个） 写SQL语句 SQL语句：创建数据库，删除数据库，删除表，修改表，查询表（核心）； 查询：select,根据条件筛选数据； 表名：staffselect from staff: 从staff表中选择select from staff Where sex=’female’:从staff整张表中选出sex为female的数据；select from staff Where name like ‘%江’ : 从表中选出名字中含有江的数据；select from staff ORDER BY cup DESC :将整张表降序排列；select *from staff ORDER BY cup ASC: 将整张表按升序排列；select name, age from staff ORDER BY cup DESC:选出名字和年龄并按年龄降序排列； 数据库 索引： 事务：提供一种机制，即一件事必须做完，如果中间出了差错就会清除所有痕迹初始化，可保持数据单完整性； 联合查询：几张表一起查； 对数据库database的理解：table-表，record-每一行的记录，feature-特点，每条记录的N列；一个大文件里有许多excel表格，每张表格里存储了许多条记录； 操作：增，删，改，查； why 数据库（比excel）：存取效率高，很多系统无用户界面用不了excel，SQL语句操作，事务系统，命令操作失误少； 什么样的数据适合数据库存储：结构化的数据，即数据有方便查询的feature； 设计数据库：excel思维+拆表； eg.user表和message表需拆开，否则会出现数据冗余／重复插入的结果，即低频与高频分开；create_table user(姓名 varchar（20），昵称 varchar（20）)create_table message(姓名 varchar（20，时间datetime，message text)insert into message values（‘李晓华’，now（），‘xxxxx！’） ：把用户发的新消息插入表中；select *from message where 姓名=‘李晓华’ ：查询李晓华单所有消息；select max（time）from user_table :找出使用时长最长的用户； select是精髓，其执行速度直接关系到用户体验； 数据库索引 数据库索引：排序（对数据库里的某个属性的所有值排序以加快查询速度）+二分法（取大取小再取中点，重复） 1.聚簇索引 按数据的物理存储顺序建立索引； 缺点：会改变表的存储结构和数据排序；每张表只能建一个聚簇索引； 2.非聚簇索引 开辟一块存储区域保存索引，不改变表的存储结构和顺序；每张表可建多个聚簇索引； 缺点：索引消耗硬盘存储空间； ORM（一种轮子） 能帮你把对象直接存入数据库，也能直接从数据库读取对象的工具；（对象-SQL语句-对象） 调API 数据的来源 上报数据的流程：埋点——上报——后台记录日志——计算／入库——展示 埋点：在正常功能逻辑里加统计逻辑；上报数据可采用key-value或数据组合的形式；key-value：key-区分不同事件，value-事件发生的次数／状态值；数据组合：描述一个事件或状态需要多种属性描述的场景； eg.数据组合：描述【下载成功】这个事件的数据组合包括：对应的下载地址，下载渠道／来源，耗时等。 上报：不是每统计一次事件或状态就上报，客户端统计到的数据会暂时保存在内存或磁盘上，当用户启动／退出应用时或其他合适时机，将当前周期统计到的事件批量上报到服务器（减少性能损耗和流量损耗） 后台记录日志：数据上报到服务器后，服务器将原始数据存到服务器磁盘里；非强实时性的数据不会立即参与计算，而是等服务器负载较低的时间段利用预先配置的计划任务进行离线处理（节约服务器资源／钱／不影响实时业务的处理效率）； 计算／入库：对海量数据的计算需用到数据仓库（eg.hive处理工具），当数据仓库工具计算出最终结果后，计划任务会将最终结果保存到数据库里（入库）； 展示：数据入库后报表系统通过前段页面用户的输入获取查询条件——通过后台数据查询获取结果——在前端展示； 数据统计统计数据出问的原因和方案？1.报表数据为0 在验证客户端埋点功能正常后，可能计算&amp;入库 和展示环节（与埋点功能相关，需根据不同的统计需求修改）出问题 solution：确认功能正常——联系后台负责数据统计的人员跟进，恢复异常数据；发现功能不正常导致数据没有上报——等下个版本修复 或 热补丁； 2.报表数据大面积突降 可能是日志分析系统有策略调整或日志迁移导致只收录了部分数据 solution：找运维确认 3.报表数据突增 可能是恶意刷量 solution：找数据组查原始日志，看是否有个别IP或用户ID对应的PV数量明显异常； 4.报表数据明显低于预期 埋点功能出问题，漏报数据 solution：找负责埋点的同学 5.业务流复杂的漏斗系统统计数据异常 solution：逐级找相关人员确认数据 @pm要谨记 及时关注重要数据，有问题及时发现反馈； 影响埋点的因素考虑清楚，如果数据异常是否原因可追溯，若有需要可多设置几个辅助埋点； 除自己熟悉的业务，也要了解外围业务； 渠道号是怎么统计的 需要针对不同的渠道包在同一个逻辑里上报不同的数据 如何实现：在apk里添加配置文件，注明当前apk对应的渠道号；当客户端启动时，首先从配置文件中读取渠道号信息，然后进行上报操作，即可实现用相同的代码来上报不同渠道号信息； 针对每个需要统计的分发渠道，要提供单独的渠道包，且配置文件不同； 可自动化生成； 无埋点统计系统（eg Growing10） 下载sdk并植入，超方便！]]></content>
      <categories>
        <category>Product Manager</category>
        <category>Technical Logic &amp; Knowledge</category>
      </categories>
      <tags>
        <tag>PM Communication</tag>
        <tag>Technical Logic &amp; Knowledge</tag>
        <tag>Reading Notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Interpret Technique to PM|Chapter1:Communication]]></title>
    <url>%2F2017%2F10%2F01%2FInterpret-Technique-to-PM-Chapter1-Communication%2F</url>
    <content type="text"><![CDATA[Abstract:基础篇，关于PM怎样愉快地跟程序员沟通。哪些话会惹恼程序员？提需求的正确姿势是什么？ 沟通 产品本质是获取信息（程序猿角度），信息-数据；前端：展示数据，后台：管理数据（守着服务器鼓捣数据）； 不要重复造轮子：别人写过的代码不要重复写，可以直接搬运； 全栈：前端后台样样精通； 后台 分布式：为了让各个服务器并行工作，故研究分布式算法，把大任务拆成小任务，分布给哥哥服务器单独运算； 非关系型数据库NoSQL：提高数据库的存取速度； 缓存技术：解决硬盘速度远远跟不上内存速度的问题，即数据从硬盘取出来就不放回去了； 哪些话会惹恼程序员？ 先做出来看看：随时可能被删除的需求，删代码等于割肉；（好的pm把需求的关键路径在脑海里演练上百次） 我就要这种效果，怎么实现是你的问题：有的创意功能实在不好实现／根本无法实现／不合适实现；（功能设计时要考虑：用户需求／商业模式／设计和开发实现的可能性） 不要教育程序员怎么写代码； 你就说能不能做吧：不要粗暴直接问程序员做决定；pm大可不必跟开发探讨技术细节，就讲道理／讲数据／大局观，迂回取胜。 我有一个绝妙的 idea，什么都准备好了，就差一个写代码的了：idea谁都能想，不要在言语里露出对程序员的轻视； 这个需求老大已经同意了，你照着做就是了：不要拿上级压人，易遭致反感；最好以理服人，充分pk／摩擦产生高效输出； 提需求的正确姿势是什么 往往一些看似很简单的需求，实际上会遇到很多坑：比如一个很简单的按钮的实现可能需要千行代码；（好的pm需要从用户角度想清楚一个功能的前后逻辑） 举例：实现「视频播放的时候，用户可以设置屏幕亮度」的功能 问题：调用系统提供的「设置屏幕亮度」的程序接口 存在问题： 1.如果用户在我的APP里提高了屏幕亮度，退出之后要不要给人家还原呢？如果用户只是暂时离开了我的APP，退出又回来，我是不是要给人恢复成原来设置的亮度呢？ 2.「设置屏幕亮度」的接口是一个很耗时的接口，可能会造成整个APP的卡顿，这时候你就得考虑用多线程来解决。引入多线程之后，线程之间的资源共享问题如何解决，谁先谁后的问题如何解决，等等… 程序员写代码：完成功能，代码规范，扩展性，后期的完全负责； 知道有哪些轮子： 开源社区：平时多看一些别人整理的技术博客，你可能并不需要知道里面技术上是如何实现的，你只需要记下，这个功能是有轮子可以用的，就够了。自己的项目：想知道自己项目积累了哪些轮子，去问你们的开发吧，找他们抽支烟、吃个饭，很容易就套出来了。系统平台：买一本介绍你们产品平台的技术书，比如《疯狂Android讲义》、《iOS Programming》，大体翻一下就行了，主要是了解一下这个平台到底可以做哪些事情。 提需求要跟着项目的版本周期走：每个版本会经过功能开发（有提必应）、单元测试（一个功能模块的需求做完后给测试找bug的时段，大多数有提必应）、集成测试、beta验证（提需求拉仇恨，需要评估需求的实现难度）、上线五个阶段 程序员需要什么样的PRD？ 程序员不需要文字式的需求文档，只要清晰的流程图和原型即可。 以程序猿的逻辑去沟通产品逻辑：状态机（万字不如一图） 状态机：描述事物不同状态切换的逻辑方式； 要素：现态（描述事物当前的状态）+ 次态（现态在一定条件下出发相应动作后能达到的状态）+ 条件（执行动作的前提）+ 动作（条件满足时触发状态机状态改变） 核心：各个可相互切换的状态+出入动作 tip：要为状态定义入口动作/出口动作要详细设计每一个状态，列出层次化状态结构针对具体问题，严格限制状态跳转的请求同一层次的状态数量不宜超过5个记录每一次的跳转历史，方便调试可能的话，最好使用编辑器来编辑状态机 1.有限状态机FSM：只有一个层级的状态切换 2.无限（层次）状态机HFSM：存在多个层级的状态切换（状态嵌套） 状态内的状态是不需要关心外部状态的跳转的，这样也做到了无关状态间的隔离； 为什么项目会延期？ 关于需求：pm在初期需明确每个需求的关键路径和效果预期；存在风险的需求需提前预估风险；复杂需求需与开发评估实现成本；大需求最好化成小需求，方便进度跟进和风险响应； 排期与风险控制：排期取决于交付周期+需求量+所需人力；大项目前期可采取周会+后期或短周期项目可采取晨会；延期风险：敢于做减法，分清主次需求；加班； 沟通沟通！]]></content>
      <categories>
        <category>Product Manager</category>
        <category>Technical Logic &amp; Knowledge</category>
      </categories>
      <tags>
        <tag>PM Communication</tag>
        <tag>Technical Logic &amp; Knowledge</tag>
        <tag>Reading Notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Coursera Machine Learning|Week5:BackPropagation Algorithm]]></title>
    <url>%2F2017%2F09%2F29%2FCoursera-Machine-Learning-Week5-BackPropagation-Algorithm%2F</url>
    <content type="text"><![CDATA[Abstract：本文讲在给定训练集下为神经网络拟合参数的学习算法。反向传播算法——让代价函数J(theta)最小化的算法。即，我们从输出层开始计算δ项，然后我们返回到上一层计算第三隐藏层的δ项，接着我们再往前一步来计算δ(2)。类似于把输出层的误差反向传播给了第3层，然后再传到第二层。这就是反向传播的意思。 反向传播其实就是计算所有δ项， δ是每层的激励值的误差项，δ = 这个单元的激励值 - 训练样本里的真实值。反向传播和前向传播很相似，只不过计算方向不同。反向传播的计算结果其实是δ值的加权和。权值是这些对应边的强度。 实现将参数从矩阵展开成向量，以便我们在高级最优化步骤中的使用需要。矩阵向量化：thetaVec；向量矩阵化：reshape命令。 而当使用反向传播时，易遇到很多细小的错误，梯度检验方法会帮助确定实现的向前传播和反向传播或者其他的什么算法是100%正确的。梯度检验的原理是双侧差分法求导数。随机初始化theta。以上方法都是训练神经网络的过程中需要用到的。本文后半段详细介绍了训练神经网络的步骤和方法。 代价函数Abstract：本节讲拟合神经网络的代价函数。 如上图，以神经网络在分类问题中的应用为例。假设我们有上图所示的神经网络结构。 训练集：(x(1),y(1)),(x(2),y(2)),…,(x(m),y(m))，共m个训练样本(x(i),y(i)) L：神经网络结构的总层数（total no. of layers in network） 如上图的L = 4 Sl：第l层的神经元的数量（no. of units(not counting bias unit) in layer l），不包括偏置单元 如：S1 = 3（输入层）；S2 = 5，S3 = 5（隐藏层）；S4 = SL = 4（L = 4） 接下来讨论两种分类问题：二元分类和多元分类。 二元分类(Binary classification)二元分类中的 y = 0 or 1； 1个输出单元，神经网络的输出会是一个实数；hΘ(x) ∈ ℝ； 输出单元个数：SL = 1（即第L层即输出层的神经元个数为1）； K = 输出层的单元数目。二元分类中K = 1。 多类别分类（Multi-class classification） 多类别分类问题，会有K个不同的类，即K个输出单元。 我们的输出假设就是个K维向量：hΘ(x) ∈ ℝ^K ;输出单元数就是K：SL = K； 如上图4个四维向量。 注：通常这类多元分类问题，K &gt;= 3。因为若K = 2，直接使用二元分类法即可。 定义代价函数Abstract：神经网络里用的代价函数是逻辑回归里的代价函数的一般形式。我们要把其放到神经网络里去理解，注意求J(theta)时需要除去正则化项，以避免重复计算。 我们在神经网络里，使用的代价函数，应该是逻辑回归里使用的代价函数的一般形式。 逻辑回归的代价函数： 注：其中λ2m∑nj=1θ2jλ这一项是个额外的正则化项，是一个jj从1到nn的求和形式。因为我们并没有把偏置项0正则化。 而对于神经网络，我们使用的代价函数是上式的一般形式： 神经网络输出的K维向量为hΘ(x)：hΘ(x) ∈ ℝ^K ;(hΘ(x))i表示第i个输出 ∑Kk=1：为求和项，对K个输出单元求和；如我们有4个输出单元在神经网络的最后一层，则这个求和项为K从1到4所对应的每一个逻辑回归算法的代价函数之和。 eg.hΘ(x) = [0,0,0,1]T ,则求和项的结果为0+0+0+1 = 1，表示多元分类后的最终结果； 式子的最后一项λ/2m….：为一个求和项，类似逻辑回归中用到的正则化项；它的作用在于把这些项全部加起来，即对所有的Θji(l)的值都相加，目的是除去那些对应于偏差值的项Θj0(l)，因为我们在计算神经元的激励值时已经把这些项（类似于偏置单元的项）计算进去了。类比于我们在做逻辑回归的时候，我们就不应该把这些项加入到正规化项里去，因为我们并不想正规化这些项，并把这些项设定为0。 反向传播（B-P(Backpropagation)算法）Abstract：反向传播算法——让代价函数J(theta)最小化的算法。即，我们从输出层开始计算δ项，然后我们返回到上一层计算第三隐藏层的δ项，接着我们再往前一步来计算δ(2)。类似于把输出层的误差反向传播给了第3层，然后再传到第二层。这就是反向传播的意思。 上图是上一节写好的代价函数。 目的：找到使代价函数J(Θ)最小的Θ值 方法：梯度下降计算法。 操作：写好一个通过参数Θ，然后计算： 下面将详解梯度下降计算。 梯度下降计算 情况1：只有一个训练样本。情况：假设我们整个训练集只包含一个训练样本：(x,y) 计算方法： 1.用向前传播方法计算：在给定输入时，假设函数的输出结果： 2.通过上面的层层推导，我们即可得出假设函数的输出结果。 注：a(i)表示第i层经过函数计算后输出的结果。若该神经网络共K层，则h(Θ)X = a(k). 3.然后，为了计算导数项，我们采用反向传播算法。 反向传播算法：直观上理解，就是对每一个节点求下面这一个误差项： δj(l)：代表了第l层的第j个结点的激励值的误差；而aj(l)表示第l层的第j个结点的激励值。 接下来，我们用四层的神经网络结构做例子 。 计算公式：每一项的输出单元(layer L = 4) —— δj(4) = aj(4) − yj 理解上式： 对每一个输出单元，计算δ项，则第4层的第j个单元的δ = 这个单元的激励值 - 训练样本里的真实值。此即为误差求法。故aj(4)亦可写成hΘ(x)j：δj(4) = hΘ(x)j−yj。 若把δ、a、y这三项都看作向量的话，那么上面的式子你也可以写出向量化的实现：δ(4) = a(4)−y。这里的δ(4)、a(4)和y都是一个向量，并且向量维数等于输出单元的数目。 计算过程（从后往前计算每层的误差项δ）： 1.由δj(4) = aj(4) − yj计算出δ(4) 2.由δ(3) = (Θ(3))Tδ(4).∗g′(z(3))计算出δ(3) 这里的点乘.∗.∗是我们从MATLAB里知道的对y元素的乘法操作，指的是两个向量中元素间对应相乘。其中g′(z(3))g′(z(3))这一项其实是对激励函数gg在输入值为z(3)z(3)的时候所求的导数。g′(z(3)) = a(3).∗(1−a(3))，其中1是元素都为1的向量。 3.同理由δ(2)=(Θ(2))Tδ(3).∗g′(z(2))求出δ(2) 注：这里我们没有δ(1)项，因为第一层是输入层，不存在误差。所以这个例子中，我们的δ项就只有第2层和第3层。 反向传播法的方法阐释：我们从输出层开始计算δ项，然后我们返回到上一层计算第三隐藏层的δ项，接着我们再往前一步来计算δ(2)。类似于把输出层的误差反向传播给了第3层，然后再传到第二层。这就是反向传播的意思。 情况2：当我们有一个很大的训练样本情况：假设我们有m个样本的训练集: Training set (x(1),y(1)),…,(x(m),y(m)) 计算过程： 略 反向传播算法的直观介绍神经网络计算过程 如上图神经网络，包含2个输入单元（不包括偏差单元）。在第2、3层分别有2个隐藏单元（不包括偏差单元），最后的输出层有1个输出单元。 前向传播 如上图展示了这个神经网络的前向传播的运算过程。而反向传播算法的运算过程非常类似于此，只有计算的方向不同而已。 代价函数为了更好的理解反向传播算法的原理，我们把目光转向代价函数： 上面的代价函数只有一个输出单元。若有不止一个输出单元，则我们需要对所有输出单元进行求和运算。 而且，在只有一个输出单元时，若不考虑正则化即λ=0。所以后面的正则化项也没有了。 这个求和运算括号里面与第i个训练样本对应的代价项，也就是说(x(i),y(i))对应的代价项，将有下面这个式子决定： 而这个代价函数所扮演的角色可以看做是平方误差，当然，如果你愿意，你可以把cost(i)想象成： 故，这里的cost(i)表征了该神经网络是否能准确地预测样本i的值，也就是输出值，和实际观测值y(i)y(i)的接近程度。 反向传播直观理解：反向传播算法就是在计算所有这些δ项：δ(l)j=“error” of cost for a(l)j (unit j in layer l). 可以把它们看作是这些激励值的“误差”(注意这些激励值是第l层中的第j项)。 更正式一点的说法是δ项实际上是关于zj(l)的偏微分，也就是cost函数关于我们计算出的输入项的加权和，也就是z项的偏微分。 如果我们观察该神经网络内部的话，把这些z(l)jzj(l)项稍微改一点点，那就将影响到神经网络的输出，并且最终会改变代价函数的值。 因此，它们度量着我们对神经网络的权值做多少的改变，对中间的计算量影响是多少，进一步对整个神经网络的输出h(x)h(x)影响多少，以及对整个的代价影响多少。 我们再深入一点，研究一下反向传播的过程，对于输入层，如果我们设置δδ项，假设我们进行第i个训练样本，那么：δ1(4)=y(i)−a1(4) 接下来我们要对这些值进行反向传播，算出δ1(3)、δ2(3)，然后同样的再进行下一层的反向传播，算出δ1(2)、δ2(2)。 接下来，我们来看看如何计算δ2(2)： ![13]http://studyai.site/img/16_09_18/017.png) 实际上，我们要做的是我们要用下一层的δ值和权值相乘，然后加上另一个δ值和权值相乘的结果。也就是说，它其实是δ值的加权和。权值是这些对应边的强度。 计算过程：δ2(2) = Θ12(2)δ1(3) + Θ22(2)δ2(3) 同理，若计算δ2(3): δ2(3) = Θ12(3)*δ1(4). 注：本节中的δ值仅仅是隐藏层中的没有包括偏差单元:”+1”的。包不包括偏差单元取决于你如何实现这个反向传播算法，你也可以对这些偏差单元计算δ的值，这些偏差单元总是取为”+1”的值。通常来说，我在执行反向传播的时候，我是算出了这些偏差单元的δ值，但我通常忽略掉它们，而不是把它们带入计算，因为它们其实并不是计算那些微积分的必要部分， BP算法练习将参数从矩阵展开成向量Abstract：实现将参数从矩阵展开成向量，以便我们在高级最优化步骤中的使用需要。矩阵向量化：thetaVec；向量矩阵化：reshape命令。 而当使用反向传播时，易遇到很多细小的错误，梯度检验方法会帮助确定实现的向前传播和反向传播或者其他的什么算法是100%正确的。梯度检验的原理是双侧差分法求导数。随机初始化theta。以上方法都是训练神经网络的过程中需要用到的。 目的：实现将参数从矩阵展开成向量，以便我们在高级最优化步骤中的使用需要。 function[jVal, gradient] = costFunction(theta) ... optTheta = fminunc(@costFucntion, initialTheta, options) 如上代码段，对代价函数costFunction(theta)传入参数theta，函数返回值是jVal和导数值gradient，然后将返回值再传递给高级最优化算法fminunc . 其中costFunction中的参数theta、返回值gradient和fiminunc的参数initialTheta都是一个R^(n+1)阶的向量 . 注：fiminunc不是唯一算法，可使用他法。 但，对神经网络，我们的参数将不再是向量，而是矩阵。 以一个四层完整的神经网络为例： 其参数theta所代表的参数矩阵为矩阵Θ(1) ,Θ(2),Θ(3)，在Octave中，我们可以设为Theta1,Theta2,Theta3。 类似的，这些梯度项gradient也是costFunction的返回值之一。这些梯度矩阵的计算结果是D(1),D(2),D(3)，在Octave中用D1,D2,D3来表示。 下面将介绍，如何取出这些矩阵并将它们展开成向量，以便他们最终成为恰当的格式以传入initialTheta里，并得到正确的梯度返回值gradient。 阶段1：抽象解释算法具体来说，假设我们有这样一个神经网络： 上图神经网络：10个输入单元(s1=10)，10个隐藏单元(s2=10)，1个输出单元(s3=1) 矩阵Θ的维度和矩阵D的维度将由这个神经网络的结构所决定，如：Θ(1)是一个10×11的矩阵。 因此，在octave中，矩阵向量化的步骤： 1.取出Θ(1) ,Θ(2),Θ(3)；即用下面这段代码，取出三个Θ矩阵中的所有元素，然后把他们全部展开，成为一个很长的向量，也就是thetaVec。同理，取出D矩阵的所有元素，然后展开成一个长向量DVec（：把所有元素集中到一列）。 thetaVec = [Theta1(:);Theta2(:);Theta3(:)]; AND DVec = [D1(:);D2(:);D3(:)]; thetaVec：将参数向量化，即矩阵向量化。构成为xx+Vec，Vec是向量化的意思，Vec前面的代表要被向量化的对象。 2.[返回路径]如果想从向量表达式返回到矩阵表达式，就使用reshape函数，传入向量区间和矩阵的行数和列数，即可得到对应的矩阵。 Theta1 = reshape(thetaVec(1:110),10,11); Theta2 = reshape(thetaVec(111:220),10,11); Theta3 = reshape(thetaVec(221:231),10,11); 注：reshape(thetaVec(start:end), #row, #column) 阶段2：Octave展示算法Octave展示上述计算过程： 1.假设Theta1是一个10X11的单位矩阵： &gt;&gt; Theta1 = 1*ones(10,11); &gt;&gt; Theta1 Theta1 = 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 Theta2是一个元素都为2的10X11矩阵： &gt;&gt; Theta2 = 2*ones(10,11) &gt;&gt; Theta2 Theta2 = 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 假设Theta3是一个1X11的元素为3的矩阵： &gt;&gt; Theta3 = 3*ones(1,11) &gt;&gt; Theta3 Theta3 = 3 3 3 3 3 3 3 3 3 3 3 2.把上面三个矩阵变成一个长向量thetaVec： &gt;&gt; thetaVec = [Theta1(:);Theta2(:);Theta3(:)]; &gt;&gt; thetaVec thetaVec = 1 1 1 1 1 1 1 1 1 1 1 1 ... thetaVec是一个231X1的向量，包含所有矩阵的元素： &gt;&gt; size(thetaVec) ans = 231 1 若想返回原来的3个矩阵，则可对thetaVec用resharp命令。 比如，我们可以抽出前110个元素，来重组一个10×1110×11的矩阵，即Theta1： &gt;&gt; reshape(thetaVec(1:110),10,11) ans = 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 阶段3：将算法应用到学习算法首先，假设我们有一些初始参数值：Θ(1) ，Θ(2)，Θ(3)； 目的： 1.取出这些参数并将他们展开成一个长向量作为initialTheta，代入fiminunc函数; 2.执行代价函数@costFunction，实现算法如下。 1：fiminunc(@costFunction, initialTheta, options) 2：function [jval, gradientVec] = costFunction(thetaVec) 步骤： 1.通过已知的向量thetaVec使用重组函数reshape得到Θ(1)，)Θ(2),Θ(3)； 2.执行向前传播和反向传播来计算出导数D(1)，D(2),D(3)和代价函数J(Θ)； 3.取出这些导数值，并让它们保存和我展开的ΘΘ值相同的顺序来展开它们(按照D(1)，D(2),D(3)的顺序)，得到gradientVec，这个值由我的代价函数返回，它可以以一个向量的形式返回这些导数值。 两种表达式各自的优点： 使用矩阵表达式的好处：当你的参数以矩阵的形式存储时，你在进行正向传播和反向传播时，你会觉得更加方便。当你将参数存储为矩阵时，一大好处是充分利用了向量化的实现过程。 向量表达式的优点：如果你有像thetaVec或者DVec这样的矩阵，当你使用一些高级的优化算法时，这些算法通常要求你所有的参数都展开成一个长向量的形式。 梯度检验（Gradient Checking）为什么要梯度检验： 当使用反向传播时，易遇到很多细小的错误。梯度检验方法会帮助确定你实现的向前传播和反向传播或者其他的什么算法是100%正确的。 梯度检验原理 如上图，假设有一个假设函数J(theta)，若想估计在某一点上的导数值，则此导数 = 其切线的斜率。用双侧差分法计算近似导数： 1.找到θ+ε和θ−ε这两个点，用一条直线把这两点连起来； 2.用两点的连线作导数近似值，即：(∂/∂θ)*J(θ) ≈ [J(θ+ε)−J(θ−ε)]/2ε; 通常给ε取很小的值，如ε=10^−4; Octave中实现梯度检验gradApprox = (J(theta + EPSILON) - J(theta - EPSILON))/(2*EPSILON) 梯度检验在octave中的实现要用上面的代码。你的程序要调用gradApprox来计算这个函数。这个函数会通过这个公式：[J(θ+ε)−J(θ−ε)]/2ε，它会给出这点导数的数值估计。 当θ是向量时在前面的例子中，θ是实数。接下来我们来讨论一种更普遍的情况：θ为向量参数。 假设θ是一个n维向量（它可能是我们的神经网络参数Θ1，Θ2，Θ3的展开形式），所以θ是一个有n个元素的向量。θ=[θ1,θ2,θ3,…,θn] 我们可以用类似的想法来估计所有偏导数项： 分别对θ向量的每个元素使用双侧差分来计算导数。 上面公式给出一个对任意参数求近似偏导数的方法。具体的说，要实现的是下面这个程序： //双侧差分法计算导数 for i = 1:n, thetaPlus = theta; thetaPlus(i) = thetaPlus(i) + EPSILON; thetaMinus = theta; thetaMinus(i) = thetaMinus(i) - EPSILON; gradApprox(i) = (J(thetaPlus) - J(thetaMinus))/(2*EPSILON); end; 我们实现神经网络时，我们用for循环来计算代价函数对每个网络中的参数的偏导数gradApprox，然后和我们从反向传播得到的导数DVec进行对比，看是否相等或近似于DVec。 如果这两种计算导数的方法给了你相同的结果，或者非常接近的结果，那么我就非常确信我实现的反向传播是正确的。然后我把这些DVec向量用在梯度下降法，或者其他高级优化算法里。 总结：如何实现梯度检验 实现反向传播来计算DVec(D(1)，D(2)，D(3)） 用gradApprox实现数值梯度检验 然后确定DVec和gradApprox给出的结果非常相近 在使用你的代码去学习训练你的网络之前，重要的是要关掉梯度检验，不在使用gradApprox这个数值导数公式（这么做的原因是，这个梯度检验的计算量非常大，它是一个非常慢的计算近似导数的方法。而相对的反向传播算法是一个在计算导数上效率更高的方法。） 如果你在每次的梯度下降法迭代时，都运行数值梯度检验，你的程序会变得非常慢，因为数值检验程序比反向传播算法要慢得多。 随机初始化θ为什么要随机初始化θ： 当你运行一个算法（例如梯度下降算法，或者其他高级优化算法）时，我们需要给变量θ一些初始值。 optTheta = fminunc(@costFunction, initialTheta, options) 梯度下降算法，我们需给定θ一些初始值，接下来使用梯度下降方法慢慢地执行这些步骤使其下降，使J(θ)下降到最小。 是否可将θ的初始值设置为0向量？Set initialTheta = zeros(n,1) 在逻辑回归时，初始化所有变量为0是可行的。但在训练神经网络时，这样做是不行的。 以训练下面的神经网络为例： 若将所有变量初始化为0：Θij(l) = 0 for all i,j,l. 当初始化下面这些颜色两两相同的权重时，这些权重都被赋予相同的初始值0： 则经过计算后，两个隐藏单元a1，a2的值是相同的：a1(2) = a2(2); 由于权重相同，亦可证明：δ1(2) = δ2(2)； 进一步推导，以这两条红色权重为例，代价函数的关于这两个权重的偏导数是相等的：(∂/∂Θ01(1))J(Θ) = (∂/∂Θ02(1))J(Θ) 这意味着：一旦更新梯度下降方法，第一个红色权重也会更新，等于学习率乘以这个式子:(∂/∂Θ01(1))J(Θ)；第二条红色权重更新为学习率乘以这个式子：(∂/∂Θ02(1))J(Θ)。 即使权重现在不都为0，但参数值最后也互为相等。 所以每次更新后，两个隐藏单元的输入的对应的参数将是相同的。这就意味着即使经过一次梯度下降的循环后，你会发现两个隐藏单元任然是两个完全相同的输入函数：a1(2) = a2(2)； 这也意味着，这个神经网络并不能计算出什么更有价值的东西。 想象一下，不止有两个隐藏单元，而是有很多的隐藏单元，这就是会导致所有的隐藏单元都在计算相同的特征，这是完全多余的表达，因为这意味着最后的逻辑回归单元只会得到一种特征。这样便阻止了神经网络学习出更有价值的信息。 随机初始化Θ引入目的：为了解决这个神经网络变量初始化的问题，我们采用随机初始化的方法。 具体分析：上面的问题根本在于所有权重相同的问题，即对称权重。故随机初始化解决的是如何打破这种对称性(Symmetry breaking)。 我们需要做的是对Θ(l)ijΘi的每个值进行初始化，范围在[−ε,ε][−ε,ε]之间（−ε≤Θij(l)≤ε） 在Octave中初始化Θ： Theta1 = rand(10,11)*(2*INIT_EPSILON) - INIT_EPSILON; Theta2 = rand(1,11)*(2*INIT_EPSILON) - INIT_EPSILON; 其中rand(10,11)代表一个10*11的随机矩阵，这个rand()函数就是用来得到一个任意的随机矩阵方法，并且所有的值都是介于0到1的实数。 若取0到1之间的一个数和2ε相乘再减去ε，然后得到的结果就是一个在[−ε,ε]之间的数。 总结为训练神经网络，应对权重进行随机初始化为[−ε,ε]之间的值。ε是接近于0的小数，然后进行反向传播，执行梯度检查，使用梯度下降或者高级的优化算法，试着使代价函数J(Θ)J(Θ)达到最小，从某个随机选取的参数ΘΘ开始。通过打破对称性的过程，我们希望梯度下降或者其他高级优化算法可以找到Θ的最优值。 神经网络总体回顾：算法合体Abstract：对神经网络的所有内容进行一个整体回顾，看看这些零散的内容相互之间有怎样的联系，以及神经网络学习算法的总体实现过程。 第一步，选择一个合适的神经网络结构搭建网络的大体框架，框架指神经元之间的链接模式。可能会从以下几种结构中选择： 第一种网络结构包含三个输入单元、五个隐藏单元、和四个输出单元。 第二种包含三个输入单元，两组五个隐藏单元作为隐藏层，四个输出单元。 第三种组合是三个输入单元，三组五个隐藏单元作为隐藏层，四个输出单元。 上面就是可能选择的结构，即每一层可选择多少个隐藏单元，多少个隐藏层。这些都是构建框架时的选择。 1.输入单元数量已定义：确定了特征集x(i)对应的输入单元数目 = 确定了特征x(i)的维度 = 确定输入单元的数目 2.输出单元数目：多类别分类中，输出层的单元数目将会由分类问题中所要区分的类别个数确定 若多类别分类问题中，若y的取值范围在1，2，…，10之间，则有10个可能的分类，记得要将y写成向量形式，如：y = [1,0,0,…,0] 若要表示5个分类，即y = 5，则在你的神经网络中，不能直接用5来表达，而是用10个输出单元来表示，向量为：y = [0,0,0,0,1,0,0,0,0,0] 3.隐藏层层数：默认只使用单个隐藏层；若使用超过一层，则默认每个隐藏层都拥有相同单元数。所以后面的两种神经网络结构的隐藏层都拥有相同的单元数： 4.隐藏单元数目：越多越好。不过，需要注意的是，如果有大量的隐藏单元，计算量一般会比较大。并且，一般来说，每个隐藏层所包含的单元数量还应该和输入x的维度相匹配，也要和特征的数目相匹配。可能隐藏单元的数目和输入特征的数量相同，或者是它的二倍或者三倍、四倍。因此，隐藏单元的数目需要和其他参数相匹配。一般来说隐藏单元的数目取稍大于输入特征数目都是可以接受的。 训练神经网络的步骤1.构建一个神经网络并随机初始化权值（Randomly initialize Weight）；通常把权值初始化为很小的值，接近于0； 2.执行向前传播算法，也就是对于神经网络的任意一个输入x(i)计算出对应的hΘ(x(i))； 3.通过代码计算出代价函数J(Θ)J(Θ) 4.执行反向传播算法(Backprop)来算出这些偏导数：(∂/∂Θjk(l))J(Θ) 具体来说，我们要对所有训练集数据使用一个for循环进行遍历每一个样本（实际上有更复杂的方式来替代for循环来实现，但对于第一次实现神经网络的训练过程，不建议使用for循环以为的方式，因为这种方式更有助于第一次使用时的理解） for i = 1:m 5.使用梯度检查来校验结果。用梯度检查来比较这些已经用反向传播算法得到的偏导数值(∂/∂Θjk(l))J(Θ)与用数值方法得到的估计值进行比较，来检查，确保这两种方法得到值是基本相近的。 通过梯度检查，我们能确保我们的反向传播算法得到的结果是正确的，但必须要说明的一点是，检查结束后我们需要去掉梯度检查的代码，因为梯度检查计算非常慢。 6.使用一个最优化算法（比如说梯度下降算法或者其他更加高级的优化方法，比如说BFGS算法，共轭梯度法，或者其他一些已经内置到fminunc函数中的方法），将所有这些优化方法和反向传播算法相结合，这样我们就能计算出这些偏导数项的值(∂/∂Θjk(l))J(Θ)。 到现在，我们已经知道了如何计算代价函数J(Θ)J(Θ)，我们知道了如何使用反向传播算法来计算偏导数(∂/∂Θjk(l))J(Θ)，那么我们就能使用某个最优化方法来最小化J(Θ)关于Θ的函数值。 BTW: 对于神经网络代价函数J(Θ)是一个非凸函数，因此理论上是能够停留在局部最小值的位置。实际上，梯度下降算法和其他一些高级优化方法理论上都能收敛于局部最小值，但一般来讲这个问题其实并不是什么要紧的事，尽管我们不能保证这些优化算法一定会得到全局最优值，但通常来讲，像梯度下降这类的算法在最小化代价函数J(Θ)的过程中，还是表现的很不错的，通常能够得到一个很小的局部最小值，尽管这可能不一定是全局最优值。 梯度下降法在神经网络中的直观理解 假设这个神经网络中只有两个参数值：Θ12(1),Θ11(1),则代价函数J(Θ)度量的就是这个神经网络对训练数据的拟合情况。 所以，如果你取某个参数，比如说在这样一个局部最优值： 这一点的位置所对应的参数Θ的情况是对于大部分的训练数据，我的假设函数的输出会非常接近于y(i): hΘ(x(i)) ≈ y(i) 若是这样，代价函数J(Θ)值就会很小。 而反过来，如果我们取这个值, 我们的代价函数J(Θ)J(Θ)值就会很大。 梯度下降原理：我们从某个随机的初始点开始，它将会不停地下降，那么反向传播算法的目的就是算出梯度下降的方向，而梯度下降的过程就是沿着这个方法一点点地下降，一直到我们希望得到的点，这一点即我们希望找到的局部最优点。 神经网络实现自动驾驶 如图： 左下方：汽车看到的前方路况图像，是汽车前方摄像头每2秒采集且压缩处理后得到的一张30*32图像。从中可看见道路。 左上方：第一个水平进度条显示驾驶员所选方向，第二个显示学习算法所选方向。含义为：白色区段偏左表示向左转；反之向右转。 实际上神经网络在开始学习之前，你会看到网络的输出是一条灰色的区段，覆盖着整个区域，只有在学习算法运行足够长的时间之后，亮白色的区段才能逐渐显现。]]></content>
      <categories>
        <category>Artificial Intelligence</category>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Artificial Intelligence</tag>
        <tag>Machine Learning</tag>
        <tag>Coursera ML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[新媒体掐架定律25条]]></title>
    <url>%2F2017%2F09%2F29%2F%E6%96%B0%E5%AA%92%E4%BD%93%E6%8E%90%E6%9E%B6%E5%AE%9A%E5%BE%8B25%E6%9D%A1%2F</url>
    <content type="text"><![CDATA[Abstract：最近在新媒体界很火的“新媒体掐架定律 ”，由南大新闻院院长杜骏飞老师提出。这25条定律从新媒体掐架的起端、过程、结果和掐架双方、掐架话题等维度，清晰阐释了掐架事件背后的一些普适性的定律。 1.分歧倍增定律：若双方观点有分歧，那么，每一次掐架之后，分歧都将倍增 2.镜像定律：那个喜欢与你掐架、且你也喜欢与他掐架的人/人群，通常与你颇为相似 类比加多宝与王老吉之争，二者都是凉茶品牌，争执的结果是愈演愈烈直至对簿公堂，且这一番争执拉锯战也使二者的知名度极大提升，双方在品牌知名度上均受益。 3.排斥第三方定律：掐架只会有正反方，若出现任何第三方，那么他将会同时被正反方误以为是是对方 4.题材因果定律：若掐架的题材是老题材，那么掐架的结果仍会是老结果5.题材循环定律：如果掐架的题材是新题材，那么，掐架将使它回到老题材。 6.心境定律：如果你看见掐架，说明你需要掐架；如果你经常看见掐架，说明你喜欢掐架。 7.心境定律逆定律：如果你看不见掐架，那就没有掐架。 10.分贝定律：掐架双方中分贝更高的一方，通常更为不自信。 11.活跃度定律：对于一个给定的主题T，掐架中发言最为活跃的参与者，通常对T的发言权最少。 12.分岔定律：对于一个给定的主题T，掐架都将会尽可能快的导向一系列与T无关的讨论。 13.耗散定律：一场掐架的耗散率，决定于双方的体力而非脑力消耗的进程。 14.热力学第九定律：观众的热度与掐架的深刻程度成反比，与掐架的新鲜度成正比。 15.Narcissus（水仙）定律：相较于真理，掐架者无疑更喜欢自己。 16.Echo（回声）定律：掐架者通常不能正常说出想说的话，而只能不断的重复别人的言语——包括对手的错的言语。 17.Moebius（莫比乌斯）定律：越有能力掐架者，越不愿意掐架；越不愿意掐架，就越没有能力掐架。 18.测不准定律：对于大多数的掐架，我们无法测量其讨论文本的价值。 19.测不准第二定律：对于大多数可以测量的掐架文本，我们无法测量文本作者的真实立场。 20.测不准第三定律：对于大多数可以测量其立场的掐架者，我们无法测量他在第二天的态度转变。 21.羊群定律：一场掐架中，观众最终所采取的立场通常会与他所认为的大多数保持一致。 22.人数定律：一般来说，掐架人数越多，掐架越没有结果。 23.切贝雪夫大数定律：但当掐架人数成为成为一个大数时，掐架将很快分出结果，但是其结果只匹配常人心智，而非创见或真知。 24.修辞定律：如果争论演化为掐架，那么其原因一定不在观点，而在修辞。 25.军规定律：如果你发生在线掐架，而非理性的讨论，那或许不是语言能力不足，而是大部分的在线讨论根本不适合于任何人的理性表达。]]></content>
      <categories>
        <category>Communication Science</category>
        <category>Advertising and Marketing</category>
      </categories>
      <tags>
        <tag>Communication Science</tag>
        <tag>Advertising and Marketing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Coursera Machine Learning|Week4：Neural Networks]]></title>
    <url>%2F2017%2F09%2F27%2FCoursera-Machine-Learning-Week4%EF%BC%9ANeural-Networks%2F</url>
    <content type="text"><![CDATA[Abstract：神经网络产生的原因是人们想尝试设计出模仿大脑的算法。 Neural Networks import线性回归对于非线性假设的不可行性分析 对上面的监督学习分类问题，若用逻辑回归算法解决，则需要首先构造一个包含很多非线性项的logstic regression function :g(θ0+θ1x1+θ2x2+θ3x1x2+θ4x21x2+θ5x31x2+θ6x1x22+…) ;当项数足够多时可得到一个分开正样本和负样本的分界线。 问题在于，当只有2项时，逻辑回归的确能得到不错的效果，因为x1，x2的所有组合都可被包含到多项式中。但对于这种复杂的机器学习问题，涉及的项往往多于两项，可能有成千上百的特征。而当项数过多时最终结果很可能是过拟合的，且运算量会很大。而且，若还要引入三次项，则项数将以O(n^3)的量级增长。（如100个特征——约170000个三次项组合。）故当特征个数增大时，高阶多项式项数将以几何数递增，特征空间随之急剧膨胀。所以，当特征数量过多时，这种方式构建分类器并不合适。 图像识别中使用线性回归的不可行性 场景：用ML算法训练一个分类器，用于检测一个图像是否为一辆汽车。 如图，取图像一小部分，人眼看到得失汽车，计算机看到的是一个数据矩阵。矩阵中每个元素表示像素强度值，即每个像素的亮度值。即计算机识别的方法变成：根据这个像素点亮度矩阵，来告诉我们这些数值代表一个汽车门把手。 构建汽车识别器(car detection)的方法：出一个带标签的样本集，其中各类汽车和非汽车事物各一些。将样本集输入给学习算法以训练出一个分类器。训练完毕后，输入一副新图片，让分类器判定“whats this？” 为什么一定要引入非线性分类器？—— 计算成本太高 如上图，我们在每幅图片（既包括汽车又包括非汽车）上挑出一组像素点，在坐标系上标出这幅汽车的位置(pixel1,pixel2)。随着样本数量的增多，我们将发现这两类数据分布在坐标系中的不同区域。因此我们现在需要一个非线性分类器，来尽量分开这两类样本。 这个分类问题中，特征空间的维度是多少呢？假设我们使用50∗50像素的图片，虽然这个图片尺寸很小，但依然有2500个像素点。因此我们的特征数量n是2500个，特征向量XX包含了所有像素点的亮度值。如果存储的是每个像素点的灰度值（典型的计算机图片表示方法），那么每个元素值应该介于0到255之间。如果图片存储形式是RGB模式，每个像素点包含红、绿、蓝三个子像素，那么n=7500。 因此，如果我们非要通过包含所有的二次项来解决这个非线性问题，那么，这个式子的所有条件的个数大约是300万个。这样计算成本太高了。 总结：若通过逻辑回归算法做图像识别，则项数过多会导致结果过拟合以及计算成本太高的问题，故最好使用非线性分类器来解决。 Model Representation 神经网络的表示神经网络的表示1 概念 神经元：大脑中的细胞；一个计算单元，从输入神经接受一定数目的信息，并做一些计算，然后将结果通过它的轴突传送到大脑中的其他神经元 树突Dendrite：神经元的输入神经，可接收来自其他神经元的信息 轴突Axon：神经元的输出神经，可传递神经元信号给其他神经元 动作电位：神经元利用微弱的电流进行沟通 神经网络逻辑单元 我们将神经元模拟成一个逻辑单元。上图中，黄色的圆圈，你可以理解为类似神经元的东西，然后我们通过它的树突(或者说是它的输入神经)传递给它一些信息。然后神经元做一些计算，并通过它的输出神经(即它的轴突)输出计算结果。 x和θ指的是我们的参数向量。此即一个简单的模拟神经元模型。 当绘制一个神经网络时，有时会额外增加一个x0的输入节点，这个x0节点有时也被称作偏置单位(或偏置神经元)。但由于x0=1，有时是不会画出它的，这取决于它是否对例子有利。 激励函数g(z)activation function： 其中：θ为模型的参数，也是权重weight 激励activation：由一个具体神经元读入计算并输出的值。 解读神经网络 神经网络：不同神经元组合在一起的集合。 layer1：输入层，有三个输入单元：x1,x2和x3，当然我们也可以加入值为1的x0 ；layer2：隐藏层，有三个神经元：a(2)1，a(2)2和a3(2)，同理，你可以可以加上值永远为1的偏置单元a(2)0；隐藏层的值在你的训练过程中是看不到的，它的值不是x也不是y，所以我们叫它隐藏层。神经网络可以有不止一个的隐藏层。在神经网络中，任何一个非输入层且非输出层，就被称为隐藏层。layer3：输出层，输出h(x)的计算结果hθ(x)； 神经网络运行原理ai(j) 表示第j层的第i个神经元；如a1^2表示的是第2层的第1个激励，即隐藏层的第一个激励。 Θ(j) 表示层与层之间权重的参数矩阵(或者叫权重矩阵)(比如说从第一层到第二层、或者从第二层到第三层的作用)。 隐藏层的三个神经元的计算结果： 这里我们有三个输入单元和三个隐藏单元，这样一来，参数矩阵Θ(1)Θ(1)控制了我们来自三个输入单元到三个隐藏单元的映射。因此Θ(1)的维数是R^(3×4)的(考虑x0的情况下)矩阵。 如果一个神经网络在第j层有Sj个单元，在j+1层有Sj+1个单元，那么第j层的参数矩阵Θ(j)的维度就是sj+1 × (sj+1) 最后，在输出层，我们还有一个单元，它用来计算hΘ(x)： 以上就是从数学上对一个人工神经网络的定义。 神经网络的表示2Abstract：介绍如何高效地进行计算，并展示一个向量化的实现方法。更重要的是明白神经网络的好处所在，并且介绍它是如何帮助我们学习复杂的非线性假设的。神经网络使用的前向传播计算法就是从输入层到隐藏层到输出层的层层递进计算来获取最终结果的计算路径。其与逻辑回归的区别在于，逻辑回归的输入特征值为x1，x2，x3…，而前向传播的输入值为通过隐藏层的输入函数Θ(1)来学习而得的特征项a1(2)、a2(2)、a3(2) 。隐藏层内的计算是不可见的，但看起来就像逻辑回归在做的事。 最终的目的是从输出层输出一个假设函数的值h theta(x)。 前向传播(forward propagation)的向量化实现 前向传播：就是从输入层到隐藏层到输出层的层层递进计算来获取最终结果的计算路径。 从输入层的激励开始，然后进行前向传播给隐藏层，并计算隐藏层的激励，然后我们继续向前传播，并计算出层的激励，这个从输入层，到隐藏层，再到输出层，依次计算激励的过程，叫前向传播。 推导过程：我们将上图中的假设函数计算方程简化计算。先定义一些额外的项如a1(2)=…来简化表示，然后层层推导。最后h theta(x)只需2步即可推导出来。 为什么神经网络可以帮助我们学习非线性假设 盖住左边，发现神经网络很像逻辑回归。但区别是：神经网络的输入特征值是通过隐藏层计算的，即，神经网络所做的工作看起来就像逻辑回归，但它不是使用x1，x2，x3作为输入特征，而是使用特征项a1(2)、a2(2)、a3(2)。而特征项a1(2)、a2(2)、a3(2)是通过输入函数Θ(1)来学习的，即从layer1映射到layer2的函数。这个函数由其他一组参数Θ(1)决定。所以，在神经网络中，它没有用输入特征x1、x2、x3来训练逻辑回归，而是训练逻辑回归的输入:a1(2)、a2(2)、a3(2)。 神经网络的架构神经网络的架构(Architecture)：神经网络中神经元相连接的方式。 注：隐藏层可以是不止一层。 神经网络应用实例ApplicationsAbstract：本节通过具体的例子来解释神经网络是如何计算关于输入的复杂的非线性函数的。神经网络是通过层层递进的前向传播来计算复杂的非线性函数的。隐藏层数量增多，可计算更复杂的函数。 问题引入 Q：如上图，我们有一组二进制的输入特征x1和x2；x1，x2 ∈{1,0};我们的目的是用一个非线性的决策边界来区分正负样本。那么神经网络是如何做到的呢？ Solution： 求同或（真假性相同才真，真假性相异则假），即 y=x1 XNOR x2 求异或（真假性相同则假，真假性相同则真），即NOT(y=x1 XOR x2) 简单例子：AND、OR、NOT的实现AND 条件：有二进制输入特征值x1，x2，目标函数是y = x1 AND x2 目的：得到一个具有单个神经元的神经网络来计算这个逻辑与。 步骤： 画出偏置单元x0，值为+1；不影响整体值； 给这个神经网络分配一些权重（参数），如上图； 故假设函数为hΘ(x)=g(−30+20x1+20x2)；Θ10(1) = −30，Θ11(2) = 20，Θ12(3) = 20； 在输入不同特征值时，神经元输出计算结果hΘ(x)的值； 激励函数g(z): 当z的绝对值大于4时，g(z)的值必定等于1或0.故在选取权重值时一定要保证任何情况下g(z)的值只能为1或0. 假设在各种情况下的输出,即为逻辑与的计算结果: x1 x2 hΘ(x) 0 0 g(−30)≈0 0 1 g(−10)≈0 1 0 g(−10)≈0 1 1 g(10)≈1 OR 如上图神经网络同理实现了“或”的功能。 假设函数为：hΘ(x)=g(−10+20x1+20x2) NOT 如上图神经网络同理实现了“非”的功能。 假设函数为：hΘ(x)=g(10−20x1) 更复杂的例子(NOTx1)AND(NOTx2) 如上图神经网络同理实现了(NOTx1)AND(NOTx2) 的功能。 假设函数为：hΘ(x)=g(10−20x1−20x2) 求解XNOR（组合多神经元求解） 目的：求解XNOR（组合多神经元求解） 方法：如上图将上面求解的三个神经网络（AND &amp; (NOTx1)AND(NOTx2) &amp; OR）组合成一个具有三个输出单元的神经网络，即可运算x1 XNO x2 ；为拟合x1 XNOR x2的非线性样本分布，我们可以构建双层神经网络隐藏层a1(2)、a2(2)。加入偏置单元x0(= +1)即可得到输出层。 计算路径： Neuron1：AND；Neuron2：(NOTx1)AND(NOTx2) ； Neuron3：OR **Neuron1只能输出1/Neuron2只能输出1/Neuron3只能输出0； 将特征值x1，x2同时输入3个神经元，输出结果h theta(X)即为结果。** 最终真值表如下： x1 x2 a1(2) a2(2) hΘ(x) 0 0 0 1 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 1 注:XNOR 是同或，真假性相同才真，真假性相异则假。 总结：通过一个含有输入层、隐藏层、输出层的神经网络，我们最终拟合了x1XNORx2x1。 更一般的理解是：在输入层中，我们有原始输入值，然后我们建立了一个隐藏层，用来计算稍微复杂一些的输入量的函数，然后通过添加另一个层我们得到了一个更复杂一点的函数，这就是神经网络可以计算较复杂函数的某种直观解释。 我们知道，当层数很多的时候，你有一个相对简单的输入量的函数作为第二层，而第三层可以建立在此基础上来计算更加复杂一些的函数，然后再下一层，又可以计算再复杂一些的函数. 神经网络的多类别分类Abstract：使用神经网络解决多类别分类问题，即建立一个具有多个输出单元的神经网络。它和逻辑回归时介绍的一对多方法其实是一样的，只不过现在我们有四个逻辑回归的分类器，而我们需要对每一个分类器都分别进行识别分类（为什么多神经元：因为一个神经元只能输出一个预设的结果）。 我们处理多类别分类的方法实际上是基于一个多神经网络算法而延伸出来的。 譬如：识别图片中的对象，设定4个类别：行人，汽车，摩托车，卡车。此即多类别分类问题。 方法：建立一个具有4个输出单元的神经网络(如上图)。 此时神经网络的输出是一个思维向量。 因此现在的输出需要用一个向量hΘ(x)来表示，这个向量中有四个元素，即hΘ(x) = [y1,y2,y3,y4]T; 我们要做的是对第一个输出元素进行分辨图片上是不是一个行人(Pedestrian)，然后对第二个元素分辨它是不是一辆轿车(Car)，对第三个元素分辨它是不是摩托车(Motorcycle)，对第四个元素分辨它是不是一辆卡车(Truck)。 输出结果hΘ(x)： 若图上是汽车，则输出结果：hΘ(x) = [0,1,0,0] 若图上是摩托车，则输出结果：hΘ(x) = [0,0,1,0] 若图上是卡车，则输出结果：hΘ(x) = [0,0,0,1] 若图上是行人，则输出结果：hΘ(x) = [1,0,0,0] 总结：使用神经网络解决多类别分类问题，和逻辑回归时介绍的一对多方法其实是一样的，只不过现在我们有四个逻辑回归的分类器，而我们需要对每一个分类器都分别进行识别分类。]]></content>
      <categories>
        <category>Artificial Intelligence</category>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Artificial Intelligence</tag>
        <tag>Machine Learning</tag>
        <tag>Coursera ML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《走近2050：注意力、互联网和人工智能》阅读笔记]]></title>
    <url>%2F2017%2F09%2F27%2F%E3%80%8A%E8%B5%B0%E8%BF%912050%EF%BC%9A%E6%B3%A8%E6%84%8F%E5%8A%9B%E3%80%81%E4%BA%92%E8%81%94%E7%BD%91%E5%92%8C%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Abstract：本书主要讲互联网、注意力和人工智能三大方面。从长尾理论到注意力经济、计算广告学、体验经济，从注意力到占意理论；从众包和人类计算到游戏改变世界；最后阐述了占意与人工智能的关系。书还是不错的，适合对互联网、人工智能形成宏观的了解。其中占意理论是核心，以占意为核心去阐述各种概念。而占意的定义是人的外在注意力和内在意愿的集合。 Chapter1:互联网与社会长尾理论 长尾理论是网络时代兴起的一种新理论，由于成本和效率的因素，当商品储存、流通、展示的场地和渠道足够宽广，商品生产成本急剧下降以至于个人都可以进行生产，并且商品的销售成本急剧降低时，几乎任何以前看似需求极低的产品，只要有卖，都会有人买。 这些需求和销量不高的产品所占据的共同市场份额，可以和主流产品的市场份额相当，甚至更大。 如有瑕疵的国际婴儿服装 小众是相对的概念，小众的集合是千万人的大市场，可形成可观的经济效益； 跨界 如互联网链接其他领域；微信免费电话与传统移动通信行业，微信红包带动移动支付； 彩云天气：运用机器学习实现精准定点天气预报； 翻转 对传统的颠覆 mooc：以学生为中心的教学设计；选秀：评委与观众的颠倒； 互联网的动力——注意力 人的注意力为机器世界提供动力：人之所以为人，就是因为每个人的灵魂中都有一个正在思考问题的自我。就像著名的哲学家兼数学家笛卡儿所说的：我思故我在。世界上唯一可以确认的东西就是那个正在思考的我的存在。那我是什么呢？我无非就是一段持续演进的意识流，即人的注意。于是，我们的结论就是：人类的注意力推动了整个互联网的进化。人将自身的注意力投射给互联网，从而为机器世界提供着源源不断的动力；而机器则为人提供了不间断的娱乐和沉浸的服务。 想想看，一天 24 小时的生活，你除了睡觉、吃饭，是不是有80%~90%的时间是在面对各种各样的屏幕？当你注视这些屏幕时，你已经把你的注意力投射给了屏幕背后的机器世界，只有被你注意到的程序和应用，才会向前发展，才会更新换代。 遗传算法：一种高效的问题求解和搜索的自动化算法。它通过将计算机中的程序类比为自然界的生物体，模仿自然界中 DNA 串的组合和变异，对计算机中的程序进行类似的操作，并让它适应某种特定的目标，从而达到高效的寻优和问题求解。 信息与注意力的相反流动：信息与注意力构成了相反的流动，我付出了注意力给你，你传递信息给我。当信息过剩的时候，注意力就会成为主导，于是注意力流动将会更加显著，这就会导致很多事物发生翻转。粉丝把握了舞台的主导，学生们把握了课程的主导，就是因为他们拥有大量的注意力。与其让资本和权威的力量主导娱乐节目或者课程的发展，不如直接让注意力的力量来形成控制和疏导。所以，参与感（人乐意自发地付出注意力）成为了一种重要的因素。 社群是存储注意力流的有效方式：社群恰恰是一种“存储”注意力流的有效方式。人更喜欢跟有生命的人打交道，而不是冷冰冰的机器。人与人之间的注意力交换可以产生远大于软件或游戏的吸引力和黏性。因此，若要留住用户，不能仅仅依靠机器的“虚假”注意力，还应该通过社群的方式，让用户之间的注意力交换起来，让他们自发产生交互黏性，从而储备大量的注意力资源。可以说，社群就是“注意力电池”。 占意：人们相互争夺的不仅仅包括真实的物理空间和商业利益，还包括人类的意识空间。由于人类注意力具有天然的独占属性，因此，对意识空间的争夺而引发的竞争将会更加激烈。 注意力：第一推动 什么是注意力：一种人独有的稀缺资源，如免费的搜索引擎公司如何获利，就是通过贩卖用户的注意力给广告商来获取经济效益；微信、QQ占有大量注意力，利用用户的注意力去盈利（广告，生活服务）；实质上互联网产品想获得的不是用户本身，而是用户的注意力（数据化理解是：使用时长、点击量等）；流量=用户=注意力资源； 注意：指一种选择性的心理过程。当生物体面对复杂的外界环境的时候，这种机制可以将更优质的信息处理资源分配到更重要、更相关的信息上去，从而使人或者动物具有更高的适应性优势。注意力构成了互联网进化的动力。 计算广告学实践了如何让货币流和注意力流能够自发地相向流动。 别人的关注是一种必需品，而且越多越好。获得一个人的注意力就意味着你的影响力要超过他对你的影响力，如果你能得到一个人全部的注意力你就可以引导他的行为，让他做你希望他做的任何事。注意力意味着控制。 谈话=交换注意力：两个人见面会问：吃了吗？但其实并不是真的想知道对方是否吃饭了，而是希望引起对方的注意，而对方回复“我吃了”，即是回复了你的注意力。所有的对话都是在交换注意力。 人造的虚假注意力（illusion attention）：即营造一种关注某人的假象，如电视节目的主持人盯着镜头讲话就会给观众以被关注的幻觉。很多AI产品也可以创造出非常个性化的体验=虚假注意力。随科技发展。人会创造出越来越多的品质越来越高的虚假注意力。 注意力可传递、交换：如广告系统，用注意力交换广告（钱）。 注意力管理的难度：信息过剩。时间碎片化使人注意力无法集中。 注意力经济学：研究如何交换、分配注意力这种资源。 计算广告学 广告：利用货币换取注意力的载体。 why广告：必须让别人知道你的商品。 传统广告的弊端：效率低，资源浪费。“漏斗模型”：给100%的人推送广告，赚取50%对它感兴趣的人的注意力，最后只有25%的人购买。 计算广告学：根据广告所在的上下文环境和用户决定投放的广告内容和形式。结合信息检索、数据挖掘、统计分析、机器学习等技术，可以自动寻找出一种将广告、上下文环境 用户结合起来的最佳匹配模式。 免费背后的广告经济逻辑：提供免费服务，然后运用注意力（=流量）来换取货币。支撑这种逻辑的核心技术是计算广告学，其解决的问题是通过让渡一部分用户注意力来换取经济利益，精妙之处在于能够根据用户特征定制化地推荐广告，从而将大量网民的注意力精妙地兑换成现金流。 推荐系统：本质上推荐出的商品即广告。 度量集体注意力的方式：依靠人类行为数据。 眼动仪：人直视的点就是注意的点，眼睛的运动反映注意力的转移。 脑电测量装置：读取脑电信号从而测量出人的注意力集中程度。 意愿 意愿：人对内在需求和渴望的聚焦，与注意力（局限在对外在事物的关注）相对。 意愿经济：把关注焦点放在消费者上，与注意力相对（焦点在厂商，通过广告让厂商获取更多的注意力），消费者本身的意愿会带来消费的可能。更多从有需求的一方出发，围绕买家产生，更多着眼于市场。 史蒂夫派里纳：注意力可催生万物。把注意力放在你的意愿上，大声宣传让所有人都知道它，且持续关注这个愿望，最后愿望可能实现。本质：吸引法则（持续关注并充分表达，这种力量就会把你想要的东西吸引到你的生活中）。 Chapter2:体验与体验经济 物质世界越富裕，人就越来越多把注意力放在精神世界，即体验。 分享会是一种体验的交换，分享与聆听。 用户体验分析：我们可以把大量用户在一个产品（网站、App 或者游戏、软件）中穿梭游荡比喻为一大股水流在冲刷着河流盆地。这样的水如何流动显然受制于产品的内容与结构，同时也决定了用户的体验。用户行为分析就是希望通过这些水流路径寻找出改善产品内容、结构的方法，从而提升用户体验。 留存分析：一种常用而重要的分析用户行为的工具。形象地说，如果我们将一个用户比喻成一个水滴的话，留存分析就是要追踪每粒水滴在系统中的留存时间。有的用户喜欢这款产品，它的停留时间就长，有的用户不喜欢，它的停留时间就短。留存分析通过绘制留存率曲线而获得关于用户以及产品的信息。如果我们设定初始时刻的用户数为基准，然后考察这批用户中，有百分之多少的人停留了 1 天，百分之多少的人停留了 2 天，……，即留存率，那么我们将这些留存率绘制成一条曲线，就得到了留存曲线 。 留存曲线：横坐标为用户的停留时间，纵坐标为留存率。这条曲线通常会拖上一条长长的尾巴。曲线一般分成了震荡期、淘汰期和稳定期三个阶段。在一开始，有大量的用户涌进系统，但是由于人类注意力的保持时间较短，于是开始有用户流失，所以该曲线随着时间的增长而快速下降，这一阶段被称为震荡期。接下来就是一个平滑的转变阶段，这被称为淘汰期。停留时间能够到达这里的用户就是软件产品的潜力用户。最后，曲线平稳了，剩下的用户都是一些铁杆粉丝，他们才是这款产品的核心用户。所以，通过追踪每一个曲线部分所对应的用户，我们就能获知哪些用户是我们的铁杆。除此之外，曲线的总体形状也能够反映产品的一些特征。例如，曲线的稳定期如果越高，说明这款产品越能够黏住用户，从而保持住足够多的铁杆支持者。 体验经济是第四种经济形态：体验经济是与注意力经济、意愿经济相平行的一套理论，它认为体验本身就是一种宝贵的资源，因而也可以被交换、买卖。体验经济被认为是继农业经济、工业经济、服务经济之后的第四种经济形态。 Chapter3:占意理论betention 了解意识的重要性和意识的反作用：科学对外在世界的了解已深入到原子、夸克的微软层面，但对内心世界的解读却刚刚开始。意识对外在物质世界的反作用会越来越大。体现为想法变为现实的周期会越来越短，意识对物质世界的掌控能力越来越强。 互联网对意识空间的争夺：农耕时代争夺土地，工业时代争夺资源，体验经济时代争夺意识；意识是一种稀缺资源，占有意识可以站在经济的制高点； 占意：广义上的注意，体现为对外在事物的注意+对内在需求的意愿。意识的流动则构成人的外在或内在的体验。可理解为构造了这样一个概念，去做注意+意愿的统称。 集体占意流：用用户行为数据进行近似度量和分析的流动。 占意什么是占意 人之所以为人，其本质在于人类有自我意识，会思考。 神经认知科学的全局空间理论：把人的意识比喻为舞台，人的注意就是舞台上的聚光灯，人的记忆、情感、思维片段是演员，聚光灯照到哪里，哪里的想法就会进入意识视野。演员有两种：1.来源于人类从各种感官得到的外在刺激；2.来源于人的内在需求或记忆；聚光灯的照耀就是对外在事物的聚焦或对内在需求的聚焦。占意本质是广义的注意，即意识聚光灯对意识舞台上某种事物的聚焦。 eg.看电影——意识被电影内容占据，即对电影的注意；怀着强烈愿望规划自己的婚礼——意识被心中强烈的憧憬和想象占据，即意愿； 体验：意识在一系列的时间中的流动；占意之流即体验； 占据意识 占意：解读为动词，即占据人的意识这个动作；意识可指挥人的身体执行各种动作，故意识是人的第一推动力； 对意识的占据——未来商业社会的主要竞争目标；采用一切手段占据人类的意识世界——粉丝经济、社群经济、免费服务； 占意事物的层次划分： 外部感官刺激—-视觉冲击占意——低层次占意社交体验——满足人的社会尊重——高层次占意符合人类美学的设计——提供美感享受——更高层次占意宗教——建立足够高端的信仰从而大道更大时间和空间尺度来占意——最高层次占意 人们对意识空间的争夺有高低上下之分： 粉丝经济、注意力经济——对感官层面的占意——占据总量大、粘性低、持续时间短社交网络、社群经济——满足社交需求苹果、微信的注重情怀和审美的设计——占据人类更高层次的精神空间 高层次占意具有更大的能量，可自动转化为低层次占意（如品牌的联动效应）； 占意的性质相关性：联想相关概念 品牌效应背后的逻辑：将品牌与多种属性、事物相连，从而只要有一项事物成功占意，其他事物就会自发形成占意；（google，腾讯） “单点突破，做到极致”的逻辑：以有限的精力把单一产品或特点做到极致，才能使人对产品留下深刻印象，从而让用户形成产品和某一优良品质的相关联系；成功以单一占据意识后即可利用品牌的相关性扩展到其他领域；（如小米） 连续性和心流 连续性：意识在流动过程中有很大惯性，当意识高度集中时不愿被打扰以及很难适应被打断后的新场景；（水流惯性，运动惯性） 心流状态：意识高度集中时意识高度流动、达到忘我境界和完全沉浸； 连续性和相关性的矛盾关系 矛盾点：当希望进入心流状态，过多的联想会干扰思考；当需要进行创造性思考，过于拘泥于细节又会让思维受限； 因时制宜：需要根据具体场景，在稳定性和相关性间选取恰当平衡； 创造性 创造性：通过注意过程指导行动从而改造外在世界的过程； 如水流和河道的耦合演化：水的流动要顺河道而行 + 水流反过来不断冲刷河道改变河道形状 占意流 占意流：用户注意力在不同概念、观点上的转移，即用户体验；获取用户在概念、心理状态上的占意流数据——传达关于用户体验的信息——做用户体验优化 占意流的优化（产品用户体验评价维度） 流畅感：用户为达到某目的所经历的平均最短时间；时间越短，用户感觉越爽； 黏性：用户从源到汇的平均路径长度；用户在系统中停留的时间越长，游走的节点越多，系统对用户越有粘性；（游戏、娱乐应用中，用户不带有明确目的，仅为娱乐和社交） 沉浸感：好的设计不仅能黏住用户，更重要的是让用户产生沉浸感； 占意流网络是基于宏观的类比性的思考，而不是针对用户行为的具体细节；可帮助宏观把握系统的结构； Chapter4 解读互联网 找不到一条主线能把纷呈的互联网现象科学地串联起来 用高维理论解释低维现象 意本家：以占意资本运作为主导；追去完美和发烧的pm是占意资本的寡头，增长黑客是深谙占意运作之道的专家； 从占意理论角度：社群是通过人和人之间的社交关系来增加用户黏性从而存储占意的容器；因为社交本质是注意力的交换； 流动和翻转 车流与空隙的反向流动：当汽车密度达到一定程度，它们中间就会产生一些空隙，且空隙会反向流动（自下而上）。原因：切换了观察事物的角度。 流动的反向、前景和背景的切换 资本运作的奥秘：以钱生钱； 农耕时代：G-M-G；货币经济：M-G-M；信息化社会：A-B-A2；资本换取占意：M-B-M；全新的反向流动：B-M-B； 社会翻转：以信息、内容、知识为核心——以普通民众的注意力资源为核心；（反向流动：流量为王） 思考货币流与注意力流分别在物质世界和信息世界中的反向流动； 货币流贺占意流互联网公司估值 背景：新兴创业公司没有资金和人员支持需要通过一整套风险投资拉动的方法实现；风险投资最核心的问题：在尽可能短的时间内对初创公司尽可能客观、合理、准确地估值； 一个企业今天的估值是它以后创造利润的总和； 面对初创公司如何确定对它未来收益的预期：根据公司在现阶段的占意程度决定，包括日活（日活跃用户数）、频次（用户平均访问次数）、留存（老用户占比）等关键数据【表征占意流】；一个产品的现在的占意大小 = 对该产品的未来收益预期 = 估值； 流量为王的逻辑：培养用户使用习惯—占据用户意识空间—用户养成新习惯抛弃旧习惯—在未来实现大规模的实际购买行为； 免费逻辑 让渡货币，换取占意流 eg.360 VS 金山：技术上比不过老牌杀毒软件，但由于免费+易用性，故获得用户认可从而快速占领市场；360的逻辑：通过免费吸引用户的占意，并形成在段时间内对整个市场的垄断性优势，然后通过其他手段来变现，以谋取经济利益； 意本：能够增值的占意；意本家：以获取占意为目的的企业家； 阿里重定义：本质是一家扩大数据价值的公司。数据指用户数据，从占意理论角度来说，用户数据是占意流的一种表现。 增长黑客 growth hacker 试图用更聪明、更有创造力、更低成本的方式，解决产品用户增长，即有效地获取占意流的问题。 通常手段：搜索引擎优化，电子邮件召回，病毒营销； growth-AARRR转化漏斗：由于占意的耗散性，注意力会形成漏斗形状，筛选用户，占意品质身高、用户数量相对减少； A:acquistion获取用户-用户通过各种渠道来到你的产品A：activation激发活跃-用户享受第一次使用，get到产品价值R：retention提高留存-用户再次使用，并多次回访R：revenue增加收入-用户产生付费行为R：referral传播推荐-用户把产品传播给其他人 获取用户占意：像利用水汞从水池抽水，同时水管是漏的；最终能抽多少取决于：1.池塘的容量（市场的大小）和状态（正在成长还是正在萎缩）；2.水汞的功率（需求强弱）；3.水管漏水情况（用户的流失）；增长黑客可有效提高水汞功率，增设水汞，发现并维修水管漏洞； hotmail：在hotmail发出的邮件尾签名处增加一条附言：“我爱你，快来注册hotmail免费邮箱吧”；于是用户数量以几何数增长；以一行文字撬动用户为其免费宣传； 占意制高点产品：占领意识制高点 产品的温度：产品设计师的意识制高点要首先被产品占据；追求极致完美； 社群—注意力电池社群的特征：成员具有某种共同喜好或信念；成员之间存在着很强的互动性；成员结构是自组织形成的； 注意力流互动的不同模式： 粉丝明星：一对多；社交网络：一对一；社群：互相连接但又一对一的网； 小米-打造产品型社群；依靠高性价比手机吸引用户，将米粉通过MIUI集结到一起，让米粉形成相互连接的庞大社群；经营好米粉，即可挖掘整个产业链上的增值服务； 从A／B测试到自动化创业A／B测试：为同一个想调研的目标制定2个不同方案，让一部分用户使用A方案，一部分B，记录用户使用情况，通过数据分析对比确定哪个方案更优； 灰度测试：先测试一小部分用户； 精益创业的三大法宝： 1.MVP（minimum viable product）； 2.用户反馈； 3.快速迭代；开发团队通过开发MVP，用最快最简明的方式实现一个可用的产品原型，帮助用户解决问题的最小功能集合，快速投放市场让目标用户上手使用，测量用户数据，收集用户反馈，通过快速迭代来修正产品、添加功能、完善细节； 精益创业的循环过程：认知-想法-开发-产品-测量-数据-认知； 从体验到共享如果体验和经理才是最重要的东西，那么拥有将变得越来越不重要；当站在意识主体角度来说，所谓的“我拥有某个东西”这种所属权只不过是一种虚假的符号而已——人主动让渡所有权 共享社会：新的方式为我们创造出额外的经济利益，人会更有动力放弃拥有权而实现共享 共享经济比传统经济更高效的原因：通过流动加速，把时间维度引入经济体系，从而使物质的量能产生虚假的提高； Chapter5:众包与人类计算人脸生成程序用于罪犯识别：目击者在电脑屏幕上不断点选那些更像罪犯的脸孔，程序会一点点把真正的罪犯面孔进化出来。（属于机器学习还原图片吗？） 与人相关的领域相互交叉、覆盖： 众包crowsourcing：从外包outsourcing的概念扩展而来，指把传统的由一个人或公司完成的工作利用互联网外包给一大群人的做法。 人类计算human computation：通过互联网和其他技术手段，将人的智力组织控制起来，用于解决计算机尚未解决的问题。 社会计算social computation：指有助于帮助人们完成社会性活动（集体行动、社会交互、信息交换、知识聚集）的应用和服务,其唯一目的是促进人与人间的交互。如blog，weibo，wiki，quora… 群体智能collective intelligence：一大群个体集合在一起做出一些看起来具有智能表现的行为，如蚂蚁通过简单的互动交流而找到食物并沿着一条最短的路把食物搬运回家。 数据挖掘data mining：从海量数据中挖掘出一些有价值的信息和知识的技术手段。如推荐算法和用户数据分析。 占意可直接转化为信息结构。 众包定义：化大为小并通过互联网社区将小任务分发给大众。 交叉验证：当N个人的答案完全一致时才接纳答案，此法可避免恶意破坏。 人类计算：运用游戏化（gamification）的方式调用人类的占意资源，目的是完成复杂任务的求解。 Verbosity :一个利用游戏的方式解决语义网络构建问题的人类计算系统.比如，我要建立一个笔记本电脑（laptop）这个概念的知识，会让两个玩家一起玩这个游戏，他们在不知不觉中就会完成知识的输入。具体玩法是，计算机在玩家 A 的屏幕上打出 laptop 这个词（如图 5-11 所示），并要求 A 用一些标准的语句描述 laptop，比如 A 会说它包含一个键盘、或者拥有一块屏幕等。与此同时，在玩家 B 的电脑上，B 不能看到 laptop 这个原始词，但却可以看到 A 输入的那些描述 laptop 的词。于是 B 开始尽自己的最大努力猜玩家 A 看到的原始词是什么。最后，直到 B 猜出来那是 laptop，这一轮游戏结束，系统会根据时间的长短给两个玩家增加分数。而在 A 和 B 共同娱乐的同时，系统已经收集了大量关于 laptop 的语义知识，至少系统已经知道笔记本电脑包含键盘和屏幕了。 ESP21:以游戏的方式让玩家来做图片标记的任务。玩法是两个玩家在看到同一张图片后一起输入一些词来描述图片，直到两个人输入的单词一模一样则双方各自加分。如此可得大量的图片标签，有助于图片分类、检索、理解。 从众包到人类计算区别： 众包：人直接将占意付出给原任务本身。 人类计算：人将占意赋予给一个游戏或一个与原任务没有关系的任务（如reCAPTCHA中的验证码），然后系统会自动将这个不相关的任务映射到原任务。（游戏——发动机——高效提取用户占意流的发动机） 众包模式： 1.微任务模式：如Turkit、reCAPTCHA，将大任务分解成分散的相对独立的小微任务。 2.工作流模式：串行处理，流程化，将不同人的计算能力串联起来。 3.求解生态系统的模式：为完成任务需要搭建共享工作空间，并调集整合各种不同人的意见、评估结果，从而完成高度复杂的相互协调。 占意的比喻：人是计算机，占意是CPU。 Chapter6：游戏的世界要点：游戏的特性、本质，如何将游戏元素融入到各行各业；游戏化；游戏设计；VR、AR； 什么是游戏定义 wiki：游戏是玩的一种结构化形式，通常用于娱乐和教育……游戏中的关键元素包括：目标、规则、挑战以及交互。 一种交互的、具有明确目标的、由积极主动的参与者玩的，并且参与者可以彼此交互的活动。 目标、规则、反馈系统和自愿参与是游戏中最重要的四个元素。 游戏的另外几个重要元素：趣味、分离性、不确定性、非产出性、由规则限定，以及幻想性。 特性 交互性 趣味性和自愿参与性 目标性：不过有例外，如体验式的游戏 隔离性：与现实隔离的完全虚拟的世界 规则性：自己的规则系统，自由与约束的平衡 不确定性：环境的不确定性和对手 沉浸性：进入心流状态（精神高度集中，浑然忘我） 游戏改变世界 可解决社会问题：老龄化，反腐 游戏+ 游戏设计游戏机制game mechanicsdefinition：游戏的根本，由一组规则或方法组成，这些规则可解决游戏状态之间的转换以及玩家的交互。游戏机制提供了游戏的基本可玩性。 游戏的本质：底层规则和逻辑，即游戏机制。而不是外观或建构它的代码。 1.游戏的平衡性game balance设计师在游戏机制层面要考虑的最主要因素之一。 指游戏中各个组成单元都能得到有效充分的利用。 如何创造平衡的游戏： 游戏设计上保持对称性：如所有玩家都具备完全相同而对称的初始状态 随机化带来均匀的公平：如洗牌 创造动态平衡，尤其是玩家的表现和游戏难度之间的反馈：当玩家的游戏表现越好，游戏难度相应增加 eg.没有哪个角色相对于其他角色具有绝对的优劣势，角色间有制约关系。窦泽就会导致绝对占优的角色出现，于是玩家都会自愿只选择这个角色，从而浪费了其他角色。譬如石头剪刀布。 破坏游戏平衡的机制： 游戏必须动态平衡，即其本身要存在一种破坏游戏平衡的机制；游戏过程处于不断建立平衡、打破平衡的重复之中，从而游戏可玩性得以提高。eg.围棋：黑白双方在初始处于完全对称的地位，但一旦某一方吃掉对方的一大片棋子后，这种对称性就会瞬间破坏掉，被吃掉棋子的一方就落于被动地位。 平衡与打破平衡的机制应调节到一定比例范围内才会好玩。过于平衡的游戏易让玩家生厌，过多的破坏游戏平衡机制会让玩家丧失信心。 2.游戏的涌现性涌现（emergence）：大量微观个体通过交互所展现出来的超越底层单元的集体属性或规律。引申为系统的运行表现对于底层运行机制的超越。 3.设计心流体验心流体验的作用：沉浸感；当难度和玩家技能相互匹配时，可达到最佳的心流体验。 when： 挑战过于简单、玩家技能过高，玩家感觉无聊 挑战过高、玩家技能过低，玩家感觉焦虑 How： 设计与玩家能力相匹配的游戏，如使用游戏AI使游戏本身有自适应的人工智能机制，从而使游戏的难度会根据玩家的水平而自行调节。 其他辅助：炫酷画面，bgm 涌现与沉浸的对比： 涌现emergence：自内而外的涌出，em- 沉浸immersion：玩家通过外在的游戏过程或刺激达到一种内在的心流体验，属由外而内 复杂系统的涌现现象往往对应着围观的简单规则。当规则足够简单，游戏才更好玩。 游戏程序设计游戏与模拟程序的最大区别：游戏允许玩家的输入和参与 关键变量：运算——f(x,I),游戏状态——x,玩家的input——I每一个时间步，游戏程序根据用户的输入信息I以及游戏的状态x完成一次运算f(x,I)，这个运算结果反映为游戏新一步的状态，而这个状态又会在下一步反馈给系统。游戏的进度就体现在每个周期的x变量的不同。而游戏的规则则体现为f的不同。通常情况下，f不能写出解析的函数形式，而必须用算法的语言进行表达：它往往包含了游戏逻辑、人工智能、物理模拟、图形渲染等功能。 从数学上看，一个函数迭代：xt+1=f(xt)是一个标准的动力系统（Dynamical System）[8]。但是，该框图与动力系统最大的不同在玩家输入I的存在，因此我们称此框图表达的是一种开放式的动力系统（Open Dynamical System）。玩家输入信息I对于动力系统的设计者来说完全是不确定的（不包含在规则f之内），所以这种开放式的计算系统具有天然的不确定性。 现实虚拟化AR技术的普及：现实世界所叠加的虚拟世界会越来越复杂；现实与虚拟世界融为一体。 虚拟世界游戏（VR）的后果：人类社会被分割成独立部落，推进文化分离。 Chapter7 占意与人工智能人工智能人工智能：研究如何设计智能主体的学科，智能主体是指能够感受环境并采取行动以获得最大可能成功的系统（input——计算——output）。 神经网络为什么有强学习能力 网络上，每条边上都有个强度大小，网络可不断调解这些强度，从而使网络在给定输入信息的条件下可输出人们认为正确的信号。只要有足量的数据，训练足够长的时间，网络就能调节到人们想要的状态。 神经网络的运转 训练阶段：足量数据训练网络，改变网络各个边的连接强度 运行：将训练好的网络用在未训练的数据上，得到想要的输出 深度学习 改良的神经网络模型，具有更深的神经网络结构 又称表征学习representative learning，可自动学习描述物体的抽象特征。如识别图片，传统学习方法需要对原始图片作预处理（边缘检测，图像分割etc），而深度学习可将这些本来由人工处理的步骤全部自动化。 人是占意之源 人可以什么都不做，只要付出自己的注意力玩机器。人的注意力是机器的能量。 人工智能与人类智能人工智能 训练数据来自于人 游戏化是获取数据的上佳方式 真正价值：不在于其存储的知识，而在于它对外界的适应和学习过程。 过滤器 目的：降低过度信息对我们注意力的干扰和破坏 个性化推荐系统是典型过滤器：当我们作为匿名用户登录淘宝等网站，我们看到的内容会非常大众化、流行化。而当我们输入用户名和密码再次登录，会看到完全不一样的界，其中战士的商品大多是跟用户长期已购买商品类别有关的个性化推荐产品。反映其背后有强大的个性化推荐引擎，根据用户以往的浏览行为数据而产生过滤规则，自动帮用户筛选信息。 实质：利用用户的占意流优化自身 搜索引擎：通过关键词把超大规模的无用信息过滤掉 排序算法：把重要而相关的网页排在前面 反垃圾邮件：挡掉垃圾邮件的干扰 智能代理（“许愿树”） 通过人机对话的方式了解每一个客户的需求，为其提供个性化的服务和帮助 机器人或软件 愿望的实现过程：复杂问题的求解过程。将大问题分解为一系列小问题，然后一个个地求解，最终帮用户达成心愿。 自动化游戏设计automatic game design让AI更好地利用人类的占意资源的最高境界：让人类沉浸于一个AI游戏中并促使人在与机器互动中产生心流。 属于计算机创造学（computational creativity）：专门研究如何用机器算法来生成人认为具有美感的东西，图形、音乐、诗歌等。 1.游戏元素生成 游戏自动生成需要分不同层次：1.游戏机制 ；2.游戏规则；3.游戏关卡；4.游戏参数； 2.可玩性评价（user ex） 可玩性度量：计算相应评价指标 玩家行为数据 玩家直接反馈 自动化游戏设计框架 通过玩家反馈社交网络进行自动游戏设计 Chapter8 参与者的宇宙 交互不确定性 量子力学 平台与观测：用户与互联网产品构成闭环，起点和终点都是用户 互联网的最小驱动单元是人与机器的互动 人机交互 测量网：两类节点，由玩家的选择和机器的选择分别展开不同路径（选择即节点） 附录：占意流网络的定量规律克雷伯定律生物体的新陈代谢F和该生物体的体重M之间满足一个很好的3/4幂律关系，即：F = c * M^b。c是比例常数，b为幂律指数（= 3/4）。 所有生物体都满足克雷伯定律。 数字资源也满足克雷伯定律的推理： 贴吧——生物体 贴吧内的一个个帖子——细胞 用户在不同帖子之间的跳转——生物体内的血液流动 单位时间内该贴吧吸引的用户数(UV)——其从外界获取的新陈代谢能量流 该贴吧在一个时间段内获得的总浏览量(PV)——贴吧的体积 计算UV与PV之间是否遵循以3/4为指数的幂律关系 交互黏性]]></content>
      <categories>
        <category>Artificial Intelligence</category>
      </categories>
      <tags>
        <tag>Artificial Intelligence</tag>
        <tag>Reading Notes</tag>
        <tag>Communication Science</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[新媒体标题写作：如何成为标题党？]]></title>
    <url>%2F2017%2F09%2F27%2F%E6%96%B0%E5%AA%92%E4%BD%93%E6%A0%87%E9%A2%98%E5%86%99%E4%BD%9C%EF%BC%9A%E5%A6%82%E4%BD%95%E6%88%90%E4%B8%BA%E6%A0%87%E9%A2%98%E5%85%9A%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[Abstract：传统媒体标题以简洁直白明确为要，而新媒体时代的标题则以吸引人眼球为特征。尤其以咪蒙为代表的自媒体写作者常篇篇文案动不动10万+，且引起大规模转发，其中很重要的原因是其标题拟得精彩、戳中人心。一个好的标题可获取极高的点击率，价值千金。那么“标题党：究竟是怎样炼成的呢？ 什么样的人适合做新媒体？ 不要脸的人：真诚交流 有同理心：能感悟各种人生经历，有代入感，驾驭各种题材；理解 有观点 ：独特观点 超强意志力和执行力 自黑，自恋，自强，自信 自媒体微信公号类型 “我有一个朋友型”——“来，我们来聊聊” 冷兔lengtoo（朋友型） “取悦”型——“我了解你” “毒舌”型——“来吧，开骂吧，爽！” 咪蒙：“致贱人…” “恶补”型——“书读得太少，悄悄读几篇，去人前装B” 新媒体标题写作的关键技巧 分众：确定标题面向的受众群体，语言表达要贴近受众喜好，运用对话式 构建具体的用户场景，使产生强烈的场景代入感 eg.“如何突出文字的力量”——“做PPT时，如何突出中文字体的力量与美？” 耸人听闻的字眼触动用户敏感神经：受害者，死亡，性，私生活，弱者，失败，结局，底层 露大腿不露底裤:犹抱琵琶半遮面，吸引人点进去了解更多 识别度 &gt; 知名度：贴有辨识度和吸引力的标签 eg.张幼仪：徐志摩前妻 有共鸣，让用户感同身受，觉得这是说给自己听的 eg.你有没有玩命爱过一个女生？你有没有疯狂暗恋过一个帅气的男老师？ 精彩案例1.翟欣欣骗婚事件 翟欣欣：有我在，马蓉算老几？ 天才程序员的墓志铭：这一生修正过无数bug，却还是跨不过女人这道坎 渣男还在玩感情，渣女却在玩命。 生而为程序员，我很抱歉。钱多话少死得早，程序员难道不配被爱吗？ 前有翟欣欣，后有李雨桐，一千万引发的生死情仇 2.孕妇跳楼事件 中国式渣男：生不了孩子你算什么女人 孕妇跳楼，致未出生的孩子——欠你的命来世再还 你只管怎么X，你管我怎么生？！ 我的三次下跪都没有换来你的一声安慰 我以为这是爱情，却只是你传递香火的程序 产妇跳楼案：孕妇的肚子到底由谁做主？ 昨天你下跪向我求婚，今天我下跪向你求生 我把你当老公，你把我当子宫 3.正经政府新闻：住户存款 住户存款大揭秘，你的城市排在哪儿？ 15个存款最多的城市，这个省竟然上榜了4个！ 各城市平均存款出炉，看看哪些人比你穷 差距大：23个城市竟掌握了全国五分之一的存款 4.其他经典标题： 曾经他是王者，现在爹来了 你有这只猫勤奋吗 这篇文章，给咪蒙5小时都取不出10W+标题 别人都写出10W+的爆款文章了，你却连标题都拟不好？ 如何谋杀你的妻子 “我们男人出轨，不是为了性” 捉奸的时候，该涂什么口红 在没有约炮的年代，人们如何谈恋爱？ 女追男隔层纱？这种屁话你也信！ 看到直男癌言论，我的脏话不够用了 什么门当户对，不就是爱得不够 生活不只有诗和远方，还有傻逼甲方 矮子们快来幸灾乐祸！原来长得高也很凄惨啊！ 老子努力奋斗，就是为了包养一个男人 我能想到最性感的画面，就是你跪键盘的样子]]></content>
      <categories>
        <category>Communication Science</category>
        <category>Creative Writing and Planning</category>
      </categories>
      <tags>
        <tag>Communication Science</tag>
        <tag>Creative Writing and Planning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集合覆盖问题]]></title>
    <url>%2F2017%2F09%2F26%2F%E9%9B%86%E5%90%88%E8%A6%86%E7%9B%96%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[Abstract：集合覆盖问题是集合论里的一个知识点，这里做简要的概念解释。集合覆盖问题是要找到S的一个最小子集，使得其并集等于全集。 给定全集U，和一个包含n个集合且这n个集合的并集为全集的集合S。集合覆盖问题要找到S的一个最小子集，使得其并集等于全集。 集合覆盖的最优化问题：给定(U,S),求使用最少集合的一个覆盖。]]></content>
      <categories>
        <category>Math</category>
        <category>Discrete Mathematics</category>
      </categories>
      <tags>
        <tag>Computer Science</tag>
        <tag>Discrete Mathematics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解算法的时间和空间复杂度]]></title>
    <url>%2F2017%2F09%2F25%2F%E7%90%86%E8%A7%A3%E7%AE%97%E6%B3%95%E7%9A%84%E6%97%B6%E9%97%B4%E5%92%8C%E7%A9%BA%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[Abstract：算法分析包括事后统计和事前分析估算。事后统计由于依赖于计算机软硬件环境等因素故不太好。事前分析估算是以估算算法的时间复杂度的方式来衡量算法优劣。时间复杂度T(n) = O(f(n))，计算结果只需关注n的最高次幂的数量级即可。而算法的空间复杂度则是对算法在运行过程中临时占用存储空间大小的量度。 算法分析 证明算法正确性 分析算法时间复杂度：反映算法优劣，通过依据该算法编制的程序在计算机上运行时所消耗的时间来度量 方法 事后统计：依赖于计算机的软硬件环境等因素，易掩盖算法本身的优劣 事前分析估算 事前分析估算一个用高级语言编写的程序在计算机上运行时所消耗的时间取决于下列因素： 算法采用的策略、方法； 编译产生的代码质量； 问题的输入规模； 机器执行指令的速度。 算法的时间量度 WHAT：以基本操作的重复执行次数为算法的时间量度，便于比较同一问题的不同算法 WHY：算法由控制结构（顺序、分支、循环）和原操作（固有数据类型的操作）组成，算法时间取决于两者的综合效果 时间复杂度时间频度T(n)：算法中语句执行次数，n为问题规模 时间复杂度：T(n) = O(f(n)),为算法的时间复杂度 f(n)为辅助函数，使得当n趋近于无穷大时，T(n)/f(n)的极限值为不等于0的常数；该式表示存在一个常数C，使得在当n趋近于正无穷时总有T(n)&lt;=C * f(n)，即T(n)在n趋近于正无穷时最大，与f(n)差不多; eg.O(2n2+n +1) = O (3n2+n+3) = O (7n2 + n) = O ( n2 ) ，一般都只用O(n2)表示即可。注意到大O符号里隐藏着一个常数C，所以f(n)里一般不加系数，只保留主干。如果把T(n)当做一棵树，那么O(f(n))所表达的就是树干，只关心其中的主干，其他的细枝末节全都抛弃不管。 时间频度不同，时间复杂度有可能相同，如T(n)=n2+3n+4与T(n)=4n2+2n+1它们的频度不同，但时间复杂度相同，都为O(n2)。 按数量级递增排列,常见的时间复杂度有：常数阶O(1),对数阶O(log2n),线性阶O(n),线性对数阶O(nlog2n),平方阶O(n2)，立方阶O(n3),…， k次方阶O(nk),指数阶O(2n)。 随着问题规模n的不断增大，上述时间复杂度不断增大，算法的执行效率越低。应尽可能选用多项式阶O(nk)的算法，而不希望用指数阶的算法。 常见的算法时间复杂度由小到大依次为：Ο(1)＜Ο(log2n)＜Ο(n)＜Ο(nlog2n)＜Ο(n2)＜Ο(n3)＜…＜Ο(2n)＜Ο(n!) 求解算法时间复杂度的步骤： 找出算法中的基本语句：即最内层循环的循环体 计算基本语句的执行次数的数量级：只要保证基本语句执行次数的函数中的最高次幂正确即可，其他细枝末节可忽略 得出O(n) btw:1.如果算法中包含嵌套的循环，则基本语句通常是最内层的循环体，如果算法中包含并列的循环，则将并列循环的时间复杂度相加。2.Ο(1)表示基本语句的执行次数是一个常数 计算时间复杂度的简单程序分析方法 简单I/O语句或赋值语句：O(1) 顺序结构：求和法则； 若T1(m)=O(f(m)), T2(n)=O(g(n)),则 T1(m)+T2(n)=O(f(m) + g(n)) 选择结构：时间主要耗费在then/else语句上，以及检验条件也需要O(1)时间 循环结构：乘法法则；运行时间主要体现在多次迭代中执行循环体以及检验循环条件的时间耗费 若算法的2个部分时间复杂度分别为 T1(n)=O(f(n))和 T2(n)=O(g(n)),则 T1T2=O(f(n)g(n)). 复杂算法:可以将它分成几个容易估算的部分,然后利用求和法则和乘法法则计算整个算法的时间复杂度 1.O(n^2) 12345sum=0； （一次） for(i=1;i&lt;=n;i++) （n+1次） for(j=1;j&lt;=n;j++) （n2次） sum++； （n2次） //Θ(2n2+n+1)=n^2;只保留主干得：T(n)= =O(n^2) 算法的空间复杂度Space Complexity定义：对算法在运行过程中临时占用存储空间大小的量度，包括存储算法本身所占用的存储空间，算法的输入输出数据所占用的存储空间和算法在运行过程中临时占用的存储空间这三个方面。 算法在运行过程中临时占用的存储空间随算法的不同而异，有的算法只需要占用少量的临时工作单元，而且不随问题规模的大小而改变 举例： 当算法的空间复杂度为常量，即不随被处理数据量n的大小而改变时，可表示为O(1)； 当一个算法的空间复杂度与以2为底的n的对数成正比时，可表示为0(10g2n)；当算法的空间复杂度与n成线性比例关系时，可表示为0(n). 若形参为数组，则只需要为它分配一个存储由实参传送来的一个地址指针的空间，即一个机器字长空间； 若形参为引用方式，则也只需要为其分配存储一个地址的空间，用它来存储对应实参变量的地址，以便由系统自动引用实参变量]]></content>
      <categories>
        <category>Computer Science</category>
        <category>Data Structure</category>
      </categories>
      <tags>
        <tag>Computer Science</tag>
        <tag>Data Structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构|Chapter2：线性表]]></title>
    <url>%2F2017%2F09%2F25%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-Chapter2%EF%BC%9A%E7%BA%BF%E6%80%A7%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[摘要：本文是第二章线性表的笔记。线性表是最基本、最简单、也是最常用的一种数据结构。 线性结构 存在唯一的首位元素a1 存在唯一的末位元素an ai+1是ai的直接后继（1&lt;i&lt;=n） ai-1是ai的直接前驱（1&lt;=i&lt;n） 线性表的定义线性表的逻辑结构1.线性表：由n个类型相同的数据元素（a1，a2,…,an）构成的有限序列。记作L = （a1，a2,…,an）。2.表长（表的长度）：线性表中数据元素的数目。3.空表：不含数据元素的线性表。 抽象数据类型的线性表 ADT：三元组，包括数据对象，数据关系，基本操作。 ADT List{ 数据对象：D={ai|ai∈ElemSet,i=1,2,,…n,n&gt;=0} 数据关系：R1={| ai-1,ai∈D,i=2,,…n} 基本操作： 1.IniList(&amp;L) //构造空表L。 2.DestroyList(&amp;L) //销毁线性表L 3.ClearList(&amp;L) //置L为空表 4.ListEmpty(L) //判断L是否为空表 5.ListLength(L) //求表L的长度 6.GetElem(L,i,&amp;e) //取元素ai,由e返回ai 7.LocateElem(L,e,compare()) //查找符合条件的元素 8.ListInsert(&amp;L,i,e) //元素ai之前插入新元素e 9.ListDelete(&amp;L,i,&amp;e) //删除第i个元素 10.DeleteElem(&amp;L,x) //删除元素值为x的元素 … … }ADT List btw:为什么有的在L前加了&amp;而有的没加：当操作对线性表或其中的元素作了修改，则是&amp;L；若无修改，则为L。 定义：一个数学模型以及定义在该模型上的一组操作。 作用：抽象数据类型可以使我们更容易描述现实世界。 关键：使用它的人可以只关心它的逻辑特征，不需要了解它的存储方式。定义它的人同样不必要关心它如何存储。 操作 基本操作 可利用基本操作组成更复杂的操作 EG.将线性表Lb中的且不在线性表La中的数据元素合并到La中。 123456789101112//算法：判断两个线性表的长度——用循环结构将符合条件的元素插入La中（判断Lb中的每个元素是否在La中，不在的话就插入） void union(List &amp;La, List &amp;Lb) //合并线性表 &#123; La_len = ListLength(La); Lb_len = ListLength(Lb); for (i=1;i&lt;=Lb_len;i++) &#123; GetElem(Lb,i,e) //取Lb的第i个数据元素赋给e；即依次取出所有Lb中的元素 if(!LocateElem(La,e,equal)) ListInsert(La,++La_len,e); //判断e在La中是否存在，不存在则插入 &#125;&#125; 线性表的顺序表示 顺序分配：将线性表中的元素依次存放到计算机存储器中一组连续地址的存储单元中，这种分配方式称为顺序分配或顺序映像。由此得到的存储结构称为顺序存储结构或向量(一维数组）。 一般形式：i——an——存储地址：LOC(i) = b+（i-1） p （1&lt;=i&lt;=n）b：表的首地址/元素a1的地址；p：1个数据元素所占存储单元的数目；maxleng：最大长度，为某个常数 [序号maxleng——存储地址：b+(maxleng-1)p] 顺序访问 随机访问 线性表顺序结构在C语言中的定义（静态分配）? 123456789101112#define maxleng 100 typedef struct &#123; ElemType elem[maxleng]；//下标:0,1,...,maxleng-1 int length； //表长 &#125; SqList； SqList La； .......... 其中：typedef---别名定义，SqList----结构类型名 La----结构类型变量名 La.length---表长 La.elem[0]----a1 La.elem[La.length-1]---an 线性表顺序结构在C语言中的定义（动态分配）? 123456789101112 #define LIST_INIT_SIZE 100 #define LISTINCREMENT 10 typedef struct &#123; ElemType *elem；//存储空间基地址 int length； //表长 int listsize; //当前分配的存储容量 //（以sizeof(ElemType)为单位 &#125; SqList； SqList Lb； 其中： typedef--别名定义， SqList---结构类型名 Lb---结构类型变量名 Lb.length---表长 Lb.elem[0]---a1 Lb.elem[Lb.length-1]--an 插入算法的实现举例1.插入操作移动元素次数分析：在（a1,a2,…,ai,…an)中ai之前插入新元素e当插入点为： 1 2 … i … n n+1 需移动元素个数: n n-1 … n-i+1 … 1 0则插入一个元素时移动元素的平均值是： n+1 Eis=∑pi(n-i+1)=[1/(n+1)]*(n+(n-1)+…+1+0)=n/2 i=1注：算法时间复杂度为O(n) 2.删除操作及移动元素次数的分析：将元素ai删掉，并得到删除后的线性表 123456789//算法：先判断元素ai的合法性——若不合法则返回Error/若合法则继续——定位ai元素（在第i-1号地址）并赋给变量e——用循环结构将ai后的所有元素往前挪一位（共L.length-1个元素；赋值法）int ListDelete_Sq(SqList &amp;L,int I,ElemType &amp;e) &#123;if (i&lt;1 || i&gt;L.length) return ERROR; e=L.elem[i-1]; for(j=i;j&lt;=L.length-1;j++) L.elem[j-1]=L.elem[j]; L.length--; return OK; &#125; 移动元素个数= n-1 n-2 … n-i … 0假定qi是在各位置删除元素的概率，且：q1=q2=…=qn=1/n 则删除一个元素时移动元素的平均值是: n Edl=∑qi(n-i)=1/n*((n-1)+…+1+0)=(n-1)/2 i=1注：算法时间复杂度为O(n) 顺序结构的评价优点： 随机存取结构，存取任何元素的时间都是一个常数，速度快 结构简单，元素逻辑相邻也物理相邻 不适用指针，节省存储空间 缺点： 插入和删除元素需要移动大量元素，消耗时间 需要一个连续的存储空间 插入元素可能发生“溢出” 自由区中的存储空间不能被其他数据占用/共享 线性表的链式存储结构单链表1.单链表的一般形式 不带表头结点的单链表：在首节点前无头结点的单链表 带表头结点的单链表：在首节点有无头结点的单链表 1.头结点：在单链表的第一个结点之前附设的一个结点。头结点的数据域可以不存储任何信息，其指针域存储指向第一个结点的指针（即第一个元素结点的存储位置）。作用是使所有链表（包括空表）的头指针非空，并使对单链表的插入、删除操作不需要区分是否为空表或是否在第一个位置进行，从而与其他位置的插入、删除操作一致。[头结点的指针域指向首节点的指针从而使此指针非空] 2.链表的结点结构：┌───┬───┐│data │next │└───┴───┘data域—存放结点值的数据域next域—存放结点的直接后继的地址（位置）的指针域（链域）]]></content>
      <categories>
        <category>Computer Science</category>
        <category>Data Structure</category>
      </categories>
      <tags>
        <tag>Computer Science</tag>
        <tag>Data Structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[社交情感分析与机器学习]]></title>
    <url>%2F2017%2F09%2F18%2F%E7%A4%BE%E4%BA%A4%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E5%88%9D%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[摘要：关于社交媒体中的情感分析的关键技术、发展趋势和历史（btw：整理自糖姐姐的blog） 一、Tecent AI LAB CV + ML + Speech + NLP（更难，还需10-20年） 基础研究 + 产品（游戏，社交，音乐视频） + 开放（ai open platform） 爬取data——数据清洗（抽取关系、事件等，整理和处理）——可视化 个性化推荐：需要和文本相关，词的标签、分类；如何构建起用户兴趣； 舆情分析 情感分析deep emotion parsing 二、社交媒体中的情感分析情感分析发展的7项关键技术 情感分类 情感元素抽取 跨领域情感分析 个性化情感分析 隐式情感分析 情感原因发现 情感生成 情感分类 1.基于传统ML方法的情感分类 2.基于DL的情感分类 3.面向评价对象的情感分类 输入seq或篇章，输出情感类别 基于层次化神经网络的篇章建模 篇章表示；篇章语义组合；句子表示；句子语义组合；词语表示 传统做的话能达到80%-&gt;rnn在依存树基础上得到根节点的情感分类（基于句法分析）-&gt;cnn直接做全局和局部的上下文信息且速度快-&gt;得到情感词典、得到语言学约束加到损失函数中 基于DL的情感分类文档级情感分析任务 层级关系：词到句子，句子到篇章 情感抽取3个任务： 情感词语抽取 评价对象抽取 评价搭配抽取（二元组对应） 情感词典 评价对象搭配：&lt;评价对象，评价表达&gt; 跨领域情感分析 从源领域到目标领域，相当于迁移学习 问题：评价对象不同，评价表达不同，情感表达极性不同（程度不同） 个性化情感分析基于用户用词习惯：不同群体情感倾向，主观想法和个人身份立场 基于认知理论的想法： 用户画像（属性维度，性格维度，行为维度）； 把用户和产品用低维向量和矩阵表示，融入已有神经网络框架，应用到篇章级文本情感分类任务 基于网络结构的方法：社交媒体上用户之间的连接关系、相同情感倾向性 隐式情感分析 中文情感表达方式复杂 事实型隐式情感分析基于上下文的方法：挖掘句子外部信息；与句子内部信息相融合（基于图的融合算法） 基于特征+规则的意见挖掘模型： 隐含情感特征可从形容词获知 与情感词典中情感词贡献的得分信息 利用conj处理上下文相关的特征 利用两种直接依存关系来抽取名词产品特征 去除那些直接同时被褒义、编译情感词修饰的产品特征 修辞型反讽：大连理工林鸿飞的隐喻语料库 情感原因发现基于文本数据来源哈工大深圳徐睿峰 基于个体立场现有方法难以解决立场问题 基于群体立场民众情绪的自动归因（对焦点事件）、可能有子话题：比如沉船有人被救起 情感生成评论文本生成Affect-LM：基于LSTM的语言生成；情感类别信息/强度Attri2Sequence+Attention 情感回复生成：emotional chatting machine 情绪对话生成评测：NLPCC2017比赛 首届情绪对话生成评测 给定微博及用户指定情感，生成一条与该情感类型一致的回复。 训练数据：100万对&lt;微博,回复&gt; 情感分析6大趋势 从粗粒度到细粒度 从单领域到跨领域 从文本到社会媒体 从显示情感到隐式情感 从情感分类到情感原因 从情感分析到情感生成 总结历史20世界前：社会学为主：1967六度分割、1973weak tie、1995结构度 20世纪左右：物理学：hits、pagerank、smallworld、scale free 21世纪：计算机学：link prediction、network evolution：复杂化发展（加时间加地点）densification、social influnence分析 2009computational social science（giles） 社会计算学：很多节点和边组成的社交图 信息1.0：data：像是给一个query把文档排序（google是1.0重要代表） 大数据来了：it时代公司发展云，传统公司跳进来把数据存进去 信息2.0：数据➕用户，像是信息推荐（如今日头条是2.0时代重要代表） 信息3.0（未来）：融合+智能 socail network = info space + social space 数据语义 + 用户语义 = 知识 ——&gt; 智能 大数据需要知识，一定需要deep learning这个锄头去挖数据 以交互驱动 节点是用户，边是关系，以用户为核心和以边为核心两种建模方式 难点：社会理论融合到概率图模型上，不仅需要how，更需要why 应用之一：舆情未来：从单舆论场到多舆论场的融合（微博微信联合在一起）、从分析到预测（预测未来可能会发生什么，需要知识库推理）、综合人行为空间时间一起综合分析、从单语种到跨语种（全球化）、机器数据也是舆情、对重点事件做多维度分析（传播的指标） 舆情报告的智能化（ai写数据驱动的舆情报告）]]></content>
      <categories>
        <category>Artificial Intelligence</category>
        <category>Natural Language Processing</category>
      </categories>
      <tags>
        <tag>Artificial Intelligence</tag>
        <tag>Sentiment Analysis</tag>
        <tag>Natural Language Processing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AI来临，应试教育何去何从]]></title>
    <url>%2F2017%2F09%2F18%2FAI%E6%9D%A5%E4%B8%B4%EF%BC%8C%E5%BA%94%E8%AF%95%E6%95%99%E8%82%B2%E4%BD%95%E5%8E%BB%E4%BD%95%E4%BB%8E%2F</url>
    <content type="text"><![CDATA[摘要：AI的大数据训练与应试教育的“题海战术”在训练模式上具有高度的相似性，但AI由于具有更为强大的计算能力和数据处理能力，故具有人脑无法企及的天然优势。AI正将一步步替代高流程化和重复性的工作岗位，并将从就业市场倒逼教育转型。传统应试教育的固有弊端在AI时代下将会愈发凸显，转型势在必行。 AI学习的特点AI的主要构成：算法-大脑，算力-肌体，大数据-养分 AlphaGo可以看成是最新深度学习方法（算法）、最新超算体系（算力）、棋谱大数据的总和. AI在算力和大数据上已超越人脑，并可以高度理性应对人类的非理性 AI与人类学生的各维度对比 学生：AI勤奋、记忆力超群、响应极速 老师：AI的老师是最顶尖的精英 教材：AI的教材难度远超普通教育的教材 学校：AI的学校是世界顶尖的实验室 教育模式：AI是大数据训练算法，还可自我优化 教育成果：经充分训练的AI可迅速上岗，降低社会工作成本 AI的领域应用 金融、财会 医疗 新闻编辑 驾驶 翻译 客服 物流 应试教育的缺点 数学：缺少逻辑推理训练、抽象思维、原理背景介绍、知识点间的逻辑关系；而非简单解题能力 人文：强调死记硬背，缺少纵深、尺度、思辨 理：强调结论性，忽视问题的提出、过程的探究、实验设计的原始思路 大学本科教育：缺少融会贯通 威胁 对我国教育和人才培养结构形成巨大冲击 若仍以应试教育“死记硬背”“题海战术”来教育学生，则与AI相比“出才效率”“出品质量”低，达不到学校、家长、社会的期望值，还可能重挫学生自信心。 如何应对 对现行学科教育和教育理念坐整体的变革，走差异化、个性化的全面教育创新之路，发挥人区别于AI的优势 重视逻辑思维、抽象思维的培养，重视将实际问题转化为数学语言的能力 引导在“问题层面”的求索 重视兴趣引导和培养 AI基础教育，让孩子具备与AI携手工作的能力]]></content>
      <categories>
        <category>Artificial Intelligence</category>
        <category>AI Application</category>
      </categories>
      <tags>
        <tag>Thoughts</tag>
        <tag>Artificial Intelligence</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何打造完美的仪式感]]></title>
    <url>%2F2017%2F09%2F17%2F%E5%A6%82%E4%BD%95%E6%89%93%E9%80%A0%E5%AE%8C%E7%BE%8E%E7%9A%84%E4%BB%AA%E5%BC%8F%E6%84%9F%2F</url>
    <content type="text"><![CDATA[摘要：《小王子》中狐狸说，“（仪式）它就是使某一天与其他日子不同，使某一时刻与其他时刻不同。”仪式感是用庄重认真的态度去对待生活里看似无趣的事情，设计关键是承载文化内涵。好的仪式设计具备专属性、独特性象征性，仪式传播需要给人带来卓越的体验、情感的共鸣，以及延续品牌价值。 什么是仪式感仪式感有个很官方的解释，就是用庄重认真的态度去对待生活里看似无趣的事情。 记得《小王子》中狐狸说，“（仪式）它就是使某一天与其他日子不同，使某一时刻与其他时刻不同。”仪式感是对生活的重视，把一件单调普通的事变得不那么单调。 一个人的早餐可以在地铁上嚼包子解决，也可以早起个十几二十分钟慢慢准备，铺上你最爱的蓝白格子餐垫，精心挑选的餐盘里每一个水果的摆放都恰到好处。给自己一点仪式感，让生存变成生活，不要浪费点滴。就如《蒂凡尼的早餐》里，赫本身着小黑裙，优雅地吃着可颂的样子，美极了。 什么样的仪式感构造仪式感以及使仪式感有完美的二次传播效果的关键是使仪式感承载特殊的文化内涵。 产品是载体，情感和价值观是启动载体的纽带。 价格 = 产品 + 价值 + 价值观 台湾可净手更可净心得到肥皂：坚决不用塑料剂，静置60天，放心经2.TOMS:ONE FOR ONE 仪式设计的三个要点： 专属产品服务EPO：定制化，重视消费体验，专属感 独特感官体验RSO：意义比夸张的表现形式重要；五感；足够惊喜 象征物Icon：人物，建筑物，地理景观，物品 仪式传播的效果： 卓越salience：体验 共鸣responce：拨动心弦 外延价值residual value：延续对品牌价值有提高价值的影响]]></content>
      <categories>
        <category>Communication Science</category>
      </categories>
      <tags>
        <tag>Communication Science</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构|Chapter1：绪论]]></title>
    <url>%2F2017%2F09%2F17%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-Chapter1%EF%BC%9A%E7%BB%AA%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[摘要：今天《数据结构》开课，本文是第一章绪论的笔记。 什么是数据结构 计算机中存储和组织数据的方式，意味着接口或封装：一个数据结构可被视为两个函数之间的接口，或者是由数据类型联合组成的存储内容的访问方法封装。 相互之间存在一种或多种特定关系的数据元素的集合，数据元素间的关系为结构。 程序设计 = 数据结构 + 算法 基本概念和术语 数据data：所有能输入到计算机中并被计算机程序加工、处理的符号的总称;如整数、实数、字符串、图像、声音等。 数据元素data element：数据的基本单位（元素、记录、结点、顶点），在计算机程序中通常作为一个整体进行考虑和处理。 数据项data item：一个数据元素可由若干个数据项组成，数据项是数据的不可分割的最小单位。如：姓名、年龄等；一个数据元素可由一个或多个数据项组成。如: (姓名、年龄) 数据对象data object：由性质/类型相同的数据元素组成的集合；是数据的一个子集。 数据结构data structure：相互之间存在一种或多种特定关系的数据元素的集合。数据元素间的关系为结构。四类基本结构：集合（元素同属于一个集合的关系），线性结构（一个对一个的关系），树状结构（一个对多个的关系）和图状结构（多个对多个的关系）。 数据的逻辑结构：各元素间的逻辑关系 物理结构：指数据的逻辑结构在计算机中的存储形式；逻辑结构面向问题，物理结构面向计算机，其基本目标是将数据及其逻辑关系存储到计算机的内存中。 数据的存储结构：数据结构在计算机存储器中的映象(mapping)；存储结构也称为：存储表示,物理结构,物理表示。 结点/元素：计算机中由若干位bit组成的一个位串表示一个元素，这个位串即元素或结点（node）。 1.顺序存储结构：用元素在存储器中的相对位置来表示数据元素之间的逻辑关系。 .非顺序/链式存储结构：用指示元素存储地址的指针表示数据元素间的逻辑关系；把数据元素存放在任意的存储单元里，这组存储单元可以是连续的也可以是不连续的。数据元素的存储关系不反映其逻辑关系，用指针存放数据元素的地址，我们通过地址可以找到相关联数据元素的位置。如： 数据类型data type：是一个值的集合和定义在这个值上的一组操作的总称。 抽象数据类型(Abstract Data Type):与计算机的实现无关的数据类型，指一个数学模型以及定义在该模型上的一组操作，其定义只取决于它的一组逻辑特性，与其在计算机内部如何表示和实现无关。一个抽象数据类型定义了一个数据对象、数据对象中各数据元素之间的关系及对数据元素的操作。 抽象数据类型（Abstract Data Type, ADT）可用三元组表示（D，S，T）：ADT抽象数据类型名{ 数据对象：（数据对象的定义） 数据关系：（数据关系的定义） 基本操作：（基本操作的定义） ​ 基本操作的定义格式：基本操作名（参数表） 初始条件：（初始条件描述） 操作结果：（操作结果描述） 数据结构的层次： C语言核心子集 预定义变量和类型：#define typedef表示数据结构，元素类型为ElemType 基本操作语句 赋值 选择：if …else; switch…case…default 循环：for;while;do…while 结束：return (函数结束语句)；break（case结束）；exit（异常结束） 输入输出：scanf([格式串]，变量1，…，变量n)；printf 注释：// 基本函数：max;min;abs;floor;ceil;eof;eoln 逻辑运算约定：&amp;&amp;；|| 算法Algorithm和算法分析定义：对特定问题求解步骤地描述。 算法的描述工具：自然语言；程序设计语言（C）；流程图；伪码语言；类C； 算法的时间复杂度：算法中基本操作重复执行的次数的总和 语句频度：所有语句的执行次数f(n) 时间复杂度：T(n)=O(f(n)) 根据时间复杂度衡量算法效率的前提是：针对同一问题 冒泡排序的C语言算法 算法的存储空间需求：空间复杂度S(n) = O(f(n)); 算法改进]]></content>
      <categories>
        <category>Computer Science</category>
        <category>Data Structure</category>
      </categories>
      <tags>
        <tag>Computer Science</tag>
        <tag>Data Structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并行计算与SIMD]]></title>
    <url>%2F2017%2F09%2F16%2F%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E4%B8%8ESIMD%2F</url>
    <content type="text"><![CDATA[摘要：2017-9-15 苟兄关于SIMD的分享，次日查阅资料并整理成记录。并行计算是为了解决大批量数据的处理问题，使用时间并行或空间并行的方式实现数据的并行处理。而SIMD是通过采用单控制器控制多处理器从而实现空间并行的技术。流水线化是SIMD的重要思想，但其过程中可能会发生分支跳转问题，分支预测可通过预测分支是否会跳转从而较好地解决这一问题。 一、并行计算parallel computing 许多指令同时进行的模式，化整为零以并发方式解决。 分类 时间并行：流水线技术； 空间并行：使用多处理器执行并发计算； 数据并行：化大为小 并行算法 算法设计的最常用方法：PCAM（划分-通信-组合-映射） 划分：化一为多，让各处理器去同时执行 通信：分析执行过程中要交换的数据和任务的协调情况 组合：将小问题组合解决以提高性能和减少任务开销 映射：将任务分配到每个处理器上 二、SIMD（single instruction multiple media） 实现数据级并行的技术 单指令多数据流；数据级别的并行；无需多线程配合；可与多线程配合； 单指令流多数据流：是一种采用一个控制器来控制多个处理器，同时对一组数据（又称“数据向量”）中的每一个分别执行相同的操作从而实现空间上的并行性的技术。 关键：在1条单独的指令中同时执行多个运算操作，以增加处理器的吞吐量。 特别适合于多媒体应用等数据密集型运算 pipelining 流水线化 延时：单条指令从开始执行到执行结束所花的时钟周期数 吞吐量（CPI：cycles per instruction）：平摊到每条指令所花费的时间 优化方法：解除依赖，减少代码体积 形象举例：洗衣服的步骤通：洗第1件-烘干-洗第2件-烘干…并行高效(流水线化)：洗第1件-洗第2件+烘干第1件-洗第2件+烘干第2件… 以加法指令的处理流程为例说明：指令单数据流SISD：对加法指令译码——&gt;单处理器访问主存、取得第1个操作数——&gt;再访问主存、取得第2个操作数——&gt;求和运算；IMD：指令译码——&gt;几个处理器同时访问主存、一次性获取所有操作数——&gt;求和运算 optimization 浮点转整型计算；尽量采用单精度浮点；（浮点数的表示方法,浮点数分离，消耗性能） 精简指令，减小代码体积（乱序执行-并行执行） 内存对齐 why：CPU按双字、字、字节访问存储内存，并通过总线进行传输，若未经一定规则的对齐，CPU的访址操作与总线的传输操作将会异常复杂，故编译器中都会对内存进行自动对齐。 sizeof：判断数据类型或表达式长度的关键字；字节数的计算在程序编译时即进行； 类型长度：如C++中使用sizeof得到的结果； 数据成员对齐：struct首地址是struct内第一个数据成员的地址； 数据成员对齐规则：在第一个成员之后，每个成员距离struct首地址的距离 offset， 都是struct内成员自身长度（sizeof） 与 #pragma pack(n)中的n的最小值的整数倍，如果未经对齐时不满足这个规则，在对齐时就会在这个成员前填充空子节以使其达到数据成员对齐。 整体对齐：与数据对齐相似但不是完全相同，如果数据对齐完成时struct的大小不是 struct内成员自身长度最大值（sizeof） 与 #pragma pack(n)中的n的最小值的整数倍。就要在struct的最后添加空字节直到对齐。 AoS or SoA？ SOA（Structure of arrays）数组的结构与AOS（Array of structures）结构的数组，是面向数据和面向对象设计的区别之一。 在需要高频率（如渲染循环中）访问数据的时候，一般情况下SOA的效率高于AOS，因为将需要频繁访问的数据连续存放会大大提高访问速度。虽然AOS的结构可能更适合面向对象设计，但是在高度依赖效率的地方应该使用SOA。 分支预测策略 why：流水线技术处理分支指令会遇到根据判定条件不同可能产生跳转从而打断流水线指令的处理的问题。分支预测即为解决此问题诞生，预测分支是否会跳转。 猜对/猜错 静态分支预测：简单：任选一条分支，平均命中率50%；精确：根据原先运行的结果进行统计从而尝试预测分支是否会跳转。 jxx指令：向上跳/向下跳 内存 缓存 都是2的幂次方 缓存：内存中的虚映射；缓存读取快； 为什么最好填L1/L2缓存的一半：避免数据互相挤兑的问题（内存震荡/摇摆）；]]></content>
      <categories>
        <category>Computer Science</category>
        <category>Distributed Computing</category>
      </categories>
      <tags>
        <tag>Computer Science</tag>
        <tag>Distributed Computing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[纸媒的出路：向死而生？]]></title>
    <url>%2F2017%2F09%2F15%2F%E7%BA%B8%E5%AA%92%E7%9A%84%E5%87%BA%E8%B7%AF%EF%BC%9A%E5%90%91%E6%AD%BB%E8%80%8C%E7%94%9F%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[摘要：本文探讨了现如今纸媒的发展现状、未来预测及转型策略。 什么是纸媒纸媒，是媒介，是广告的承载工具。包括：报纸、杂志、期刊、商品说明书、火柴盒、包装纸等各类印刷出版物。 纸媒的现状 行业：寒冬 新式媒体如火如荼，以纸为载体的传统媒体已经夕阳西下 数据：报业广告刊登额断崖式下滑 据广电总局发布的《2015年新闻出版产业分析报告》显示，去年，纸媒营收额平均降低了10.3%，利润总额降低了53.2%。据梅花网广告监测数据显示，2016年第三季度的报刊广告投放率在去年的基础上又下跌了40%，而且还在持续下滑。/传统广告市场规模 :2014年传统广告市场规模首次出现负增长，依靠新媒体广告的强势表现，整体广告市场还保持增长。2015年，整体广告市场表现依然呈颓废趋势，首次出现负增长，跌幅为2.9%。其中，传统广告跌幅达7.2%。2016年上半年的中国杂志广告市场较去年同期大跌29.4%，由于中国经济持续放缓及奢侈品及零售行业的本地消费疲软，导致国内的传统时尚杂志广告收入持续下降。 个体：或抱残守缺，或消失死域，或向死而生 分举3例阐述3种现状：京华时报；东方早报——澎湃新闻；《壹读》——号称“中国最具互联网思维的平媒” 早在2014年7月份，《东方早报》就已然孵化澎湃新闻网，而采编班底也出自东早，堪称网络版《东方早报》。含着金汤匙出生的澎湃在褪去纸质媒介的外壳，以一系列深度、严肃的新闻报道重新进入大众视野。而东早的保留节目：时评、文化等栏目仍旧原汁原味。 《壹读》宣布不再出版纸质杂志，集中全力运营新媒体，现在在微信公号和流媒体视频中，已然佼佼者。而国际上的纽约时报、华尔街日报等都是传统媒体向新媒体转型的典范代表。 “两微一端”的媒介融合：74%以上的报纸均在新浪微博开通了官方微博，其中，其中，有116家报纸的官方微博粉丝量超过了100万，394家报纸在新浪微博的粉丝总量累计达到5.5亿。以《人民日报》为首的党报在新浪微博的粉丝总量超过1.2亿人次（不包括“人民微博”粉丝量）。新兴媒体赋予传统报纸不同的传播介质，也带来了更快速、更广泛的传播效果。 纸媒衰落的原因用户 互动性：纸媒无差异单向传播信息，忽略个性化需求，受众与媒体间缺乏良性互动；纸媒的受众是被动接受信息，新媒双向互动传播，受众是主动寻找信息；无法满足差异化阅读需求 时效性：海量的信息量；实时传播；手机和电脑带来信息获取的便利性 没有培养读者群的概念，读者流失严重：纸媒普遍缺乏培养读者群和用户忠实度的概念，加之互联网的各种冲击，读者就更难以存留。目前中国的订报人群，平均年龄都在60岁以上，而且他们每天也只是翻翻家装、教育、健康等版面。而年轻人的信息来源主要都源自于互联网，就更不会去看报纸，就连五六线城市中的中年人都普遍从移动终端获取资讯。 成本与营收 传播成本：纸媒“人海战术”，人工成本投入大，编辑和印刷成本大；新媒无需加工任何资源即可直接数字化传播； 免费性：新媒以免费侵占市场，攫取用户注意力 广告营收愈发困难 内容 信息内容表现力：新媒体的传播方式更为丰富，文字、图像、声音等多媒体化成为一种趋势，“图文并茂”“声画结合”。 利益格局 利益格局难以变革：内部制度僵化，固有的利益格局难以打破，负责传统业务者是既得利益者，新媒体业务的收入永远比不上做传统业务的人；传统媒体决策者对新技术的地位认识不足，无法建立产品思维（不懂得在互联网时代中UI的重要性，更不懂为什么要研究用户习惯，纸媒决策者们无法适应互联网时代所带来的变化）；纸媒中的人对改变往往怀着恐惧与排斥的态度，宁愿抱残守缺也不愿离开舒适区。 纸媒的未来：向死而生载体或死，内容永存 载体或死: 大众对信息的需求从未停止，因此媒介永不会过时。只不过随着信息时代的到来，载体注定改变。就算不死也是夕阳西下的产业。不能和时代、潮流对抗，就像当年柯达舍不得放下胶片相机、发展数码相机一样，到头来还是自己受伤。 内容永存：纸媒作为一种媒体形态，无论是出现纸上还是网上，都不会改变其本质属性——固定的内容生产方式；介质与内容可分离。 必须转型，壮士断腕以求破茧重生；茧是旧载体，无破不立。 纸死了，“媒”依然会永续存在。有价值的信息永远都有市场，只是载体在改变。未来信息的传播模式将更加多元化。讨论纸媒是不是会死，已经没有必要了，当信息创造者和优质信息都向互联网集中，纸媒必将衰落。被整合和转型变革就成了求生的必经之路。 破茧重生：纸媒转型策略1.全媒体：打通形式界限 打通形式界限，纸媒、网站、客户端、APP均开设专栏，对外一体服务。 内容付费产品——精品付费阅读：在内容付费大行其道的当下，互联网新媒体在生产力上并不占优势，纸媒所拥有的专业内容生产者，逻辑严密的生产环节，都更能把控内容质量。而分答珠玉在前，知乎问答、逻辑思维得到、微博问答等内容付费产品在后，这些互联网产品实际上已经完成了「优质内容值得付费」的概念，又或者说，优质内容仍旧是稀缺资源，而这又正是纸媒优势。 “全媒体”指媒介信息传播采用文字、声音、影像、动画、网页等多种媒体表现手段（多媒体），利用广播、电视、音像、电影、出版、报纸、杂志、网站等不同媒介形态（业务融合），通过融合的广电网络、电信网络以及互联网络进行传播(三网融合)，最终实现用户以电视、电脑、手机等多种终端均可完成信息的融合接收（三屏合一），实现任何人、任何时间、任何地点、以任何终端获得任何想要的信息（5W）。 2.基因重组 != 形式转场 打破形式界限不是非平移式转型，要用心揣摩用户和市场。从受众需求、内容理念、组织结构、生产和发行体系的变革，而不是一个简单的形式，一定是基因的重组，而不只是局部的创新。内容架构师而非内容搬运工 3.内容：坚持生产优质内容，巩固行业影响力 相对于新媒体，纸媒的核心优势是人才资源和专业内容生产体系：一流的编辑、作者团队，深度的调查报道、良好的新闻媒介素养，让传统媒体人在面对新媒体转型时，能迅速意识到新媒体传播的核心价值所在。专业的稀缺性让传统媒体人价值得以被凸显，尤其是在信息过剩时代，纸媒的安全边际逐渐付出水面。 文字内容的原创性和权威性：网络信息良莠不齐，身份的难以追查性 互联网零工经济带来的良莠不一的从业者们，这其中不可否认有优秀头部创作者的隐现，但湮没在重复、过时、过剩的信息之中，犹如零星点缀，本质仍旧混沌。而在流量考核的压力下，门户网络编辑“以效率取代质量，以夺人眼球罔顾新闻伦理”的现象常有发生。这样的标题党初期或许能够俘获一定量的读者，但终究不是长久之计。 生产内容的人，依然需要大批的传统媒体从业者。比如，微信的新媒体公号中，很多“100000+”点击量的文章，很多都是曾经的纸媒从业者打造的。此外，很多目前移动新闻平台的自媒体板块上，订阅量高、颇受欢迎的自媒体也大多是由传统媒体人业余打理或专事经营。 危机-人才的流失：传统媒体持续失血，大批媒体精英毅然离开传统媒体，独立单干。这也加剧了很多纸媒的衰落，甚至逐渐成为恶性循环。 4.重视用户需求分析 以用户体验为中心，放低媒体架子。 面向消费者而非读者：传统媒体把受众称为读者，则媒体和受众间只有一层“信息消费”的关系；但是当媒体拥有大量的发行以及一定忠诚的受众的时候，媒体需要做的是去发掘更多的需求，并能够通过产品的创新去满足需求消费者。传统媒体通常以一种“满足需求”的线性思维方式提供内容，而互联网思维所给的启示是“创造需求”。纸媒需要考虑的是如何以媒体作为介质，来满足更多的用户需求，提供超越于信息内容的服务和体验。 数据思维，精准营销：用大数据分析海量用户习惯，实现广告业务的精准投放和营销 5.整合：媒体内部整合 + 资本整合 媒体内部整合合并：如去年10月，上海最大两大报业集团，解放报业集团和文新报业集团完成合并，今年6月的时候曾经传出湖南报业迎来大整合，老牌的《潇湘晨报》《法制周报》要划归湖南日报报业集团。 资本整合。这种情况在国外比较普遍。2013年，《华盛顿邮报》被卖给亚马逊;就在同一个月，《纽约时报》旗下的《波士顿环球报》也被卖掉。在法国，我们熟知的LV就收购了知名财经报纸《回声报》;英国的“百年财经大报”《金融时报》早前也被出售给了日本的《日本经济新闻》。在国内，因为媒体的性质，一般全部卖掉的情况不会出现，但是投资入股的案例不断涌现。阿里这两年布局文化产业，投资12亿元人民币参股SMG旗下的第一财经。 6.盈利 分平台建构：在传统纸质媒体赢利能力衰退，新媒体渠道又缺乏足够赢利能力的情况下，报业打造新型主流媒体的努力必须考虑到分平台建构，一个报业集团有多个平台，每个平台在集团中的位置不同，所起的作用、角色也不同。有些平台是很难赢利的，比如微博、微信公众号、移动终端等新媒体；但它们在媒体融合理念的指引下，能够提高整个集团的影响力和品牌价值，或者为其他单元的影响力服务。而有些平台则是可以实现赢利的，比如报纸、网站、游戏平台、房地产、物业、财务投资会展、户外媒体。因此，在当前的媒体融合发展中，就不一定需要每个平台都有经济指标的要求，但是需要两个平台共同发力，形成合力，共同助推新型主流媒体集团舆论与赢利职能的实现。 7.VR内容：体验式新闻 VR内容：利用VR的沉浸感打造体验式新闻；私人虚拟图书馆；交互自由阅读；因其削弱了空间的局限性故或可创新广告形式、增加广告收入 VR为纸媒带来的改变集中体现在两个方面，首先体验式新闻产生，利用VR技术的沉浸感，会将观众天然地、自然地带入场景中，带来新的、更客观真实、更富感染力冲击力的内容体验。相比于视频，VR 媒介增加了空间的维度，能够传达更丰富的信息内容。 旅游、时尚、购物、新闻、体育等领域的VR内容充满想象。 读者可以通过VR进入书中的情境，甚至与书中的人、物完成简单的互动。在VR的加成下，书本已不再是摊开的平面纸张，变成了清晰立体的情景。VR与众不同的交互方式无疑会使阅读本身变得更生动也更有趣。 如：HTC推出了全球首个基于Vivepaper创新技术的增强式虚拟阅读体验。Vivepaper技术把VR和AR整合到了同一个硬件平台，并将VR阅读与融媒体相结合，为读者提供了360度全景照片、视频、3D模型、2D图文及音频等互动内容。目前，该技术可以将大量现存的书籍、杂志、报纸、期刊等现实内容带到VR世界中。 其VR应用商店Viveport在30个国家上线，除了游戏，它也提供教育、设计、艺术、社交、视频、音乐、体育、健康、时尚、旅游、新闻、购物及创意工具等垂直领域内容。希望用户能体验到那些由业内顶尖的内容创作者发布的VR App。汪丛青表示，未来Vivepaper也可以在Viveport上获取。 VR也将影响广告收入。受制于纸媒的版面，广告和内容总处在抢位置的冲突中。而VR将不再局限于空间，或许能给纸媒带来更多营收。新的广告形式也容易引起读者兴趣，或者吸引读者进行互动。 总结 纸媒需往数字媒体与新媒体拓展，构建矩阵化的全媒体竞争体系，以人工智能、大数据等信息手段巩固城防；如要突围，则须从电商、投资等更多领域出发，切入交易本身。]]></content>
      <categories>
        <category>Communication Science</category>
        <category>Medium Study</category>
      </categories>
      <tags>
        <tag>Medium Study</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[观点：深度学习也许远非人工智能的未来]]></title>
    <url>%2F2017%2F09%2F12%2F%E8%A7%82%E7%82%B9%EF%BC%9A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%9F%E8%AE%B8%E8%BF%9C%E9%9D%9E%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%9A%84%E6%9C%AA%E6%9D%A5%2F</url>
    <content type="text"><![CDATA[摘要：如今每个人都在学习或打算学习深度学习，但其实深度学习只是人工智能-机器学习领域的极小一部分。深度学习本身存在的“只会学习所有数据，不会否定任何数据”的缺陷会导致坏数据样本引发的偏见偏误，使其很有可能被新一代的人工智能科技所取代，可以说深度学习并不是人类可创造的人工智能科技的终点。 深度学习的火热是世界有目共睹的，但绝大多数人仿佛迷失在这种过热中，忽略了深度学习其实只占机器学习领域的1%，而机器学习又只占人工智能领域的1%，余下的99%则被用来处理实践中的绝大多数任务。 人工智能领域存在着“深度误传”。深度学习绝不是人工智能的同义词，深度学习专家也无法与人工智能专家划上等号。大众误以为所有人工智能的突破都由深度学习实现，可能部分是由于Google、FB等公司宣传的最多的人工智能工具主要是深度学习。可事实远非如此。媒体暗示AlphaGo的成功全部归功于深度学习，但其实它是蒙特卡洛树搜索+深度学习。 深度学习的致命缺陷在于它只是简单地相信训练数据，而无法辨别训练数据本身的真与假、善与恶、现实与想象、公平与非公。深度学习会学习并模仿最具缺陷的逻辑，包括恐怖主义。其决策会比训练数据的平均样本包含更深刻的偏见，如种族歧视、性别歧视。而且深度学习模型是机器学习算法中最难测试、检测、控制和调整的。这个问题很难解决，这引起很多深度学习实验突然取消，从聊天机器人变得纳粹化、充满仇恨，到美图软件中给黑人照片美白。你无法通过训练后打补丁来修复一个带有偏见、种族歧视的深度学习模型，而必须使用全新的、完全公正的、稀有的数据对该神经网络进行重新训练。总之，深度学习只会学习和模仿所有数据，但不会否定任何数据。 深度学习有一天会过时，当可解释的AI流行，深度学习将会被抛弃。 深度学习也缺乏绝对的稳定性，至少相比于人类。对抗样本的使用可以轻易使深度学习系统出错，比如为一张猫图加上一些特殊的噪点使机器把它误认为是不相关的东西。更危险的是，如果街边的路牌被对抗样本黑掉了，自动驾驶将不再安全。所以，新一代的人工智能系统必须克服这些问题，同时它也将取代深度学习。 我们可以选择等待下一代人工智能系统的兴起，直接跳过深度学习1.0时代。同时最好深入了解人工智能及机器学习领域的知识，而不仅仅是深度学习。 愈火热，愈需要冷静思考。]]></content>
      <categories>
        <category>Artificial Intelligence</category>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Artificial Intelligence</tag>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[情感化设计之移动动效设计]]></title>
    <url>%2F2017%2F09%2F11%2F%E6%83%85%E6%84%9F%E5%8C%96%E8%AE%BE%E8%AE%A1%E4%B9%8B%E7%A7%BB%E5%8A%A8%E5%8A%A8%E6%95%88%E8%AE%BE%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[情感化设计三大载体：界面，交互，动效。本文阐述其三——动效的作用及其设计要点，并以等待的动效设计为例辅助说明。 动效作用过渡流畅：通过界面及其元素的出现和消失，以及大小、位置和透明度的变化，使用户和产品的交互过程更流畅。 高效反馈：通过动效让用户了解程序当前状态，同时对用户操作（平移、放大、缩小、删除）做出及时反馈。 引导操作：通过动效对功能的方向、位置、唤出操作、路径等进行暗示和指导，以便用户在有限的移动屏幕内发现更多功能。 层级展现：通过焦点缩放、覆盖、滑出等动效帮助用户构建空间感受。就像iOS7一样，通过动效上来构建了整个系统的空间结构。 增强操纵：一些动效通过动效对现实世界的模拟并且不需要任何提示，迎合用户的意识认知。使产品的交互方式更接近真实世界。用户通过对现实世界的认知来理解动效，增强了用户对应用的操纵感和带入感。 创新体验：在可用性良好的前提下，通过细节设计和交互方式创新为产品增加亮点，可以带来更惊喜的体验和表达产品的气质与态度（X格）。 动效设计要点有度：不要让设计使用户忽略你本来想表达度内容和作用（不增加操作；不干扰用户；不超过1秒） 考量实现，平衡设备和方案：不卡，不跳，不闪；做出优秀且绝大多数设备流畅运行度动效 自然，符合现实运动规律 整体编排，循序教育：1个好的欢迎界面+0-2个细节动效+不多于6个说明动效。 举例：等待的设计界面先行：在业务动作未真实完成之前，界面先进入到完成/进行状态，以弱化用户的等待和烦躁。进度条。 资源代替：比如先加载出低质量的图片，再加载清晰资源。 趣味吸引：通过有趣的内容分散用户注意力。]]></content>
      <categories>
        <category>Product Manager</category>
        <category>Design</category>
      </categories>
      <tags>
        <tag>UI Design</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android Design Standards]]></title>
    <url>%2F2017%2F09%2F11%2FAndroid-Design-Standards%2F</url>
    <content type="text"><![CDATA[本文从设计原则、风格、界面、模式、控件五个维度阐释安卓设计规范。通篇阅读下来可建立初步的安卓设计概念。 1.设计原则A.着迷 惊喜（漂亮的界面和悦耳的音效，提高易用性和掌控强大功能的感觉） 真实的对象比菜单和按钮更有趣（人性化操作） 个性（个性化，提供合理漂亮的默认样式的同时尽可能提供个性化功能） 了解用户的喜好：记住用户喜好，不要重复询问 B.简洁 简洁的句子：got Google？ 一图胜千言：尽量用图片去解释想法 只展示用户需要的：不要给用户展示太多选择，将不重要的选项隐藏起来并让用户慢慢学习 让用户知道ta在哪：通过页面样式的区别和页面切换的效果来告诉用户页面的位置 不要弄丢用户的东西：记住用户创造的内容，并使之随时随地获取； 颜行一致：不要给用户产生认知混淆 只在重要时刻打断用户 C.惊奇 用户使用引导 用户永远没有错：提示用户做出改正时要保持和蔼和耐心；如果能系统自己解决就不要麻烦用户； 给予哪怕微小的鼓励：反馈 帮用户完成复杂的事情：通过步骤分解 迅速完成最重要的事：功能展示的优先级 2.安卓用户界面：结构 主屏幕+所有应用+最近应用； 系统栏：状态栏，导航栏 通知抽屉：通知抽屉显示一些简要的信息，用户在任何时候都可以从状态栏访问它们。它收纳应用升级、提醒以及其他一些重要但不至于直接打断用户的信息。向下滑动状态栏可以打开通知抽屉。点击消息将会打开相应的应用。 通用的应用ui：操作栏，导航抽屉，内容区域； 3.风格A.设备与显示 灵活（尺寸大小变化），优化布局，适用于各种设备（通过像素密度dpi实现） 如何开始为多种屏幕设计呢？一种方法是以一个基本的标准 (中等尺寸，MDPI) 开始，之后将其缩放到不同的尺寸。另一种方法是从最大的屏幕尺寸开始，之后为小屏幕去掉一些 UI 元素。 B.主题 主题是一种使得 Android 应用保持统一风格的机制。风格样式定义了各种构建用户界面所需要的视觉元素，包括颜色、高度、边界填充和字体大小。为了提升各种应用的统一性，Android 为你的应用提供了两种系统主题:浅色 Holo 主题深色 Holo 主题将这些主题应用于设计中将使得应用更好的和 Android 设计语言融合起来。 C.触摸反馈 视觉／触觉／听觉上的反馈 按钮的状态： 交流：当控件需要对复杂手势做出响应时，应帮用户了解该操作的结果；即通过反馈告诉用户操作会产生的结果（用户引导） 边界：边界反馈 D.度量单位和网格 按像素密度分类的类别有 LDPI、MDPI、HDPI 和 XHDPI。为不同的设备优化你的应用界面，为不同的像素密度提供不同的位图。 可触摸控件以48dp为基础单位：注意留白：界面元素之间的留白应该是8dp； E.字体 roboto字体：专为界面渲染和高分辨率屏幕设计； 默认字体颜色 Android 界面使用以下的色彩样式: textColorPrimary 和 textColorSecondary。在浅色主题中则使用 textColorPrimaryInverse 和 textColorSecondaryInverse。在设计框架中还包括了几种自带的触摸反馈效果。 字体的缩放 为不同控件引入字体大小上的反差有助于营造有序、易懂的排版效果。但在同一个界面中使用过多不同的字体大小则会造成混乱。Android 设计框架使用以下有限的几种字体大小: 用户可以在“设置”中调整整个系统的字体大小。为了支持这些辅助特性，字体的像素应当设计成与大小无关的，称之为 (sp)。 F.颜色 目的：强调信息，视觉对比效果； G.图标 图标在不同设备上的显示：提供不同尺寸的图标，dp; 在设计图标时，对于五种主流的像素密度（MDPI、HDPI、XHDPI、XXHDPI 和 XXXHDPI）应按照 2:3:4:6:8 的比例进行缩放。例如，一个启动图标的尺寸为48x48 dp，这表示在 MDPI 的屏幕上其实际尺寸应为 48x48 px，在 HDPI 的屏幕上其实际大小是 MDPI 的 1.5 倍 (72x72 px)，在 XDPI 的屏幕上其实际大小是 MDPI 的 2 倍 (96x96 px)，依此类推。 提示: 虽然 Android 也支持低像素密度 (LDPI) 的屏幕，但无需为此费神，系统会自动将 HDPI 尺寸的图标缩小到 1/2 进行匹配。 启动图标：启动图标在“主屏幕”和“所有应用”中代表你的应用。因为用户可以设置“主屏幕”的壁纸，所以要确保你的启动图标在任何背景上都清晰可见。 操作栏图标：图像按钮；手机操作栏图标的整体大小应该是3232dp，图形区域为2424dp； 颜色 颜色: #333333可用: 60% 的透明度禁用: 30% 的透明度 颜色: #FFFFFF可用: 80% 的透明度禁用: 30% 的透明度 小图标：在应用的主体区域中，使用小图标表示操作或者特定的状态。例如在 Gmail 应用中，每条信息都有一个星型图标用来标记“重要”。小图标尺寸：整体大小1615dp，图形区域1212dp；样式：中性，平面，简单，最好填充图标而非细线条勾勒；颜色：若图标可操作，则用和背景色形成对比的颜色 通知栏图标：大小2424dp，图形区域2222dp；样式：简单平面，和启动图标相似；颜色：必须白色； H.设计小技巧 尽可能使用矢量图；避免缩放损失； 使用更大的画布：为了更好的适配不同的像素密度，最好使用数倍于目标图标尺寸的画布。例如，启动图标在 MDPI、HDPI、XHDPI 和 XXHDPI 下的宽度为 48、72、96 和 144 px，使用 864x864 px 的画板可以降低缩放图标时的工作量。 缩放时，按需重绘位图图层： 如果需要放大的图标中包含位图图层，这些图层需要进行手动重绘，以便在更高的像素密度下获得更好的显示效果。例如，为 MDPI 所绘制的 60x60 px 的圆，在适配 HDPI 屏幕时需重绘成 90x90 px。 图标命名约定有助于图标文件的管理：图标类型 命名前缀 例图标 ic_ ic_star.png启动图标 ic_launcher ic_launcher_calendar.png菜单图标和操作栏图标 ic_menu ic_menu_archive.png状态栏图标 ic_stat_notify ic_stat_notify_msg.png标签选项卡图标 ic_tab ic_tab_recent.png对话框图标 ic_dialog ic_dialog_info.png 按像素密度对图标进行归档 删除最终版本中无关的元信息 I.自我标识 适当使用高对比度端配色强带哦 J.写作风格简短只告知用户嘴必要的信息，避免冗长的表述，尽可能缩短文本长度； 简明使用短词语／行为动词／简单名词；先说重要的事，一句话开头两个词应该表达出重要信息；仅说明必要信息； 友好使用缩写；使用第二人称对话；保持随意轻松的腔调但避免使用俚语； 标点符号句号: 如果toast、标签或通知消息等控件中只包含一句话，无需使用句号作为结尾。如果包含两句或更多，则每一句都需以句号结尾。 省略号: 使用 省略号 (…) 表示以下含义: 未完成的状态, 例如表示操作进行中 (“下载中…”) 或是表示文本未能完全显示。 菜单中需要进一步 UI 操作的条目 (例如 打印… or 分享…) 。 例外: 如果该条目的描述文字中已经对进一步的操作进行了一定的描述则无需添加省略号, 例如 页面内查找 或 选定日期。 4.模式A.应用结构典型的 Android 应用由顶层视图和详细信息/编辑视图组成。如果出现深度且复杂的层级结构，使用分类目录视图连接顶层和详细信息。 顶层屏幕首先展示内容；设计导航栏； 固定标签同时显示多个顶层视图，且必须保证用户可通过左右滑动在不同视图之间切换；适合使用固定标签的情景 应用需要频繁的切换视图。应用只有最多三个顶层视图。希望向用户强调还有其他功能视图存在。 下拉菜单适合使用下拉菜单 (Spinners) 的情景: 不希望标签栏占用过多的垂直空间。用户切换视图时，数据集不发生变化，或者数据集的类型一样。 导航抽屉从旁边划入；适合使用导航抽屉的情景: 不希望标签栏占用过多的垂直空间。应用有很多顶层视图。应用需要从低层视图直接切换到其他顶层视图。应用需要在多个没有直接联系的视图之间切换。应用可能会有较深的导航路径。 不要混合使用多种模式请仔细斟酌应用的顶层视图设计，不要混合使用多种不同的模式。例如使用了固定标签，就不要为应用添加导航抽屉，否则用户可能会觉得复杂且混乱 分类目录使用标签整合多个分类类别和数据视图；当分类相似时克采用滚动标签，不是很相似时用固定标签；优点：1.减少层层导航；2.使数据一直保持在用户关注的中心； 允许穿过多个层级端操作在当前页面使用户可完成部分下级页面端操作； 同时对多个项目进行操作详细信息布局 使详细信息视图之间的导航变得简单如果用户经常顺序浏览多个项目，那么应当让他们在详细信息视图中直接做到。考虑使用滑动视图 (swipe view) 或者其他替代方法实现这种操作。比如滑动视图； 在主页上显示有用的信息使用操作栏提供统一的导航体验。 通过水平导航和快捷方式压缩导航层次。 通过多选使用户可以操作多条数据。 使用滑动视图 (swipe views) 在不同的详细信息视图中导航。 B.导航返回键的功能1.品目之间端导航； 2.“返回”键可以关闭弹出窗口 (对话框或者弹出信息) 3.返回”键可以关闭上下文操作栏，还可以清除当前的选择4.“返回”键可以关闭屏幕键盘 (IME) 应用内的导航导航到有多个入口的屏幕； 屏幕中切换视图； 从主屏幕或者通知进入你的应用的深层次屏幕； 间接通知：当你的应用需要同时通知多条信息时，可以通过单条通知将用户带入一个列表屏幕。这个屏幕列出所有的事件，并且可以让用户直接进入应用。这种通知叫做间接通知，在间接通知的列表屏幕按“返回”键将回到开启通知之前的屏幕； 弹出通知：跳过通知抽屉，直接将通知呈现在用户面前。不要滥用这种通知，除非用户必须立刻对信息做出响应时，才使用弹出通知。 应用间导航应用之间有能力互相调用，用户可以直接从一个应用进入另一个应用。 安卓框架体系activity定义一个包含信息和用户能够执行的所有相关操作的屏幕。你的应用就是一些 activity 的集合，包括了你编写的 activity 和从其他应用中复用的 activity。 Task 用于表示用户完成一个任务需要执行的 activity 队列。一个 task 可以只使用来自同一个应用的 activity，或者也可以使用来自不同应用的 activity。 Intent 作为一种机制，表示一个应用希望其他应用帮助它完成一个操作。应用中的 activity 可以向系统告知自己可以响应哪些 intent。例如比较常见的 intent “分享”，用户安装的许多应用可能都可以响应这个 intent 完成操作。 C.操作栏操作栏的目的：突出重要的操作 (例如“新建”和“搜索”) 并且可以方便的使用。在应用内提供统一的导航和视图切换体验。将较少使用的功能收集到“更多操作”菜单中，减少界面上的杂乱布局。为你的应用提供一个展示其特点的空间。 基本布局1.应用图标+向上按钮； 2.视图控制（多个视图切换）； 3.操作按钮；4.更多操作（隐藏较少呗用到的操作） 判断按钮是否能进入操作栏的标准（满足任何一条即可）F — 常用 人们进入该屏幕，70% 是为了使用该功能吗？人们经常连续使用这些功能吗？完成这些操作多花一些时间很烦人吗？ I — 重要 这个功能很酷或者是你应用的卖点？这个功能操作起来困难吗？ T — 典型 在类似的应用中，这是不是最主要的功能？ 某些情况下，人们会不会对这个操作跑到更多功能菜单中而感到惊讶 上下文操作栏浮于操作栏之上的临时操作兰，放置特定的子任务，一般在项目选择和文字选择时出现； 用户可以: 通过触摸选择项目。在上下文操作栏中选择操作，并应用于所有已选项目。之后上下文操作栏自动消失。通过导航栏的“返回”按钮关闭上下文操作栏，也可以通过点击上下文操作栏的选择图标关闭它。关闭上下文操作栏的同时要取消所有的选择。 当设计操作栏时，考虑以下问题对于当前的任务，视图切换很重要吗？ 如果视图切换对你的应用很重要，那么使用标签或者下拉菜单。 哪些操作应当一直出现在操作栏上，哪些可以放在“更多操作”菜单中？ 按照 FIT 原则，考虑哪些操作放在操作栏上，哪些放在“更多操作”菜单中。如果操作栏中的图标太多了，使用底部的副操作栏。 哪些内容足够重要需要一直显示？ 有时一些上下文信息对于你的应用很重要。比如收件箱中未读信息数量或者正在播放的歌曲信息。仔细考虑操作栏的布局安排。 D.导航抽屉：可折叠关闭导航抽屉当导航抽屉打开时，用户可以通过以下方式关闭它 触摸导航抽屉以外的内容 从屏幕的任何位置向左滑动 (包括从屏幕的右边缘开始) 触摸应用操作栏的图标或者标题 按“返回”按钮；使用导航抽屉的情景：有超过三个顶层视图，在深层视图之间导航；当导航抽屉打开时，清理操作栏，移除上下文相关的操作按钮，并显示应用的名称。 当导航抽屉打开后，隐藏上下文操作栏 首次启动时提示导航抽屉当你的应用首次启动时，请打开导航抽屉，直到用户手动关闭它。这样确保用户了解如何使用导航抽屉，并且看到导航抽屉中有哪些项目。以后的应用启动可以不再自动打开导航抽屉。 给用户小提示当用户触摸屏幕左边缘的时候 (左边缘 20dp 之内的位置)，稍稍显示一点导航抽屉，提供有效的反馈，并且提示用户。 高亮如果当前视图是导航抽屉中的一项，打开导航抽屉时高亮该项目，如果不是则不要高亮任何项目。 样式导航抽屉的宽度可以根据应用的内容做出调整，但是请保持在 240dp 到 320dp 之间，其中每个项目的高度也不应低于 48dp。 E.多视图布局预先为不同的屏幕尺寸和方向设计好应用的布局。 仔细考虑在不同的屏幕方向时，何种复合视图的布局是应用最好的选择。 寻找各种可能，将应用的视图组合成复合视图。 确保在屏幕旋转后，仍然提供相同的功能。 F.滑动视图 详细信息视图的滑动切换 标签的滑动切换 G.全屏模式2种进入全屏模式的方式： 1.横屏模式：适用于不需要交互的应用，点击屏幕上的任何位置，调出系统栏。 2.沉浸模式：适用于需要频繁交互的应用，从屏幕的上/下边缘外向屏幕内滑动，调出隐藏的系统栏。 H.选择如果你的应用可以执行多选，请使用上下文操作栏 (CAB)。 长按手势只用于选择，不要显示古老的上下文菜单。 如果不支持多选，那么长按手势应当没有任何的效果。 按照你在操作栏上排列内容的标准排列上下文操作栏中的内容。 I.确认和提示确认是确保用户的确想要做出指定的操作。有些情况下，确认是以警告或者重要信息对话框的形式出现的，用户需要考虑再做出选择。 提示通过显示一些文字来告诉用户刚才的操作完成了。这样做可以避免后台操作带来的不确定性。有些情况下，提示可以提供一个撤销操作的选项。 一下下情况不需要提示：操作有其他形式的反馈或是可撤回的；凑走本身比较复杂，体现用户是深思熟虑过的； J.通知基本布局：所有通知的布局在基础上都包含: 发送通知的应用图标或者发送人的头像 通知标题和消息 时间戳 当主图标显示发送人头像时，在副图标位置显示应用图标 扩展布局：通过扩展布局显示消息的前几行或者图片的预览。 操作Android 支持在通知底部显示附加操作。通过这些操作，用户可以对通知直接执行常见的任务，而不用打开应用。这样可以加快操作，配合上滑出消失操作，使用户的通知抽屉体验更加顺滑。 可以放入通知中的操作有以下特点: 对于该通知重要、常用和典型的操作 时间紧迫的 不会与相邻的操作重复的 不要放置:模糊的和点击通知得到的效果一样的操作，例如阅读或者打开 个性化如果通知的内容来自于另一个用户 (比如一条消息)，放个头像在通知上。 记住把应用作为次要图标放在通知内，让用户知道是哪个应用发出的通知。 去正确的地方用户触摸通知后，打开你的应用并且将用户带到可以直接操作该通知内容的界面上。大多数情况下，应当是一个详细信息视图 (例如一条消息)，不过也有可能是多条内容的列表 (处理 合并的通知 时)。如果进入的不是应用的顶级屏幕，那么在后退历史中增加导航路径，使用户可以通过“返回”键回到应用的顶级屏幕。更多信息，请参考 导航 一节的 系统到应用的导航。 正确设置通知的优先级从 Jelly Bean 开始，Android 为通知增加了优先级标志。这样你可以使重要的通知相对于其他通知，总是显示在第一个。 合并通知如果已经有正在等待处理的相同类型通知了，那么你的应用不应当再创建一条新的通知，而是将多条通知合并。 合并的通知提供了总体的信息描述，并且告知用户有多少条通知正在等待处理。 可以使用扩展布局为合并的通知提供更多信息。这样用户可以知道被合并的消息细节，并选择在应用中阅读通知内容。 保证通知是可选的: 用户应当可以控制通知。你的应用应当提供选项让用户可以关掉通知或者选择通知的方式，包括是否震动之类的。 使用独特的图标: 用户应当可以迅速的了解是哪个应用的通知正等待处理。 应当这么做: 观察已有的通知图标，为你的应用选择一个比较独特的。 应当这么做: 在通知中使用合适小图标样式，并且在通知操作中选择合适的 Holo Dark 操作图标。 应当这么做: 使你的图标保持简洁，不要使用难以分辨的细节。 不要这么做: 使用彩色突出你的应用。 正确的闪烁通知指示灯 许多 Android 都有个小灯，称之为通知指示灯，可以在屏幕关闭时告诉用户有通知在等待操作。有 MAX、HIGH 和 DEFAULT 优先级的通知会闪烁指示灯，低优先级的 (LOW 和 MIN) 通知不应该闪烁指示灯。 用户对通知的控制应该包括指示灯。通知指示灯应当显示的是白色。应用的通知不应该显示其他颜色，除非使用户指定 构建用户真正关心的通知： Android 通知系统的设计就是要尽量减小对用户注意力的影响，但是仍要注意同时是会打断用户的操作的。当你设计通知时，请仔细考虑是否值得去打断用户。如果你不确定的话，给用户选择是否弹出通知，或者设置优先级标志。 好的应用只在需要它说话的时候才说话，但是有一些情况下有必要使用通知来打断用户。通知主要被用在时间敏感的事件上，特别是和其他人有关的同步事件。 何时不要显示通知有些情况下不要使用通知影响用户: 不要用一些和他们没有直接关系或者不是时间敏感的事件干扰用户。例如社交网络的一些新鲜事，它们都不是很需要实时回应的。不过对于那些关注这些新鲜事的用户，给他们选择。 如果这个事件已经显示在屏幕上了，就不要再用通知了。而是在应用界面中通知用户。例如用户正在使用聊天应用的聊天的时候，就没必要再使用通知告知用户对方有回应消息了。 不要用技术细节来打扰用户，比如保存、同步或者升级。如果应用可以自己处理，就不要问用户。 不要用应用的错误消息打扰用户，如果应用可以快速恢复，那就不要问用户。 不要搞没有内容的通知，也不要打广告。通知是用来告诉用户某个事件，而不是仅仅为了打开一个应用。 不要仅仅是将你的应用显示在用户眼前。这样的通知只会使你的用户疏远你的应用。如果想让你的用户持续看到你的应用，做个小工具让用户可以选择放在主屏幕上。 通知的交互操作图标区分通知，时间逆序从上到下排列；“正在进行的”通知不能被手动删除。 小部件： 分类：信息小部件（显示重要信息，随时间改变；触摸打开相关应用）+列表小部件（浏览列表和在应用内打开制定项目的详细信息，可纵向滑动）+控制小部件（在主屏幕就打开常用说设置）+混合小部件（综合上面部件的特点） 手势：触摸，纵向滑动 小部件大小可让用户人为调整 考虑布局：如何适应不同屏幕的大小 K.设置不要把什么都放在设置里，按下面流程图仔细考虑： 人类能够短时间内记住的条目一般是 7±2 项。如果你给出了个超过 10 条条目的设置 (在通过上面的流程图筛选后)，用户会觉得难以浏览、理解和对它们做出选择。 你可以将这些设置分组，把长列表变成短列表。同一组中相似的设置以下面的某一种方式排列: 放在同一个分隔符下 放在另一个屏幕中你可以任意组合使用上面两种分组方式来合理组织应用的设置项。 少于 7 项:不用分组了，否则用户会觉得更复杂了。 8 到 10 项:试着通过一两个分隔符分组。如果有的设置看起来比较独立，和其他的没有什么联系，那么可以按照以下的方案分组:如果是比较重要的设置，直接把它们放在设置屏幕的最顶端。否则在最下面用一个叫“其他”的分组摆放它们。 11 到 15 项:类似上面的规则，试着用 2 到 4 个分隔符分组。 通过以下方法来减少分组数量:如果有多于 2 个设置是给高级用户的，那么在主设置屏幕中放置一个“高级”按钮，将这些设置放置在点击后出现的子屏幕中。在“更多操作”中放置一个名为“高级”的条目以便导航。 找出“配对”的设置，尽量按照下面的“设计模式”将它们合并。例如你可以把两个相关的设置 (之前使用复选框) 合并为一个单选设置。 多于 16 项:如果你可以把 4 项以上的设置组合在一组中，可以将它们放置在子屏幕中，在主屏幕用一个按钮连接到那里，这样可以有效的减少主屏幕中的设置项。 复选模式，单选，滑块，日期／时间，子屏幕，列表子屏幕，主开关，独立开关，相互依赖（一个设置的可用状态依赖于另一个设置的值；如果有多于 3 个设置依赖同一个选项，考虑使用由主开关控制的子屏幕，以便你的主设置屏幕不会被许多禁用的条目所弄乱。） 默认值选择安全、中性、适合大多数用户的默认值。如果没有默认值，用户最倾向于选择哪个值？ 哪个值是中性的？哪个值最没有风险，最没有争议，最不过分？哪个值消耗最少的电量和最少的数据流量？哪个值最符合设计原则？哪个值最符合设计原则？ 为每个设置提供一个清晰而明确的标题，并且编写合适的副标题内容。副标题是用来表示状态的，除非它是个复选框才是做详细描述的. L.帮助假设每次请求帮助都是非常紧急的，不要增加额外的步骤增加用户的操作成本； 帮助内容的撰写原则：仔细考虑每一个像素，仅展示重要信息；图片比文字更容易理解；浏览而非阅读，请在排版上更友好；直接提供答案； M.无障碍性使导航符合直觉 使用推荐的控件大小 视觉元素的文字标签要有意义 Provide alternatives to affordances that time out 使用标准的开发框架控件或者让自定义控件支持 TalkBack Android 包含了一些为视觉障碍人士提供方便的功能，这样就不需要你的应用作出较强烈的视觉变化。 TalkBack 是一个预安装的 Google 屏幕阅读器。通过阅读来反馈操作的结果，例如运行应用或者事件通知。 Explore by Touch 是与 TalkBack 结合使用的一个系统功能，你可以触摸屏幕，并且听到触摸的内容。 辅助功能设置让你可以设置设备的显示和声音，例如大号字体、语音阅读速度等。 N.纯粹的android不要模仿其他平台上的ui元素； 不要使用专为其他平台设计的图标; 不要在列表中使用向右箭头; 5.控件A.标签选项卡滚动标签：可左右滑动；固定标签； B.列表区域分割和列表条目 C.网格列表基本网格列表的条目排列顺序：垂直滚动（从左到右从上到下）；水平滚动（从上到下再从左到右）； D.滚动容器滚动容器和滚动索引（按首字母查找条目） E.下拉菜单F.按钮单图标；图标加文字（重要操作）；仅文字； G.文本框当文字输入超出边界后，单行文本框会自动向左边滚动，使最右边的文字一直能够显示。当文本长度超过文本框宽度时，多行文本框会自动换行，当行数超出文本框高度时，会自动向上滚动，使用户能够看到最后一行。 自动完成文本框: 使用自动完成文本框时，它将会实时显示自动完成或者搜索结果，用户可以更容易和准确的输入内容。 文字选择：上下文操作栏；选择控制；（长按文本框选择文字进入文本选择模式） H.滑块交互式滑块 I.进度条和活动指示器进度条应当表示从 0% 到 100% ，而且永远不会往回变成一个更小的值。如果有多个操作按顺序发生，使用进度条来表示整体的延时。 活动指示器用于那些不确定时间的操作。它们告诉用户要等一会儿操作才能完成，而不用将具体的细节说出来。 有两种活动指示器: 活动进度条和活动圈。这两种提供了自适应大小和 Holo Light/Holo Dark 两种主题。请在上下文周围选择合适的风格和大小。例如较大的活动圈比较适合大面积的空白区域而不是一个小对话框。每一个操作应该只使用一个活动提示器。 当使用活动圈时，不要配以文字。旋转的圆圈已经表明了正在进行后台操作。 自定义活动指示器：通过颜色区分状态； J.开关3种开关控件：复选框，单选按钮，开关； K.对话框警告对话框：分有标题栏和无标题栏，谨慎使用无标题栏，仅在有可能引起数据丢失、连接断开、收费等高风险的操作时才使用。并且标题应当是一个明确的问题，内容区提供一些解释。 弹出对话框：是一种轻量级的对话框，一般只让用户做出一个选择。弹出对话框不使用确定或取消按钮，而是让用户通过简单的触摸，在整个工作流中做出选择。 Toast 提供了轻量级的反馈信息。例如，当你在发送邮件前跳出撰写页面，“草稿已保存”的 Toast 让用户知道待会儿可以继续编辑。Toast 会在一定时间后自动消失。 L.选择器提供了一种简单的方式，让用户在多个值中选择一个。除了可以通过点击向上/向下按钮调整值以外，也可以通过键盘或者手势。 选择器可以内嵌在一个表单中，但是由于它占的空间比较大，把选择器单独放在对话框里比较好。如果要嵌入表单，考虑使用文本框或下拉菜单以节省空间。]]></content>
      <categories>
        <category>Product Manager</category>
        <category>Design</category>
      </categories>
      <tags>
        <tag>UI Design</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阅读笔记《漫谈人工智能》]]></title>
    <url>%2F2017%2F09%2F11%2F%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%8A%E6%BC%AB%E8%B0%88%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E3%80%8B%2F</url>
    <content type="text"><![CDATA[《漫谈人工智能》从人工智能的历史谈起，讲述了此领域最新的思想成果、深度学习和机器学习理论、自然语言处理、人与机器的关系以及人工智能领域的最新应用。 序言 part1:整个人工智能学科发展历史的介绍； part2-4:介绍了人工智能中最古老的部分，包括图灵机模型（第2章）、冯·诺依曼计算机体系结构（第3章）以及怪圈与哥德尔定理（第4章） part5-12:介绍了人工智能领域最新的思想和成果 part5:马库斯·胡特（Marcus Hutter）的通用人工智能理论 part6:深度学习理论 part7:探讨人工智能与人脑在信息处理等若干方面的异同之处 part8-9:讨论了一种非常另类的人工智能——人类计算，即通过互联网众包的方式，让人类自己帮助计算机程序来实现“人工智能”。 part10:自然语言处理 part11:展现了一种另类的理解、构思人工智能的视角。这里关注的不再是个体机器人，而是这群机器人通过相互作用而涌现出来的集体行为。人类的智力不也是来自于成千上万个神经元互动的涌现模式吗？通过巧妙地设计机器人的相互作用规则，我们可以在集体层面获得智能. part13-14:实践人工智能的应用案例 part 1图灵机模型 假设有两间密闭的屋子，其中一间屋子里面关了一个人，另一间屋子里面关了一台计算机：进行图灵测试的人工智能程序。然后，屋子外面有一个人作为测试者，测试者只能通过一根导线与屋子里面的人或计算机交流——与它们进行联网聊天。假如测试者在有限的时间内无法判断出这两间屋子里面哪一个关的是人，哪一个是计算机，那么我们就称屋子里面的人工智能程序通过了图灵测试，并具备了智能。事实上，图灵当年在《机器能思考吗？》一文中设立的标准相当宽泛：只要有30%的人类测试者在5分钟内无法分辨出被测试对象，就可以认为程序通过了图灵测试。 学科 自动定理证明；模式识别；机器学习；自然语言理解；计算机视觉；自动程序设计 part2 图灵的计算王国Turing Machine and Theory of Comoutation小虫模型 从信息处理角度建立小虫行动的模型：对小虫所处环境建模；假设小虫所处环境是一个无限长的纸带，纸带被分成若干小方格（只有黑白色），假设小虫只有眼睛能感知方格颜色，方格的颜色即它的输入信息； 输入信息：方格的颜色，I={black，white}；输出动作：在纸带上前进或后退一个方格，O={forward,backwards}；程序：从I集合到O集合的映射；内部状态：黑方格为吃饱状态，白方格为吃饱状态，再次读入黑方格是饥饿状态，S={饥饿，吃饱}； 模型复杂化【复杂性来自于组合】：把小虫的输入集合（多种感官，环境复杂化）、输出行动集合（移动，改造）、内部状态集合进行扩大； 信息处理的根本：输入集合、输出集合、内部状态、固定的程序； 计算 图灵机是理论计算模型；也是一个会对输入信息进行变换给出输出信息的系统；功能是完成对输入信息进行变换得到输出信息的计算； 如果我们把一切都看作信息，那么广义上讲，计算就是对信息的变换。你会发现，其实自然界充满了计算。如果我们把一个小球扔到地上，小球又弹起来了，那么大地就完成了一次对小球的计算。因为你完全可以把小球的运动都抽象成信息，它无非是一些位置、速度、形状等能用信息描述的东西，而大地把小球弹起来无非是对小球的这些信息进行了某种变换，因而大地就完成了一次计算。你可以把整个大地看作一个系统，而扔下去的小球是对这个系统的输入，那么弹回来的小球就是该系统的输出，因而也可以说，计算就是某个系统完成了一次从输入到输出的变换。 因为我们完全可以把所有的自然界存在的过程都抽象成这样的输入输出系统，所有的大自然存在的变量都看作信息，因而计算无处不在 计算的组合：可以把若干个计算系统进行合并，构成更大的计算系统。比如还是那个小球，如果往地上放了一个跷跷板，小球掉到地上会弹起这个跷跷板的另一端，而跷跷板的另一端可能还是一个小球，于是这个弹起的小球又会砸向另一个跷跷板； 最简单的信息就是0和1，最简单的计算就是对0或1进行的布尔运算。而布尔运算本质上其实就三种：与、或、非。从最简单的逻辑运算操作最简单的二进制信息出发，我们其实可以构造任意的图灵机。这点不难理解：任何图灵机都可以把输入、输出信息进行01编码，任何一个变换也可以最终分解为对01编码的变换，而对01编码的所有计算都可分解成前面说的三种运算。也许，现在你明白了为什么研究计算机的人都要去研究基本的布尔电路。奥秘就在于，用布尔电路可以组合出任意的图灵机； 模拟 图中A是你的鬼脸动作，B是你朋友做出来的鬼脸动作，C是日记本上的描述。你朋友的动作B模拟了你的动作A，而B的动作信息是通过执行C上的描述得到的，也就是说，存在着一个从C到B的信息变换。这样我们认为C也对A进行了模拟。 一台图灵机A如果要模拟B，并不一定要模拟B中的所有输入、输出、内部状态、程序规则表这些元素，而只要在给定输入信息的时候能够模拟B的输出信息就可以了； 在给定相同输入信息的情况下，只要输出信息 o’ 能够模拟信息 o，也就认为B模拟了A。而信息 o’ 对信息 o 的模拟又符合我们上面对一般集合之间模拟的定义。也就是说，如果存在另外一台图灵机能够把信息 o’ 计算并映射成信息 o，就认为 o’ 模拟了 o。说白了也就是 o’ 可以与 o 不一样，但是只要你能用一个图灵机把 o’ 经过一系列运算变换到相同的 o，就认为 o’ 模拟了 o。因而也就是图灵机B模拟了图灵机A。 假设A和B输入的信息也不一样，一个是 i，另一个是 i’，那么如果 i 和 i’ 之间也存在着模拟对应关系的话，我们仍然认为B可以模拟A 计算等价性 如果把中国人、英国人、机器人都看作图灵机，把那三句话看作对它们的输入信息，那么最终的结果就是图灵机计算的输出。这个时候我们看到三种结果是相同的，也就是说这些图灵机之间是可以相互模拟的 能够相互模拟的图灵机是计算等价的。而这种计算等价性就像前面说到的加法规则一样是独立于计算系统和执行机构的。 自食其尾 万能图灵机可以模拟自身：把自己的编码信息输入给自己； 存在某种图灵机可以完成自我复制，其构造：如果一台图灵机是X，那么它的编码就记为。这样能够自我复制的图灵机T的功能是把T的编码写到纸带上输入万能图灵机，那么万能图灵机就能根据读入的执行T，在纸带上再次输出的一份副本‘，并且 = ‘。下面就来解释如何构造这样的T。首先T由两部分构成：A和B。第一部分A的功能是指导万能图灵机把B的编码原封不动地打印到纸带上，纸带上就有了，如果这个时候你想用同样的方法打印到纸带上是不行的，因为A就会打印自己了。然而B却可以这样做：读入纸带上的信息X，生成能够打印X的图灵机p(X)的编码，打印到纸带上，并把X和的内容前后调换，有定理保证这样的图灵机是存在的。这样当B读到纸带上的信息之后，就会打印出能够打印的图灵机的编码也就是，然后把和位置对换，就构成了，也就是，所以P把自己进行了一次复制。初看起来，这种自我复制的程序是不可能的，因为这包含了无穷无尽的怪圈。P要能产生它自己，就意味着P中至少包含了一个，而这个中又包含了至少一个……最后P必然是一个无限大的程序，然而我们却能够证明P是可能的。 停机问题 我们不妨设P(X,Y)表示P判断程序X作用到数据（纸带）Y上是否存在死循环的结果。如果X作用到Y上存在死循环，那么P(X,Y)就输出一个yes；否则就输出一个no。 这很像刚开始叙述的那个猜硬币的游戏。你想猜对我的硬币，就必须告诉我你的答案是左手还是右手，然而问题是我总能根据你给出的答案进行动态调整，让你永远也猜不对！停机问题也是如此，我总能根据你的程序P来构造P判定不出来的问题Q，我总会赢！很简单，因为你总要在我之前构造好P，就相当于你总要先说出硬币在哪个手中。 图灵停机问题也和复杂系统的不可预测性有关。我们总希望能够预测出复杂系统的运行结果。那么能不能发明一种聪明的程序，输入某个复杂系统的规则，输出的是这些规则运行的结果呢？从原则上讲，这种事情是不可能的。它也和图灵停机问题等价。因而，我们得出来的结论就是：要想弄清楚某个复杂系统运行的结果，唯一的办法就是让这样的系统实际运作，没有任何一种计算机算法能够事先给出这个系统的运行结果。 part4 一条永恒的金带 core：缠结的层次结构 层次，高低（一种典型的层次结构），尺度（另一种层次），虚拟层次（eg.梦中梦，从前有座山） 层次的混淆／缠结 本该属于不同层次的东西却由于某种原因混淆到了一起；eg.莫比乌斯带；埃舍尔的僧侣利用视觉错觉和绘画技巧达到二维三维化的效果； 数学公理化：数学最大的好处就在于，它可以从一些基本而简洁的前提假设（公理）出发，通过严密的推理导出所有可靠的结论——这就是数学公理化方法。 哥德尔句子 本数学命题不可以被证明。 下面我们将展开推理。根据逻辑排中律，这条数学命题要么正确，要么错误。那么，我们不妨先假设它是正确的，然后再看看会发生什么。于是，“本数学命题不可以被证明”就暂时是正确的，也就是说这个数学命题是一条数学真理，并且根据它自己的论述，它不能被证明。于是，我们得到了一条真理，但却不能被我们的数学公理化系统所证明，因此，希尔伯特要求的完备性不能得到保证。 下面我们再从另一个角度展开讨论，假设该命题是不正确的。那也就是说，“本数学命题不可以被证明”这个命题是可以被证明的。于是，从公理出发，我们能够得到“本数学命题不可以被证明”这一命题。而按照假定，“本数学命题可以被证明”是真理，所以根据完备性，它也必然是系统中的定理。于是，正命题和反命题同时都是系统中的定理，一致性遭到了破坏。 理发师悖论：某个小村庄里有一名理发师，他给自己制定了一条奇特的规矩：他不给那些给自己理发的人理发。这条规矩看起来没什么问题，但是一旦我们问这位理发师他该不该给自己理发的时候，他就会立即陷入自相矛盾的境地。因为，如果他给自己理发，那么按照他的规矩，他属于给自己理发的人，那么他就不该给自己理发。而如果他不给自己理发，那么根据他的规矩，他又应该给自己理发。所以，这个可怜的理发师将无所适从。 递归定理 对于任意的程序F，总存在一段程序代码c，使得我们执行代码c的结果完全等价于把源代码c作为数据输入给程序F执行的结果。 part5 从算法复杂性到通用人工智能 人的智能涉及模式识别、分类、学习、记忆、归纳、类比、泛化、联想、规划、优化、创新、演绎推理、问题求解、语言处理、生存、繁衍等方面，试图通过模拟人脑或模拟人类智能的各种功能模块而构建智能主体 AIXI（通用智能主体）的应用 序列预测。显然，索洛莫诺夫的序列预测可以看作AIXI的特例，所以AIXI拥有序列预测的功能，像股票走势、天气预报、彩票投注之类的问题都可以转化为类似的序列预测问题，只要现实世界中这些问题真的是可计算的，那么AIXI都可以成功预测。 最优化。比如寻找某个函数的最小值问题，这时AIXI可以权衡计算该函数的所有程序，然后只要把AIXI的效用函数设为跟自变量的函数值相关的某个函数就好了。函数值越小效用越高，为了寻求最大效用，AIXI会自动寻找函数的最小值。计算经过所有城市路途最短的旅行商问题、求解生产某产品的最小成本问题等都属于此类问题。 策略博弈。AIXI还可以进行各种策略博弈，只需要把博弈的另一方看作“环境”就好了，比如象棋之类的二人零和游戏。而且，如果对手是“理性人”的话，AIXI的期望最大化策略会收敛到通常的极小极大化策略。传统的博弈理论只能处理理性主体间的博弈，如果博弈的另一方有一些非理性的行为，只要这些行为仍然具备某种模式，那么AIXI都可以探索出来，然后加以利用，从而谋利。也就是说，AIXI可以对抗有限理性或非理性玩家。 监督学习。给定一系列(z, f(z))，AIXI可以轻易地预测(z’ , ?)，所以监督学习也很容易处理。比如识别物体的属性和根据属性分类的问题都可以划归为这种样式。比较复杂一点儿的如给定一些状态，然后教它在合适的状态下做合适的动作，这意味着给定一个(state, action)序列，然后AIXI就可以学会遇到什么状态该采取什么行动了。诸如此类的几乎所有问题都可以划归为AIXI能处理的问题，与智能相关的各种要素也都应该可以从中涌现出来。这里最关键的还是对环境的压缩问题，而压缩包含着对任何模式的探索，一般来说具体的问题往往只是针对某些或某类具体的模式。 操作系统 输出：为了使从1加到100的计算结果能够显示在计算机屏幕上，我们需要在内存中留出特定的区域存放用于显示的内容，在CPU通过指令的运行把数据存放在特定的内存位置上以后，操作系统负责不断地将这些特定区域的内容在屏幕上显示出来。在这个过程中，要适应不同的分辨率，计算在显示器上输出的位置。为此，操作系统需要适应不同的显示设备，根据不同的设备运行不同的驱动程序。 输入：同样，操作系统需要接收键盘的输入，在键盘发生了按键按动作时，需要得到触发的通知，将按键的电信号转换为相应的字符，并不断将接收到的字符存在指定内存区域，供计算机中运行的程序使用。 在简单的计算机模型中，操作系统主要负责的功能有两点：一是封装对于底层的硬件实现，二是提供更多的函数支持更多的功能 层次的概念 层次与其愈上愈简：当一个计算机程序运行时，我们从软件到硬件来观察，就会发现每一层的表现形式完全不一样，但是本质上都是计算，而且每一层都是建立在下一层的基础上的。虽然分了这么多层，但所有的层都是等价的。层和层之间有清楚明确的边界，越到下层牵涉到的基础单元越多，越到上层越简洁。 真实的世界是在无数的层次上运行着的； 实例望梅止渴中反应出的反应层次：望梅止渴”中，士兵们被想象中的梅子引诱得不住流口水的过程实际上就是一个非常复杂的多层次系统运行的过程（如图3-23所示）。由梅子这个概念到引起口水的分泌，我们平常都只从概念的层面来提及，但实际上从概念到身体反应，作用是一层层产生的。在身体系统之下，还有更低的层次，如器官、细胞、蛋白质、DNA分子。概念和思维层（视觉，形象回忆，口感回忆）+ 身体系统层次（大脑神经激活，神经系统递质传递，唾液分泌） 通用智能 通用智能的核心是通用归纳。通用归纳将归纳转化为预测，而预测的关键是压缩。压缩可以理解为对数据的建模或编码表示，它依赖于对模式的掌握，模式可以用算法来衡量。 从数据到程序是编码，从程序到数据则是解码。编码越好（即压缩越短）则预测越准，预测越准行为就越有效。与智能相关的其他要素，诸如分类、类比、联想、泛化等都可以理解为对模式的追求，这些都可以在追求最大压缩的过程中涌现出来，所以不是基本的。但找寻最短编码的过程不是一个能行的过程，所以我们只能通过试错不断逼近。逼近的过程可以理解为一个信念修正的过程，这可以通过贝叶斯更新来处理，信念修正之前的“先验信念”的大小则取决于模式自身的简单性。 复杂的事物要想简单地表达必须先压缩。压缩可以通过编码来实现 科学是对经验的理解，理解就是压缩，预测可以看作某种解压缩 主体与环境的交互：考虑一个面对未知环境的主体，它与环境不断交互（如图5-9所示）。在每一个回合中，主体都对环境作出某种动作，然后这个动作激发环境作出某种反应，反过来给主体一些反馈。主体感知到这种反馈，同时从中体会到某种正面（幸福）或负面（悲伤）的效用，然后计划下一回合的交互该采取哪种动作，主体的所有信息都来自过去与环境交互的历史，它对未知环境的评估也主要依赖于这些信息。在主体与环境交互过程中，主体最优的行为方式就是依照算法概率（主观信念）评估未知环境、寻求期望累积效用最大化。 生活是一系列选择的总和，如果你选择了做ak，你就可能面对ok 、品尝rk 。要想收获更多，就需要慎重选择，立足当下，评估未来。在主体与环境交互过程中，主体最优的行为方式就是依照算法概率（主观信念）评估未知环境、寻求期望累积效用最大化。 关于这个世界，最不可理解的是——它竟是可以理解的。——爱因斯坦 深度学习 机器学习就是通过算法，使得机器能从大量历史数据中学习规律，从而对新的样本做智能识别或对未来做预测。 如果把深度学习比作一个物种，和其他机器学习物种相比，它有两个特点：(1) 不挑食。无论原始数据属于图像识别、语言识别、NLP、生物医药等哪个领域，都可以“喂”给神经网络学习处理。这和大脑的工作原理很相似，大脑用同一套算法解决视觉、听觉、嗅觉等感知问题。(2) 胃口大。喂给它的数据越多，它就变得能力越强，越聪明，并且只会吃不饱，不会消化不良。 BP算法（反向传播算法）：利用BP算法可以让一个人工神经网络模型从大量训练样本中学习出统计规律，从而对未知事件进行预测。这种基于统计的机器学习方法比起过去基于人工规则的系统，在很多方面显示出了优越性。这个时候的人工神经网络，虽然也被称作多层感知机（Multi-layer Perceptron），但实际上是一种只含有一层隐层节点的浅层模型。 卷积神经网络：本知识多层感知机；成功的关键可能在于它所采用的局部连接和分享权值的方式不仅减少了权值的数量，而且降低了过拟合的风险。 从特征表示到深度学习 特征提取和表示学习：机器学习特定问题时，一般采集到数据之后会进行一些特征提取的处理，例如机器视觉里的 SIFT特征 + 词袋模型（Bag of Words），或者语音识别里的 MFCC频谱之类的特征，再把提取出的特征（即原始数据的一个表示）丢到各种机器学习模型（如SVM）里做分类或预测； 简单图片识别任务：识别一张图片上是否有鸟。如果能预先知道鸟的视觉特征，例如尖嘴、羽毛、翅膀等，并检测一幅图片上是否包含这些特征，作为这幅图片的表示（例如如果有尖嘴，对应位置为1，否则为0）。不难想象，在这个新的表示上识别，比直接在原始像素上识别，准确率要高得多。 深度学习的实质：通过构建具有很多隐层的机器学习模型和海量的训练数据，来学习更有用的特征，从而最终提升分类或预测的准确性。所以“深度模型”是手段，“表示学习”才是目的。 深度学习与传统的浅层学习的不同在于：(1) 强调了模型结构的深度，通常有5层或6层，甚至10多层的隐层节点；(2) 明确突出了表示学习的重要性，也就是说，通过逐层特征变换，将样本在原空间的特征表示变换到一个新特征空间，使分类或预测更加容易。 自编码器（autoencoder）是含有一个隐层的神经网络。从概念上讲，它的训练目标是“重新建立”输入数据；换句话说，让神经网络的输出与输入是同一样东西，只是经过了压缩。例如，有一个由28×28像素的灰度图像组成的训练集，且每一个像素的值都作为一个输入层神经元的输入（这时输入层就会有784个神经元）。输出层神经元要有相同的数目（784），且每一个输出神经元的输出值和输入图像的对应像素灰度值相同。 RBM（受限玻尔兹曼机） 一种可以在输入数据集上学习概率分布的生成随机神经网络；标准的RBM中，隐含和可见层的神经元都是二态的，即神经元的激活值只能是服从伯努力分布的0或1。 RBM的权重更新公式中包含正学习和逆学习两项，其中逆学习项需要可见层和隐含层的交替随机采样（gibbs sampling），直至网络达到平衡态。但这样计算很慢，使得模型只具有理论价值而不够实用。但这种问题难不倒辛顿，他提出了一个叫作对比散度（contrastive divergence）的近似方法，只需采样很少的次数（如1次）就可以更新权重，且对最终的学习效果几乎没有影响。 自编码器或RBM以无监督的方式学到数据中的特征 深度学习与传统的神经网络之间既有相同的地方也有很多不同。二者的相同之处在于深度学习采用了相似的分层结构，系统由包括输入层、隐层（多层）、输出层组成的多层网络，只有相邻层节点之间有连接，同一层以及跨层节点之间相互无连接。这种分层结构是比较接近人类大脑的结构的。 在传统神经网络中，人们采用的是BP算法训练整个网络，随机设定初值，计算当前网络的输出，然后根据当前输出和训练的标签之间的差改变前面各层的参数，直到收敛（整体是一个梯度下降法）。 BP算法作为传统训练多层网络的典型算法，实际上在仅含几层网络的时候就已经很不理想了。深度结构（涉及多个非线性处理单元层）的非凸目标代价函数中普遍存在的局部最小是训练困难的主要原因。BP算法存在以下几个主要问题。1.梯度越来越稀疏：从顶层越往下，误差校正信号越来越小；2.收敛到局部最小值：尤其是从远离最优区域开始的时候（随机值初始化会导致这种情况的发生）；3.一般只能用有标签的数据来训练：但大部分数据是没有标签的，而大脑可以从没有标签的数据中学习。 栈式自编码器网络 (1) 使用自底向上（图6-5的从左向右）非监督学习，就是从底层开始，一层一层地往顶层训练。采用无标定数据（有标定数据也可）分层训练各层参数，这一步可以看作一个无监督训练过程，也可以看作是特征学习过程，是和传统神经网络区别最大的部分。 （2）自顶向下（图6-5的从右向左）的监督学习，就是通过带标签的数据去更新所有层的权重，误差自顶向下传输，对网络进行微调。 语音识别 采用深度神经网络可以充分描述特征之间的相关性，可以把连续多帧的语音特征并在一起，构成一个高维特征。最终的深度神经网络可以采用高维特征训练来模拟。另外，它拥有更为稳定的表述（Invariant Representation）特性，层级越多，抽象能力越强。 迁移学习：DNN不但大幅度提高了准确率，还间接解决了语音识别模型训练的一个实际问题：对于一些小语种，无法收集到足够多的训练语料数据。谷歌的研究人员发现了一个有趣的现象，先针对有足够训练数据的大语种（如英语）训练一个识别网络，然后将网络最顶层的英语音素分类层去掉，代之以某个新语言的音素分类层，而重用下层产生的特征（即把原网络去掉最顶层后当成一个语音特征提取器），这样只要花非常少的训练代价，就可以得到一个效果非常好的新语言识别网络。特征重用带来的迁移学习能力体现了表示学习的巨大威力。 自然语言处理NLP 相比于声音和图像等底层原始信号，语言是一种非自然信号，是完全由人类大脑产生和处理的符号系统，属于人类认知过程中产生的高层认知抽象实体。 词向量 自然语言理解的问题要转化为机器学习的问题，第一步肯定是要找一种方法把这些符号数学化。 One-hot表示法：NLP 中最直观也是到目前为止最常用的词表示方法是 One-hot稀疏表示，这种方法把每个词表示为一个很长的向量。这个向量的维度是词表大小，其中绝大多数元素为 0，只有一个维度的值为 1，这个维度就代表了当前的词。eg.“话筒”表示为 [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 …];“麦克”表示为 [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 …];每个词都是茫茫 0 海中的一个 1。问题：词汇鸿沟”现象，任意两个词之间都是孤立的。光从两个向量中看不出两个词是否有关系，哪怕是“话筒”和“麦克”这样的同义词也不能幸免于难。 词向量（word embedding）：分布式表示（distributed representation）的一种低维实数向量；这种向量一般长成这个样子：[0.792, -0.177, -0.107, 0.109, -0.542, …]；优点；让相关或者相似的词在距离上更接近。向量的距离可以用最传统的欧氏距离来衡量。用这种方式表示的向量，“麦克”和“话筒”的距离会远远小于“麦克”和“天气”的。 part7 人工智能与人脑智能的思考 认知计算与脑识别功能：现在可能已经没有必要讨论人和计算机谁更好这个问题了，更重要的是利用好两者的优势，更好地解决实际问题。在处理人和计算机的关系问题上，一种办法是模拟人的思维过程，利用从结构到功能的特点，实现类似的智能。人类在这一关系中处于服务对象角色，最近十分热门的深度学习属于这一类的实例。还有一种想法是直接利用人的认知功能，将部分计算机不适宜完成的复杂的计算任务由人来完成，人直接参与到了计算之中，与计算机各自分工。这类有人脑辅助的“认知计算”研究也已经取得了很多成果。例如，美国EGI公司利用人的超强图像处理能力从海量遥感卫星获取的图像中检索异常物体，用快速视觉呈现的方式将大量卫星图片呈现给参与实验的人。与传统人工操作相比，这家公司在人进行检查的同时记录了人脑活动的脑电波，一旦发现异常物体，脑电信号中会有特异性反应成分。而这套系统的另一个优势是直接检测脑活动，可以不需要人的意识参与，能够在刺激呈现后短时间内发现异常，并且在前一幅图片加工完之后马上呈现另一幅图片，而无需等人按键决策。通常需要人作出决策、按键反馈等高级认知加工需要耗费很多时间，而直接利用脑的识别功能，绕开耗时的决策和反应，与计算机对接，就极大地提高了检测效率。类似地，美国哥伦比亚大学的Paul Sajda教授研究组进行了很多利用视觉信息进行物体快速归类的实验。近年来还利用人对物体类别感知过程的脑活动特征建立了数据库，实现了利用人特征进行图片检索的引擎。还有一部分认知计算应用是利用人的认知活动直接为人服务的，例如计算出人的情绪活动的状态，给予人实时的反馈调节，协助用户找到保持良好状态的策略（例如Neurosky的系列产品）。这种神经反馈的应用已经用于情绪调节和注意力训练等领域，无一不是恰当地利用了人的认知优势，实现了有机的人机协同。 人的视觉系统可以很好地处理这个问题，即便背景在变化！这涉及视网膜空间特殊表征，即便物体位置、尺寸、朝向、角度发生了很大改变，仍然能够保持‘视觉不变性’，不会因为输入的客体信息改变而影响识别输出结果，这是稳定追踪的基础。” 计算机在处理速度方面有优势，可以连续工作，另外大型数据库可以存储的容量远远大于人脑，只不过人脑的信息处理模式可能极为智能 我们首先可以看出信息加工是层次化的，逐级加工从简单到复杂的特征，从V1到V2、IT区，每个层级都对视觉信息有各自的表征，提取从线段朝向、简单特征组合到复杂特征等不同的特征，感知、识别等认知加工也是伴随着每个层级的表征进行的。而这些表征和加工都是通过多个层级的神经元群体的同步活动来实现的。单个神经元负责表征极为简单的信息，但是通过群体组合在一起（通过神经电活动有节律的同步震荡完成组织）就能表达复杂的功能。从信息科学的角度看，整个加工过程的实现可以理解为复杂的多次特征提取过程，提取的特征从简单到复杂，多次组合，甚至“概念”这种十分抽象的特征也可以被提取。因此我们不难看出现有计算机视觉领域物体识别算法的局限性，也不难理解与人的识别过程有类似之处的深度学习等算法一定有内在结构特点做保证。 part8 人工智能：从人机交互到人类计算 从图灵测试看智能的定义：人们往往将注意力集中在那台机器上面：我们如何改进机器的算法来蒙骗人类。但实际上，起到关键作用的恰恰不是机器而是测试机器的测试者——一个活生生的人，因为他是最终的判断者。因此，与其说我们要开发一个具有智能的机器，不如说我们要开发一个能够让人类测试者认为具备智能的机器。虽然后面一种说法只比前面的说法多出了一个限定词：“人类测试者认为”，但是，这已经道破了关于人工智能的一条真理：智能不是一个可以客观定义的属性，而是一种依赖于观察者——人类测试者——的属性； 只有玩家的交互输入才是让机器实现自我改进、实现进化的唯一动力来源。 一种有效地让用户“编程”的模式很快被人们用来解决一些更有意义的实际问题。例如，美国新墨西哥州立大学的约翰斯顿（Victor S. Johnston）教授在他的著作《情感之源》一书中就描述了这样一个人类面孔生成的程序。首先，他把人脸的各个组成部分，如鼻子的形状、眼睛的大小、额头的宽度等按照它们的特征进行编码。之后，与“生物变形”程序一样，计算机在随机地选择一组参数之后就能在屏幕上生成一系列人脸，并让人进行选择。但是，与“生物变形”不同的是，这个人脸选择软件可以用于解决实际问题，而不仅仅是为了娱乐。约翰斯顿教授将这款软件用于辅助目击者寻找杀人凶手。人们通常不知道如何描述罪犯的面部特征，但却可以轻松地识别出哪一张脸更像凶手。于是，只要杀人案目击者在电脑屏幕前不停地点选那些更像罪犯的脸孔，就会一点点地把真正的罪犯面孔“进化”出来。 程序的引擎——人类的注意力 诺贝尔经济学奖得主郝伯特·西蒙（Herbert Simon）早在1971年的时候就指出：“在一个信息丰富的世界中，拥有信息财富就意味着另外一些东西的匮乏：所有那些信息所消耗的东西。这就是注意力。因此，信息的富足必然导致注意力的稀缺，这也就使得如何能够在过载的信息资源上面有效地分配注意力变得极其重要。” 不妨把机器的内存环境看作一个大的养鱼池，内存中活跃的程序就好像是这个池子中的各类小鱼小虾。正如所有的鱼都需要吃食物一样，所有的程序体都需要系统给它们分配CPU执行时间。在目前主流的多任务操作系统中，只有竞争到足够多CPU时间的程序段才能够更好地存活，并且有更高的机会被执行和修改。因此，CPU时间对于程序体就像是能量流对于鱼池里面的鱼一样。 爱德华开始发挥他经济学家的特长。首先，他发现虚拟角色每升级一次，玩家就可以在eBay上多卖出13美元。其次，他估算出玩家让自己的角色升一级大概需要51.4小时，那么平均每个小时每个玩家就能创造13/51.4 ≈0.25美元的价值。而且，每天EQ游戏中都有60 381个玩家在线，那么，整个游戏在一年内创造出的价值，也就是GDP年均值是：60 381×24×365×0.25 ≈1亿3千万美元。这是一个可观的数字，因为按照这个计算，EQ作为一个虚拟的国家，GDP排名竟然是2001年全球所有国家的第77位。爱德华的研究的一个核心假设就是：玩即生产。因为玩消耗了注意力，而注意力相对于EQ世界来说就是一种资源。没有人玩的网络游戏必然会死掉。所以，玩家看似消费一样的玩的行为实际上构成了一种生产，而这种生产的价值恰恰可以体现在eBay上虚拟角色的拍卖价格上面。 注意力经济学 互联网广告的运转原理就是将人类的注意力转变成实实在在的资金流。 谷歌印钞机：利用人工智能技术精准地引导这种注意力流动和相应反向的资金流。谷歌开发了两个系统，分别称为AdWords和AdSense。首先，谷歌公司意识到每天成千上万的网民给谷歌输入了大量的关键词以搜索网页，这实际上是一种商业机会。因为，如果将这些关键词作为广告去出售，这就会是一大笔收入。于是，AdWords系统就完成了这一任务，它将搜索关键词按照重要程度排序，以不同的价格出售给广告商。其次，AdSense负责将正确的广告投放到合适的网站上。它根据关键词，搜索到点击排名靠前的个人网站（博客），并从这些网站站长或博主那里购买广告位，然后将AdWords中的大量广告按照关键词打包投放到这些广告位上。由于采取了先进的人工智能技术，所有广告的投放都能达到精准的定位。这样，当你浏览有关人工智能的网站的时候，你将不会看到有关交友和成人用品的广告。于是，谷歌的AdWords和AdSense系统可以精准地引导大量的注意力流动和资金的流动，同时，也赚取了可观的广告收益。 真正的人工智能程序应该是一种平台，能够充分利用人类的注意力资源而滋养大大小小的程序，与此同时，该平台还可以牢牢地抓住玩家的心，将它们粘到这个虚拟的数字世界中。、 part9 美丽的注意力之流 万物皆流在现实与虚幻世界的对应：古希腊人赫拉克利特说：“万物皆流。”在我看来，这不是一个模糊的隐喻，而是蕴含着深刻的洞察。真实物理世界的河流、城市交通流以及虚拟世界的注意力流三者之间具有很多相似性，而且在各类流系统中普遍存在着标度律。 part10 自然语言处理NLP的应用 信息检索；社交网络分析（依靠海量数据基于统计的研究方法与社会网络分析方法的融合）； 自然语言处理的研究涵盖：词法分析和句法分析，语音识别、机器翻译、自动问答、文本摘要、情感分析、舆情分析等应用和社交网络中的数据挖掘、知识理解等。 自然语言处理与搜索引擎：原始的网页其实是非结构化的信息，充斥着计算机本身无法理解的“自然语言”。搜素引擎首先需要将这些自然语言“转换”成规则的可被解析的“机器文本”。这些文本有许多存储形式，最简单的就是分词后的短语形式：搜索引擎将大篇幅的网页自然语言切割成一个个短小的词语，并将词语按照重要程度进行排序，再与用户输入的关键词进行匹配，才能返回匹配程度最高的网页。 NLP与社交网络：1.基于社交网络的自然语言理解研究有了更多的应用。一方面，研究聚焦在如何通过人们在社交网络中的表现更好地理解人们的行为模式，最广为人知的应用就是广告点击预测——预测具体的单个用户或单一用户群体是否会点击某一特定广告，从而实现广告收益最大化。另一方面，社交网络中的文本由于其实时性，使许多预测任务成为可能，如利用推特上的关注度预测美国总统大选结果、微软研究院预测奥斯卡获奖情况，以及著名的高盛公司预测世界杯比赛结果。虽然高盛预测的正确率只有可怜的34%，但是社交网络的兴起还是使人们的意图想法反映在了容易获得的文字之中。 2.以我们熟悉的新浪微博为例，用户可以关注别人，获得关注，还可以发表原创内容、评论或转发别人的微博……这些都可以转化为可以“计算”的数据。但若想挖掘出一定的潜在信息，还需要一些靠谱而大胆的假设。比如，人们会更倾向于关注自己感兴趣或和自己相似的人，更愿意转发自己感兴趣的内容。这样，在一定程序上，我们在新浪微博上关注的人“代表”了我们的兴趣爱好，我们转发的内容也就“表达”了我们的观点立场。于是，挖掘了某些文本中蕴含的潜在主题，就挖掘出了一些用户的兴趣爱好，就可以在此基础上做广告推荐。同样，分析出了某些文本中蕴含的情感倾向，再结合一些特定的表情和有情感倾向的词语，就分析出了一些用户对某件事的态度，于是他到底支持哪个候选人或哪个球队，也就都了解了。 移动应用交互与NLP：如siri，google now，cortana； NLPNLP的本质是结构预测 自然语言处理的终极问题是分析出“处理”一门自然语言的过程。它包含自然语言理解和自然语言生成，前者是将自然语言语句转化成形式语言语句，后者相反。 自然语言处理的本质是结构预测；无论使用何种方法探究处理自然语言的过程，都需要面临最根本的问题：理解语法和语义——语法表现为句法结构，语义表现为语义结构。可以说，句法结构分析和语义结构分析是公认的自然语言处理（语言计算并不仅仅是现代自然语言处理）的基础任务。 句法分析 流程：语言结构的基本单位是词语（words），第二个层级是构词法（morphology），第三个层级是词性，进而是语法和语义，最后由多个句子组成篇章。相应地，语言计算的任务对应地由分词得到词，由取词根（Stemming）和词形还原（Lemmatization）分析构词法，再由词性标注得到词性（Part-of-Speech），然后由句法分析（Syntax Parser）得到语法结构树（Parse Tree）。至此，句法分析结束。 语义分析 在句法分析的基础上，语义分析可以得到语义理解（semantics），最后再运用篇章分析（discourse）理解句子与句子之间的关系。语义分析即分析自然语言的意义，这里的自然语言可以是词语、句子、篇章等不同级别的语言单位。语言学的语义分析目的在于找出语义表达的规律性、内在解释、不同语言在语义表达方面的个性及共性。逻辑学的语义分析是对一个逻辑系统的解释，着眼点在于真值条件，不直接涉及自然语言。认知科学对语义的研究在于人脑对语言单位的意义的存储及理解的模式，而与计算机科学相关的语义学研究就在于机器对自然语言的理解。 与较为成熟的句法分析不同，语义分析仍然依应用场景的不同有很大不同。从粒度来说，语义分析包含词汇语义分析、句子语义分析和篇章语义分析；从应用场景看，语义分析包含概念语义提取、指称语义分析、情感语义计算、情景语义分析等。这里我仅将语义拆解成概念、主题和情感 李桐观点：梅西永无可能到达球王的高度。 概念：概念是一种浓缩的信息，是一种约定俗成。有了概念，人们在提及一个具体的人或事物时，就不再需要长篇大论的描述。人们不再需要用很长的定义去解释什么叫观点，也不再需要定义什么是球王。这种压缩的信息，不正是一种语义的体现吗？ 主题：如果说概念是文字中直接出现的浓缩信息，那么主题则是一种潜在信息。一篇文章可能围绕着一个具体的社会现象来展开，一次辩论可能以一句名言来交锋，一个网站也许是因一个爱好而建立……这些都可以被称为“主题”。主题是一段自然语言下潜在的中心点，是一个语义上的主体，但它并不一定直接出现在这段自然语言中。比如，还是上面那个例句，显然主题是世界杯，但“世界杯”三个字并没有出现在原句中。如果没有上下文，没有历史语料，机器就无法理解这句话的含义，也无法将其和“世界杯”关联在一起，甚至无法判断出球王是一个约定俗成的词语。所以，挖掘出这样的潜在主题，进一步提炼自然语言的信息，也是语义分析的重要任务。理解了主题，就可以根据主题进行更多地关联扩展，就能提高搜索引擎的相关性，也能用于挖掘社交网络中的某个用户的兴趣爱好。 情感：人类的自然语言中还常常包含情感倾向。对于刚刚例句中的“李桐”的观点，他并不看好梅西，或者说不认可梅西。这种观点是一种广义的“负面”情感。常见的情感大体上可以分成“正面”“中性”和“负面”三类。这种包含情感倾向的自然语言在社交网络中尤为普遍，但让机器去判断自然语言的情感并不那么简单。简单的词典匹配方法只能解决少数这类语料，比如，“梅西是球王”，这是一个正面判断；“梅西不是球王”，这是一个负面判断；“难道梅西不是球王？”，这又回到正面判断。更复杂的，一些特殊比喻可能被用作反讽，如“西班牙防线可以与国足媲美了”，要让机器去理解，即使是大规模的语料也相当困难。 向量表示与相似度计算 分类问题：概念提取和情感分析是两个不同的语义分析任务，但其实它们都是一种分类任务。“是”或者“不是”某一个概念，是一种特定的二分类问题；而情感分析可被简单看作“正面”“中性”“负面”的三分类问题。与分类问题同样重要的是自然语言处理中的另一类问题——相似度衡量问题，潜在语义分析便是这一问题的一种变形和应用。 相似度的衡量：产生于语义分析中的词汇语义研究。词汇语义的研究分为两类：如何表示词汇的涵义（meaning）；如何表示词汇与词汇语义之间的关系。前者一般依据词典定义的方法，后者的研究大致将词义基本关系分为同义词（Synonymy）、反义词（Antonymy）、上位词（Hypernomy）、下位词（Hyponomy）、整体（Holonymy）和部分（Meronymy）。以同义词和反义词为例，两个词的两个词义（许多词有多个词义）相同或接近相同即是同义词；反之，词义相反即为反义词。词义的相近和相反可能是多种角度的，比如，“长（long）”和“短（short）”作为一对语义上的反义词，在度量长度的用法角度是有共性的。于是，词汇相似度（Word Similarity）也是探究词义关系的重要问题。现在一般将词汇相似度或语义距离（Word Semantic Distance）定为词汇相似度。 词汇相似度大致有两类计算方法：基于语义词典（Thesaurus-based）的方法；基于语料统计（Distributional/Statistical algorithms）的方法，即比较词语在语料库中的上下文。中文语义词典有同义词词林、中文概念辞书（CCD）和知网（HowNet）。但词典中许多词并不被包含，且大部分词典定义的方法依赖于上下位层次关系，对于特定词性的词汇表达有限。 基于语义词典的方法：One-hot Representation是最常用的。这种方法把每个词表示为一个很长的向量。这个向量的维度是词典大小，其中绝大多数元素为0，只有一个维度的值为1，这个维度就代表了当前的词。比如，“猫”表示为 [1, 0, 0, …, 0, 0, 0]，“狗”表示为 [0, 1, 0, …, 0, 0, 0]。这种 One-hot Representation 可以采用稀疏方式存储，相当于给每个词分配一个数字 ID。这时，“猫”的 ID 就是 0，“狗”的 ID 就是1。 基于语料统计的词汇语义计算定义了上下文向量（Context Vector），并将词语语义表示为稀疏特征向量，然后即可方便地运用向量距离或相似度公式进行计算。因为词汇的“共同出现”定义、词语权重度量和相似度计算公式的不同，基于语料统计的词汇语义有很大的变形扩展空间。用向量表示语义空间的思想也随后被使用在基于空间向量模型（线性代数）的潜在语义分析（Latent Semantic Analysis）中，而上文提到的潜在主题分析则是潜在语义分析的一个变形。 词汇相似度或者语义相关性的计算包含在许多自然语言处理任务之中。1.短串（Term）分析技术，是后续查询和语义的相关度计算做一些基础的分析。由于查询需求有很多不同的表示方法，我们会对查询进行改写，使其能比较好地召回。其中最主要的技术是短串的语义相关性。2.语义规化，即相同语义用不同方法表示，这种语义规化技术在搜索引擎中应用广泛。语义短串在这里可以被很好地应用，用一种相同的形式表示，然后计算它们之间的关联。3.用户意图分析，对查询意图的识别能针对性地满足用户不同的需求，可理解为对查询语义类别的识别，即短串的分类。4.排序上的应用，包含了查询和网页的相关性计算。 自然语言处理的过程首先要拥有分析的语料，比如网络新闻、微博、正式出版物等，然后根据这些语料建立半结构化的文本库（text database）。紧接着重要的一步就是生成包含词频的结构化的词条-文档矩阵（term-document matrix）； 向量表示：将语料的向量表示为词条-文档关系矩阵（见图10-3）。词条-文档关系矩阵，顾名思义，就是将矩阵的行与列分别表示为词条和文档的索引，从而表现其关系。词条-文档关系矩阵是后续构建模型的基础。 向量空间模型（也称词组向量模型）作为向量的标识符（比如索引），是一个用来表示文本文件的代数模型。它应用于信息过滤、信息检索、索引以及关联规则。 主题模型（Topic Model）作为近年来最受关注的统计语言模型之一，进一步发展了潜在语义模型，将“语义”维度表示为“主题”的多项式分布。通过引入主题空间，主题模型不仅考虑了传统向量空间模型和语言模型中文档在词典空间的维度，也实现了文档在主题空间上的表示。语义挖掘的利器是主题模型，在主题模型中，“主题”表示一个概念、一个方面，表现为一系列相关的单词，是这些单词的条件概率。 文档-词语矩阵表示每个文档中每个单词的词频，即出现的概率；主题-词语矩阵表示每个主题中每个单词的出现概率；文档-主题矩阵表示每个文档中每个主题出现的概率。给定一系列文档，通过对文档进行分词，计算各个文档中每个单词的词频就可以得到左边的文档-词语矩阵。主题模型就是通过左边这个矩阵进行训练，学习出右边两个矩阵。 潜在主题分析在微博的应用：已经广泛运用于社交网络的自然语言处理中。最为热门的应用当属微博关键词，微博关键词致力于挖掘出用户在微博上关注的主题。最初的版本只是根据高频词计算，但随后便加入了潜在语义分析的技术。最终的关键词展示也用到了词云的可视化方法，较大的词代表较高的频率（概率），较小的词代表较低的频率（概率）。通过这种自然语言处理方法和可视化展示，很清楚地展现了用户在社交网络上的兴趣爱好； 深度学习与nlp 将以前自然语言处理中的字面匹配（词典、One-hot Representation）转变成了基于上下文的语义匹配（Distributed Representation）。 在大数据时代，将会出现新的语言模型甚至是理论框架。深度学习通过学习模型的“深层结构”从而对数据中存在的复杂关系进行建模，虽不能显著降低计算要求，但在小规模有标注样本和极大规模无标注样本的融合学习中，可能会给语义分析带来突破性的成果。 NLP的应用核心技术 1.自然语言处理的各个任务中，首先需要有数据收集。常见的有三种类型的数据：词典，分词和词法分析内可用到；知识库，多用于高级语义分析；语料，用于统计词汇共现等数据。本章第二节已提到大数据收集的必要性。 2.随后，第二层是词条级，这是语义理解的基础层面。其中，词法分析包含分词分词、词性标注和未登录词识别；词条语义的表示；词条关系表示和知识库构建。在词条基础上进行的浅层词法分析、句法分析和信息检索中的查询扩展替换仍在各类应用中占据重要地位。比如，近年来，基于用户意图分析的查询研究越来越热。 3.在词语语义计算的基础上，第三层就是篇章级的语义理解，分为单文档分析和多文档分析，pLSA和主题模型都属于此范畴。]]></content>
      <categories>
        <category>Artificial Intelligence</category>
      </categories>
      <tags>
        <tag>Artificial Intelligence</tag>
        <tag>Reading Notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[人工智能概念初导]]></title>
    <url>%2F2017%2F09%2F11%2F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A6%82%E5%BF%B5%E5%88%9D%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[本文阐述了人工智能的内涵和外延，从概念到层次到技术体系到应用领域，以及顺带提了下机器学习和深度学习。 内涵和外延什么是人工智能 与人类大脑的生物学近似性，与人类思考逻辑的近似性，与人类行为的近似性； 实用主义定义：研究、开发用于模拟、延伸和扩展人的智能理论、方法、技术及应用系统的一门新的技术科学，是计算机科学的一个分支； 教科书定义：人工智能科学的主旨是研究和开发出智能实体；是一门综合学科，数学、逻辑学、归纳学、系统学、控制学、工程学、计算机科学、哲学、心理学、生物学、神经科学、仿生学、经济学、语言学； 人工智能的层次 弱人工智能：擅长于单个方面，如Alphago； 强人工智能：各方面可与人类比肩；能进行思考、计划、解决问题、抽象思维、理解复杂理念、快速学习和从经验中学习等； 超人工智能：超越人类智慧； 未来学 阿西莫夫机器人三定律 人类命运与费米悖论：宇宙中智慧生命存在的必然可能性与至今未发现智慧生命的悖论； 人类与机器的关系：伦理学； 智慧的本质：哲学； 宇宙的命运：物理宇宙学； 对社会经济的影响 产业变革 失业和社会保障 贫富差距 地区发展不平衡：高科技产业的地域性； 产业结构调整：人与机器的分工； 服务业 教育、职业培训、再教育：对Ai不擅长的领域进行针对性培训； 对个人影响：失业和社保；心理层面（人类的自我价值，自我实现，人机协同时代的人类心理学）；个人教育；择业； 技术体系 数学基础：微积分，线代，概率论，信息论，集合论和图论，博弈论； 技术机基础：计算机原理，程序设计语言，操作系统，分布式系统，算法基础； 机器学习算法：机器学习基础（估计方法，特征方程），线性模型（线性回归），逻辑回归，决策树模型（GBDT），支持向量机，贝叶斯分类器，神经网络（深度学习——MLP，CNN，RNN，GAN），聚类算法（K均值算法）； 机器学习分类：监督学习（分类任务，回归任务），无监督学习（聚类任务），迁移学习，强化学习； 问题领域：语言识别，字符识别（手写识别），机器视觉，自然语言处理（机器翻译），自然语言理解，知识推理；自动控制，游戏理论和人机对弈；数据挖掘； 机器学习架构：加速芯片（CPU，GPU，FPGA，ASIC），虚拟化（容器-Decker），分布式结构（Spark），库和计算框架（TensorFlow，Caffe，MXNET，Torch，Microsoft CNTK，scikt-learn）；可视化解决方案，云服务（Amazon ML，Google Cloud ML，Microsoft Azure ML，阿里云ML）； 数据集和竞赛：ImageNet，MSCOCC，Kaggle，阿里天池； 其他相关技术：知识图谱，统计语言模型，专家系统，遗传算法，博弈算法（纳什均衡）； 应用领域 互联网和移动互联网：搜索引擎，内容推荐引擎，精准营销，语音和自然语言交互，图像和视频内容理解、检索，用户画像，反欺诈； 自动驾驶、智慧交通、物流、共享出行：自动驾驶汽车（传感器、感知、规划、控制、整车集成、车联网、高精度地图、模拟器），智慧公路网络和交通标志，共享出行，自动物流车辆和物流机器人，智慧物流规划； 智能金融：银行业（风控和反欺诈、精准营销、投资决策、智能客服），保险业（风控和反欺诈，精准营销，智能理赔，智能客服），证券、基金、投行（量化交易，智能投顾）； 智慧医疗：医疗影响识别，辅助诊断，病例理解和检索，康复智能设备，智能制药； 家用机器人和服务机器人：智能家居，老幼伴侣，生活服务； 智能制造业：工业机器人，智能生产系统； 人工智能辅助教育：智慧课堂，学习机器人； 智慧农业：智慧农业管理系统，智慧农业设备； 智慧新闻写作：写稿机器人，收集资料机器人； 机器翻译：文字翻译，声音翻译； 机器仿生：动物仿生，器官仿生； 智能律师助理：智慧法律咨询，案例数据库机器人； 人工智能驱动的娱乐业； 人工智能艺术创作； 智能客服； ML &amp; DL1.机器学习：研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能； 2.深度学习： 通过组合低层特征形成更加抽象的高层表示属性类别或特征，以发现数据的分布式特征表示； 强化学习、迁移学习、生成对抗网络等新技术的发展；]]></content>
      <categories>
        <category>Artificial Intelligence</category>
      </categories>
      <tags>
        <tag>Artificial Intelligence</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[UI 设计规范]]></title>
    <url>%2F2017%2F09%2F10%2FUI-%E8%AE%BE%E8%AE%A1%E8%A7%84%E8%8C%83%2F</url>
    <content type="text"><![CDATA[从设计稿尺寸、屏幕适配、切图、ios／android设计规范等方面讲解UI设计规范的入门知识。 1.设计稿尺寸(1)app界面构成：布局层，图文排版层，图标层； (2)ios设计搞尺寸：750*1334 px （4.7英寸） 图标用栅格化系统设计 (3)设计字体：iOS 9：英文字体为Helvetica Neue iOS 9：中文字体由为冬青黑 iOS 10：英文字体为San Francisco iOS 10：中文字体为苹方 关于字体大小的问题：顶部操作栏文字大小 34-38px标题文字大小 28-34px正文文字大小 26-30px辅助性文字大小 20-24pxTab bar文字大小 20px 文字大小只是一个范围，这要根据设计的视觉效果来决定，不要死记硬背，但是切记，字体大小要用偶数。 2.UIUI定义：user interface，系统和用户指甲 进行交互和信息交换的媒介，是实现信息的内部形式与人类可接受形式之间当转换； 3.ios&amp;安卓设计标准规范: 4.尺寸和切图各屏幕适配——不同的dpi及屏幕尺寸 (1)专用词 dpi（像素密度）dpi一般包含：mdi~160dpi、hdpi~240dpi、xhdpi~320dpi、xxhdpi~480dpidp即为dip:device independent pixels(设备独立像素)表示长度、高度等属性时用。sp=scaled pixels(放大像素). 主要用于字体显示best for textsize。设置字体时用 (2)换算公式dp=px /（dpi/160）dpi=（√（横向分辨率^2+纵向分辨率^2））/屏幕尺寸sp计算同dp (3)常用dpi举例 (4)原则：简洁，高效，移动化，情感化，卡片化设计，反馈，一致性，亲密性 (5)android设计规范：]]></content>
      <categories>
        <category>Product Manager</category>
        <category>Design</category>
      </categories>
      <tags>
        <tag>UI Design</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[离散数学概念初导]]></title>
    <url>%2F2017%2F09%2F09%2F%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6%E6%A6%82%E5%BF%B5%E5%88%9D%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[离散数学研究基于离散空间而不是连续的数学结构。 其研究对象——例如整数、图和数学逻辑中的命题——不是光滑变化的，而是拥有不等、分立的值。因此离散数学不包含微积分和分析等连续数学的内容。离散对象经常可以用整数来枚举。离散空间：离散空间是特别简单的一种拓扑空间，在其中点都在特定意义下是相互孤立的。 拓扑空间：topological space，赋予拓扑结构的集合。拓扑空间就是最一般的空间，上面的所有性质都是由上面定义的拓扑，或者说，开集决定。拓扑空间是一种数学结构，可以在上头形式化地定义出如收敛、连通、连续等概念。 拓扑空间的定义：设X是一个非空集合，若U是X的子集组成的一个集族，满足： O1：￠、X∈U O2：T1，…，Tn∈U→∩（i=1,…,n）Ti∈U O3：Tα∈U，α∈I→∪（α∈I）Tα∈U 则称U是X上的一个拓扑，称（X，U）是一个拓扑空间，U中的集合称为开集. 公理T1-T3称为是开集公理. 拓扑结构：如果对一个非空集合X给予适当的结构，使之能引入微积分中的极限和连续的概念，这样的结构就称为拓扑，具有拓扑结构的空间称为拓扑空间。引入拓扑结构的方法有多种，如邻域系、开集系、闭集系、闭包系、内部系等不同方法。 ​ // 今天离散数学开课了(/ω＼)]]></content>
      <categories>
        <category>Math</category>
        <category>Discrete Mathematics</category>
      </categories>
      <tags>
        <tag>Math</tag>
        <tag>Discrete Mathematics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解集合嵌套]]></title>
    <url>%2F2017%2F09%2F09%2F%E7%90%86%E8%A7%A3%E9%9B%86%E5%90%88%E5%B5%8C%E5%A5%97%2F</url>
    <content type="text"><![CDATA[本文从集合论的角度理解集合嵌套，并例举其在Python里的用法。 集合与集合间的关系： 子集：若集合A中的每个元素都是B的元素，则A是B的子集。 真子集：A是B的子集且B中至少有一个元素不属于A，则A是B的真子集。 元素与集合的区分：元素不带花括号，集合带花括号。 元素与集合是从属关系，如x ∈ A；而集合与集合间是包含关系，如A⊆B。但集合与集合之间只能有包含关系的前提是：集合位于同一层次。 集合嵌套：当集合A里的元素由集合B、C构成，则B ∈ A是正确的（将集合里的集合看成元素，则其可与父集合有从属关系）。 Python——多层嵌套list的递归处理方法Q：用Python处理一个多层嵌套list 1[&apos;and&apos;, &apos;B&apos;, [&apos;not&apos;, &apos;A&apos;],[1,2,1,[2,1],[1,1,[2,2,1]]], [&apos;not&apos;, &apos;A&apos;, &apos;A&apos;],[&apos;or&apos;, &apos;A&apos;, &apos;B&apos; ,&apos;A&apos;] , &apos;B&apos;] 处理需求： 展开成一层 删除重复元素，包括重复的list 12345678910111213141516171819202122232425262728293031def onelist(ll):&quot;&quot;&quot;功能：用递归方法删除多层列表中重复元素&quot;&quot;&quot;result = []for i in ll: if isinstance (i, list): if onelist(i) not in result: result.append(onelist(i)) else: if i not in result: result.append(i) return resultdef flatten(ll): &quot;&quot;&quot; 功能:用递归方法展开多层列表,以生成器方式输出 &quot;&quot;&quot; if isinstance(ll, list): for i in ll: for element in flatten(i): yield element else: yield lltestcase= [&apos;and&apos;, &apos;B&apos;, [&apos;not&apos;, &apos;A&apos;],[1,2,1,[2,1],[1,1,[2,2,1]]], [&apos;not&apos;, &apos;A&apos;, &apos;A&apos;],[&apos;or&apos;, &apos;A&apos;, &apos;B&apos; ,&apos;A&apos;] , &apos;B&apos;]print onelist(testcase)print list(flatten(testcase)) output: 123[&apos;and&apos;, &apos;B&apos;, [&apos;not&apos;, &apos;A&apos;], [1, 2, [2, 1], [1, [2, 1]]], [&apos;or&apos;, &apos;A&apos;, &apos;B&apos;]][&apos;and&apos;, &apos;B&apos;, &apos;not&apos;, &apos;A&apos;, 1, 2, 1, 2, 1, 1, 1, 2, 2, 1, &apos;not&apos;, &apos;A&apos;, &apos;A&apos;, &apos;or&apos;, &apos;A&apos;, &apos;B&apos;, &apos;A&apos;, &apos;B&apos;]]]></content>
      <categories>
        <category>Math</category>
        <category>Discrete Mathematics</category>
      </categories>
      <tags>
        <tag>Computer Science</tag>
        <tag>Python</tag>
        <tag>Discrete Mathematics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解尾调用优化]]></title>
    <url>%2F2017%2F09%2F09%2F%E7%90%86%E8%A7%A3%E5%B0%BE%E8%B0%83%E7%94%A8%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[上篇讲了递归的理解，里面涉及到了对尾递归优化的阐述，不过不够详尽。这篇将详细阐述从尾调用到尾递归。 尾调用（Tail Call）概念：某个函数的最后一步是调用另一个函数。 function f(x){ return g(x) 123 } 上面函数的最后一步是调用函数g，此即尾调用。若调用之后还有其他操作，则不属于尾调用。譬如： //condition 1 fucntion f(x){ let y = g(x); return y; } //condition 2 function f(x){ return g(x) + 1; } 尾调用不一定出现在函数尾部，只要是最后一步即可。如下面代码中函数m、n都是尾调用。 function f(x){ if (x &gt; 0){ return m(x) } return n(x)} 尾调用优化调用栈：函数调用会在内存形成一个“调用记录（调用帧／call frame）”，用以保存调用位置和内部变量等信息。在函数A内部调用函数B，则会在A的调用记录上方形成一个B的调用记录。等到B运行结束， 将结果返回A，B的调用记录才会消失。若函数B内部还调用C，则还有一个C的调用记录栈，依此类推。所有的调用记录形成一个调用栈（call stack）。 上图表示一个调用栈及递归调用的过程。矩形块表示一个个调用帧／调用记录／call frame，箭头表示状态，从左至右是流程。 尾调用优化：只保留内层函数的调用记录（实际上是以内层函数直接取代外层函数的调用记录）。尾调用可大大节省内存，因为调用记录只有一项。 function f(){ let m = 1; let n =2; return g(m + n); } f(); //is equal to function f(){ return g(3) } //is equal to g(3) 如上例，若函数不是尾调用，f函数就需保存m和n的值以及g的调用位置等信息。但由于调用了g后，函数f就结束了，故最后完全可删除法f()的记录，只保留g(3)的调用信息。 尾递归函数调用自身，称为递归。如果尾调用自身，就称为尾递归。 为什么用尾递归：递归非常耗费内存，因为需要同时保存成千上百个调用记录，很容易发生”栈溢出”错误（stack overflow）。但对于尾递归来说，由于只存在一个调用记录，所以永远不会发生”栈溢出”错误。 //递归求阶乘的算法，最多需要保存n个调用记录，复杂度O(n) function fact(n){ if (n == 1) return 1; return n * fact(n -1); } fact(5) //改成尾递归,只保留一个调用记录，复杂度O(1) function fact(n, total){ if (n == 1) return total; return fact(n -1, n * total) } fact(5, 1) 递归函数的改写柯里化（currying）：将多参数的函数转化为单参数形式。 //由于参数total有默认值1，故调用时不用提供此值。 function fact(n, total = 1){ if (n == 1) return total; return fact(n - 1, n * total) } fact(5) 递归本质是循环操作，所有的循环都用递归实现。一旦使用递归，最好使用尾递归。 趣味例子摘自某乎上的一个形象解释尾递归与递归的区别的例子： function story() { 从前有座山，山上有座庙，庙里有个老和尚，一天老和尚对小和尚讲故事：story() // 尾递归，进入下一个函数不再需要上一个函数的环境了，得出结果以后直接返回。} function story() { 从前有座山，山上有座庙，庙里有个老和尚，一天老和尚对小和尚讲故事：story()，小和尚听了，找了块豆腐撞死了 // 非尾递归，下一个函数结束以后此函数还有后续，所以必须保存本身的环境以供处理返回值。}]]></content>
      <categories>
        <category>Computer Science</category>
        <category>Data Structure</category>
      </categories>
      <tags>
        <tag>Computer Science</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解递归思想]]></title>
    <url>%2F2017%2F09%2F08%2F%E7%90%86%E8%A7%A3%E9%80%92%E5%BD%92%E6%80%9D%E6%83%B3%2F</url>
    <content type="text"><![CDATA[什么是递归递归（Recursion），指在函数的定义中使用函数自身的方法，即程序的自身调用。 递归一词还较常用于描述以自相似方法重复事物的过程。例如，当两面镜子相互之间近似平行时，镜中嵌套的图像是以无限递归的形式出现的。也可以理解为自我复制的过程。 递归算法的特点 递归就是方法里调用自身。 出口：在使用递增归策略时，必须有一个明确的递归结束条件，称为递归出口。 效率：递归算法解题通常显得很简洁，但递归算法解题的运行效率较低。所以一般不提倡用递归算法设计程序。 栈溢出：在递归调用的过程当中系统为每一层的返回点、局部量等开辟了栈来存储。递归次数过多容易造成栈溢出等，所以一般不提倡用递归算法设计程序。 递归程序的基本步骤1.初始化算法。递归程序通常需要一个开始时使用的种子值（seed value）。要完成此任务，可以向函数传递参数，或者提供一个入口函数， 这个函数是非递归的，但可以为递归计算设置种子值。1 2.检查要处理的当前值是否已经与基线条件相匹配。如果匹配，则进行处理并返回值。 3.使用更小的或更简单的子问题（或多个子问题）来重新定义答案。 4.对子问题运行算法。 5.将结果合并入答案的表达式。 6.返回结果。 总结流程：初始化——检查当前值与基线条件的匹配——化小重定义——对子问题运行算法——结果归总——返回结果 递归与循环的比较 Properties Loops Recursive functions 重复 为了获得结果，反复执行同一代码块；以完成代码块或者执行 continue 命令信号而实现重复执行。 为了获得结果，反复执行同一代码块；以反复调用自己为信号而实现重复执行。 终止条件 为了确保能够终止，循环必须要有一个或多个能够使其终止的条件，而且必须保证它能在某种情况下满足这些条件的其中之一。 为了确保能够终止，递归函数需要有一个基线条件，令函数停止递归。 状态 循环进行时更新当前状态。 当前状态作为参数传递。 例子计算阶乘n! = 1 x 2 x 3 x … x n，用函数fact(n)表示： def fact(n): if n == 1: return 1 return n * fact(n - 1) 尾递归—解决栈溢出栈溢出：使用递归函数需要注意防止栈溢出。在计算机中，函数调用是通过栈（stack）这种数据结构实现的，每当进入一个函数调用，栈就会加一层栈帧，每当函数返回，栈就会减一层栈帧。由于栈的大小不是无限的，所以，递归调用的次数过多，会导致栈溢出。 尾递归（tail-call）优化：在尾部进行函数调用时使用下一个栈结构覆盖当前栈结构，同时保持原来的返回地址。 本质是对栈进行处理，删掉活动记录（activation record），在函数返回的时候，调用自身本身，并且return语句不能包含表达式。这样，编译器或者解释器就可以把尾递归做优化，使递归本身无论调用多少次，都只占用一个栈帧，不会出现栈溢出的情况。 要使调用成为真正的尾部调用，在尾部调用函数返回前，对其结果不能执行任何其他操作。 不管递归有多深，栈大小保持不变。尾递归属于线性递归的子集。 用尾递归优化改造上面的阶乘算法，主要是要把每一步的乘积传入到递归函数中： def fact(n): return fact_iter(n, 1) def fact_iter(num, product): if num == 1: return product return fact_iter(num - 1, num * product) 可以看到，return fact_iter(num - 1, num product)仅返回递归函数本身，num - 1和num product在函数调用前就会被计算，不影响函数调用。 尾递归事实上和循环是等价的，没有循环语句的编程语言只能通过尾递归实现循环。]]></content>
      <categories>
        <category>Computer Science</category>
        <category>Data Structure</category>
      </categories>
      <tags>
        <tag>Computer Science</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫技术入门知识]]></title>
    <url>%2F2017%2F09%2F08%2FPython%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF%E5%85%A5%E9%97%A8%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[涉及的技术 熟练一门编程语言python html知识（HTML 就是一个文档树结构） http／https协议基本知识（爬虫基本原理就是通过网络请求从远程服务器下载数据的过程，而这个网络请求背后的技术就是基于 HTTP 协议。） 正则表达式 数据库 常用抓包工具的使用 爬虫框架的使用 分布式概念 消息队列 常用数据结构和算法 缓存 机器学习的应用 作用 爬虫只是为了 获取数据，分析、挖掘数据才是价值； 数据分析、数据挖掘； 学习阶段 1.入门：掌握基础知识 2.模仿：跟着别人的爬虫学，弄懂每行代码 3.自己动手，独立设计爬虫系统 基础知识 流程：从数据的抓取到清洗再到存储 Requests ：一个模拟浏览器发送 HTTP 请求的网络库 了解 HTTP 协议之后，你就可以专门有针对性的学习和网络相关的模块了，比如 Python 自带有 urllib、urllib2（Python3中的urllib），httplib，Cookie等内容，当然你可以直接跳过这些，直接学习 Requests 怎么用，前提是你熟悉了 HTTP协议的基本内容。 数据爬下来，大部分情况是 HTML 文本，也有少数是基于 XML 格式或者 Json 格式的数据，要想正确处理这些数据，你要熟悉每种数据类型的解决方案，比如JSON数据可以直接使用 Python自带的模块 json，对于 HTML 数据，可以使用 BeautifulSoup、lxml 等库去处理，对于 xml 数据，除了可以使用 untangle、xmltodict等第三方库。 Python 的 re 模块可用来处理正则表达式 数据清洗完最终要进行持久化存储，你可以用文件存储，比如CSV文件，也可以用数据库存储，简单的用 sqlite，专业点用 MySQL，或者是分布式的文档数据库 MongoDB，这些数据库对Python都非常友好，有现成的库支持。 很多网站都设有反爬虫策略，他们想方设法阻止你用非正常手段获取数据，比如会有各种奇奇怪怪的验证码限制你的请求操作、对请求速度做限制，对IP做限制、甚至对数据进行加密操作，总之，就是为了提高获取数据的成本。这时你需要掌握的知识就要更多了，你需要深入理解 HTTP 协议，你需要理解常见的加解密算法，你要理解 HTTP 中的 cookie，HTTP 代理，HTTP中的各种HEADER。 爬虫常用库 urllib、urlib2（Python中的urllib）python内建的网络请求库 urllib3：线程安全的HTTP网络请求库 requests：使用最广泛的网络请求库，兼容py2和py3 grequests：异步的requests BeautifulSoup：HTML、XML操作解析库 lxml：另一种处理 HTML、XML的方式 tornado：异步网络框架 Gevent：异步网络框架 Scrapy：最流行的爬虫框架 pyspider：爬虫框架 xmltodict：xml转换成字典 pyquery：像jQuery一样操作HTML Jieba ：分词 SQLAlchemy：ORM框架 celery ：消息队列 rq：简单消息队列 python-goose ：从HTML中提取文本 书籍《图解HTTP》《HTTP权威指南》《计算机网络：自顶向下方法》《用Python写网络爬虫》《Python网络数据采集》《精通正则表达式》《Python入门到实践》《自己动手写网络爬虫》《Crypto101》《图解密码技术》]]></content>
      <categories>
        <category>Computer Science</category>
        <category>Data Mining and Analysis</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阅读笔记：《术与道：移动应用UI设计》]]></title>
    <url>%2F2017%2F09%2F08%2F%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%EF%BC%9A%E3%80%8A%E6%9C%AF%E4%B8%8E%E9%81%93%EF%BC%9A%E7%A7%BB%E5%8A%A8%E5%BA%94%E7%94%A8UI%E8%AE%BE%E8%AE%A1%E3%80%8B%2F</url>
    <content type="text"><![CDATA[书籍简介：对移动应用设计的规范和技巧形成入门级的了解。 1.UI设计 用户界面：WUI &amp; GUI：web user interface；graphics user interface； UE：性能（运行快，稳定，占资源），内容（内容是否能解决问题），交互（交互流畅，无障碍，界面（logo，主题，颜色，布局等是否整齐一致高质量）； UED：designer UCD：user centered design BTU：business，technique，user 工具：after effects动效制作；mark man标注；尺寸概念 英寸inch 像素pixel 分辨率：屏幕像素数量；iphone6:750*1334； 网点密度dpi（dot per inch） 像素密度ppi（pixel per inch）：每英寸像素数量；ppi计算公式； 逻辑分辨率：软件可达到的分辨率； 像素倍率：@1x，@2x，@3x，分别用在2倍率和3倍率到Retina屏幕上； 传感器 加速度传感器 距离传感器 光线传感器 气压传感器 三轴陀螺仪：获取角度信息 2. 设计风格 产品气质——确定主色——图标插图——选取符合产品气质的字体——排版（按主题+美学）——文案 颜色 主色（logo，导航栏etc）+辅助色（控件图标插图）+点睛色（醒目，提示性到小图标） RGB HSB：hues色相（颜色），saturation饱和度（鲜艳程度），brightness明度（明暗度，深浅度）； 颜色搭配：色环（12色圆环），互补色（色环中相对色），三色搭配原则； 选色：白（纯洁神圣，善良，信任，开放；疏离，梦幻；大量留白有格调）；蓝（灵性与知性，希望梦想，独立；暗蓝：诚实，信赖，权威）；红（热情性感，权威，自信）；橙（亲切坦率，开朗健康，激情和参与）；黄（刺激，艳黄：征信心，聪明，希望，淡黄：天真浪漫娇嫩）；绿（安全，自由和平，新鲜舒适，沉稳知性）；粉（温柔甜美，浪漫，洒脱大方，女性）；黑（权威高雅，低调创意，冷漠防御）；灰（诚恳，考究，智能，沉静，成功，认真）；紫（优雅浪漫，哲学家，魅力）；褐色棕色咖啡色（安定，沉静，亲切，稳定，友善） 字体 衬线体（serif）和无衬线字体（sans-serif）：衬线——字母结构之外到装饰性笔画， 内嵌字体：文件过大 3.图标与图片图标特点 独特：识别性； 表意准确 谨慎用色 避免文字：用首字母设计图标易雷同 避免照片 适应不同尺寸 图标设计流程 寻找隐喻：抽象 抽象图形 竞品分析：避免雷同 确定风格：层次感，质感 调整细节：观感评价 场景测试：适配，不同尺寸下的识别性 设计法 正负形组合：主图+副图，结合叠加抠除； 折叠图形：局部折叠处理； 局部提取：提出局部有代表性到元素设计 线性图标：提炼图形到轮廓设计各种线性图形； 透明渐变 色块拼接 图形复用：复制得新形 背景组合 ios图标规范 ios图标适配：做大不做小，做大尺寸图标再缩放成小； app图标；app store图标（10241024，圆角像素160）；标签栏导航图标（5050）；导航栏图标（4444）；工具栏图标（4444）；设置图标（5858）；web clip图标（120120）； android图标规范 低密度屏幕LDPI（7575），中低密度屏幕MDPI（100100），高低密度屏幕HDPI（150150），超高低密度屏幕XHDPI（200200），超超高低密度屏幕XXHDPI；密度关系：3:4:6:8:12； 图标视觉统一 图标大小 设计线性图标： 风格：大圆角风格；直角风格；断线风格； 像素对齐 图片的使用 格式：jpg有损；png无损压缩格式；png8（256色）；png24（1600万）； 图片比例：1:1（用于头像，产品列表），3:2（产品详情页），4:3/6:9； 4.ios系统界面栏 状态栏status bar 导航栏navigation bar 标签栏tab bar：最底部 工具栏toolbar：最底部（一个视图中只能存在一个标签栏／工具栏 内容视图 表格视图：纵向表格（table view）；辅助说明型表格（有补充说明的副标题）；内容强调型（主标题左对齐，副标题右对齐，强调项目当前状态） 文本视图（text view） web视图（web view）：H5页面，可极快发布内容而不用等版本更新； 临时视图 对话框 操作列表：侧边或向下滑动 5.android系统界面导航机制界面布局消息推送 推送方式：iOS（本地只需要与苹果服务器通信，苹果服务器作为中转站与其他应用服务器通信，再告诉系统，android（每个需要后台推送的应用都有各自的后台进程，各自与各自到服务器通信），由于ios只用与一个服务器通信，所以内存占用少，安卓的特点是把推送的控制权放在开发者手里； 通知方式：ios（条幅式通知，icon右上角，弹出式，声音；每种方式给用户到通知强度和心理影响不同，设计时需认真考虑层级，谨慎使用），android（通知区域，进行中，弹出式，提示条） 操作方式：iOS（单击编辑进入编辑模式），android（长按进入选择模式，选择内容后出现情景操作栏覆盖在当前操作栏上） 选择：iOS（单击出现滚轮盘），android（弹出浮层）； 删除：ios（左滑出现删除按钮），android（单击删除按钮）android系统插件 桌面插件 弱化插件 6.建立规范设计规范： 个性化，树立品牌威信 规范的组成：全局图形（app图标，全局图形）；调色板；常用控件（图片，图标，按钮，单选，复选，表格，输入框，下拉框）；常用组件（加载动画，翻页，加载更多，页面指示器，浮层反馈，信息提示，选项卡，导航栏）；典型页面（app到一级页面）； 颜色规范：推荐用HSB；创建调色板：主色+辅助色+灰度色； 全局色（主色，辅助色），背景色，分割线用色，文字用色，图标用色； 文字规范：iOS（黑体-简；helvetica neue），android（方正兰亭黑，roboto）； 系统字号：导航栏-36px；标题／大按钮—30px；主文字／正文／小按钮-28px；提示性文字-24px；底部标签栏-22px；提示文字-18px； 布局：边距20px，模块间距-30px，顶部导航栏，顶部标签栏 图片规范 无数据图片：（在无数据时显示的底图）商品底图，头像底图，无数据图； 7.设计组件控制元素 活动指示器：表达持续时间不明的进程 进度指示器：展示可预测完成度的任务 页码控制器：page control，显示有多少页视图，不超过5个点，灰色为20%透明度； 刷新控件：refresh control，刷新当前页面，下拉刷新+上拉刷新； 滑动器：slider，包含滑轨+滑块； 开关switch 步进器筛选器 选择器picker 日期时间选择器date picker 分段控件segmented control 选项卡 排序 地区选择表单控件 单选框radio 复选框checkbox 文本框textfield 下拉框drop list：需要有默认文字提示； 表格按钮规范 背景+文字；背景+图标；图标+文字；文字；其他组件 加载更多：配合活动指示器使用 非模态浮层：轻量级提示 模态浮层：透明度90%的黑色或白色，重量级提示； 8.导航设计扁平导航 标签导航：标签栏 舵式导航：中间的标签作为重要且操作频繁的入口，凸显； 宫格式导航：登陆界面正中心的菜单即进入各部分功能的起始点；内容主导式导航 陈列式导航：直接在界面显示各个内容项实现导航； 旋转木马式导航：轮播式；列表式导航： 通常用于二级页面，清晰高效 列表式：列表菜单 抽屉式：侧滑导航，核心：隐藏非核心功能使用户更专心当前页面；其他导航 点聚式导航：单击一个按钮出来多个按钮 瀑布式：卡片浏览 9.界面设计首页 搜索栏：主色／白色透明作为背景 标签栏：白色／灰黑背景，浅灰／深灰图标； 卡片式设计：内容区域分割 楼层设计 列表页 消息列表 横向卡片 瀑布流：每个模块高低不同 详情页 全局按钮：将页面的最终目的一直展现在用户眼前，如加入购物车； 快速通道：如在深层次页面设置直达首页的按钮，可在导航栏右侧加一个弹出菜单； 个人中心 头像：居中&amp;居左 个人信息 功能模块：表单&amp; 图标 启动页 情感化设计，品牌宣传，彰显个性；占时间，不效率 品牌宣传类：产品名+标语 首页样式类：类似首页的静态图作为过渡，使用户以为已经启动；or 用首页渡局部元素； 情感故事类：说明一个故事or情怀 节日氛围类：应用基础元素不变的基础上与节日主题结合； 引导页：第一次使用告诉用户产品渡主要功能与特点 功能介绍类：截取主要界面配文字；插图配文字； 使用说明类：对用户使用中可能遇到的困难／误操作等提前告知；手绘风格为主；1.透明蒙层+插图；2.动态图+文字； 情感故事类：传达产品态度，功能引导；1插图+文字；2.视差动画，文+音+动画，可创新页面切换方式；3.视频展示； 10.设计适配ios系统适配 ios系统适配 设计基准 设计适配：1.设计基准稿（i6:750*1334）；2.界面调试（不适配的界面要重新设计）3.切图输出（图标，控件；等比放大1.5倍，输出@3x的切图）； 适配规则：文字流式+控件弹性+图片等比缩放 android系统适配 密度独立像素DP（density independent pixels）：以160dpi（MDPI）为基准，则1dp=1px；320dpi（XHDPI）为基准，1dp=2px； 设计基准 适配方法： 1.720*1280（XHDPI）做基准稿，除图片外所有设计元素用矢量路径做，图片用智能对象方便缩小放大，在7201280上做标注，输出标注图 ； 对图标+控件等元素等比放大1.5倍，生成XXHDP到@3x切图，HDPI切图不用单独提供，由开发调试系统生成； 3.输出基于XHDPI到@2x切图+放大后到@3x切图+相应控件到点九图；标注使用给予XHDPI做的标注； ios系统适配android系统 目的：省力，故设计以iOS设计为主导，只输出一套设计； 换算关系：XHDPI模式下，android与iOS切图尺寸一样； 标注设计 标注基准：android（4dp，8，16，32，48；如边距16dp，元素间留白值8dp，导航栏／标题栏／按钮48dp） 标注方法：1.标注图素到大小+间距；2.标注图素到颜色+透明度；3.标注图素的状态变化； 点九图：使图片横竖都可完美展示； 图标切图：标注要与实际输出的切图的尺寸保持一致； 命名规则：界面／功能+控件名+状态+补充描述； 11.手势应用 标准，简单，慎重创新；引导； 图册链接]]></content>
      <categories>
        <category>Product Manager</category>
        <category>Design</category>
      </categories>
      <tags>
        <tag>UI Design</tag>
        <tag>Reading Notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[结巴分词及其使用]]></title>
    <url>%2F2017%2F09%2F08%2F%E7%BB%93%E5%B7%B4%E5%88%86%E8%AF%8D%E5%8F%8A%E5%85%B6%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[分词模式 精确模式：精确切开句子，适合文本分析； 全模式：把句子中可扫描的句子都扫出来，速度很快，但不能解决歧义； 搜索引擎模式：在精确模式上对长词再切分，适合搜索引擎分词； example： 123456789101112import jiebaseg_list = jieba.cut(&quot;我来自北京清华大学&quot;，cut_all=True)print(&quot;Full Mode:&quot; + &quot;/&quot; &quot;.join(seg_list)&quot;)seg_list = jieba.cut(&quot;我来到北京清华大学&quot;, cut_all=False)print(&quot;Default Mode:&quot; + &quot;/&quot;.join(seg_list))seg_list = jieba.cut(&quot;他来到了网易杭研大厦&quot;)print（&quot;, &quot;.join(seg_list))seg_list = jieba.cut_for_search(&quot;小明硕士毕业于中国科学院计算所，后在日本京都大学深造&quot;)print(&quot;, &quot;.join(seg_list)) output: 1234567【全模式】: 我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学【精确模式】: 我/ 来到/ 北京/ 清华大学【新词识别】：他, 来到, 了, 网易, 杭研, 大厦 (此处，“杭研”并没有在词典中，但是也被Viterbi算法识别出来了)【搜索引擎模式】： 小明, 硕士, 毕业, 于, 中国, 科学, 学院, 科学院, 中国科学院, 计算, 计算所, 后, 在, 日本, 京都, 大学, 日本京都大学, 深造 分词语法 jieba.cut 方法接受三个输入参数: 需要分词的字符串；cut_all 参数用来控制是否采用全模式；HMM 参数用来控制是否使用 HMM 模型 jieba.cut_for_search 方法接受两个参数：需要分词的字符串；是否使用 HMM 模型。该方法适合用于搜索引擎构建倒排索引的分词，粒度比较细待分词的字符串可以是 unicode 或 UTF-8 字符串、GBK 字符串。注意：不建议直接输入 GBK 字符串，可能无法预料地错误解码成 UTF-8 jieba.cut以及 jieba.cut_for_search 返回的结构都是一个可迭代的 generator，可以使用 for 循环来获得分词后得到的每一个词语(unicode)，或者用 jieba.lcut 以及jieba.lcut_for_search 直接返回 list jieba.Tokenizer(dictionary=DEFAULT_DICT) 新建自定义分词器，可用于同时使用不同词典。jieba.dt 为默认分词器，所有全局分词相关函数都是该分词器的映射。 算法 基于前缀词典实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图 (DAG) 有向无环图：如果一个有向图从任意顶点出发无法经过若干条边回到该点，则这个图是一个有向无环图（DAG图）。 采用了动态规划查找最大概率路径, 找出基于词频的最大切分组合 对于未登录词，采用了基于汉字成词能力的 HMM 模型，使用了 Viterbi 算法 viterbi:动态规划算法。它用于寻找最有可能产生观测事件序列的维特比路径——隐含状态序列. 基于TF-IDF 算法的关键词抽取import jieba.analyse jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=()) sentence 为待提取的文本topK 为返回几个 TF/IDF 权重最大的关键词，默认值为 20withWeight 为是否一并返回关键词权重值，默认值为 FalseallowPOS 仅包括指定词性的词，默认值为空，即不筛选 原文链接]]></content>
      <categories>
        <category>Computer Science</category>
        <category>Data Mining and Analysis</category>
      </categories>
      <tags>
        <tag>Computer Science</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解C语言指针]]></title>
    <url>%2F2017%2F09%2F08%2F%E7%90%86%E8%A7%A3C%E8%AF%AD%E8%A8%80%E6%8C%87%E9%92%88%2F</url>
    <content type="text"><![CDATA[数据、指令和内存 程序的数据和指令存放在同一内存空间 虚拟存储空间的两个关键要素：内存地址+类型 指针是对内存区域的抽象：指针变量存放目标对象的内存地址 定义和使用指针指针的定义 使用* 标记指针，定义方式与定义变量一样 在同一个变量定义语句中，基本数据类型只能有一个，但是可以有多个形式相同或不同的声明符。这也就是说，同一个语句可以定义出不同类型的变量。 12int *ip1, *ip2; // ip1 和 ip2 都是指向 int 类型变量的指针变量double d, *dp; // d 是 double 类型变量，dp 是指向 double 类型变量的指针变量 可以定义一个指向这一指针的指针 123int val = 1024int *p = &amp;valint **pp = &amp;p pp 是一个指向指向 int 类型变量的指针的指针。 获取对象的地址 用&amp;取地址符号来获取对象地址 指针的类型和对象的类型需要严格匹配12int val = 42;int *p = &amp;val; // &amp;val 返回变量 val 的地址，记录在指向 int 类型变量的指针里 访问指针指向的对象 通过解引用指针 p 来访问变量 val123int val = 1004int *p = &amp;valcout &lt;&lt; *p &lt;&lt; endl 空指针和空类型的指针 空指针是不指向任何对象的指针,字面值是 NULL，它定义在 stdlib 当中 空类型的指针，指的是形如 void *pv 的指针。这是一类特殊的指针；这里的空类型，不是说没有类型，而是说空类型的指针，可以用于存储任意类型对象的地址。 由于空类型的指针可以接受任意类型对象的地址，所以，当编译器拿到一个空类型的指针的时候，它无法知道应该按照何种方式解释和使用指针中记录地址中的内容。因此，空类型指针能够做的事情非常有限：做指针之间的比较、作为函数的输入或输出、赋值给另外一个空类型指针。 1234567int *p1 = NULL; // C 风格的空指针初始化int *p2 = nullptr; // C++ 风格的空指针初始化int *p3 = 0; // 使用字面值常量 0 初始化空指针if (nullptr == p1) &#123; // 思考一下为什么不是 p1 == nullptr ; // do something&#125; 1234567double pi = 3.14;void *pv = &amp;pi; // 使用 void * 存放了一个 double 类型对象的地址double *pd = &amp;pi;pd = pv; // 错误：不能将空类型的指针赋值给其他类型的指针pv = pd; // 正确：空类型的指针可以接受任意类型的指针赋值pd = (double *)pv; // 正确：C 风格的强制类型转换pd = reinterpret_cast&lt;double *&gt;(pv); // 正确：C++ 风格的强制类型转换 const与指针 定义常量，只需要在基本类型前，加上 const 关键字即可 常量的值在生存期内不允许改变 const与指针连用有4种情况： 123456789101112131415161718192021int val = 0; // int 型变量const int cnst = 1; // int 型常量int *pi = &amp;val; // pi 本身是变量，通过 pi 访问的也是变量 // 正确：将变量地址赋值给变量的指针pi = &amp;cnst; // 错误：不允许将常量的地址赋值给变量的指针const int *pci = &amp;cnst; // pci 本身是变量，通过 pci 访问的是常量 (point to const) // 正确：将常量地址赋值给常量的指针pci = &amp;val; // 正确：允许将变量地址赋值给常量的指针int *const cpi = &amp;val; // cpi 本身是常量，通过 cpi 访问的是变量 // 正确：允许将变量地址赋值给变量的指针int fake = 2; // int 型变量cpi = &amp;fake; // 错误：cpi 本身是常量，不能在定义之外赋值const int *const cpci = &amp;val; // cpci 本身是常量，通过 cpci 访问的也是常量 // 正确：允许将变量地址赋值给常量的指针cpci = &amp;fake; // 错误：cpci 本身是常量，不能在定义之外赋值cpci = &amp;cnst; // 错误：cpci 本身是常量，不能在定义之外赋值，哪怕是常量的地址 变量可以是常量，而指针本身也可以是常量。因此在变量和指针两个维度，都可以选择是否为常量 为了区分这两个维度，我们引入顶层 const 和底层 const 的概念： 顶层 const：指针本身是常量。此时，指针在定义初始化之外，不能被赋值修改。称指针为指针常量。层 const：指针指向的变量是常量。此时，不能通过解引用指针的方式，修改变量的值。称指针为常量的指针。 指针与数组123456789int nums[] = &#123;1, 2, 3&#125;;int *p = &amp;(nums[0]);if (p == nums) &#123; printf("true!\n");&#125;size_t i = 0;for (i = 0; i != 3; ++i) &#123; printf("%d\n", p[i]);&#125; 数组指针可自增，可加减运算 123456int nums[] = &#123;0,1,2,3,4,5&#125;;size_t len = sizeof(nums) / sizeof(nums[0]);int *iter, end = nums[len]; // end 是尾后指针for (iter = nums; iter != end; ++iter) &#123; printf("%d\n", *iter);&#125; 两个指针如果指向同一个数组中的元素，那么它们可以做差 数组指针与整数的加减，实际是将指针沿着数组进行移动，得到的结果还是一个指针。既然结果是指针，那么就可以解引用，访问数组中的元素 函数与指针让函数返回一个数组的指针 函数在返回时会对返回值拷贝 数组不能被拷贝，函数无法直接返回数组，但可返回数组的指针 如何定义一个返回数组指针的函数：element_type (*func(param_list))[dimension] 123int arr[10]int *parr[10] // parr 是一个数组，长度是 10，元素类型是 int *，也就是数组中存的是指针int (*p)[10]=&amp;arr 1234int (*func(param_list))[10]; // 正确：func 是一个函数，param_list 是它的参数 // 它返回的是一个指针 // 这个指针指向了一个长度为 10 元素类型是 int 型的数组 函数的指针 一个函数的类型，取决于它的输入和输出。这也就是说，一个函数的类型，应当包含它的返回值类型和参数列表 定义一个指向某类型的函数指针 123bool isEqual(int, int);bool (*pfunc)(int, int) = &amp;isEqual; // 定义了一个函数指针，指向 isEqualbool (*pfunc)(int, int) = isEqual; // 一个等价定义 pfunc 就是一个函数指针，它指向一个 bool (int, int) 类型的函数。也就是说，这类函数接收两个 int 型的参数，并返回一个 bool 类型的值。 当函数名字作为值使用时，它会自动地转换成指针（有点像数组名字，不是吗）。因此，在函数指针的初始化或者复制的过程中，取值运算符是可选的。于是，上述两个定义语句是等价的。另一方面，函数指针作为函数调用使用时，它会自动转换成函数名（有点像数组指针，不是吗） 123456bool isEqual(int, int);bool (*pfunc)(int, int) = isEqual;bool res1 = isEqual(1, 2); // 通过原函数名调用bool res2 = (*pfunc)(1, 2); // 一个等价调用：通过函数指针，解引用调用bool res3 = pfunc(1, 2); // 另一个等价调用：函数指针自动转换成函数名 将函数指针作为参数传入另一个函数 函数不能拷贝，但函数指针可以123456void addIfEqual(int lhs, int rhs, bool pfunc(int, int)); // addIfEqual 的第三个参数是一个函数定义 // 它会自动地转换成一个函数指针的参数void addIfEqual(int lhs, int rhs, bool (*pfunc)(int, int)); // 一个等价定义：显式地注明第三个参数是函数指针&gt;&gt;addIfEqual(1, 1, isEqual); 让函数返回一个函数的指针 outer_return_type (*func(param_list))(outer_param_list) func(param_list) 是当前需要定义的函数；outer_return_type 和 outer_param_list 分别是当前定义的函数返回的函数指针对应函数的返回值类型和参数列表。 其他 ((void()())0)()：访问内存地址 0，将它作为一个参数列表和返回类型均为空的函数，并执行函数调用 void(*pfunc)(); 定义了一个函数指针 pfunc，它指向的函数参数列表为空、返回值类型也为空 类型强制转换符：12(double) c; // 将变量 c 强制转换为 double 类型(double *) d; // 将变量 d 强制转换为 double * 类型]]></content>
      <categories>
        <category>Computer Science</category>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>Computer Science</tag>
        <tag>C/C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解内存地址]]></title>
    <url>%2F2017%2F09%2F08%2F%E7%90%86%E8%A7%A3%E5%86%85%E5%AD%98%E5%9C%B0%E5%9D%80%2F</url>
    <content type="text"><![CDATA[物理内存和物理地址物理内存：内存条实际提供的内存空间 内存寻址：在内存上找到正确的位置以便进行存取的过程 内存地址：在内存空间中描述位置的方法 物理内存地址：无符号的整数编号，以byte划分，从0开始编号，逐渐线性增大 硬编码：通过物理地址操作物理内存的写码方式 线性内存和线性地址线性内存和物理内存： 相似点：从0编号，线性增加； 不同：1.物理地址一一对应于实际物理内存空间的位置，而线性地址可多对一（多个线性地址对应一个物理地址）2.物理地址的增加，总是线性地对应着内存空间的位置；但是线性地址的增加，对应到物理地址上，可以分段跳跃。 逻辑内存（虚拟内存）和逻辑地址（虚拟地址）分段：不同级别的程序、程序的不同数据类型，存放在不同的「段」上面，然后再定义在「段」上的偏移量。现代程序看到的地址都不是线性的，而是分段过的地址，形如：segment:offset。 逻辑内存空间：段 + 偏移量组成的空间 逻辑地址：二元组 宽度宽度：位宽；某东西在同一时刻能处理的数据量；单位：位（bit） CPU寻址能力对于 xx 位 CPU 来说，它一次性能够表示的无符号数的范围是 [0,2x−1][0,2x−1]。因此，对这枚 CPU 来说，它以字节（Byte）为单位寻址时，最多能在 2x Bytes2x Bytes 的内存空间中找到它需要的数据。如果在寻址时，不加入其它信息，那么这是它的寻址能力上限。 CPU 需要通过地址总线去内存寻址。若地址总线的带宽为 yy 位，那么在地址总线中传输的物理内存地址的范围是 [0,2y−1][0,2y−1]。也就是说，地址总线的可寻址空间是 2y Bytes2y Bytes。地址总线中的地址，是与物理内存地址保持一致的。 因此，如果在寻址时，不加入其它信息，CPU 具体的寻址能力取决于 CPU 本身的位宽和它连接的地址总线的带宽：2min(x,y) Bytes2min(x,y) Bytes。 内存分页分页就是人为地在逻辑上将连续的内存空间，按照固定大小切分成一段一段。对于线性内存来说，这样切分出来的固定大小叫做「页（Page）」；对于物理内存来说，这样切分出来的固定大小叫做「页帧（Page Frame）」。 分页机制将线性内存分为若干页，将物理内存分为若干帧，并建立从页到帧的映射关系。这个映射关系，是一个「多对一」的映射。 高速缓存高速缓存将最近的访存页面对应的内容保存其中。当 CPU 访存时，首先在缓存中查询是否有目标页内容：若有，则直接存取内容；否则，再进行二级页表的查询，到内存中存取内容。如果缓存存满了，则根据一定的算法（退场机制），将缓存中的过期数据退场。 根据 Intel 自己的统计，大约 98% 的访存请求可以通过高速缓存处理。亦即，只有 2% 的访存请求，需要进行两次页表查询。可见，高速缓存的存在，大大降低了数据交换的量和频次。因此，高速缓存大大提升了寻址/访存速度。]]></content>
      <categories>
        <category>Computer Science</category>
      </categories>
      <tags>
        <tag>Computer Science</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阅读笔记《网络传播导论》]]></title>
    <url>%2F2017%2F09%2F08%2F%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%8A%E7%BD%91%E7%BB%9C%E4%BC%A0%E6%92%AD%E5%AF%BC%E8%AE%BA%E3%80%8B%2F</url>
    <content type="text"><![CDATA[新媒体 旧媒体时代：内容为绝对核心，其他各环节为附属；新媒体：强调内容为王的同时，发挥网络、渠道、平台、终端的作用，服务与市场的理念逐步强化。 互联网的发展 TCP/IP:通信传输协议 TCP(transmission control protocol传输控制协议）：规定一种可靠的数据信息传递服务，负责聚集信息或把文件拆分成更小的包；这些包提高网络传送到接收端的TCP层，接收端的TCP层把包还原为原始文件。 IP(Internet protocol)互联网协议：支持网间互联的数据包协议，处理每个包的分发地址部分，使这些包正确到达目的地；网络上的网关计算机根据数据包的地址来进行路由选择。 下一代互联网：ipv6技术（128位编码），未来信息社会的重要基础设施；端对端的传输速度至少100Mbps，为现在的速度的1000倍；远程教育：基于交互协同视频会议技术进行远程授课辅导；基于高清晰度视频广播技术进行授课内容回放；基于按内容流媒体检索和点播技术实现随时随地按需学习，实现海量信息存储与检索等； 网络经济1.自我膨胀性： 摩尔定律（moore‘s law）：单片硅芯片的运算处理能力每18个月翻一番，与此同时价格减半； 梅特卡夫法则（metcalfe’s law）：网络经济的价值=网络节点数的平方，说明网络产生的效益将随着网络用户的增加而呈指数形式增长。马太效应(matthew effect)：网络经济中由于人们的心理反应和行为惯性，在一定条件下，优势或劣势一旦出现并达到一定程度，就导致不断加剧而自行强化，出现“强者更强，弱者更弱”的垄断局面。 吉尔德定律 2.边际收益递增性： 由于网络信息的重复使用性、信息传递的边际成本为0和平均成本明显递减趋势，故网络信息的平均成本随上网人数的增加而明显递减，边际成本随之缓慢递减，但网络收益却随上网人数的增加而同比例增加。网络规模越大，总收益和边际效益就越大。 网络经济的累积增值性。 网络恶搞流行的原因：其夸张的发泄方式迎合了网络青年的心理需求；社会环境日益宽松、娱乐化的文化氛围提供了滋生土壤； 网站运营 网站盈利的主要方式：在线广告盈利，企业信息化服务盈利（帮助企业建站维护等），电子商务盈利（B2B,B2C,C2C）,网络游戏盈利，无线增值服务盈利（移动运营商），会员制营销服务盈利； 网站运营的主要方式：内容导向，产品导向（提供独特有价值的产品），用户导向（主营业务与用户的结合非常紧密，高粘度，如QQ聊天，微博），服务导向（提供行业化或专业化的服务，如淘宝）；注重用户体验：速度要快，美感要强，域名易记，内容精华，注册快捷，用户中心，发布信息，互动交流； 网站内容管理 八项基本工作：编辑，撰写，发布，修改，删除，查看，整理，汇总； 三个要点：内容版权管理，评论管理，专题管理（利用一件事或一个点进行深度挖掘，制作好的专题吸引用户的焦点关注） 三项基本原则：可用性（为目标群体服务），可寻性，专业性（内容高度集中于所表现的行业，精细化、专业化运营）； 网站安全：网站服务器安全，网站程序安全，网站信息安全； 网民 行为类型：信息型，社交型，娱乐型，逃避型，交易型，求助型，自我实现型； 网民的网络行为特征：虚拟性，技术依赖性，自主性，去抑制性，解构性（实质：反传统，反理性，反中心），猎奇性，从众性 匿名：埃瑟戴森说：匿名是一种非常有用的机制，人们可以把后果讲到最轻微的程度的同时，肆无忌惮地发表自己的看法，对各种主张和幻想甲乙摸索和尝试，并避开世界的非议。 网民的需求特征：环境认知需求，社会交往需求，彰显个性需求，社会参与需求，自我实现需求（成就感，胜任感），逃避现实需求，实用工具需求； 活跃分子 知识分享中的活跃分子的动机：乐趣，求知，互惠利他，获得声誉； 网络书写中的活跃分子：自我表现，生活记录，自由表达，知识管理与分享，被关注； 数字鸿沟 在外延上，数字鸿沟是一个由技术层面、经济层面、知识层面和社会层面共同构成的综合性的差距。 技术层面，技术鸿沟，反映不同主体在接入新兴信息技术方面存在的差距； 经济层面，经济鸿沟，反映国际国内经济不平等和贫富差距在信息时代的延续； 知识层面，知识鸿沟，反映不同群体使用新兴信息技术获取和利用信息资源方面的差距； 社会层面，社会鸿沟，反映信息社会的阶层分化和社会分化现象。]]></content>
      <categories>
        <category>Communication Science</category>
      </categories>
      <tags>
        <tag>Reading Notes</tag>
        <tag>Communication Science</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解矩阵]]></title>
    <url>%2F2017%2F09%2F07%2F%E7%90%86%E8%A7%A3%E7%9F%A9%E9%98%B5%2F</url>
    <content type="text"><![CDATA[矩阵的本质是运动的描述 “空间”是容纳运动的一个对象集合，而变换则规定了对应空间的运动。 空间是一个对象集合，线性空间也是一个对象集合，其对象的特点是：线性空间中的任何一个对象，通过选取基和坐标的办法，都可以表达为向量的形式。 线性空间里的运动被称为线性变换。在线性空间中，当你选定一组基之后，不仅可以用一个向量来描述空间中的任何一个对象，而且可以用矩阵来描述该空间中的任何一个运动（变换）。而使某个对象发生对应运动的方法，就是用代表那个运动的矩阵，乘以代表那个对象的向量。简而言之，在线性空间中选定基之后，向量刻画对象，矩阵刻画对象的运动，用矩阵与向量的乘法施加运动。 矩阵的本质是运动的描述。 这里的“运动”的概念不是微积分中的连续性的运动，而是瞬间发生的变化。比如这个时刻在A点，经过一个“运动”，一下子就“跃迁”到了B点，其中不需要经过A点与B点之间的任何一个点。因此更准确地说，矩阵是线性空间里跃迁/变换的描述；而所谓变换，其实就是空间里从一个点（元素/对象）到另一个点（元素/对象）的跃迁。 所谓相似矩阵，就是同一个线性变换的不同的描述矩阵。按照这个定义，同一头猪的不同角度的照片也可以成为相似照片。 矩阵不仅可以作为线性变换的描述，而且可以作为一组基的描述。而作为变换的矩阵，不但可以把线性空间中的一个点给变换到另一个点去，而且也能够把线性空间中的一个坐标系（基）表换到另一个坐标系（基）去。而且，变换点与变换坐标系，具有异曲同工的效果。 矩阵与向量相乘，就是实施运动（变换）的过程。 同一个变换，在不同的坐标系下表现为不同的矩阵，但是它们的本质是一样的，所以本征值相同。 矩阵乘法 固定坐标系下一个对象的变换等价于固定对象所处的坐标变换。 Ma = b：“有一个向量，它在坐标系M的度量下得到的度量结果向量为a，那么它在坐标系I的度量下，这个向量的度量结果是b。”I是指单位矩阵，就是主对角线是1，其他为零的矩阵。 Ma = Ib的意思就是说：“在M坐标系里量出来的向量a，跟在I坐标系里量出来的向量b，其实根本就是一个向量啊！” 你选择的坐标系（基）不同，得出来的向量的表示就不同。向量还是那个向量，选择的坐标系不同，其表示方式就不同。 “对坐标系施加变换的方法，就是让表示那个坐标系的矩阵与表示那个变化的矩阵相乘。”矩阵的乘法变成了运动的施加。只不过，被施加运动的不再是向量，而是另一个坐标系。]]></content>
      <categories>
        <category>Math</category>
        <category>Linear Algebra</category>
      </categories>
      <tags>
        <tag>Linear Algebra</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[就海底捞危机浅谈危机公关]]></title>
    <url>%2F2017%2F09%2F07%2F%E5%B0%B1%E6%B5%B7%E5%BA%95%E6%8D%9E%E5%8D%B1%E6%9C%BA%E6%B5%85%E8%B0%88%E5%8D%B1%E6%9C%BA%E5%85%AC%E5%85%B3%2F</url>
    <content type="text"><![CDATA[我是很讨厌所谓的危机公关的。尽管它对企业的发展也许有漂亮的扭转作用，可那跟我一个消费者有毛线关系？ 只问一句，危机公关会让这个世界变好吗？它仅表明一个态度：可能态度很好，可能不好，但不改变任何实质性的东西，没有解决任何危机本身揭露出的问题。而一个危机揭露出的漏洞，是实在需要企业采取行动去改变的，以给消费者一个交代，而非只会说漂亮的公关话语来fool消费者。 华丽漂亮的辞藻，诚恳示弱的态度，抖抖机灵的创意，这些实为trick的危机公关，能改变这个社会对消费者层出不穷的恶意么？它们能让世界变好么？ 近日又出海底捞危机，昔日餐饮界的良心被爆出食品卫生问题，大众一片哗然。然而没几日，事件出现反转，海底捞诚恳大方的道歉态度让消费者很受用，部分消费者表示“当然是选择原谅他啦”，“海底捞已经很良心了，要是海底捞我们都不放心，那中国就没什么餐厅能吃了”等诸如此类的话。我不禁愕然，中国的消费者怎么如此善良？因为国内目前的餐饮食品安全现状就可以降低对餐饮食品安全的要求？可如果就此放低标准，又如何能改善这个现状呢？须知人善被人欺，受危害的还是消费者自身啊。 我认为，危机公关这四个字应拆开看，一半是危机，一半是公关。危机爆出，企业以“公关”之手段维护企业形象、扭转危机是可以理解的，但同时也必须想办法实实在在地解决“危机”，给消费者一个交代。 人的自我约束力天生是薄弱的，在利益面前不堪一击。这就需要制度的约束，希望国家层面能在食品安全制度上完善相关法律，以约束企业行为。假设诸如海底捞的后厨卫生问题会导致相关上下级人员下狱，试问谁不会对后厨卫生万分上心？]]></content>
      <categories>
        <category>Communication Science</category>
        <category>Public Relationship</category>
      </categories>
      <tags>
        <tag>Public Relationship</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度：深析知识付费领域中的KOL]]></title>
    <url>%2F2017%2F09%2F05%2F%E6%B7%B1%E5%BA%A6%EF%BC%9A%E6%B7%B1%E6%9E%90%E7%9F%A5%E8%AF%86%E4%BB%98%E8%B4%B9%E9%A2%86%E5%9F%9F%E4%B8%AD%E7%9A%84KOL%2F</url>
    <content type="text"><![CDATA[摘要：2017年知识付费浪潮来袭，此领域从“四国之战”到群雄逐鹿，市场发展态势迅猛。而KOL作为知识付费服务的核心提供者，在这一场知识付费浪潮中为什么可以实现知识变现？这一领域有何发展趋势？KOL的知识变现之路有何瓶颈？以及KOL如何能够成功打造自己的知识付费产品？ 一、知识付费领域的现状：从“四国大战”到群雄逐鹿先看一组数据： 年初，互联网行业媒体虎嗅、36kr、钛媒体分别推出了各自的知识付费会员服务。 5月6日，36kr发布知识付费年度报告，指出该领域目前总体经济规模有望达到300-500亿元，成为“新风口”。 5月17日，知乎上线“知识市场”，推出类似淘宝的“7天无理由退款”等规则。知乎live上线11个月，已经举行2900场，超过300万人参加，主讲人平均时薪达到了11000元。 5月18日，得到举办知识发布会，罗胖发布了12款知识产品，并发布了内部品控手册，其订阅专栏销售数量已经达到206万份。 6月3日，千聊召开“知识变现破局”峰会，邀请300多位KOL，该平台已有注册讲师80万，做次课程且收入超过500元的讲师达到5万人。 6月6日，喜马拉雅推出66会员日，围绕知识内容付费推出会员服务、强化付费用户粘性，三天召集会员342万，会员销售额达到6114万。根据该平台公布的消息，今年以来其付费用户的月均ARPU值已经超过了90元。 观以上数据可析，去年喜马拉雅、知乎live、分答、得到”四国“之战，今年上半年尤其5-6月的“群雄”频繁动作无不昭示着知识付费浪潮的到来以及其成为下一个风口的强大潜力。而在这场浪潮中KOL扮演着什么样的角色呢？ 二、KOL是什么？KOL意为关键意见领袖（Key Opinion Leader），常是某行业领域是权威人士或相关群体，他们拥有更多、更准确的知识和信息，且为所接受或信任，并在相关群体里有较大影响力。 三、为什么KOL可以实现知识变现？“为什么KOL可以实现知识变现”大致等同于这个问题：知识付费浪潮兴起的原因是是什么？本文从主客观两方面分析。 客观原因1.新媒体工具、支付手段的便捷化，知识传授的去平台化。2.信息爆炸，免费知识门槛过低，导致虚假无用信息泛滥，导致优质内容稀缺。中国互联网方兴至成熟，惯于低价甚至免费的产品和服务的提供模式为其带来了指数级增长。资本家以流量和广告变现，而 免费思维深根于国民内心、为大众所认可甚至习以为然，传统经济行业的规则遂逐渐被颠覆。 然而，知识付费浪潮袭至，当知识内容被贴上付费的标签时，缘何创业者们纷纷涌入浪潮？为什么明明有大量免费内容，却要对内容收费？这场知识付费浪潮究竟是昙花一现还是成为下一个风口？ 原因在于，传统知识内容往往通过报纸、书籍等途径进行输出，往往只有行内专业人士才能创作，准入门槛较高；而互联网时代则倡导人人都能进行内容创作，准入门槛较低。一旦门槛降低，将意味着大量无效信息涌入，内容产出也没有衡量标准，优质内容在如此大的信息密度下也逐渐变得稀缺。 在信息无限、精力有限的情况下，如何快速获取高价值的有效信息成为了用户新的痛点，在这个大背景下，基于知识经验的付费分享，慢慢地成为一种全新的信息交互模式。因为利用付费这一门槛，一定程度上帮助用户筛选有效信息，同时也能让优质内容得到最大程度的曝光，将双边价值最大化。 3.社会化分工精细，人趋向于精深于专一领域社会化大生产导致社会分工的变化，社会分工将越来越趋于精细化，每个人在自己的专业领域深入挖掘，而既在其他领域求得通识又缺乏充足时间自己去钻研，这此即知识付费的需求来源之一；而一些外行人想通过快速学习某一领域知识从而快速进入该领域谋生，提供系统化的专业技能培养服务即为知识付费的又一需求来源。 4.知识付费的商业模式正在成熟目前成熟的商业模式有三种：专栏付费订阅，线上沙龙付费，付费问答。分别体现在现今中国知识付费的几大巨头的运作上。喜马拉雅FM和得到的专栏是付费订阅，知乎live是线上沙龙付费，而分答和在行则是付费问答。 下图是来自易观网的四大知识付费巨头的对比分析： 若以电商喻之，则喜马拉雅是天猫模式，知乎live更接近淘宝，而得到则是精品店。下面详细分析知乎live、得到、分答和喜马拉雅FM的商业模式。 （1）知乎live：淘宝模式知乎知乎live是推出的实时问答互动产品，因答主身份的广泛性和可自主创建live、提供付费服务，类似淘宝平台，故喻之为淘宝模式。其运作方式为：KOL入驻平台，发起话题live，设置简介、内容大纲、开始时间、参与票价；然后用户看到 live，感兴趣即支付服务；最后live开始，语音直播，音图文互动。 （2）得到：京东模式得到是逻辑思维团队推出的专栏付费订阅APP。因其付费专栏作品从KOL物色到专栏策划到作品售卖均由得到团队完成，与京东自营颇像，故喻之为京东模式。其运作方式为：平台自己找KOL，自己策划专栏售卖，收入分成。其优点是不需要太依赖KOL，不用长期维护其活跃度，可重复售卖；缺点是用户与KOL的互动少，粘性偏差，使用频次不高。 （3）喜马拉雅FM：天猫模式喜马拉雅FM是一款听书软件，同时售卖音频知识课程与内容。特点是有较为成熟的系统性和连贯性，就像是跻身名家大课现场“偷偷蹭课”；作为节目，会专门突出主讲者的个人IP、打造“知识网红”，内容形式更加现场化、故事化、干货化和娱乐化。 （4）分答：分答是一个付费语音问答平台。其运作方式为：KOL入驻，自己设置回答问题的费用；用户支付费用、发起提问；KOL语音问答，其他用户可支付1元偷听KOL的回答；提问者得1元中的0.5元的分成。 5.KOL的知识变现需求。主观原因1.行业竞争激烈，中产阶级越发焦虑。中产高知人群的付费意识逐渐崛起，希望通过投资自我来提升其核心竞争力。 用户除了可以利用知识付费这种模式去筛选有效信息外，还可以利用这种模式促使自己学习，为自己未来进行投资。根据真象大数据调查显示，有63.3%更愿意为“能提高工作效率或收入的知识和经验”这类知识进行付费。归根结底，还是因为用户身处于行业竞争激烈的时代，希望通过最快速有效的方式学习行业专业知识，进一步提升自己的核心竞争力，知识付费则只是一种途径，是他们通往晋升之路的垫脚石。 2.碎片化信息时代，用户注意力稀缺，需要别人帮助筛选和提炼知识。3.用户希望对碎片化时间进行更高效的利用以及对碎片化信息进行快速获取。四、用户购买的付费服务是什么？用户购买的付费服务究竟是什么？从用户和付费服务提供者两方面分析。 1.用户消费什么一般来说，信息可大致分为四类： 学术知识，如大学图书馆里的学术书籍。 加工过的“干货型”的知识，比如各类商业畅销书，以及如逻辑思维之类的知识传播类节目，特点是易于理解记忆操作性强，弱点是咀嚼嚼碎后喂给用户的食物难免失之精深，且加工的过程难免“扭曲变形”，使读者失去了自发思考循序渐进的这一过程。 结论，运用知识解决实际中的某个问题，给出可操作的解决方案，常见于各种咨询中。 资讯，工具性质或娱乐八卦。2.KOL提供哪些付费服务 在付费问答社区，KOL提供的是单次付费回答问题的服务。 在付费专栏，KOL提供的是一次性的专栏售卖服务。 在付费订阅，KOL提供的是付费包一定时间额度的专辑订阅服务。 KOL在付费社群中提供的服务：推荐内容；发布观点；回答问题；主持讨论，即发起话题讨论。在付费社群中KOL将输出的内容结构化，从而形成以KOL为顶点的知识传播社群。 五、用户为什么愿意为KOL的知识付费？人们为什么愿意消费内容，无非出自几种动机： 学习求知； 了解最新的时事新闻，掌握有用的资讯； 获得精神满足感和审美体验。 和作者观点碰撞，交流； 欣赏作者人格魅力； 获取社交谈资；六、知识付费领域的发展趋势 1.整体增长，用户愈加活跃，平台优势渐显 上图为Questmoblie近期发布的《移动知识付费行业发展现状与趋势》，由图表可见，喜马拉雅FM、知乎、得到、分答整体都处于高速增长态势，其中喜马拉雅FM月活第一，大概是第二名知乎的三倍。而根据此前QM《2016春季APP实力榜》数据显示，彼时喜马拉雅FM的月活为2089万，知乎为753万。这两大平台在短短一年时间内均实现了1.6倍以上的增长，其中知识付费的拉动作用不可谓不大。 垂类知识平台，在发展初期需要不断的拉拢头部KOL，并耗费大量精力用于用户的留存与拉新，而知乎与喜马拉雅FM天然就拥有知识粘性用户，并且由于用户的精英性质故本身就拥有付费潜质，加入知识付费功能则属于水到渠成，平台优势逐渐在后期显示出来。 2.垂类细分领域开始崛起，腰部KOL机会来临知识付费已经从最初最火的商业财经、技能培养，向更多更丰富的细分领域扩展，未来几年内每个垂直领域都可能出现“头部”。早期的知识付费更像是头部KOL的变现特权，李翔、马东等一线知识明星KOL一开课就能获得百万到千万级别的收入。但如今这样标杆现象却已经不复存在，而各个垂类的知识内容也逐渐受到用户关注，各个平台也有的放矢地对垂类领域的内容进行扶持。下面以各大知识付费平台为例进行分析。 （1）喜马拉雅FM：在过去一年中，一边拉拢头部KOL，一边在专注孵化腰部KOL，期间孵化了2000位知识网红以及超过10000节付费课程；且在价格的定位上，喜马拉雅FM价格从0.1元到199元，属于全品类覆盖。 （2）知乎live：知乎live发展早期主要依靠李笑来等头部KOL来打造知识爆款，在有一定用户基量后知乎Live彻底改变策略，化身“淘宝模式”，开始扶持自身平台的腰部KOL，鼓励大量KOL进驻开课。目前已经举行了超过2900场Live，主讲人的平均时薪超过11000元。而其针对自身优势所定制的10—30元的差异化定价策略也助其吸引更多用户购买各种垂类课程。 （3）得到：得到初期的专栏课程主要为商业财经、创业成功学，而后意识到垂类的重要性并开拓垂类知识内容。但KOL垂类付费内容比较少，优势不大。而得到的垂类优势和核心竞争力在于独家发售解读版电子书。以独家解读版电子书（文字+音频）的形式，来切入垂类知识内容的运营，涉及历史、心理、亲子、科学等各个方面，是得到的一种差异化竞争策略。 （4）分答：因受到微博付费问答、知乎付费问答的冲击，故分答也针对性地建立起垂类差异化优势，在产品设计上，APP对用户提问的方向作出清晰的引导，围绕健康、职场、科普、法律、育儿、心理几大门类。 3.基于算法进行个性化推荐相较于文字可快速浏览的特性，音频内容对于用户来说需要高成本投入。将不相干的内容推送给用户会，造成用户体验的严重下降，但是反之则能够极大程度地提高用户满意度，并增加用户使用时间。故基于算法进行个性化推荐非常重要。下面以各大知识付费平台为例进行分析。 （1）喜马拉雅FM：喜马拉雅一直有“猜你喜欢”的功能，为用户推荐其可能感兴趣的内容，而目前喜马拉雅FM也已将大量的付费内容整合到该功能中，帮助用户决策，为用户推荐付费内容属于无缝对接。 （2）知乎live：知乎live已将付费知识内容整合成独立“商城”业务，“商城”中包括live讲课、电子书、问答的内容全部是付费内容，其中“猜你喜欢”功能，为用户推荐出其可能感兴趣的live课程。且知乎拥有对用户行为数据的全方位记录，可对用户实现更为精准的推荐。 （3）得到：由于得到的专栏书目较少用不着故不需要对专栏进行精准推荐。不过为了提高用户的粘性和使用率，得到开发了“随时听”这个智能推荐功能，基于用户数据喜好为其推荐其已经购买过的尚未听过的内容，包括专栏音频、电子书等。且得到也会通过“猜你喜欢”功能为用户推荐各种垂类电子书。 （4）分答：分答目前处于内容积淀的阶段，鼓励用户更多主动提问，而当内容积淀到一定程度，分答必会推出智能推荐功能。 4.会员制度的应用会员制的建立于平台于会员于KOL三方都极为有利。 平台可以通过建立会员制度，更好地吸引潜在用户进行消费，知识KOL能获利更多，而消费者可以用更低的价格购买内容， 5.声音渐成首选模式无论喜马拉雅、得到售卖的课程，还是知乎live、千聊的直播，声音都是首选模式，这主要因为： 都市白领工作繁忙，听声音可解放双手，使用户能边听学边做其他的事情。声音可以传递更丰富的信息，比如说话的节奏、力度，从而更具感染力。比起图文能较好地保护版权；比起视频制作成本门槛更低。 6.用户付费习惯逐步养成7.内容爆款模式已经成熟各个知识付费平台都已经有大卖的案例，从喜马拉雅的好好说话，到得到的李翔商业内参，到知乎live的李开复分享，各家都有了当红的”爆款“标杆。 七、KOL进行知识变现的三大威胁1.与免费竞争付费产品的第一个难题就是说明付费的理由，说明自己比免费内容的价值更高，有什么独特和稀缺的地方，但实际上这个价值往往很难衡量。人各有志，有的KOL追求的是“传播和流量”，通过广告和其他方式变现；有的KOL追求的是“直接赢利和变现”，即让用户为知识付费。举个例子，《李翔商业内参》为得到上的热门付费专栏，那假如有一个和李翔名气相当的人做了个类似的免费内参，那么李翔的内参是否具备竞争力呢？所以，追求流量和传播的免费优质内容生产者，是知识付费的第一个威胁。 2.爆款模式难以广泛复制KOL想应用爆款模式，前提是这个KOL适合包装和被打造成IP。对很多商业实践的大咖来说，这个模式时间、机会成本太高，吸引力不足。 3、用户是否会选择复购？（1）来自免费产品的竞争对大多数用户来说，信息爆炸时代下免费的信息都看不过来，付费的更是无暇顾及，且一些免费公众号和头条号的内容质量不必付费差，以及付费产品的使用频次太低。 （2）如何保证用户的体验？满意是大多数复购的基本前提。但是现在很多知识付费产品的第一个问题是用户“买而不读”或者“根本消化不完”。试想当冰箱里还有鸡蛋，那么复购的可能性也大大降低。另外，不同于教育培训有考试作为衡量标准，由于知识付费产品的效果无法以统一的标准衡量，故会导致用户的预期难以统一。 （3）是否能提供持续的需求场景？很多知识付费的尴尬在于，用户“学完”的那一刻，也就是不再需要的时刻。不能提供持续的需求场景，复购则无从谈起。 （4）被替代的风险假使用户第一次体验很满意，但是第二次他在淘宝上发现了“盗版”，只要花很少的钱就能从网盘上下载全部内容，那么复购正版的可能性就只取决于用户维护作者版权的意识了。 八、KOL如何打造自己的知识付费产品？从KOL、用户和内容三个层面阐述作为KOL该如何打造自己的知识付费产品。 1.KOL的IP潜质KOL的名声是用户是否购买的决定性因素，KOL自身需要具备一定的影响力以及被打造成IP的潜力。 2.用户层 用户人群的选取：为能实现长期盈利，打造的产品针对的人群必须足够广。 直击用户痛点：解决现实生活中的具体问题往往更吸引人，故打造的产品最好能直击用户迫切想解决的痛点，且阐明效果的显著性。 抓住用户心理：a.群众具有从众心理，故打造的产品需获取该群体尽可能多的人的认同；b.抓住中产阶级的焦虑心理：好的知识爆款一定能缓解中产们心中的焦虑，给用户提供心理满足感，进而构建某种身份标签和优越感，无论是否真的能给用户带来知识和眼界的增长。 做好用户预期管理：使自己能最舒服的姿势输出内容，同时能做好用户的预期管理，保持住用户的粘性。 3.内容设计与管理 碎片化、易用性。课程必须短平快，不能耗时太长或者信息量过大，得让人有一种“每天十分钟就能进步”的易得感，从而满足用户对碎片化时间进行更高效的利用以及对碎片化信息进行快速获取的需求。且内容的表现形式要生动有趣、寓教于乐，并配上出色的演讲和讲故事、渲染气氛的能力。 进行持续优质内容输出：唯有持续的优质内容输出才是保证用户粘性和提高复购率的王道。 产品包装：知识付费产品需要进行精心的打磨、设计和包装，要能让人“一见钟情，再见倾心”。 产品促销：人是贪小便宜的，适当适时的促销活动可拉观望者入坑。 本文从KOL角度切入，深入分析了现今知识付费领域的现状与发展。2017是知识付费野蛮生长的一年，知识付费浪潮已起，而风口未至。可预见的未来越来越多的KOL将涌入这一领域，成为知识传播者和实现知识变现的一代。]]></content>
      <categories>
        <category>Product Manager</category>
        <category>Internet &amp; Product</category>
      </categories>
      <tags>
        <tag>Internet &amp; Product</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[众人眼中“荒诞低俗”的快手，为何能火爆跻身短视频王者？]]></title>
    <url>%2F2017%2F09%2F05%2F%E4%BC%97%E4%BA%BA%E7%9C%BC%E4%B8%AD%E2%80%9C%E8%8D%92%E8%AF%9E%E4%BD%8E%E4%BF%97%E2%80%9D%E7%9A%84%E5%BF%AB%E6%89%8B%EF%BC%8C%E4%B8%BA%E4%BD%95%E8%83%BD%E7%81%AB%E7%88%86%E8%B7%BB%E8%BA%AB%E7%9F%AD%E8%A7%86%E9%A2%91%E7%8E%8B%E8%80%85%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[摘要：日前看了一篇某博士所作的批判快手的文章，才得知有快手的存在。而文章里描述的快手使我产生兴趣，明明荒诞低俗，可为何在短短几年内成燎原之势，跻身短视频市场王者？我百思其中缘由，遂有此文。 一、快手是什么？1.基本简介类型：短视频分享app 内容：生活百态 2.我们对快手的普遍印象在一二线城市人眼中，快手代表着低俗、low、荒诞。如图为快手上的普遍所见的视频。 3.快手的火爆下面观一组数据反映快手的火爆状况。 上图为2015-2016短视频APP的排名情况，其中快手在2015年居短视频排行第三，2016年和2017年跃居第一。 上图为1月垂直短视频app市场渗透率top10 app的日增用户量排行，其中快手日增用户量平稳，约为95.6，仅稍次与小影，发展态势良好且稳定。 上图为主流垂直短视频APP的每用户日使用频率排行，其中快手日均使用频次2.1，排行老大。 上图为1月垂直短视频app市场渗透率top10排行，其中快手市场渗透率最高，称霸垂直短视频领域。 上面数据无不反映快手在短视频APP中的王者地位，那么问题来了：为什么快手这么“低俗”却有那么多人在用？快手到底是什么？是什么魔力让它使它一步步成燎原之势？ 二、快手到底是什么？1.快手到底是什么？（1）产品定位：一款短视频制作、分享、浏览的社交分享应用，满足普通用户记录和展现生活中的有趣瞬间的需求。 （2）产品功能介绍：下图为快手的一级界面： 下图为快手的产品功能架构： 然后，如果从用户角度来分析，则用户在快手上的操作行为主要有3种： A.浏览视频关注：展示你关注的网红的视频，按照时间的顺序倒序进行排列。发现：按照你的浏览习惯结合发布时间进行个性化推荐。同城：同城展示和你同城人的视频，排列顺序按兴趣推荐；交钱的用户会向前排；然后按照时间的倒序和浏览的热度进行倒叙排列。 B.拍摄视频逐帧编辑，自选封面。 C.菜单抽屉式导航，给视频留足展示空间。 了解了快手的产品定位和功能，下面是“重头戏”，我们来分析下快手走红的背景以及真正原因。 三、快手走红背景及原因分析1.走红的社会背景：互联网社会进入新居民社会文化的分层是客观真实存在的，不以任何人的意志为转移。总体的讲可以分为上层与下层，其界线是城市与乡村。 （1）社会背景：A.农村近三年智能手机的普及、数据流量的减价、WIFI的普及和生活质量的提高。我国农村近几年来的生活质量得到了大幅度的提高。随着数据流量的降价，以及国务院出台一系列措施在农村普及互联网，农村上网变得方便了很多。与此同时，农村的Wifi覆盖率也大大提升，Wifi甚至成为了许多村庄小卖铺农村、理发店、菜馆的标配。 从2015年开始，农村的智能手机普及率也实现了井喷式的增长。 积极抢占农村市场的手机企业 B.三四线城市的消费升级：过去几年外出打工的人流回归。四线城市自身的产业升级。90后人口的上升。这部分群体的消费欲望更强，也推动了三四线城市的消费升级。 （2）互联网新居民的现状（窘境）：已经较为健全的互联网社会并没有他们的容身之地，价值观及学识的巨大差异使得他们难以融入现有的圈子。 2.走红的原因分析（1）对市场用户群的选择：重视屌丝用户群，即占中国人口绝大多数的非主流市场（欠发达地区，农村，三四线城市，城乡结合部）。下图展示的是中国主要短视频平台的用户结构： 根据QuestMobile的数据：2017年4月，快手日活跃用户数为4172万，4个一线城市的日活跃用户数总计约1000万。这说明，快手平台的至少73%用户来自非一线城市。这主要是由于当代中国的人口结构，中国只有约7%的人口生活在一线城市。因此，快手大部分用户来自低线城市。 而这部分低线用户也有自己的购物需求、社交需求、娱乐需求，他们也在融入移动互联网。不同的是，他们融入的并非高大上的、与硅谷接轨的移动互联网，而是符合他们需求的、满足他们趣味和诉求的移动互联网。他们需要有描述自己农村生活、打工生活，反映底层生活逻辑、底层日常情绪的社区和产品。快手满足了四五六七线城镇和农村屌丝青年们自我展示、内容分享的娱乐需求，这也恰恰的是快手成长为中国非一线城市用户最集中的视频社区的机遇。 快手走红的背后揭示了中国社会的鸿沟；其独特的市场切入点获取了广大未被开发的互联网处女地，这可谓是一个非典型的、逆向生长的中国互联网产品成长故事——绝大多数互联网产品，在中国都是由一线城市向二三四线与农村渗透的。 （2）找到了用户群的需求背后的本质心理诉求，满足了了底层民众被压抑的“欲望”。张小龙说，伟大产品应该满足人的情感需求、满足人性贪嗔痴。没有被解构、洞穿的需求都是耍流氓的伪需求，必须要归结到人的最本质人性。而需求来自对当下生活潮流的理解。 A.被人尊重和关注的需求下图是马斯洛需求理论的图解，其第四层为“尊重需求”，其中包含自我尊重、对他人尊重、被他人尊重、信心、成就。 尊重的需要可分为内部尊重和外部尊重。内部尊重是指一个人希望在各种不同情境中有实力、能胜任、充满信心、能独立自主。总之，内部尊重就是人的自尊。外部尊重是指一个人希望有地位、有威信，受到别人的尊重、信赖和高度评价。每个人都希望自己有稳定的社会地位，要求个人的能力和成就得到社会的承认。 而快手做的就是提供给每个用户一个平等地展现自我的舞台。在【发现】和【同城】板块用户只要发布视频就有机会被上万人看到，人人平等；且与微博以KOL为核心带起用户量的模式不同，快手是去KOL、去中心化的，它让用户不用去追逐明星，不去作KOL的追逐者和附属品，而是仅把快手当做记录和展现自己的平台。 B.猎奇心理人总是有寻找、探索新奇事物的好奇心理，这是天性。【发现】和【同城】就以很粗暴的展现方式满足了用户窥探他人生活的欲望。 C.穿透社会阶层结界，融入城市生活的内在渴望清华大学的研究人员发现，互联网帮助中国数千万名离开农村进城务工的农民工保持与亲戚和同学的联系，但对他们真正融入城市生活没什么帮助。而他们其实对融入城市生活存在着渴望。 D.群体从众心理：群体效应易传播，人是跟风的，群体智商低于个体智商。人是社会的，人们生活在一定的社会群体之中，会受到群体态度、价值观、宗教信仰、风俗习惯、文化、伦理道德规范、社会舆论、人际交流、规章制度等的影响，个体希望被群体接纳、肯定，得到角色认同，避免被群体抛弃和否定。在这种心理状态下，个体行为总是尽可能与群体中的大多数行为保持一致。比如当你身边的人都在打王者时，大概率下你也会跟从一起打。道理相似，这部分用户群体看到身边人都玩快手玩得不亦乐乎，自然大概率下也会从众。于是快手遂成燎原之势。 （4）快手对本初定位的坚持快手团队做这个产品的初心是记录世界，给身边的每一个人的记录生活的分享工具，记录每个个体一生中最美好的回忆，点点滴滴、生活的片断、喜怒哀乐、悲欢离合都能够记录下来，并且能够分享给所有人。 为了坚持这个初心，快手做了很多努力。如去KOL,去中心化。这一点前文也有提及。快手并未采用如微博一样的以明星为中心的战略，没有将资源向粉丝较多的用户倾斜，没有设计级别以分类用户，没有对用户进行排名，员工不允许接触粉丝数多的用户，也没有去签约平台上热门的直播者。 之所以做出这些限制，是因为快手希望给平台营造轻量级、休闲化的氛围，平台上的所有人都敢于表达自我，分享生活中的视频。 （5）快手的产品功能和设计符合人性：强克制，基于算法的完全个性化推荐机制，去中心化。A.功能克制，极简。体现在以下5点： 功能板块少：删除或削弱了被某些主流社交应用认为不可或缺的功能。不对视频类型作细分：这一点有助于不擅长玩智能手机的用户使用快手。不提供转发功能 ： 鼓励用户专注于制作原创内容。隐藏私信功能：一方面是为了鼓励用户分享更多内容，而不是在平台上花太多时间去聊天；另一方面用户最终不可避免地会转移至其他成熟的社交网络（QQ和微信）。没有提供单独的直播页面：快手只允许约10%的用户进行直播，从而将直播弱化成了附属功能。因为快手认为直播不是一种记录和分享日常生活的良好方式，而只是对用户互动的补充。 B.基于算法的完全个性化推荐机制。自动推荐的关键在于机器理解规则的程度。快手设计的算法能理解视频内容、用户特征，以及用户行为，包括内容浏览和互动历史。基于对以上信息的理解，模型就可以将内容和用户匹配在一起。用户的积累越多，数据就越多，于是推荐就越精确。而且目前快手正专注于对智能匹配的优化。 算法推荐的优势在于： 第一，通过算法推荐机制，所有用户和视频都有机会在“发现”页面中得到展示，即使该用户只有1个粉丝。视频获得的点赞越多，被机器选择的概率就越大。第二，通过分析用户以往的点击、观看和点赞历史，算法就可以实现视频的推荐，根据用户此前的偏好来提供“发现”页面中的内容。 四、快手的未来发展方向1.短视频战场的格局目前短视频战场上，三大短视频王者并行（美拍，秒拍，快手），快手占首位；各路新秀不断涌入市场，抢占市场份额和用户量，如微视，抖音短视频，今日头条家的火山小视频等。 2.对快手未来发展的建议（1）提升内容调性和品牌调性。快手目前处于短视频市场的行业主导者地位，继续发展的出路在于继续扩大市场需求。而对行业主导者来说，扩大市场需求的途径通常为为产品寻找新用户、新用途或促使现有用户增加使用量。 提升内容调性可以扭转市场认知，帮助快手实现商业变现；而提升品牌调性的方式可以是往更高端的市场打广告和生产相对高端的内容。 （2）与大企联手，获取大企的生态资源。3月份快手获腾讯投资3亿，可预见未来获得腾讯加持的快手可能接入腾讯生态体系，获取到鹅厂资源促其发展。 本文主要从几个角度对快手走红的原因做了分析，笔者思虑不全，若读者有更多想法，欢迎在评论区留言交流。]]></content>
      <categories>
        <category>Product Manager</category>
        <category>Internet &amp; Product</category>
      </categories>
      <tags>
        <tag>Internet &amp; Product</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[以小窥大：从拧不开的瓶盖到产业链全局思维]]></title>
    <url>%2F2017%2F09%2F05%2F%E4%BB%A5%E5%B0%8F%E7%AA%A5%E5%A4%A7%EF%BC%9A%E4%BB%8E%E6%8B%A7%E4%B8%8D%E5%BC%80%E7%9A%84%E7%93%B6%E7%9B%96%E5%88%B0%E4%BA%A7%E4%B8%9A%E9%93%BE%E5%85%A8%E5%B1%80%E6%80%9D%E7%BB%B4%2F</url>
    <content type="text"><![CDATA[摘要：这是笔者由一次拧不开瓶盖的体验而引发的一连串思考，从小瓶盖窥大产业链的利益博弈。 前两周去杭州旅游，酷暑，渴，进某便利店买水，打开冰箱，被小茗同学的外形所吸引（这是笔者第一次买小茗同学），于是买下。掰开盖子，发现里面还有一个里盖，可是里盖太紧，费了九牛二虎之力还是扭不开（笔者力气一向比较小），最后是店家阿姨帮忙拧开的。 经这件事，笔者对小茗同学里外双盖的设计产生了些许愤怒：这种设计看起来漂亮，但却没有解决我的问题：轻松拧开瓶盖。 但是，这次经历引发了我的思考：为什么小茗同学要设计成里外双盖呢？ 对这个问题的原因，我有五个猜测： 1.美观：小茗同学的外形设计很卡通风，年轻化，可见其主要市场应该是年轻人，年轻人喜欢独特，而双盖设计本身既与众不同又漂亮美观，容易吸引年轻消费者（很多人说5块钱的小茗同学跟2块5的统一冰红茶的口感也没什么区别，大概我是用2块5买了个瓶盖吧，微笑）； 2.省力：细心会发现，外盖的内侧与里盖的外侧的齿纹相同，套在一起可以增强开瓶摩擦力，使人可以更省力地扭开瓶盖（可是没有考虑到；1.如果消费者并不知道是双盖设计，先开外盖，然后可能就会像笔者一样…怎么拧都拧不开… 2.小茗同学的外盖边缘是尖尖的，被冰冻后去拧的话，手会很痛…） 3.给喜欢收集瓶盖的人提供素材(不过，会产生收集效应的产品一般具有稀缺性、多样性等特点，小茗同学还不至于…） 4.利用瓶盖的特殊性搞促销活动，比如集齐十个瓶盖换一瓶小茗同学… 5.瓶盖本身具有娱乐性的玩法（据说…可以滚着玩？） 可不管是哪个原因，都不能浇灭我的“愤怒”，因为它没有解决我的核心需求：拧开瓶盖解渴…于是，“愤怒”的我下了个结论：这个里外双盖是个多余且失败的设计。 接着，我突然意识到好像有哪里不大对劲，有问题：像我一样扭不开瓶盖的用户实际有多少？我以个人感受和经历去草率定义它为失败的，是不是以偏概全、一叶障目？ 于是我采访了一些同学，几乎所有人都表示：如果我不说的话，他们都没有发现小茗同学有两个盖子（因为大部分人是两个盖子一起拧开的）…他们也没有遇到过我这样拧不开小茗同学的盖子的情况…以及在网上输入相应的关键词检索出的网页，虽然有用户也吐槽过这个问题（拧不开里盖）和这个双盖设计，但也只有少量用户。可见，我的确犯了以偏概全的逻辑错误，误会了小茗同学。 接着，我又开始思考：力气小的人拧不开瓶盖的痛点一直存在，有哪些方案可以解决这个问题呢？ 首先，要分析解决方案，得先了解现有方案是什么。现有的瓶盖设计是：多为圆形；为方便开启或拧紧，在瓶盖的圆柱形外壁上设有若干条轴向设置的肋条以增大瓶盖的摩擦力；但由于瓶盖内部与瓶身为螺纹连接结构，一般小孩或力气较小的女士很难将瓶盖扭开，且肋条会促使手指上留下深深的勒痕，引起疼痛。 其次，为什么拧不开瓶盖呢？ 1.自己力气小（像笔者一样…） 自己力气不足，可以借助外力啊，外力包括：人力+物力，人力即可以求小哥哥们帮忙拧开（说不定成就一段良缘，微笑），物力即借助工具，比如一张叠起来的纸巾，一块手帕甚至是衬衫外套的衣角等等，包住瓶盖，再拧开，抑或这种开瓶盖神器： 2.瓶盖本身有问题，这又分为两种情况： “后天畸形”：瓶盖在生产、流通环节时出现了人为／意外偏差，导致不符合规格，所以不好拧开；“先天不足”：瓶盖的设计方案就不省力； “后天畸形”是厂家自己的质检、运输管理的责任，不过“先天不足”则是可以通过产品设计来解决的，可从两个方面思考： （1）改瓶盖外形：如改成圆角方形或给瓶盖加个“把手”， 某届台湾国际创意设计赛冠军作品——省力瓶盖 可口可乐的创意省力瓶盖 （2）增加辅助的外盖：像小茗同学、屈臣氏蒸馏水等少数双盖设计的饮料，利用杠杆原理，的确可以起到省力的效果； 解决方案分析完了，到这里看似圆满了…可问题又来了：厂商又不傻，为什么拧不开瓶盖的问题一直存在，却并没有被大范围解决呢？为什么这些瓶盖设计方案挺好，却没有被广泛应用到实践呢？ 我们把这个问题抽象一下，就是：为什么没有做出改变？那么，我们分析下不改变的原因，有两条： 1.技术能力不够； 2.改变的成本远大于收益； 第一条直接pass…现在的科技水平会生产不出一个小小的瓶盖吗？显然不可能。 那么来分析第二条：假设某厂商把瓶盖设计成省力的方形瓶盖，收益是一小部分（力气小的）消费者拧瓶盖的体验提升，该产品的口碑提升（也许因为特殊且贴心的设计而获取消费者的赞誉）；但成本无疑是巨大的。小小的一个瓶盖也是有完整的产业链的，单从瓶盖的生产环节来看，下面是瓶盖生产线： 成本 1：圆形盖与方形盖哪个更适合在传送带中传送不言而喻（且如果里面卡盖设备直接停机，停一分钟就是烧钱）； 成本 2：封盖机内有封盖头，通过由上到下封盖头卡主盖子然后在瓶口进行旋转达到封盖效果。方形盖和圆形盖哪个容易卡住且不损坏瓶盖表面，肯定是圆形盖。如果是方形盖则极易造成瓶盖表面有刮痕磨损的情况，影响美观影响销售(外表美观不美观也是标准之一…毕竟是个看脸的社会…)；如果真的有厂家有钱任性要用方形盖且瓶口依旧为圆形，用现有设备(即生成正常圆形盖饮料的)进行改造的话设计的成本有以下： 制造瓶盖的模具全换，重新调教瓶盖形状、直径、色素等数据至达标，然后设定标准规模化生成;盖检查设备、导轨重新调教，清洗消毒使用的无菌水、消毒液比例重新设定，以及无菌水设备和消毒药剂的调教；封盖机封盖头全换以及调教；还有人员培训，标准适应周期，无菌验证等等;全新设计的设备故障停机的概率肯定比原来要大，所以还有设备故障停机的成本；总结一下 总之，想做一个全新的瓶盖，需要整个产业链为之服务。而且就算花血本做出来了，也有可能在它还未发育成熟的时候就因产量、产品合格率等问题被市场淘汰了，落得个血本无归。 产品设计者一般都是从美学或者人体工程学来考虑问题和设计产品，但是并不了解生产厂商的需求，即节约成本创造利益最大化。商人以利为先，以利润最大化为根本目的，如果打不开瓶盖的人数量没有足够高到需要花成本去改善、改成省力瓶盖的成本远大于收益，自然没有厂家愿意做这种改变。再者对大多数消费者来说，喝饮料的核心目的是为了解渴，解渴属于马斯洛需求理论中最底层的生理需求，而省力属于更高层次的情感需求，只要满足了最核心的生理需求，解了渴，那么之前的开瓶盖过程省力与否就不甚重要了。 因此，在整个瓶盖产业链这么完备和成熟的情况下，依然还存在不能方便拧开这个违反最基本的功能的BUG。 以上即笔者由一次拧不开瓶盖的体验而引发的一连串思考，对笔者也有不小的启发： 1.凡事多思，从生活细节中窥见产品问题； 2.重视用户调研，勿以偏概全，妄下结论； 3.PM不应当把眼界局限在单纯的产品设计（广义的设计）上，产品策略的制定需要考虑到产品的整个生命周期（设计-生产-分配-交换-消费）的各个环节，甚至产业链上的各方利益；4.不仅要考虑用户需求，还要考虑产品的商业模式（盈利）； ​笔者思虑不周，若读者有想法，欢迎留言探讨。]]></content>
      <categories>
        <category>Product Manager</category>
        <category>Internet &amp; Product</category>
      </categories>
      <tags>
        <tag>Internet &amp; Product</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[媒体分析的对象选取]]></title>
    <url>%2F2017%2F09%2F05%2F%E5%AA%92%E4%BD%93%E5%88%86%E6%9E%90%E7%9A%84%E5%AF%B9%E8%B1%A1%E9%80%89%E5%8F%96%2F</url>
    <content type="text"><![CDATA[整理了一下具有较高分析价值的大V／媒体。 首先，选取分析的对象最好具有如下特征： 关注量大的大V／媒体； 有鲜明特色，可挖掘的点比较多，最好有一定争议或在同领域做得很出色。 其次，分类列表： 1.鸡汤／成功学： 采铜：知乎／公众号大V，畅销成功学书籍作者 咪蒙：毒鸡汤，著有一大波百万+文章，像色鬼感应大胸一样感应选题，极具争议性，当之无愧的年度最热公众号大V，日前被封号后解封 2.正经有价值的知识／资讯媒体： 晓松奇谈：文化类脱口秀 虎嗅网：聚合优质创新信息和人群的新媒体，专注原创高质量作品 刺猬公社：聚焦内容产业的垂直资讯平台 KnowYourself：一个陪你科学认识自己的公众号，以浅显话讲人生道理。 3.娱乐／有一定争议的媒体： Sir电影（毒舌电影）：犀利的影评公众号，前几月和咪蒙一起被封号过（足见其争议性） 差评：公众号大V，擅营销和文案包装 王左中右：国际新闻记者转段子手，公众号大V 4.产品： 知乎：有争议性，全民知乎，内容风格已自成一体“知乎体” 喜马拉雅：最大的音频知识付费平台 今日头条／天天快报：大众资讯平台]]></content>
      <categories>
        <category>Communication Science</category>
      </categories>
      <tags>
        <tag>Communication Science</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PR-Illustration]]></title>
    <url>%2F2017%2F09%2F04%2FPR-Illustration%2F</url>
    <content type="text"><![CDATA[今天《公共关系原理》开课，说下自己的对公共关系这个行业的理解。公共关系，英文为public relationship，顾名思义，可理解为是社会组织通过一定的传播管理来和公众交流，目的是改善与公众的关系、促进公众对其认识，以树立良好形象、取得公众的理解及支持。 目前在公共关系行业中，我们听闻最多的是危机公关。梳理了下中国危机公关的处理方案： 1.不置一词，等待媒体关注度自然消亡。因为在中国，危机的关注周期最多1周，当新危机出现则会掩盖原有危机，此时连公关都不用做危机自渡。2.彻底否认，等待危机消亡。在没有足够证据的情况下，真相模糊不清，不能定罪。所以许多公司的应对方案是写篇公关稿或开宣讲会，矢口否认，使真相更加扑朔迷离。等公众的新鲜感一过去，就不会再有人关注。3.站出来承认，以真诚和勇于承认来博取公众原谅，并积极采取措施减轻危机影响和作出保证。比如近来海底捞积极承认食品安全问题，如此反而让公众欣赏这种态度。4.抛出模糊不清的说法，使真相扑朔迷离。 有人会把公关与广告的概念混淆，我的理解是：公关是我不说自己好，但我能让别人替我说话。广告是我一个劲儿地说自己好。 公关不单是普通公众眼里的“为公司、艺人洗白”“公关小姐”等过时、肤浅或狭窄的概念，它有更高大上的层次，只是社会公众对这个行业的理解还没到达罢了。 小到线上线下的个人形象塑造，大到跨国交际的国家形象塑造，其实都是公关的范畴。公关无处不在。 比如你在朋友圈里晒生活发动态，是对你个人形象的塑造，是在向你的朋友传递你的形象信息的公关。 再比如，08年的北京奥运会被评为是中国近现代以来世界影响最大的公关事件。为什么呢？因为它是唯一一次让整个世界都将视线聚焦在中国，从而通过这一契机让世界开始或更了解中国。一个英国在华留学生在北奥前被采访对中国的看法时，她说她来中国前带了2年的肥皂；还有人说以为现在的中国人还如清朝时代一样扎着辫子。从诸多例子中可见许多外国人对中国了解甚少。中国人以为世界了解中国，就像中国了解世界一样。但其实人家觉得并没有必要了解中国，因为生活中与中国并没有交集。公关于国家层面也是至关重要的手段。 推荐几本公共关系的书籍：《公共关系学》——张克非《现代公共关系学》——陈先红《卓越公共关系与传播管理》——詹姆斯《公共关系实务》——弗雷泽《新规则：用社会化媒体做营销和公关》——戴维《公共关系：职业与实践》——丹-拉铁摩尔]]></content>
      <categories>
        <category>Communication Science</category>
        <category>Public Relationship</category>
      </categories>
      <tags>
        <tag>Public Relationship</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Welcome]]></title>
    <url>%2F2017%2F08%2F02%2FWelcome%2F</url>
    <content type="text"><![CDATA[Hello, Welome to visit my blog. My name is Scarlett.]]></content>
  </entry>
  <entry>
    <title><![CDATA[三十二]]></title>
    <url>%2F2017%2F07%2F10%2F%E4%B8%89%E5%8D%81%E4%BA%8C%2F</url>
    <content type="text"><![CDATA[第一次看慰安妇题材的片子，总觉得这样的主题太沉重，让人不敢轻易去触碰。 《三十二》，一想到这个数字背后的意义，心里就像压着块巨石，沉抑得喘不过气来。二十万慰安妇，至一二年导演拍这部片子时，仅剩三十二人；一三年，二十二人；现在… 这部纪录片讲的是韦绍兰老人的故事。五分多钟，故事还未开始讲述，背景音乐响起，眼泪便无法抑制地流下。在看之前，对于老人的苦难，我的心里其实已经有了些许预见和想象。然而，也没预料到，这个老人身上有这样触人心弦的力量。 日军侵入广西，她被抓去做慰安妇三个月；伺机，逃出，跋涉，回家；丈夫说她学坏，邻居流言蜚语；自杀，未遂；怀上了日本人的孩子，而自己的女儿夭折；丈夫和孩子相继去世；每个月靠九十块低保度日，全家每日靠四两饭度日，吃的最多的是野菜；混血儿子从小到老生活在流言和周围人异样的眼光中，读不起书，一辈子都是看牛，讲了七个妹子，都因他是日本人而未成，至老未婚，病而无侍;这名誉背了一辈子，坏了一辈子。 她说，没有人比她更苦了。 她说，眼泪，是往心里流的。该是经历过多大的苦难，才能说出这样沉重的句子。 老人反复吟唱着这首童谣：天上下雨路又滑，自己跌倒自己爬，自己忧愁自己解，自留眼泪自抹干。仿佛在哭唱着自己的人生。 然而，被问到对将来怎么看，她却说：“这世界红红火火的，世界这么好，到现在我都没想死……”“这世界真好，吃野东西都要留出这条命来看。”这样的乐观，令人惊讶而心痛。 战争难人，无论是胜利还是失败，经受苦难的永远是这些底层的小人物。这世界亏欠她们的，没有补偿，她们却负债一生。 有两个中国，想象中的中国欣欣向荣，国富民强；现实中的中国，繁荣背后有广袤大地上的苦难。我们看过，哭过，悯过，却无法真正感同身受，只能发出“民生多艰”的哀叹。 昨晚坐在钱塘江边，想，我到底想要成为一个什么样的人呢？大概是一个推墙人吧。]]></content>
      <categories>
        <category>Dairy</category>
      </categories>
      <tags>
        <tag>Film</tag>
        <tag>Thougnts</tag>
      </tags>
  </entry>
</search>
