<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="en,zh-Hans,default">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Artificial Intelligence,Machine Learning,Reading Notes," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta name="description" content="Abstract：这本书面向的是对机器学习和数据挖掘实践及竞赛感兴趣的读者，以python为基础从零开始，在不涉及数学模型和复杂编程知识的前提下，逐步学习和掌握机器学习、数据挖掘、自然语言处理工具，如scikit-learn、NLTK、Pandas、TensorFlow等。 全书分为4章：  简介篇：机器学习概念和python编程知识 基础篇：如何使用scikit-Learn作为基础机器学习工具">
<meta name="keywords" content="Artificial Intelligence,Machine Learning,Reading Notes">
<meta property="og:type" content="article">
<meta property="og:title" content="阅读笔记：《Python机器学习及实战》">
<meta property="og:url" content="http://ScarlettHuang.cn/2018/03/05/阅读笔记：《Python机器学习及实战》/index.html">
<meta property="og:site_name" content="Scarlett Huang | Blog">
<meta property="og:description" content="Abstract：这本书面向的是对机器学习和数据挖掘实践及竞赛感兴趣的读者，以python为基础从零开始，在不涉及数学模型和复杂编程知识的前提下，逐步学习和掌握机器学习、数据挖掘、自然语言处理工具，如scikit-learn、NLTK、Pandas、TensorFlow等。 全书分为4章：  简介篇：机器学习概念和python编程知识 基础篇：如何使用scikit-Learn作为基础机器学习工具">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://ws2.sinaimg.cn/large/006tKfTcly1fo0008es9mj30gf09kq4z.jpg">
<meta property="og:image" content="https://ws4.sinaimg.cn/large/006tKfTcly1fo01586rqvj30d108y0tk.jpg">
<meta property="og:image" content="https://ws4.sinaimg.cn/large/006tNc79ly1fo05eoil4jj30e408uq3q.jpg">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/006tNc79gy1fo2dpg8s4cj30za0l3dmx.jpg">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/006tNc79gy1fo2hc6x7ukj30ws0h5n13.jpg">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/006tNc79gy1fo2hc7jt5oj30ow0i3wgf.jpg">
<meta property="og:image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fo2hafjr0xj30d00a0jsa.jpg">
<meta property="og:image" content="https://ws3.sinaimg.cn/large/006tKfTcgy1fo4s6o0j24j30ic0aygnn.jpg">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/006tKfTcgy1fo5rqikbsoj30kv08adi8.jpg">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/006tKfTcgy1fo5wh0szkej30sa0nfq9l.jpg">
<meta property="og:image" content="http://img.blog.csdn.net/20160403174832218?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
<meta property="og:updated_time" content="2019-06-07T11:41:23.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="阅读笔记：《Python机器学习及实战》">
<meta name="twitter:description" content="Abstract：这本书面向的是对机器学习和数据挖掘实践及竞赛感兴趣的读者，以python为基础从零开始，在不涉及数学模型和复杂编程知识的前提下，逐步学习和掌握机器学习、数据挖掘、自然语言处理工具，如scikit-learn、NLTK、Pandas、TensorFlow等。 全书分为4章：  简介篇：机器学习概念和python编程知识 基础篇：如何使用scikit-Learn作为基础机器学习工具">
<meta name="twitter:image" content="https://ws2.sinaimg.cn/large/006tKfTcly1fo0008es9mj30gf09kq4z.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.2',
    sidebar: {"position":"left","display":"always","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://ScarlettHuang.cn/2018/03/05/阅读笔记：《Python机器学习及实战》/"/>





  <title>阅读笔记：《Python机器学习及实战》 | Scarlett Huang | Blog</title>
  




<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-141530033-1', 'auto');
  ga('send', 'pageview');
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->





</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Scarlett Huang | Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description"></h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-know-me">
          <a href="https://www.scarletthuang.cn" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            know me
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://ScarlettHuang.cn/2018/03/05/阅读笔记：《Python机器学习及实战》/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Scarlett Huang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Scarlett Huang | Blog">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">阅读笔记：《Python机器学习及实战》</h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-03-05T16:28:01+08:00">
                2018-03-05
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index">
                    <span itemprop="name">Artificial Intelligence</span>
                  </a>
                </span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/03/05/阅读笔记：《Python机器学习及实战》/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/03/05/阅读笔记：《Python机器学习及实战》/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>Abstract：这本书面向的是对机器学习和数据挖掘实践及竞赛感兴趣的读者，以python为基础从零开始，在不涉及数学模型和复杂编程知识的前提下，逐步学习和掌握机器学习、数据挖掘、自然语言处理工具，如scikit-learn、NLTK、Pandas、TensorFlow等。</p>
<p>全书分为4章：</p>
<ul>
<li>简介篇：机器学习概念和python编程知识</li>
<li>基础篇：如何使用scikit-Learn作为基础机器学习工具</li>
<li>进阶篇：怎样借助高级技术或模型提高机器学习系统的性能</li>
<li>竞赛篇：如何完成kaggle竞赛</li>
</ul>
<a id="more"></a>
<h1 id="1-简介篇"><a href="#1-简介篇" class="headerlink" title="1.简介篇"></a>1.简介篇</h1><h2 id="1-1-机器学习"><a href="#1-1-机器学习" class="headerlink" title="1.1 机器学习"></a>1.1 机器学习</h2><p>任务：</p>
<ul>
<li><p>监督学习 （supervised Learning）：预测事物未知表现，包括分类问题(classification)、回归问题(regression)，根据目标预测变量类型的不同可分为：</p>
<ul>
<li>分类问题：预测类别（已知数量，类别离散），如已知一个人的身高、体重和三围等数据预测其性别</li>
<li>回归问题：预测连续变量，如根据房屋的面积、地理位置、建筑年代等预测销售价格(C.V)</li>
</ul>
</li>
<li><p>无监督学习（Unsupervised Learning）：倾向于分析事物本身特性，常用技术：降维(Dimensionality  Reduction), 聚类 (Clustering)</p>
<ul>
<li><p>数据降维：对事物的特性进行压缩和筛选，如对图像进行降维以保留最有区分度的像素组合</p>
</li>
<li><p>聚类：依赖于数据的相似性从而把相似的数据样本划分为一个簇（区别于分类，簇的数量和含义非已知），如电子商务中对用户信息和购买习惯进行聚类分析，一旦找到数量多且背景相似客户群，则可针对其投放广告促销</p>
<p>​</p>
<p>​</p>
</li>
</ul>
</li>
</ul>
<p>经验：</p>
<ul>
<li>特征（feature）：反映数据内在规律的信息</li>
</ul>
<ul>
<li>监督学习中的经验：特征、标记(label)<ul>
<li>用一个特征向量描述一个数据样本</li>
<li>label的表现形式取决于监督学习的种类</li>
<li>数据标注需耗费大量资源，故数据量少</li>
<li>训练集（training set）：带label的数据集，用来训练学习系统</li>
</ul>
</li>
<li>无监督学习中的经验：无label故无法做预测，但适合对数据结构作分析</li>
<li>原始数据转化为特征向量的过程中会遭遇多种数据类型（需全部转化为具体数值运算）：<ul>
<li>类别型特征（categorical）</li>
<li>数值型特征（numerical）</li>
<li>缺失数据（missing value）</li>
</ul>
</li>
</ul>
<p>性能（performance）：</p>
<ul>
<li>评价学习模型完成任务质量的指标<ul>
<li>分类问题：准确性（accuracy）——预测正确类别的百分比</li>
<li>回归问题：衡量预测值与实际值之间的偏差大小</li>
</ul>
</li>
<li>测试集（testing set）：与TS具备相同特征，没有被用于训练</li>
<li>how：用测试集测试预测的准确率（用具备相同特征的数据，模型在测试集上的预测结果与正确结果进行比对）</li>
</ul>
<h2 id="1-2-python编程库"><a href="#1-2-python编程库" class="headerlink" title="1.2 python编程库"></a>1.2 python编程库</h2><ul>
<li>python</li>
<li>numpy：高级数学运算机制，高效向量与矩阵运算</li>
<li>scipy：在numpy基础上更强大、应用更广泛的科学计算包，依赖numpy</li>
<li>Matplotlib：数据分析与可视化的绘图工具包</li>
<li>scikit-learn：封装了大量ML模型</li>
<li>pandas：数据处理和分析的工具包</li>
</ul>
<h2 id="1-3-python基础"><a href="#1-3-python基础" class="headerlink" title="1.3 python基础"></a>1.3 python基础</h2><p>常用数据类型</p>
<ul>
<li>数字</li>
<li>布尔值</li>
<li>字符串</li>
<li>元组（turple）：以()表征，元组数据不允许修改</li>
<li>列表：以[]表征，允许修改列表数据</li>
<li>字典（dict）：以key-value构成，以{}表征</li>
</ul>
<p>数据运算</p>
<p>流程控制</p>
<p>函数设计</p>
<h1 id="2-基础篇"><a href="#2-基础篇" class="headerlink" title="2.基础篇"></a>2.基础篇</h1><p>Abstract：机器学习模型的使用方法、性能评价和优缺点。</p>
<p>模型阐述角度：模型简介、数据描述 、编程实践、性能测评、特点分析。</p>
<h2 id="2-1-监督学习经典模型"><a href="#2-1-监督学习经典模型" class="headerlink" title="2.1 监督学习经典模型"></a>2.1 监督学习经典模型</h2><h3 id="2-1-1-监督学习任务的基本流程"><a href="#2-1-1-监督学习任务的基本流程" class="headerlink" title="2.1.1 监督学习任务的基本流程"></a>2.1.1 监督学习任务的基本流程</h3><ul>
<li><p>准备训练数据——抽取所需特征、形成特征向量(Feature Vectors)——把特征向量连同labels一起送入学习算法(Machine Learning Algorithm)中——训练出一个预测模型(Predictive Model)</p>
</li>
<li><p>采用同样特征抽取方法作用于测试集、得到用于测试的特征向量——使用预测模型对待测试的特征向量进行预测并得到结果(Expected Label)</p>
<p>​</p>
</li>
</ul>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fo0008es9mj30gf09kq4z.jpg" alt="监督学习任务的基本流程"></p>
<h3 id="2-1-2-分类学习"><a href="#2-1-2-分类学习" class="headerlink" title="2.1.2 分类学习"></a>2.1.2 分类学习</h3><p>分类问题：</p>
<ul>
<li>二分类（binary classification）：二选一</li>
<li>多分类（multiclass classification）：判断一个样本是否同时属于多个不同类别</li>
</ul>
<h4 id="2-1-2-1-线性分类器（linear-classifiers）"><a href="#2-1-2-1-线性分类器（linear-classifiers）" class="headerlink" title="2.1.2.1 线性分类器（linear classifiers）"></a>2.1.2.1 线性分类器（linear classifiers）</h4><p>线性分类器：假设特征与分裂结构存在线性关系的模型，通过累加计算每个维度的特征与各自权重的乘积来帮助类别决策（<strong>基于线性假设的分类器</strong>）</p>
<p><strong>logistics regression模型</strong>：</p>
<p>x = <x1,x2,...,xn>：n维特征向量<br>w = <w1,w2,...,wn>: 特征向量对应的权重/系数(coefficient)<br>b:截距(intercept),为避免过原点</w1,w2,...,wn></x1,x2,...,xn></p>
<p>线性关系表示为：</p>
<script type="math/tex; mode=display">
f(w,x,b) = w^T  x + b, f∈R</script><p>我们所需处理的二分类问题中f∈(0,1)，故需将函数的f∈R映射到(0,1)，故有logistics函数：</p>
<script type="math/tex; mode=display">
g(z) = \frac{1}{1+e^{-z}}, ,z∈R，g∈(0,1)</script><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fo01586rqvj30d108y0tk.jpg" alt="logistic"></p>
<p>综上，若将z替换为f，则logistics regression模型如下：</p>
<script type="math/tex; mode=display">
h_{w,b}(x) = g(f(w,x,b)) =\frac{1}{1+e^{-f}} = \frac{1}{1+e^{-(w^Tx+b)}}</script><p>该模型处理一个待分类的特征向量：若z=0，则g=0.5；若z&lt;0,则g&lt;0.5,此FV被判别为一类，反之则为另一类。</p>
<p>当使用一组m个用于训练的FV $X=<x^1,x^2,…,x^n>$ 和其对应的分类目标$y=<y^1,y^2,…,y^n>$  ，我们希望该模型能在这组训练集上取得最大似然估计(Maximum Likelihood)的概率 $L(w,b)$, 或者说至少要在训练集上表现为：</y^1,y^2,…,y^n></x^1,x^2,…,x^n></p>
<script type="math/tex; mode=display">
argmax_{w,b} L(w,b) = argmax \prod_{i=1} (h_{w,b}(i)^{y^i} (1-h_{w,b}(i)))^{1-y^i}</script><p>为学习到决定模型的参数(parameters)，即系数w和截距b，我们普遍使用一种精确计算的解析算法和一种快速估计的随机梯度上升算法(stochasitic gradient ascend)。</p>
<blockquote>
<p>1.任何模型在训练集上的表现都不一定能代表其最终在未知待测数据集上的性能，但至少要先保证模型可以被训练集优化。</p>
<p>2.SGA和SGD都属于梯度法迭代渐进估计参数的过程，梯度上升(SGA)用于目标最大化，梯度下降(SGD)用于目标最小化。</p>
</blockquote>
<p>数据集说明：</p>
<ul>
<li>数据集地址：<a href="https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/" target="_blank" rel="noopener">Address</a></li>
<li>描述：良/恶性肿瘤预测数据</li>
<li>目的：分类预测，并使用精细的测评指标评价模型性能</li>
</ul>
<p><strong>1.数据预处理</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"></span><br><span class="line"><span class="comment"># create feature list</span></span><br><span class="line">column_names = [<span class="string">'Sample code number'</span>, <span class="string">'Clump Thickness'</span>, <span class="string">'Uniformity of Cell Size'</span>, </span><br><span class="line"><span class="string">'Uniformity of Cell Shape'</span>, <span class="string">'Marginal Adhesion'</span>,<span class="string">'Single Epithelical Cell Size'</span>,<span class="string">'Bare Nuclei'</span>,<span class="string">'Bland Chromatin'</span>,<span class="string">'Normal Nucleoli'</span>,<span class="string">'Mitoses'</span>,<span class="string">'Class'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># read data</span></span><br><span class="line">data = pd.read_csv(<span class="string">'/Users/scarlett/repository/projects/breast_cancer/breast_cancer.csv'</span>,names=column_names)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># clean data</span></span><br><span class="line">data = data.replace(to_replace=<span class="string">'?'</span>, value=np.nan)</span><br><span class="line">data = data.dropna(how=<span class="string">'any'</span>)</span><br><span class="line"><span class="keyword">print</span> data.shape</span><br><span class="line"><span class="keyword">print</span> data.info()</span><br></pre></td></tr></table></figure>
<p>output：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">(683, 11)</span><br><span class="line">&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;</span><br><span class="line">Int64Index: 683 entries, 0 to 698</span><br><span class="line">Data columns (total 11 columns):</span><br><span class="line">Sample code number              683 non-null int64</span><br><span class="line">Clump Thickness                 683 non-null int64</span><br><span class="line">Uniformity of Cell Size         683 non-null int64</span><br><span class="line">Uniformity of Cell Shape        683 non-null int64</span><br><span class="line">Marginal Adhesion               683 non-null int64</span><br><span class="line">Single Epithelical Cell Size    683 non-null int64</span><br><span class="line">Bare Nuclei                     683 non-null object</span><br><span class="line">Bland Chromatin                 683 non-null int64</span><br><span class="line">Normal Nucleoli                 683 non-null int64</span><br><span class="line">Mitoses                         683 non-null int64</span><br><span class="line">Class                           683 non-null int64</span><br><span class="line">dtypes: int64(10), object(1)</span><br><span class="line">memory usage: 64.0+ KB</span><br><span class="line">None</span><br></pre></td></tr></table></figure>
<p>由于原始数据没有提供对应的测试样本，故需要对带有label的样本进行分割，一般是25%作为测试集，75%作为训练集。</p>
<p><strong>2.准备训练、测试数据</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># prepare training set and testing set</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># use train_test_split in sklearn to split data</span></span><br><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="comment"># randomly sample 25% for testing,75% for training</span></span><br><span class="line">X_train,X_test,y_train,y_test = train_test_split(data[column_names[<span class="number">1</span>:<span class="number">10</span>]],data[column_names[<span class="number">10</span>]],test_size=<span class="number">0.25</span>,random_state=<span class="number">33</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># check number and class</span></span><br><span class="line"><span class="keyword">print</span> y_train.value_counts()</span><br><span class="line"><span class="keyword">print</span> y_test.value_counts()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">2    344</span><br><span class="line">4    168</span><br><span class="line">Name: Class, dtype: int64</span><br><span class="line">2    100</span><br><span class="line">4     71</span><br><span class="line">Name: Class, dtype: int64</span><br></pre></td></tr></table></figure>
<p>训练样本：512条（344条良性肿瘤数据+168恶性肿瘤数据），测试样本171条（100+71）</p>
<p><strong>sklearn.model_selection.train_test_split解释</strong></p>
<p>from sklearn.cross_validation import train_test_split</p>
<p>一般形式：X_train,X_test, y_train, y_test = cross_validation.train_test_split(train_data,train_target,test_size=0.4, random_state=0)</p>
<p>参数解释：</p>
<ul>
<li>train_data：所要划分的样本特征集</li>
<li>train_target：所要划分的样本结果</li>
<li>test_size：样本占比，如果是整数的话就是样本的数量</li>
<li>random_state：是随机数的种子</li>
</ul>
<blockquote>
<p>随机数种子：其实就是该组随机数的编号，在需要重复试验的时候，保证得到一组一样的随机数。比如你每次都填1，其他参数一样的情况下你得到的随机数组是一样的。但填0或不填，每次都会不一样。</p>
<p>随机数的产生取决于种子，随机数和种子之间的关系遵从以下两个规则：</p>
<p>种子不同，产生不同的随机数；种子相同，即使实例不同也产生相同的随机数。</p>
</blockquote>
<p><strong>3.使用线性分类模型进行分类预测</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment"># standarlize data,make sure DE=1,EX=0,so that the outcome wont be influenced by big feature</span></span><br><span class="line">ss = StandardScaler()</span><br><span class="line">X_train=ss.fit_transform(X_train)</span><br><span class="line">X_test=ss.transform(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># init logisticregression and SGDClassifier</span></span><br><span class="line">lr = LogisticRegression()</span><br><span class="line">sgdc = SGDClassifier()</span><br><span class="line"></span><br><span class="line"><span class="comment"># use fit() of LR to train paras</span></span><br><span class="line">lr.fit(X_train,y_train)</span><br><span class="line"><span class="comment"># use trained lr to predict X_test</span></span><br><span class="line">lr_y_predict = lr.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># use fit() of SGDC to train paras, use trained lr to predict X_test</span></span><br><span class="line">sgdc.fit(X_train,y_train)</span><br><span class="line">sgdc_y_predict=sgdc.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> sgdc_y_predict</span><br><span class="line"><span class="keyword">print</span> lr_y_predict</span><br></pre></td></tr></table></figure>
<p>output：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[4 2 4 4 2 2 2 4 2 2 2 2 4 2 4 4 4 4 4 2 2 4 4 2 4 4 2 2 4 4 4 4 4 4 4 4 2</span><br><span class="line"> 4 4 4 4 4 2 4 2 2 4 2 2 4 4 2 2 2 4 2 2 2 2 2 4 4 2 2 2 4 2 2 2 2 4 2 2 4</span><br><span class="line"> 2 2 2 2 4 2 2 2 4 2 2 2 4 2 4 2 4 4 2 2 2 2 4 4 2 2 2 4 2 2 4 2 2 2 2 2 4</span><br><span class="line"> 2 2 2 2 2 2 4 2 2 4 4 2 4 2 2 2 4 2 2 4 4 2 4 4 2 2 2 2 4 2 4 2 4 2 2 2 2</span><br><span class="line"> 2 4 4 2 4 4 2 4 2 2 2 2 4 4 4 2 4 2 2 4 2 4 4]</span><br><span class="line">[2 2 4 4 2 2 2 4 2 2 2 2 4 2 4 4 4 4 4 2 2 4 4 2 4 4 2 2 4 4 4 4 4 4 4 4 2</span><br><span class="line"> 4 4 4 4 4 2 4 2 2 4 2 2 4 4 2 2 2 4 2 2 2 2 2 4 4 2 2 2 4 2 2 2 2 4 2 2 2</span><br><span class="line"> 2 2 2 4 4 2 2 2 4 2 2 2 4 2 4 2 4 4 2 2 2 2 4 4 2 2 2 4 2 2 4 2 2 2 2 2 4</span><br><span class="line"> 2 2 2 2 2 2 4 2 2 4 4 2 4 2 2 2 4 2 2 4 4 2 4 4 2 2 2 2 4 2 4 2 4 2 2 2 2</span><br><span class="line"> 2 4 4 2 4 4 2 4 2 2 2 2 4 4 4 2 4 2 2 4 2 4 4]</span><br></pre></td></tr></table></figure>
<p>混淆矩阵：<br>二分类任务中，预测结果(predicted condition)与正确标记(true condition)之间存在4种不同的组合：</p>
<ul>
<li>真阳性(true positive)：预测正确的恶性肿瘤</li>
<li>真阴性</li>
<li>假阳性(false positive)：误判为恶性肿瘤</li>
<li>假阴性</li>
</ul>
<p><strong>4.性能评价</strong></p>
<p><strong>性能评价指标</strong></p>
<p>评价指标1：准确率</p>
<p>$ Accuracy = \frac{TP + TN}{TP+TN+FP+FN}$</p>
<p>评价指标2：召回率(Recall)和精确率(Precision),F1 指标(F1 measure)</p>
<script type="math/tex; mode=display">Precision = \frac{TP}{TP+FP}</script><script type="math/tex; mode=display">Recall = \frac{TP}{TP+FN}</script><script type="math/tex; mode=display">F1 measure = \frac{2}{\frac{1}{Precision}+\frac{1}{Recall}}</script><p>F1指标：两指标的调和平均数，以综合考量两指标</p>
<p>对肿瘤识别，我们更关心召回率，即应该被正确识别的恶性肿瘤的百分比。</p>
<p><strong>使用线性分类模型进行肿瘤预测任务的性能分析</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用逻辑回归模型自带的评分函数score获得模型在测试集上的准确性结果</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'Acurracy of LR Classifier:'</span>, lr.score(X_test,y_test)</span><br><span class="line"><span class="comment"># use classification_report to get the other 3 measures of LR</span></span><br><span class="line"><span class="keyword">print</span> classification_report(y_test,lr_y_predict,target_names=[<span class="string">'Benign'</span>,<span class="string">'Malignant'</span>])</span><br></pre></td></tr></table></figure>
<p>output：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Acurracy of LR Classifier: 0.9883040935672515</span><br><span class="line">             precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">     Benign       0.99      0.99      0.99       100</span><br><span class="line">  Malignant       0.99      0.99      0.99        71</span><br><span class="line"></span><br><span class="line">avg / total       0.99      0.99      0.99       171</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用随机梯度下降模型自带的score评分函数模型在测试集上的准确性结果</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'Acurracy of SDG Classifier:'</span>, sgdc.score(X_test,y_test)</span><br><span class="line"><span class="comment"># use classification_report to get the other 3 measures of SGDC</span></span><br><span class="line"><span class="keyword">print</span> classification_report(y_test,lr_y_predict,target_names=[<span class="string">'Benign'</span>,<span class="string">'Malignant'</span>])</span><br></pre></td></tr></table></figure>
<p>output：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Acurracy of SDG Classifier: 0.9824561403508771</span><br><span class="line">             precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">     Benign       0.99      0.99      0.99       100</span><br><span class="line">  Malignant       0.99      0.99      0.99        71</span><br><span class="line"></span><br><span class="line">avg / total       0.99      0.99      0.99       171</span><br></pre></td></tr></table></figure>
<p>综上，发现，LR比SGDC在测试集上有更高的准确性，因为sklearn中采用解析的方式精确计算LR的参数，而使用梯度法估计SGDC的参数</p>
<p>特点分析：</p>
<ul>
<li>LR model：精确解析参数，计算时间长但模型性能略高</li>
<li>SGDC model：随机梯度上升算法估计参数，计算时间短但模型性能略低</li>
<li>训练数据规模在10万量级以上的数据，考虑到时间耗用，推荐使用随机梯度算法对模型参数进行估计</li>
</ul>
<h4 id="2-1-2-2-支持向量机（Suport-Vector-Classifier）（分类）"><a href="#2-1-2-2-支持向量机（Suport-Vector-Classifier）（分类）" class="headerlink" title="2.1.2.2 支持向量机（Suport Vector Classifier）（分类）"></a>2.1.2.2 支持向量机（Suport Vector Classifier）（分类）</h4><p>模型介绍：</p>
<p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1fo05eoil4jj30e408uq3q.jpg" alt="SVM"></p>
<blockquote>
<p>1.不是在所有数据集上SVM的表现一定都优于普通线性模型或其他模型，而是假设未知待测数据也如训练数据一样分布，则SVM可帮助找到最佳分类器；实际应用数据总是有偏差的。</p>
<p>2.上图，H1表现不佳（有分类错误）；H2与H3都表现完美。</p>
<p>3.但，分类模型的选取中我们需要更加关注如何最大限度为未知分布的数据集提供足够的待预测空间。如有一个黑色样本稍偏离H2，则会很可能被误判为白色，造成误差，而H3则可为样本提供更多的容忍度，故H3优于H2.</p>
</blockquote>
<p>数据描述：</p>
<ul>
<li>应用场景：邮政系统对收信人邮编进行识别与分类，以便确定信件的投送地；邮编多数为手写</li>
<li>任务：手写数字图片识别与分类</li>
<li>数据集：利用SVM处理sklearn内部集成的手写体数字图片数据集</li>
</ul>
<p><strong>1.读取数据</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据存储在digits变量中</span></span><br><span class="line">digits=load_digits()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查数据规模与特征维度</span></span><br><span class="line"><span class="keyword">print</span> digits.data.shape</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(1797, 64)</span><br></pre></td></tr></table></figure>
<p>(1797,64):有1797条图像数据，每幅图片由8*8=64的像素矩阵表示</p>
<p><strong>2.分割数据集</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train,X_test,y_train,y_test=train_test_split(digits.data,digits.target,test_size=<span class="number">0.25</span>,random_state=<span class="number">33</span>)</span><br><span class="line"><span class="keyword">print</span> y_train.shape</span><br><span class="line"><span class="keyword">print</span> y_test.shape</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(1347,)</span><br><span class="line">(450,)</span><br></pre></td></tr></table></figure>
<p><strong>3.使用SVM对手写数字图像进行识别</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从SVM里导入基于线性假设的SVM分类器LinearSVC</span></span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> LinearSVC</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对训练和测试的特征数据进行标准化</span></span><br><span class="line">ss=StandardScaler()</span><br><span class="line">X_train=ss.fit_transform(X_train)</span><br><span class="line">X_test=ss.transform(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化LinearSVC</span></span><br><span class="line">lsvc=LinearSVC()</span><br><span class="line"><span class="comment"># 进行模型训练</span></span><br><span class="line">lsvc.fit(X_train,y_train)</span><br><span class="line"><span class="comment"># 利用训练好的模型对测试样本的数字类别进行预测，预测结果存在y_predict中</span></span><br><span class="line">y_predict=lsvc.predict(X_test)</span><br><span class="line"><span class="keyword">print</span> y_predict</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[1 3 7 3 2 4 6 1 4 0 4 7 9 5 2 8 3 6 7 0 6 0 8 3 0 6 2 3 0 9 0 2 0 6 9 1 1</span><br><span class="line"> 5 8 0 6 1 5 8 9 5 1 6 2 6 6 7 6 7 7 2 7 8 0 7 3 6 3 9 6 6 5 5 4 2 9 3 7 6</span><br><span class="line"> 5 7 2 8 1 2 2 8 1 1 6 3 5 0 0 1 6 7 6 8 9 7 0 0 9 8 0 8 2 3 6 1 9 9 1 7 3</span><br><span class="line"> 9 8 8 5 9 5 1 1 7 9 3 3 2 8 1 3 8 6 4 0 0 0 7 1 5 5 1 8 5 1 8 1 6 9 9 4 5</span><br><span class="line"> 7 5 2 1 2 5 8 7 7 5 1 9 6 9 8 0 6 1 2 1 5 7 8 9 6 8 4 1 0 0 9 8 7 2 8 6 4</span><br><span class="line"> 8 9 4 2 6 1 8 5 6 7 5 1 9 2 8 3 2 9 4 3 5 5 6 2 4 3 2 6 4 8 5 8 0 8 8 6 3</span><br><span class="line"> 2 3 0 5 7 1 3 9 3 2 1 6 6 5 1 9 7 2 4 5 2 1 3 1 1 2 1 7 0 1 2 2 1 2 4 9 6</span><br><span class="line"> 6 3 9 2 8 1 5 5 1 8 6 2 5 6 0 1 4 2 1 8 9 4 3 0 6 8 3 3 2 0 2 0 6 5 6 6 4</span><br><span class="line"> 6 1 8 3 4 1 3 5 1 4 9 8 7 5 1 1 3 7 8 8 3 7 4 0 7 2 8 7 1 9 4 5 3 5 2 5 1</span><br><span class="line"> 3 0 5 8 4 7 6 9 9 3 3 4 0 6 4 7 0 6 1 2 3 3 4 5 3 3 5 2 0 9 7 1 5 5 8 4 4</span><br><span class="line"> 3 6 2 5 1 0 6 1 5 8 4 7 6 4 3 4 0 3 0 1 2 8 0 5 4 5 2 2 9 6 9 8 0 8 8 2 4</span><br><span class="line"> 6 5 6 4 3 9 8 9 7 1 7 9 4 1 9 9 5 9 8 0 8 2 5 1 4 2 6 3 7 9 3 7 4 3 7 1 8</span><br><span class="line"> 8 9 5 3 6 6]</span><br></pre></td></tr></table></figure>
<p><strong>4.性能测评</strong></p>
<p>同样使用precision、recall、accuracy、F1这四个测度来评价性能</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用模型自带的评估函数进行准确性测评</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'The Accuracy of Linear SVC is:'</span>,lsvc.score(X_test,y_test)</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">The Accuracy of Linear SVC is: 0.9533333333333334</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用模型自带的classification_report模块对预测结果做更精细的分析</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"><span class="keyword">print</span> classification_report(y_test,y_predict,target_names=digits.target_names.astype(str))</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">precision    recall  f1-score   support</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">          0       0.92      1.00      0.96        35</span><br><span class="line">          1       0.96      0.98      0.97        54</span><br><span class="line">          2       0.98      1.00      0.99        44</span><br><span class="line">          3       0.93      0.93      0.93        46</span><br><span class="line">          4       0.97      1.00      0.99        35</span><br><span class="line">          5       0.94      0.94      0.94        48</span><br><span class="line">          6       0.96      0.98      0.97        51</span><br><span class="line">          7       0.92      1.00      0.96        35</span><br><span class="line">          8       0.98      0.84      0.91        58</span><br><span class="line">          9       0.95      0.91      0.93        44</span><br><span class="line"></span><br><span class="line">avg / total       0.95      0.95      0.95       450</span><br></pre></td></tr></table></figure>
<p>由上可知，SVM可提供较高的手写数字识别性能，平均各项指标都在95%左右。</p>
<p><strong>多分类：判断一个样本是否同时属于多个不同类别；将多分类看成N个二分类任务</strong>。</p>
<p>如本例的分类目标有10个类别，即0—9这10个数字，因此无法直接计算三指标。故我们逐一评估每个类别的这三指标，把所有其他类别统一看做阴性(负)样本，则创造了10个二分类任务。</p>
<p><strong>特点分析</strong>：</p>
<ul>
<li>可帮助在海量甚至高维度数据中筛选对预测任务最有效的少数训练样本，节省数据内存，提高模型预测性能</li>
<li>但计算代价高（CPU资源与计算时间）</li>
</ul>
<h4 id="2-1-2-3-朴素贝叶斯（Naive-Bayes）（分类）"><a href="#2-1-2-3-朴素贝叶斯（Naive-Bayes）（分类）" class="headerlink" title="2.1.2.3 朴素贝叶斯（Naive Bayes）（分类）"></a>2.1.2.3 朴素贝叶斯（Naive Bayes）（分类）</h4><p>模型介绍：基于<strong>贝叶斯理论</strong>的分类器</p>
<ul>
<li>会单独考量每一维度特征被分类的条件概率，进而综合这些概率并对其所在的特征向量做出分类预测</li>
<li>基本数学假设：<strong>各个维度上的特征被分类的条件概率之间是相互独立的</strong></li>
</ul>
<p>若采用概率模型来表述，则定义$x=<x_1,x_2,…,x_n>$为某一n维特征向量，$y\in{c<em>1,c_2,…,c_k}$ 为改特征向量x所有k种可能的类别，记$P(y=c_i|x</em>) $ 为特征向量x属于类别$c_i$的概率，根据贝叶斯概率公式：</x_1,x_2,…,x_n></p>
<script type="math/tex; mode=display">
P(y|x)=\frac{P(x|y)P(y)}{P(x)}</script><p>我们的目标是寻找所有$y \in {c_1,c_2,…,c_k} $ 中$P(y|x)$ 最大的，即$argmaxP(y|x)$；并考虑到$P(x)$ 对于同一样本都是相同的，因此可忽略不计。故：</p>
<script type="math/tex; mode=display">
argmaxP(y|x)=argmaxP(x|y)P(y)=argmaxP(x_1,x_2,..,x_n|y)P(y)</script><p>若每一种特征可能的取值均为0或1，在无特殊假设的条件下，计算$P(x_1,x_2,..,x_n|y)$ 需要对$k*2^n$ 个可能的参数进行估计：</p>
<script type="math/tex; mode=display">
P(x_1,x_2,..,x_n|y)=P(x_i|y)P(x_2|x1,y)P(x_3|x_1,x_2.y)...P(x_n|x_1,x_2,...,x_{n-1},y)</script><p>但由于朴素贝叶斯模型的特征类别独立假设，故$P(x<em>n|x_1,x_2,…,x</em>{n-1},y)=P(x_n|y)$ ;若依然每种特征可能的取值只有2种，则<strong>只需要估计$2kn$个参数</strong>，即$P(x_1=0|y=c_1),P(x_1=1|y=c_1),…,P(x_n=1|y=c_1)$ .</p>
<p><strong>为估计每个参数的概率</strong>，采用如下公式，且<strong>改用频率比近似计算概率</strong>：</p>
<script type="math/tex; mode=display">
P(x_n=1|y=c_k)=\frac{P(x_n=1,y=c_k)}{P(y=c_k)}=\frac{\#x_n=1,y=c_k)}{\#(y=c_k)}</script><p>数据描述</p>
<ul>
<li>应用场景：互联网新闻文本分类</li>
<li>数据集：20类新闻文本</li>
</ul>
<p><strong>1.读取数据</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_20newsgroups</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 即时从网上下载数据</span></span><br><span class="line">news=fetch_20newsgroups(subset=<span class="string">'all'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> len(news.data)</span><br><span class="line"><span class="keyword">print</span> news.data[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>output：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">18846</span><br><span class="line">From: Mamatha Devineni Ratnam &lt;mr47+@andrew.cmu.edu&gt;</span><br><span class="line">Subject: Pens fans reactions</span><br><span class="line">Organization: Post Office, Carnegie Mellon, Pittsburgh, PA</span><br><span class="line">Lines: 12</span><br><span class="line">NNTP-Posting-Host: po4.andrew.cmu.edu</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">I am sure some bashers of Pens fans are pretty confused about the lack</span><br><span class="line">of any kind of posts about the recent Pens massacre of the Devils. Actually,</span><br><span class="line">I am  bit puzzled too and a bit relieved. However, I am going to put an end</span><br><span class="line">to non-PIttsburghers&apos; relief with a bit of praise for the Pens. Man, they</span><br><span class="line">are killing those Devils worse than I thought. Jagr just showed you why</span><br><span class="line">he is much better than his regular season stats. He is also a lot</span><br><span class="line">fo fun to watch in the playoffs. Bowman should let JAgr have a lot of</span><br><span class="line">fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final</span><br><span class="line">regular season game.          PENS RULE!!!</span><br></pre></td></tr></table></figure>
<p>​    </p>
<p>数据没有被设定特征，也无数字化的量度，因此需要在被训练前对数据做进一步处理。</p>
<p><strong>2.分割数据</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train,X_test,y_train,y_test=train_test_split(news.data,news.target,test_size=<span class="number">0.25</span>,random_state=<span class="number">33</span>)</span><br></pre></td></tr></table></figure>
<p><strong>3.使用朴素贝叶斯分类器对新闻文本数据进行类别预测</strong></p>
<ul>
<li>先将文本转化为特征向量</li>
<li>再利用朴素贝叶斯模型从训练数据中估计参数</li>
<li>最后利用这些概率参数对同样转化为特征向量的测试集进行类别预测</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入用于文本特征向量转换模块</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line">vec=CountVectorizer()</span><br><span class="line">X_train=vec.fit_transform(X_train)</span><br><span class="line">X_test=vec.transform(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入naive bayes</span></span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</span><br><span class="line"><span class="comment"># 初始化NB</span></span><br><span class="line">mnb=MultinomialNB()</span><br><span class="line"><span class="comment"># 利用训练数据对模型参数进行估计</span></span><br><span class="line">mnb.fit(X_train,y_train)</span><br><span class="line"><span class="comment"># 对测试样本进行类别预测，结果存储在变量y_predict中</span></span><br><span class="line">y_predict=mnb.predict(X_test)</span><br></pre></td></tr></table></figure>
<p><strong>4.性能评估</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"><span class="keyword">print</span> <span class="string">'The Accuracy of NBC is:'</span>,mnb.score(X_test,y_test)</span><br><span class="line"><span class="keyword">print</span> classification_report(y_test,y_predict,target_names=news.target_names)</span><br></pre></td></tr></table></figure>
<p>output：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">The Accuracy of NBC is: 0.8397707979626485</span><br><span class="line">                          precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">             alt.atheism       0.86      0.86      0.86       201</span><br><span class="line">           comp.graphics       0.59      0.86      0.70       250</span><br><span class="line"> comp.os.ms-windows.misc       0.89      0.10      0.17       248</span><br><span class="line">comp.sys.ibm.pc.hardware       0.60      0.88      0.72       240</span><br><span class="line">   comp.sys.mac.hardware       0.93      0.78      0.85       242</span><br><span class="line">          comp.windows.x       0.82      0.84      0.83       263</span><br><span class="line">            misc.forsale       0.91      0.70      0.79       257</span><br><span class="line">               rec.autos       0.89      0.89      0.89       238</span><br><span class="line">         rec.motorcycles       0.98      0.92      0.95       276</span><br><span class="line">      rec.sport.baseball       0.98      0.91      0.95       251</span><br><span class="line">        rec.sport.hockey       0.93      0.99      0.96       233</span><br><span class="line">               sci.crypt       0.86      0.98      0.91       238</span><br><span class="line">         sci.electronics       0.85      0.88      0.86       249</span><br><span class="line">                 sci.med       0.92      0.94      0.93       245</span><br><span class="line">               sci.space       0.89      0.96      0.92       221</span><br><span class="line">  soc.religion.christian       0.78      0.96      0.86       232</span><br><span class="line">      talk.politics.guns       0.88      0.96      0.92       251</span><br><span class="line">   talk.politics.mideast       0.90      0.98      0.94       231</span><br><span class="line">      talk.politics.misc       0.79      0.89      0.84       188</span><br><span class="line">      talk.religion.misc       0.93      0.44      0.60       158</span><br><span class="line"></span><br><span class="line">             avg / total       0.86      0.84      0.82      4712</span><br></pre></td></tr></table></figure>
<p>由上评估结果可知，NBC对4712条新闻文本测试样本分类的准确性约为83.977%，平均精确率、召回率、F1指标分别为86%、84%、82%。</p>
<p>特点分析：</p>
<ul>
<li>朴素贝叶斯模型广泛应用在互联网文本分类任务</li>
<li>优点：由于其较强的特征条件独立假设，使得模型预测所需估计的参数规模从幂指数量级向线性量级减少，极大节约内存消耗和计算时间</li>
<li>缺点：同样由于这种强假设的限制，模型训练时无法将各个特征之间的联系考量在内，使该模型在其他数据特征关联性强的分类任务上性能不佳</li>
</ul>
<h4 id="2-1-2-4-K近邻-k-Nearest-Neighbor，KNN-（分类）"><a href="#2-1-2-4-K近邻-k-Nearest-Neighbor，KNN-（分类）" class="headerlink" title="2.1.2.4 K近邻(k-Nearest Neighbor，KNN)（分类）"></a>2.1.2.4 K近邻(k-Nearest Neighbor，KNN)（分类）</h4><p>模型介绍：</p>
<ul>
<li>最简单的ML算法之一</li>
</ul>
<ul>
<li><p>假设有一些携带分类标记的训练样本，分布于特征空间中；蓝色、绿色样本点各自代表其类别；对一个待分类的红色测试样本点，未知其类别，按照“近朱者赤近墨者黑”的说法，我们需要寻找与这个待分类的样本在特征空间中距离最近的K个已标记样本作为参考，来帮助做出分类决策</p>
</li>
<li><p>思路:如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。</p>
<p>​</p>
<p>​</p>
</li>
</ul>
<p>数据描述：</p>
<ul>
<li>应用场景：使用K近邻算法对生物物种进行分类</li>
<li>数据集：Iris</li>
</ul>
<p><strong>1.读取数据集</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"></span><br><span class="line">iris=load_iris()</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> iris.data.shape</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> iris.DESCR</span><br></pre></td></tr></table></figure>
<p>output：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line">(150, 4)</span><br><span class="line">Iris Plants Database</span><br><span class="line">====================</span><br><span class="line"></span><br><span class="line">Notes</span><br><span class="line">-----</span><br><span class="line">Data Set Characteristics:</span><br><span class="line">    :Number of Instances: 150 (50 in each of three classes)</span><br><span class="line">    :Number of Attributes: 4 numeric, predictive attributes and the class</span><br><span class="line">    :Attribute Information:</span><br><span class="line">        - sepal length in cm</span><br><span class="line">        - sepal width in cm</span><br><span class="line">        - petal length in cm</span><br><span class="line">        - petal width in cm</span><br><span class="line">        - class:</span><br><span class="line">                - Iris-Setosa</span><br><span class="line">                - Iris-Versicolour</span><br><span class="line">                - Iris-Virginica</span><br><span class="line">    :Summary Statistics:</span><br><span class="line"></span><br><span class="line">    ============== ==== ==== ======= ===== ====================</span><br><span class="line">                    Min  Max   Mean    SD   Class Correlation</span><br><span class="line">    ============== ==== ==== ======= ===== ====================</span><br><span class="line">    sepal length:   4.3  7.9   5.84   0.83    0.7826</span><br><span class="line">    sepal width:    2.0  4.4   3.05   0.43   -0.4194</span><br><span class="line">    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)</span><br><span class="line">    petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)</span><br><span class="line">    ============== ==== ==== ======= ===== ====================</span><br><span class="line"></span><br><span class="line">    :Missing Attribute Values: None</span><br><span class="line">    :Class Distribution: 33.3% for each of 3 classes.</span><br><span class="line">    :Creator: R.A. Fisher</span><br><span class="line">    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)</span><br><span class="line">    :Date: July, 1988</span><br><span class="line"></span><br><span class="line">This is a copy of UCI ML iris datasets.</span><br><span class="line">http://archive.ics.uci.edu/ml/datasets/Iris</span><br><span class="line"></span><br><span class="line">The famous Iris database, first used by Sir R.A Fisher</span><br><span class="line"></span><br><span class="line">This is perhaps the best known database to be found in the</span><br><span class="line">pattern recognition literature.  Fisher&apos;s paper is a classic in the field and</span><br><span class="line">is referenced frequently to this day.  (See Duda &amp; Hart, for example.)  The</span><br><span class="line">data set contains 3 classes of 50 instances each, where each class refers to a</span><br><span class="line">type of iris plant.  One class is linearly separable from the other 2; the</span><br><span class="line">latter are NOT linearly separable from each other.</span><br><span class="line"></span><br><span class="line">References</span><br><span class="line">----------</span><br><span class="line">   - Fisher,R.A. &quot;The use of multiple measurements in taxonomic problems&quot;</span><br><span class="line">     Annual Eugenics, 7, Part II, 179-188 (1936); also in &quot;Contributions to</span><br><span class="line">     Mathematical Statistics&quot; (John Wiley, NY, 1950).</span><br><span class="line">   - Duda,R.O., &amp; Hart,P.E. (1973) Pattern Classification and Scene Analysis.</span><br><span class="line">     (Q327.D83) John Wiley &amp; Sons.  ISBN 0-471-22361-1.  See page 218.</span><br><span class="line">   - Dasarathy, B.V. (1980) &quot;Nosing Around the Neighborhood: A New System</span><br><span class="line">     Structure and Classification Rule for Recognition in Partially Exposed</span><br><span class="line">     Environments&quot;.  IEEE Transactions on Pattern Analysis and Machine</span><br><span class="line">     Intelligence, Vol. PAMI-2, No. 1, 67-71.</span><br><span class="line">   - Gates, G.W. (1972) &quot;The Reduced Nearest Neighbor Rule&quot;.  IEEE Transactions</span><br><span class="line">     on Information Theory, May 1972, 431-433.</span><br><span class="line">   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al&quot;s AUTOCLASS II</span><br><span class="line">     conceptual clustering system finds 3 classes in the data.</span><br><span class="line">   - Many, many more ...</span><br></pre></td></tr></table></figure>
<p><strong>2.分割数据集</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train,X_test,y_train,y_test=train_test_split(iris.data,iris.target,test_size=<span class="number">0.25</span>,random_state=<span class="number">33</span>)</span><br></pre></td></tr></table></figure>
<p><strong>3.使用K近邻算法对iris数据进行类别预测</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"></span><br><span class="line">ss=StandardScaler()</span><br><span class="line">X_train=ss.fit_transform(X_train)</span><br><span class="line">X_test=ss.transform(X_test)</span><br><span class="line"></span><br><span class="line">knc=KNeighborsClassifier()</span><br><span class="line">knc.fit(X_train,y_train)</span><br><span class="line">y_predict=knc.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> y_predict</span><br></pre></td></tr></table></figure>
<p>output：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[1 1 0 1 1 2 0 0 2 2 2 0 2 1 2 1 1 0 1 2 0 0 2 0 1 2 1 1 2 1 1 1 2 2 2 2 2</span><br><span class="line"> 1]</span><br></pre></td></tr></table></figure>
<p><strong>4.性能评估</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> <span class="string">'The Accuracy of KNC is:'</span>,knc.score(X_test,y_test)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"><span class="keyword">print</span> classification_report(y_test,y_predict,target_names=iris.target_names)</span><br></pre></td></tr></table></figure>
<p>output：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">The Accuracy of KNC is: 0.8947368421052632</span><br><span class="line">             precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">     setosa       1.00      1.00      1.00         8</span><br><span class="line"> versicolor       0.73      1.00      0.85        11</span><br><span class="line">  virginica       1.00      0.79      0.88        19</span><br><span class="line"></span><br><span class="line">avg / total       0.92      0.89      0.90        38</span><br></pre></td></tr></table></figure>
<p>特点分析：</p>
<ul>
<li>k近邻算法是非常直观简单的模型</li>
<li>是<strong>无参数模型</strong>：<strong>没有参数训练过程</strong>，即未通过任何学习算法分析数据，而只是根据测试样本在训练数据中的分布做出分类</li>
<li>缺点：<strong>高计算复杂度和内存消耗</strong>；平方级别的算法复杂度（每处理一个测试样本就要对所有训练样本进行遍历，逐一计算相似度、排序且选取K个最近邻训练样本的标记，进而做出分类决策）</li>
<li>当然也有KD-Tree这样的数据结构通过“空间换时间”思想节省KNN的决策时间</li>
</ul>
<h4 id="2-1-2-5-决策树（Decision-Tree）（分类）"><a href="#2-1-2-5-决策树（Decision-Tree）（分类）" class="headerlink" title="2.1.2.5 决策树（Decision Tree）（分类）"></a>2.1.2.5 决策树（Decision Tree）（分类）</h4><p>模型介绍：</p>
<ul>
<li><strong>描述非线性关系</strong></li>
</ul>
<blockquote>
<p>LR和SVM都要求被学习的数据特征和目标之间遵照线性假设，但现实场景下这种假设不存在。如用年龄预测流感死亡率，年龄与死亡率之间不存在线性关系。</p>
</blockquote>
<ul>
<li><p>决策树<strong>节点(node)——数据特征</strong></p>
</li>
<li><p>各节点下的<strong>分支——特征值的分类</strong></p>
</li>
<li><p>决策树的<strong>所有叶子节点——显示模型的决策结果</strong></p>
</li>
<li><p>使用多种不同特征组合搭建多层决策树时，需<strong>考虑特征节点的选取顺序</strong>，常用的度量方式有信息熵(Information Gain)和基尼不纯性(Gini Impurity)</p>
<p>​</p>
</li>
</ul>
<p>数据描述</p>
<ul>
<li>借助决策树模型预测泰坦尼克号乘客生还情况</li>
<li>数据集：乘客信息</li>
<li><a href="http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt" target="_blank" rel="noopener">Address</a></li>
</ul>
<p><strong>1.数据查验</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">titanic=pd.read_csv(<span class="string">'/Users/scarlett/repository/projects/titanic/titanic.csv'</span>)</span><br><span class="line"><span class="keyword">print</span> titanic.head()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">   row.names pclass  survived  \</span><br><span class="line">0          1    1st         1   </span><br><span class="line">1          2    1st         0   </span><br><span class="line">2          3    1st         0   </span><br><span class="line">3          4    1st         0   </span><br><span class="line">4          5    1st         1   </span><br><span class="line"></span><br><span class="line">                                              name      age     embarked  \</span><br><span class="line">0                     Allen, Miss Elisabeth Walton  29.0000  Southampton   </span><br><span class="line">1                      Allison, Miss Helen Loraine   2.0000  Southampton   </span><br><span class="line">2              Allison, Mr Hudson Joshua Creighton  30.0000  Southampton   </span><br><span class="line">3  Allison, Mrs Hudson J.C. (Bessie Waldo Daniels)  25.0000  Southampton   </span><br><span class="line">4                    Allison, Master Hudson Trevor   0.9167  Southampton   </span><br><span class="line"></span><br><span class="line">                         home.dest room      ticket   boat     sex  </span><br><span class="line">0                     St Louis, MO  B-5  24160 L221      2  female  </span><br><span class="line">1  Montreal, PQ / Chesterville, ON  C26         NaN    NaN  female  </span><br><span class="line">2  Montreal, PQ / Chesterville, ON  C26         NaN  (135)    male  </span><br><span class="line">3  Montreal, PQ / Chesterville, ON  C26         NaN    NaN  female  </span><br><span class="line">4  Montreal, PQ / Chesterville, ON  C22         NaN     11    male</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> titanic.info()</span><br><span class="line"><span class="keyword">print</span> titanic.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;</span><br><span class="line">RangeIndex: 1313 entries, 0 to 1312</span><br><span class="line">Data columns (total 11 columns):</span><br><span class="line">row.names    1313 non-null int64</span><br><span class="line">pclass       1313 non-null object</span><br><span class="line">survived     1313 non-null int64</span><br><span class="line">name         1313 non-null object</span><br><span class="line">age          633 non-null float64</span><br><span class="line">embarked     821 non-null object</span><br><span class="line">home.dest    754 non-null object</span><br><span class="line">room         77 non-null object</span><br><span class="line">ticket       69 non-null object</span><br><span class="line">boat         347 non-null object</span><br><span class="line">sex          1313 non-null object</span><br><span class="line">dtypes: float64(1), int64(2), object(8)</span><br><span class="line">memory usage: 112.9+ KB</span><br><span class="line">None</span><br><span class="line">(1313, 11)</span><br></pre></td></tr></table></figure>
<p><strong>2.数据预处理</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 特征选择</span></span><br><span class="line">X=titanic[[<span class="string">'pclass'</span>,<span class="string">'sex'</span>,<span class="string">'age'</span>]]</span><br><span class="line">y=titanic[<span class="string">'survived'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 探查所选特征</span></span><br><span class="line"><span class="keyword">print</span> X.info()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;</span><br><span class="line">RangeIndex: 1313 entries, 0 to 1312</span><br><span class="line">Data columns (total 3 columns):</span><br><span class="line">pclass    1313 non-null object</span><br><span class="line">sex       1313 non-null object</span><br><span class="line">age       633 non-null float64</span><br><span class="line">dtypes: float64(1), object(2)</span><br><span class="line">memory usage: 30.8+ KB</span><br><span class="line">None</span><br></pre></td></tr></table></figure>
<p>数据处理任务：</p>
<ul>
<li>age只有714个，有缺失项，需要补全（使用平均数或中位数来补全，造成的影响最小）</li>
<li>sex与pclass是int/object型，需要转换成数值特征，用0/1代替</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X[<span class="string">'age'</span>].fillna(X[<span class="string">'age'</span>].mean(),inplace=<span class="keyword">True</span>)</span><br><span class="line"><span class="keyword">print</span> X.info()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;</span><br><span class="line">RangeIndex: 1313 entries, 0 to 1312</span><br><span class="line">Data columns (total 3 columns):</span><br><span class="line">pclass    1313 non-null object</span><br><span class="line">sex       1313 non-null object</span><br><span class="line">age       1313 non-null float64</span><br><span class="line">dtypes: float64(1), object(2)</span><br><span class="line">memory usage: 30.8+ KB</span><br><span class="line">None</span><br></pre></td></tr></table></figure>
<p>由此可知Age特征得到了补充。</p>
<p><strong>4.数据分割</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=<span class="number">0.25</span>,random_state=<span class="number">33</span>)</span><br></pre></td></tr></table></figure>
<p><strong>5.特征转换</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用特征转换器</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> DictVectorizer</span><br><span class="line">vec=DictVectorizer(sparse=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转换特征后发现：凡是类别型的特征都单独被剥离出来独立成一列特征，数值型则保持不变</span></span><br><span class="line">X_train=vec.fit_transform(X_train.to_dict(orient=<span class="string">'record'</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">X_test=vec.transform(X_test.to_dict(orient=<span class="string">'record'</span>))</span><br></pre></td></tr></table></figure>
<p><strong>6.训练模型</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line">dtc=DecisionTreeClassifier()</span><br><span class="line">dtc.fit(X_train,y_train)</span><br><span class="line">y_predict=dtc.predict(X_test)</span><br></pre></td></tr></table></figure>
<p><strong>7.性能评估</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> dtc.score(X_test,y_test)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> classification_report(y_predict,y_test,target_names=[<span class="string">'died'</span>,<span class="string">'survived'</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">0.7811550151975684</span><br><span class="line">             precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">       died       0.91      0.78      0.84       236</span><br><span class="line">   survived       0.58      0.80      0.67        93</span><br><span class="line"></span><br><span class="line">avg / total       0.81      0.78      0.79       329</span><br></pre></td></tr></table></figure>
<h4 id="2-1-2-6-集成模型（Ensemble）（分类）"><a href="#2-1-2-6-集成模型（Ensemble）（分类）" class="headerlink" title="2.1.2.6 集成模型（Ensemble）（分类）"></a>2.1.2.6 集成模型（Ensemble）（分类）</h4><p>模型介绍</p>
<ul>
<li>综合考量多个分类器的预测结果，从而做出分类决策，<strong>综合考量</strong>方式有2种：</li>
<li>1.<strong>利用相同的训练数据同时搭建多个独立的分类模型</strong>，然后通过<strong>投票</strong>以少数服从多数的原则做出最终分类决策，如：<ul>
<li><strong>随机森林分类器</strong>(Random Forest Classifier)：在相同训练数据上同时搭建多棵决策树（每棵树都随机选取特征）</li>
</ul>
</li>
<li>2.<strong>按照一定词序搭建多个分类模型，模型间彼此存在依赖关系</strong>（每个后续模型的加入都需要对现有集成模型的综合性能有所贡献，进而不断提升更新过后的集成模型的性能，并最终期望借助整合多个分类能力较弱的分类器，搭建出具有更强分类能力的模型），如:<ul>
<li><strong>梯度提升决策树</strong>(Gradient Tree Boosting)：每棵树在生成过程中都会尽可能降低整体集成模型在训练集上的拟合误差</li>
</ul>
</li>
</ul>
<p>数据描述</p>
<ul>
<li>对比单一决策树、随机森林、梯度提升决策树三者性能差异</li>
<li>数据集：Titanic乘客数据</li>
</ul>
<p><strong>1.集成模型对titanic乘客是否生还的预测</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">titanic=pd.read_csv(<span class="string">'/Users/scarlett/repository/projects/titanic/titanic.csv'</span>)</span><br><span class="line"></span><br><span class="line">X=titanic[[<span class="string">'pclass'</span>,<span class="string">'age'</span>,<span class="string">'sex'</span>]]</span><br><span class="line">y=titanic[<span class="string">'survived'</span>]</span><br><span class="line"></span><br><span class="line">X[<span class="string">'age'</span>].fillna(X[<span class="string">'age'</span>].mean(),inplace=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=<span class="number">0.25</span>,random_state=<span class="number">33</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> DictVectorizer</span><br><span class="line">vec=DictVectorizer(sparse=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">X_train=vec.fit_transform(X_train.to_dict(orient=<span class="string">'record'</span>))</span><br><span class="line"></span><br><span class="line">X_test=vec.transform(X_test.to_dict(orient=<span class="string">'record'</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用单一决策树训练模型</span></span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line">dtc=DecisionTreeClassifier()</span><br><span class="line">dtc.fit(X_train,y_train)</span><br><span class="line">dtc_y_pred=dtc.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> dtc_y_pred</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[0 1 0 0 0 1 1 0 0 1 0 1 1 0 0 1 0 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 0 0 1 1 1</span><br><span class="line"> 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0</span><br><span class="line"> 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0</span><br><span class="line"> 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 1</span><br><span class="line"> 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0</span><br><span class="line"> 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0</span><br><span class="line"> 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 1 1 0 0 0 1 1 0 0</span><br><span class="line"> 1 1 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0</span><br><span class="line"> 1 0 0 0 1 1 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用随机森林分类器进行集成模型训练</span></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line">rfc=RandomForestClassifier()</span><br><span class="line">rfc.fit(X_train,y_train)</span><br><span class="line">rfc_y_pred=rfc.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> rfc_y_pred</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[0 1 0 0 0 1 1 0 0 1 0 1 1 0 0 1 0 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 0 0 1 1 1</span><br><span class="line"> 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0</span><br><span class="line"> 0 1 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0</span><br><span class="line"> 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 1</span><br><span class="line"> 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0</span><br><span class="line"> 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0</span><br><span class="line"> 0 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 1 1 0 0 0 1 1 0 0</span><br><span class="line"> 1 1 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0</span><br><span class="line"> 1 0 0 0 1 1 0 0 0 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用梯度提升模型训练集成模型</span></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier</span><br><span class="line">gbc=GradientBoostingClassifier()</span><br><span class="line">gbc.fit(X_train,y_train)</span><br><span class="line">gbc_y_pred=gbc.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> gbc_y_pred</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 0 0 1 1 1</span><br><span class="line"> 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0</span><br><span class="line"> 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0</span><br><span class="line"> 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 1</span><br><span class="line"> 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0</span><br><span class="line"> 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0</span><br><span class="line"> 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 1 1 0 0</span><br><span class="line"> 0 1 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0</span><br><span class="line"> 1 0 0 0 1 1 0 0 0 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0]</span><br></pre></td></tr></table></figure>
<p><strong>2.性能测评</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出单一决策树的评估指标</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'The accuracy of DTC is'</span>,dtc.score(X_test,y_test)</span><br><span class="line"><span class="keyword">print</span> classification_report(dtc_y_pred,y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出随机森林的评估指标</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'The accuracy of RFC is'</span>,rfc.score(X_test,y_test)</span><br><span class="line"><span class="keyword">print</span> classification_report(rfc_y_pred,y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出梯度提升决策树的评估指标</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'The accuracy of GBC is'</span>,gbc.score(X_test,y_test)</span><br><span class="line"><span class="keyword">print</span> classification_report(gbc_y_pred,y_test)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">The accuracy of DTC is 0.7811550151975684</span><br><span class="line">             precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">          0       0.91      0.78      0.84       236</span><br><span class="line">          1       0.58      0.80      0.67        93</span><br><span class="line"></span><br><span class="line">avg / total       0.81      0.78      0.79       329</span><br><span class="line"></span><br><span class="line">The accuracy of RFC is 0.7781155015197568</span><br><span class="line">             precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">          0       0.90      0.78      0.83       233</span><br><span class="line">          1       0.59      0.78      0.67        96</span><br><span class="line"></span><br><span class="line">avg / total       0.81      0.78      0.79       329</span><br><span class="line"></span><br><span class="line">The accuracy of GBC is 0.790273556231003</span><br><span class="line">             precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">          0       0.92      0.78      0.84       239</span><br><span class="line">          1       0.58      0.82      0.68        90</span><br><span class="line"></span><br><span class="line">avg / total       0.83      0.79      0.80       329</span><br></pre></td></tr></table></figure>
<p>上面的输出表明：在相同的训练和测试数据条件下，仅使用模型默认配置，<strong>梯度上升决策树具有最佳预测性能</strong>。一般工业界为追求更强的预测性能，会<strong>把随机森林作为基线系统(Baseline System)</strong></p>
<p>集成模型：</p>
<ul>
<li>最常见的应用；可整合多种模型</li>
<li>缺点：模型估计参数的过程受概率影响，具有不确定性</li>
<li>优点：虽然模型训练需要耗费更多时间，但得到的综合模型会具有<strong>更高的性能和稳定性</strong></li>
</ul>
<h3 id="2-1-3-回归预测"><a href="#2-1-3-回归预测" class="headerlink" title="2.1.3 回归预测"></a>2.1.3 回归预测</h3><p>回归 vs 分类：区别在于其<strong>待预测目标是连续变量</strong></p>
<h4 id="2-1-3-1-线性回归器"><a href="#2-1-3-1-线性回归器" class="headerlink" title="2.1.3.1 线性回归器"></a>2.1.3.1 线性回归器</h4><p>模型介绍</p>
<blockquote>
<p><strong>最小二乘法</strong>：数学优化方法；通过最小化误差的平方和寻找数据的最佳函数匹配。利用最小二乘法可以简便地求得未知的数据，并使得这些求得的数据与实际数据之间误差的平方和为最小。</p>
</blockquote>
<p>线性回归问题中：优化目标即最小化预测结果与真实值之间的差异（因为预测目标直接是实数域上的数值）</p>
<p>当使用一组m个用于训练的特征向量$X=<x^1,x^2,…,x^n>$ 和其对应的回归目标$y=<y^1,y^2,…,y^m>$ 时，我们希望线性回归模型可以<strong>最小二乘</strong>(Generalized Least Squares)<strong>预测的损失$L(w,b)$</strong> ，则<strong>线性回归器的常见优化目标为</strong>：*</y^1,y^2,…,y^m></x^1,x^2,…,x^n></p>
<script type="math/tex; mode=display">
argminL(w,b)=argmin\sum_{m}^{k=1}\qquad(f(w,x,b)-y^k)^2</script><p>同样为学习到决定模型的参数$w，b$ ，仍可使用一种精确计算的解析算法和一种快速的随机梯度下降(Stochastic Gradient Descend)估计算法.</p>
<p>数据描述</p>
<ul>
<li>美国波士顿地区房价预测</li>
</ul>
<p><strong>性能评估指标：</strong></p>
<p>假设测试数据有m个目标数值$y=<y^1,y^2,…,y^m>  $ 且记$\overline{y}$ 为回归模型的预测结果，则：</y^1,y^2,…,y^m></p>
<ul>
<li><strong>MAE</strong>：<ul>
<li>$SS<em>{abs}=\sum</em>{m}^{i=1}\qquad|y^i-\overline{y}|$ ,$</li>
<li>$MAE=\frac{SS_{abs}}{m}$ </li>
</ul>
</li>
<li><strong>MSE</strong>：<ul>
<li>$SS<em>{tot}=\sum</em>{m}^{i=1}\qquad(y^i-\overline{y})^2$ </li>
<li>Missing close brace MSE=\frac{SS_{tot}{m} </li>
</ul>
</li>
<li><strong>R-squared</strong>：<ul>
<li>$SS<em>{res}=\sum</em>{m}^{i=1}\qquad(y^i-(f(x^i))^2$</li>
<li>Missing close braceR^2=1-\frac{SS_{res}{tot}</li>
<li>其中，$SS<em>{tot}$ 代表测试数据真实值的方差(内部差异)；$SS</em>{res}$ 代表回归值与真实值之阿金的平方差异（回归差异）</li>
<li><strong>R-squared（拟合度）</strong>：<strong>比较预测结果与真实值的吻合程度</strong>，既考量了回归值与真实值的差异，又兼顾了问题本身真实值的变动；而MAE、MSE(差值的绝对值或平方)则会随不同预测问题而变化巨大，欠缺在不同问题中的可比性</li>
</ul>
</li>
</ul>
<p><strong>1.导入和查验数据</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line">boston=load_boston()</span><br><span class="line"><span class="comment"># 输出数据描述</span></span><br><span class="line"><span class="keyword">print</span> boston.DESCR</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"> Boston House Prices dataset</span><br><span class="line">===========================</span><br><span class="line"></span><br><span class="line">Notes</span><br><span class="line">------</span><br><span class="line">Data Set Characteristics:  </span><br><span class="line"></span><br><span class="line">    :Number of Instances: 506 </span><br><span class="line"></span><br><span class="line">    :Number of Attributes: 13 numeric/categorical predictive</span><br><span class="line">    </span><br><span class="line">    :Median Value (attribute 14) is usually the target</span><br><span class="line"></span><br><span class="line">    :Attribute Information (in order):</span><br><span class="line">        - CRIM     per capita crime rate by town</span><br><span class="line">        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.</span><br><span class="line">        - INDUS    proportion of non-retail business acres per town</span><br><span class="line">        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)</span><br><span class="line">        - NOX      nitric oxides concentration (parts per 10 million)</span><br><span class="line">        - RM       average number of rooms per dwelling</span><br><span class="line">        - AGE      proportion of owner-occupied units built prior to 1940</span><br><span class="line">        - DIS      weighted distances to five Boston employment centres</span><br><span class="line">        - RAD      index of accessibility to radial highways</span><br><span class="line">        - TAX      full-value property-tax rate per $10,000</span><br><span class="line">        - PTRATIO  pupil-teacher ratio by town</span><br><span class="line">        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town</span><br><span class="line">        - LSTAT    % lower status of the population</span><br><span class="line">        - MEDV     Median value of owner-occupied homes in $1000&apos;s</span><br><span class="line"></span><br><span class="line">    :Missing Attribute Values: None</span><br><span class="line"></span><br><span class="line">    :Creator: Harrison, D. and Rubinfeld, D.L.</span><br><span class="line"></span><br><span class="line">This is a copy of UCI ML housing dataset.</span><br><span class="line">http://archive.ics.uci.edu/ml/datasets/Housing</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">The Boston house-price data of Harrison, D. and Rubinfeld, D.L. &apos;Hedonic</span><br><span class="line">prices and the demand for clean air&apos;, J. Environ. Economics &amp; Management,</span><br><span class="line">vol.5, 81-102, 1978.   Used in Belsley, Kuh &amp; Welsch, &apos;Regression diagnostics</span><br><span class="line">...&apos;, Wiley, 1980.   N.B. Various transformations are used in the table on</span><br><span class="line">pages 244-261 of the latter.</span><br><span class="line"></span><br><span class="line">The Boston house-price data has been used in many machine learning papers that address regression</span><br><span class="line">problems.   </span><br><span class="line">     </span><br><span class="line">**References**</span><br><span class="line"></span><br><span class="line">   - Belsley, Kuh &amp; Welsch, &apos;Regression diagnostics: Identifying Influential Data and Sources of Collinearity&apos;, Wiley, 1980. 244-261.</span><br><span class="line">   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.</span><br><span class="line">   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)</span><br></pre></td></tr></table></figure>
<p><strong>2.数据分割</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">X=boston.data</span><br><span class="line">y=boston.target</span><br><span class="line">X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=<span class="number">0.25</span>,random_state=<span class="number">33</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分析回归目标值的差异</span></span><br><span class="line"><span class="keyword">print</span> np.max(y),np.min(y),np.mean(y)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">50.0 5.0 22.532806324110677</span><br></pre></td></tr></table></figure>
<p>由上发现目标房价之间的差异较大，故需要对特征和目标值进行标准化处理。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分别初始化feature和target的标准化器</span></span><br><span class="line">ss_X=StandardScaler()</span><br><span class="line">ss_y=StandardScaler()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分别对训练和测试数据的feature和target进行标准化处理</span></span><br><span class="line">X_train=ss_X.fit_transform(X_train)</span><br><span class="line">X_test=ss_X.transform(X_test)</span><br><span class="line">y_train=ss_y.fit_transform(y_train)</span><br><span class="line">y_test=ss_y.transform(y_test)</span><br></pre></td></tr></table></figure>
<p>​    </p>
<p><strong>3.使用线性回归模型和SGDRegressor分别对房价进行预测</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">lr=LinearRegression()</span><br><span class="line">lr.fit(X_train,y_train)</span><br><span class="line">lr_y_predict=lr.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDRegressor</span><br><span class="line">sgdr=SGDRegressor()</span><br><span class="line">sgdr.fit(X_train,y_train)</span><br><span class="line">sgdr_y_predict=sgdr.predict(X_test)</span><br></pre></td></tr></table></figure>
<p><strong>4.性能测评</strong></p>
<p>测量目的：衡量预测值与真实值之间的差距</p>
<p>测评指标：</p>
<ul>
<li>平均绝对误差（Mean Absolute Error,MAE)</li>
<li>均方误差(Mean Squared Error,MSE)</li>
<li>拟合度(R-squared, R平方)：拟合度检验是对已制作好的预测模型进行检验，比较它们的预测结果与实际发生情况的吻合程度</li>
</ul>
<p><strong>使用三种回归评价机制和两种调用R-squared评价模块的方法，评价此模型的回归性能</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用LR模型自带的评估模块</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'The value of default measurement of LR is'</span>,lr.score(X_test,y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入MAE和MSE评估回归模型</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score,mean_squared_error,mean_absolute_error</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用r2_score模块</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'The value of R-squared of LR is'</span>,r2_score(y_test,lr_y_predict)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用mean_squared_error模块</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'The MSE of LR is'</span>,mean_squared_error(ss_y.inverse_transform(y_test),ss_y.inverse_transform(lr_y_predict))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用mean_absolute_error模块</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'The MAE of LR is'</span>,mean_absolute_error(ss_y.inverse_transform(y_test),ss_y.inverse_transform(lr_y_predict))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用SGDR自带评估模块</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'The value of default measurement of SGDR is'</span>,sgdr.score(X_test,y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用r2_score模块</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'The value of R-squared of SGDR is'</span>,r2_score(y_test,sgdr_y_predict)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用mean_squared_error模块</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'The MSE of SGDR is'</span>,mean_squared_error(ss_y.inverse_transform(y_test),ss_y.inverse_transform(sgdr_y_predict))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用mean_absolute_error模块</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'The MAE of SGDR is'</span>,mean_absolute_error(ss_y.inverse_transform(y_test),ss_y.inverse_transform(sgdr_y_predict))</span><br></pre></td></tr></table></figure>
<p>特点分析：</p>
<ul>
<li><strong>数据规模超10万，使用随机梯度法估计参数</strong></li>
<li>在不清楚特征之间关系的前提下，可使用线性回归模型作为基线系统（baseline system）</li>
</ul>
<h4 id="2-1-3-2-支持向量机-回归"><a href="#2-1-3-2-支持向量机-回归" class="headerlink" title="2.1.3.2 支持向量机(回归)"></a>2.1.3.2 支持向量机(回归)</h4><p>模型介绍</p>
<ul>
<li>同样是<strong>从训练数据中选取一部分更加有效的支持向量</strong>，只是这少部分训练样本所提供的并不是类别目标，而是<strong>具体的预测数值</strong></li>
</ul>
<p>继续使用2.1.3.1中的训练集和测试集进行不同核函数配置的SVM回归模型训练，且分别对测试数据做出越策，会发现：</p>
<ul>
<li>不同配置下的模型在相同测试集上存在非常大的性能差异，且使用径向基(Radical basis function)核函数对特征进行非线性映射后，SVM展现最佳回归性能</li>
<li>可以多尝试几种配置，以活动最佳预测性能</li>
</ul>
<blockquote>
<p>核函数：一种特征映射技巧，即通过某种函数计算，将原有的线性不可分的低维特征映射到更高维度的空间，从而尽可能达到新的高维度特征线性可分的程度。</p>
</blockquote>
<h4 id="2-1-2-3-K近邻-回归"><a href="#2-1-2-3-K近邻-回归" class="headerlink" title="2.1.2.3 K近邻(回归)"></a>2.1.2.3 K近邻(回归)</h4><p>模型介绍</p>
<ul>
<li>在回归任务中，K近邻(回归)模型同样<strong>只是借助周围K个距离最近的训练样本的目标数值</strong>，对待测样本的回归值进行决策。</li>
</ul>
<p><strong>1.使用2种不同配置的K近邻回归模型对美国波士顿放假数据进行回归预测</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line">boston=load_boston()</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">X=boston.data</span><br><span class="line">y=boston.target</span><br><span class="line">X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=<span class="number">0.25</span>,random_state=<span class="number">33</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsRegressor</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化回归器，调整配置，使预测方式为平均回归，weights='uniform'</span></span><br><span class="line">uni_knr=KNeighborsRegressor(weights=<span class="string">'uniform'</span>)</span><br><span class="line">uni_knr.fit(X_train,y_train)</span><br><span class="line">uni_knr_y_predict=uni_knr.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化回归器，调整配置，使预测方式为根据距离加权回归，weights='distance'</span></span><br><span class="line">dis_knr=KNeighborsRegressor(weights=<span class="string">'distance'</span>)</span><br><span class="line">dis_knr.fit(X_train,y_train)</span><br><span class="line">dis_knr_y_predict=dis_knr.predict(X_test)</span><br></pre></td></tr></table></figure>
<p><strong>2.性能测评</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用R-squared、MSE、MAE三指标分别对两种不同配置的模型进行性能评估</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score,mean_squared_error,mean_absolute_error</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> <span class="string">'Uniform'</span>,uni_knr.score(X_test,y_test),mean_squared_error(y_test,uni_knr_y_predict),mean_absolute_error(y_test,uni_knr_y_predict)</span><br><span class="line"><span class="keyword">print</span> <span class="string">'Distance'</span>,dis_knr.score(X_test,y_test),mean_squared_error(y_test,dis_knr_y_predict),mean_absolute_error(y_test,dis_knr_y_predict)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Uniform 0.6418225886716102 27.773540157480316 3.7645669291338586</span><br><span class="line">Distance 0.6565370125979323 26.63256467749057 3.6251742046017417</span><br></pre></td></tr></table></figure>
<p>由上可知，<strong>K近邻加权平均的回归策略具有更好的预测性能</strong>。</p>
<h4 id="2-1-3-4-回归树"><a href="#2-1-3-4-回归树" class="headerlink" title="2.1.3.4 回归树"></a>2.1.3.4 回归树</h4><p>模型介绍</p>
<ul>
<li>在<strong>选择不同特征作为分裂节点的策略</strong>上，与决策树类似</li>
<li>不同：<strong>回归树叶节点的数据类型为连续型非离散型</strong>；决策树每个叶子节点依照训练数据表现的概率倾向决定其最终的预测类别，而回归树叶子节点是一个个具体数值，从预测值连续的意义上严格讲，回归树不能称为回归算法（因为<strong>回归树叶子节点返回的是“一团”训练数值的均值，而非具体连续的预测值</strong>）</li>
</ul>
<p><strong>1.使用回归树对波士顿房价训练数据进行学习，并对测试数据预测</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line">boston=load_boston()</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">X=boston.data</span><br><span class="line">y=boston.target</span><br><span class="line">X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=<span class="number">0.25</span>,random_state=<span class="number">33</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeRegressor</span><br><span class="line"></span><br><span class="line">dtr=DecisionTreeRegressor()</span><br><span class="line">dtr.fit(X_train,y_train)</span><br><span class="line">dtr_y_predict=dtr.predict(X_test)</span><br></pre></td></tr></table></figure>
<p><strong>2.性能测评</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score,mean_squared_error,mean_absolute_error</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> dtr.score(X_test,y_test),mean_squared_error(y_test,dtr_y_predict),mean_absolute_error(y_test,dtr_y_predict)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.5223911380673973 37.034409448818906 3.4700787401574806</span><br></pre></td></tr></table></figure>
<p><strong>树模型（回归树，决策树）</strong></p>
<ul>
<li>优点：可<strong>解决非线性拟合问题</strong>；<strong>不要求对特征标准化和统一量化</strong>（即数值型、类别型特征都可直接被训练）；可直观输出决策过程，使决策结果具有<strong>可解释性</strong></li>
<li>缺点：容易因为模型搭建得过于复杂而<strong>丧失对新数据的精确预测能力（泛化能力）</strong>；树模型从上至下的预测流程会因为数据细微的更改而发生较大的结构变化，故预测<strong>稳定性较差</strong>；在有限时间内无法找到最优解（而只是次优解）</li>
</ul>
<h4 id="2-1-3-5-集成模型（回归）"><a href="#2-1-3-5-集成模型（回归）" class="headerlink" title="2.1.3.5 集成模型（回归）"></a>2.1.3.5 集成模型（回归）</h4><p>补充：极端随机森林(Extremely Randomized Trees)</p>
<ul>
<li>每构建一棵树的分裂节点时，不会任意选取特征，而是先随机选取一部分特征，然后利用信息熵(Information Gain)和基尼不纯性(Gini Impurity)等指标挑选出最佳节点特征</li>
</ul>
<p><strong>1.使用三种集成回归模型对波士顿房间训练数据进行学习，并对测试数据进行预测</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line">boston=load_boston()</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">X=boston.data</span><br><span class="line">y=boston.target</span><br><span class="line">X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=<span class="number">0.25</span>,random_state=<span class="number">33</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor,ExtraTreesRegressor,GradientBoostingRegressor</span><br><span class="line"></span><br><span class="line">rfr=RandomForestRegressor()</span><br><span class="line">rfr=rfr.fit(X_train,y_train)</span><br><span class="line">rfr_y_predict=rfr.predict(X_test)</span><br><span class="line"></span><br><span class="line">etr=ExtraTreesRegressor()</span><br><span class="line">etr=etr.fit(X_train,y_train)</span><br><span class="line">etr_y_predict=etr.predict(X_test)</span><br><span class="line"></span><br><span class="line">gbr=GradientBoostingRegressor()</span><br><span class="line">gbr=gbr.fit(X_train,y_train)</span><br><span class="line">gbr_y_predict=gbr.predict(X_test)</span><br></pre></td></tr></table></figure>
<p><strong>2.性能评估</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score,mean_squared_error,mean_absolute_error</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> <span class="string">'RFR'</span>,rfr.score(X_test,y_test),mean_squared_error(y_test,rfr_y_predict),mean_absolute_error(y_test,rfr_y_predict)</span><br><span class="line"><span class="keyword">print</span> <span class="string">'ETR'</span>,etr.score(X_test,y_test),mean_squared_error(y_test,etr_y_predict),mean_absolute_error(y_test,etr_y_predict)</span><br><span class="line"><span class="keyword">print</span> <span class="string">'GBR'</span>,gbr.score(X_test,y_test),mean_squared_error(y_test,gbr_y_predict),mean_absolute_error(y_test,gbr_y_predict)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">RFR 0.8515156020299433 11.513672440944882 2.257322834645669</span><br><span class="line">ETR 0.8003897320777417 15.478038582677165 2.416299212598424</span><br><span class="line">GBR 0.8430286082992219 12.171764921769585 2.277247326989519</span><br></pre></td></tr></table></figure>
<h2 id="2-2-无监督学习经典模型"><a href="#2-2-无监督学习经典模型" class="headerlink" title="2.2 无监督学习经典模型"></a>2.2 无监督学习经典模型</h2><p>无监督学习（Unsupervised Learning）</p>
<ul>
<li>着重发现数据本身的特点</li>
<li>无需标记数据</li>
</ul>
<h3 id="2-2-1-数据聚类"><a href="#2-2-1-数据聚类" class="headerlink" title="2.2.1 数据聚类"></a>2.2.1 数据聚类</h3><p>数据聚类：</p>
<ul>
<li>无监督学习的主流应用之一</li>
</ul>
<h4 id="2-2-1-1-K-means算法"><a href="#2-2-1-1-K-means算法" class="headerlink" title="2.2.1.1 K-means算法"></a>2.2.1.1 K-means算法</h4><p>模型介绍</p>
<ul>
<li>最经典易用的聚类模型；要求预先设定聚类个数，然后不断更新聚类中心，经过几轮迭代，最后的目标是让<strong>所有数据点到其所属聚类中心距离的平方和趋于稳定</strong></li>
</ul>
<ul>
<li>算法执行的过程分4个阶段：<ul>
<li>1.随机布设K个特征空间内的点作为初始的聚类中心；</li>
<li>2.根据每个数据的特征向量，从K个聚类中心中寻找距离最近的一个，并且把该数据标记为从属于这个聚类中心；</li>
<li>3.在所有数据都被标记过聚类中心之后，根据这些数据新分配的类簇，重新对K个聚类中心做计算；</li>
<li>4.若一轮下来，所有数据点从属的聚类中心与上一次分配的类簇没有变化，则迭代可停止，否则回到步骤2继续循环</li>
</ul>
</li>
</ul>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fo2dpg8s4cj30za0l3dmx.jpg" alt="k-means算法迭代过程"></p>
<p>数据描述</p>
<ul>
<li>手写体数字图像识别数据集</li>
<li><a href="https://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/" target="_blank" rel="noopener">Address</a></li>
</ul>
<p><strong>聚类算法的性能评估指标</strong>：</p>
<p>1.若被评估数据已被标注正确的类别，则使用2个指标：</p>
<ul>
<li><strong>ARI指标</strong>（Adjusted Rand Index）</li>
<li><strong>Accuracy</strong>（准确性，同分类问题）</li>
</ul>
<p>2.若被评估数据无所属类别，则使用<strong>轮廓系数（Silhouette Coefficient）</strong>来度量聚类结果的质量，说明：</p>
<ul>
<li>轮廓系数<strong>兼顾聚类的凝聚度（Cohesion）和分离度（Separation）</strong></li>
<li>取值范围：[-1,1]，<strong>轮廓系数值越大，则聚类效果越好</strong></li>
<li>具体计算步骤：<ul>
<li>1.对已聚类数据中第$i$个样本$x^i$ ,计算$x^i$与其同一个类簇内的所有其他样本距离的平均值，记作$a^i$ ,用于量化簇内的凝聚度；</li>
<li>2.选取$x^i$ 外的一个簇$b$，计算$x^i$与簇$b$中所有样本的平均距离，遍历所有其他簇，找到最近的这个平均距离，记作$b^i$ ，用于量化簇之间分离度；</li>
<li>3.对于样本$x^i$ ，轮廓系数为$sc^i=\frac{b^i-a^i}{max(b^i,a^i)}$ ;</li>
<li>4.最后对所有样本$X$求出平均值，即为当前聚类结果的整体轮廓系数</li>
</ul>
</li>
<li>衡量效果：<ul>
<li>若$sc^i &lt; 0$ ,则说明$x^i$ 与其簇内元素的平均距离大于最近的其他簇，表示聚类效果不好；</li>
<li>若$a^i$ 趋于0，或$b^i$ 足够大，则$sc^i $ 趋于1 ,表示聚类效果好；</li>
</ul>
</li>
</ul>
<p><strong>1.导入和查验数据</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">digits_train=pd.read_csv(<span class="string">'/Users/scarlett/repository/projects/digits/optdigits.tra'</span>,header=<span class="keyword">None</span>)</span><br><span class="line">digits_test=pd.read_csv(<span class="string">'/Users/scarlett/repository/projects/digits/optdigits.tes'</span>,header=<span class="keyword">None</span>)</span><br><span class="line"><span class="comment"># header=None 表示第一行是数据而非文件第一行</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> digits_train.shape,digits_test.shape</span><br><span class="line"><span class="keyword">print</span> digits_train.head()</span><br><span class="line"><span class="keyword">print</span> digits_train.info()</span><br></pre></td></tr></table></figure>
<p><strong>2.使用K-means算法识别手写体图像数据</strong></p>
<p>图像数据由8*8像素矩阵表示，64个像素维度；1个目标维度用来标记每个图像样本代表的数字类别</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从训练集合测试集上都分离出64维度的像素特征和1维度的数字目标</span></span><br><span class="line">X_train=digits_train[np.arange(<span class="number">64</span>)]</span><br><span class="line">y_train=digits_train[<span class="number">64</span>]</span><br><span class="line">X_test=digits_test[np.arange(<span class="number">64</span>)]</span><br><span class="line">y_test=digits_test[<span class="number">64</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化模型，并设置聚类中心数量为10</span></span><br><span class="line">kmeans=KMeans(n_clusters=<span class="number">10</span>)</span><br><span class="line">kmeans.fit(X_train)</span><br><span class="line"><span class="comment"># 逐条判断每个测试图像所属的聚类中心</span></span><br><span class="line">y_pred=kmeans.predict(X_test)</span><br></pre></td></tr></table></figure>
<p><strong>3.性能测评</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用ARI进行K-means聚类性能评估</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">print</span> metrics.adjusted_rand_score(y_test,y_pred)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用轮廓系数评估不同类簇数量的</span></span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入计算轮廓系数的模块</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> silhouette_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分割出3*2=6个子图，并在1号子图作图；subplot(m,n,p)是将多个图画到一个平面上的工具,m行n列，p=1代表从左到右从上到下的第一个位置</span></span><br><span class="line">plt.subplot(<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化原始数据点</span></span><br><span class="line">x1=np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">5</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">7</span>,<span class="number">9</span>])</span><br><span class="line">x2=np.array([<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">8</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>])</span><br><span class="line">X=np.array(zip(x1,x2)).reshape(len(x1),<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在1号子图做出原始数据点阵的分布</span></span><br><span class="line">plt.xlim([<span class="number">0</span>,<span class="number">10</span>])</span><br><span class="line">plt.ylim([<span class="number">0</span>,<span class="number">10</span>])</span><br><span class="line">plt.title(<span class="string">'Instance'</span>)</span><br><span class="line">plt.scatter(x1,x2)</span><br><span class="line"></span><br><span class="line">colors=[<span class="string">'b'</span>,<span class="string">'g'</span>,<span class="string">'r'</span>,<span class="string">'c'</span>,<span class="string">'m'</span>,<span class="string">'y'</span>,<span class="string">'k'</span>,<span class="string">'b'</span>]</span><br><span class="line">markers=[<span class="string">'o'</span>,<span class="string">'s'</span>,<span class="string">'D'</span>,<span class="string">'v'</span>,<span class="string">'^'</span>,<span class="string">'p'</span>,<span class="string">'*'</span>,<span class="string">'+'</span>]</span><br><span class="line"></span><br><span class="line">clusters=[<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">8</span>]</span><br><span class="line">subplot_counter=<span class="number">1</span></span><br><span class="line">sc_scores=[]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> clusters:</span><br><span class="line">    subplot_counter += <span class="number">1</span></span><br><span class="line">    plt.subplot(<span class="number">3</span>,<span class="number">2</span>,subplot_counter)</span><br><span class="line">    kmeans_model=KMeans(n_clusters=t).fit(X)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> i,l <span class="keyword">in</span> enumerate(kmeans_model.labels_):</span><br><span class="line">    plt.plot(x1[i],x2[i],color=colors[l],marker=markers[l],ls=<span class="string">'None'</span>)</span><br><span class="line">    </span><br><span class="line">plt.xlim([<span class="number">0</span>,<span class="number">10</span>])</span><br><span class="line">plt.ylim([<span class="number">0</span>,<span class="number">10</span>])</span><br><span class="line">sc_score=silhouette_score(X,kmeans_model.labels_,metric=<span class="string">'euclidean'</span>)</span><br><span class="line">sc_scores.append(sc_score)</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">'K=%s,silhouette coefficient=%0.03f'</span> %(t,sc_score))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(clusters,sc_scores,<span class="string">'*-'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Number of Clusters'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'silhouette coefficient score'</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>由图可知，当聚类中心数量k=3时，轮廓系数最大；由轮廓系数与不同类簇数量的关系曲线可知，聚类中心数量为3也符合数据分布特点。</p>
<p>特点分析：</p>
<ul>
<li>K-means聚类模型采取的是<strong>迭代式算法</strong></li>
<li><strong>缺点：容易收敛到局部最优解；需要预先设定簇的数量</strong></li>
</ul>
<p><strong>局部最优解</strong>：</p>
<ul>
<li>最优化：在复杂环境中遇到的许多可能的决策中，挑选“最好”的决策</li>
</ul>
<ul>
<li>局部最优：指对于一个问题的解在一定范围或区域内最优，或者说解决问题或达成目标的手段在一定范围或限制内最优（和全局最优不同，局部最优不要求在所有决策中是最好的）</li>
<li>全局最优：针对一定条件/环境下的一个问题/目标，若一项决策和<strong>所有</strong>解决该问题的决策相比是最优的，就可以被称为全局最优</li>
<li>如下图：左边是实际数据和正确的所属类簇；右下的局部最优情况导致无法继续更新聚类中心，使聚类结果与正确结果相差很大</li>
<li>“容易收敛到局部最优解”是算法自身的缺陷，但<strong>可通过执行多次kmeans算法来挑选性能最好的初始中心点</strong></li>
</ul>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fo2hc6x7ukj30ws0h5n13.jpg" alt="K-means局部最优与全局最优比较"></p>
<p><strong>肘部观察法</strong>：</p>
<ul>
<li><p>作用：<strong>粗略估计相对合理的类簇个数</strong></p>
</li>
<li><p>思路：因为K-means模型最终期望所有数据点到其所属的类簇举例的平方和趋于稳定，所以我们可以通过观察这个数值随K的走势来找出最佳的类簇数量；理想条件下，这个折线在不断下降且趋于平缓的过程中会有斜率的拐点，即从这个拐点对应的K值开始，类簇中心的增加不会过于破坏数据聚类的结构（<strong>进一步增加K值不会再有利于算法的收敛</strong>），则<strong>此拐点K=n是相对最佳的类簇数量</strong>。</p>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fo2hc7jt5oj30ow0i3wgf.jpg" alt="3个簇"></p>
</li>
</ul>
<p><img src="https://ws4.sinaimg.cn/large/006tNc79gy1fo2hafjr0xj30d00a0jsa.jpg" alt="肘部平均距离与类簇数量的关系"></p>
<p><strong>肘部观察法示例</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"><span class="keyword">from</span> scipy.spatial.distance <span class="keyword">import</span> cdist</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用均匀分布函数随机三个簇，每个簇周围10个数据样本</span></span><br><span class="line">cluster1=np.random.uniform(<span class="number">0.5</span>,<span class="number">1.5</span>,(<span class="number">2</span>,<span class="number">10</span>))</span><br><span class="line">cluster2=np.random.uniform(<span class="number">5.5</span>,<span class="number">6.5</span>,(<span class="number">2</span>,<span class="number">10</span>))</span><br><span class="line">cluster3=np.random.uniform(<span class="number">10.5</span>,<span class="number">11.5</span>,(<span class="number">2</span>,<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制30个数据样本的分布图像</span></span><br><span class="line">X=np.hstack((cluster1,cluster2,cluster3)).T</span><br><span class="line">plt.scatter(X[:,<span class="number">0</span>],X[:,<span class="number">1</span>])</span><br><span class="line">plt.xlabel(<span class="string">'x1'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'x2'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试9种不同聚类中心数量下，每种情况的聚类质量</span></span><br><span class="line">K=range(<span class="number">1</span>,<span class="number">10</span>)</span><br><span class="line">meandistortions=[]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> K:</span><br><span class="line">    kmeans=KMeans(n_clusters=k)</span><br><span class="line">    kmeans.fit()</span><br><span class="line">    meandistortions.append(sum(np.min(cdist(X,kmeans.cluster_centers_,<span class="string">'euclidean'</span>),axis=<span class="number">1</span>))/X.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">plt.plot(K,meandistortions,<span class="string">'bx-'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'k'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Average Dispersion'</span>)</span><br><span class="line">plt.title(<span class="string">'Selecting k with the Elbow Method'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="2-2-2-特征降维"><a href="#2-2-2-特征降维" class="headerlink" title="2.2.2 特征降维"></a>2.2.2 特征降维</h3><p>特征降维</p>
<ul>
<li>特征维度过高，无法构建有效特征；无法肉眼观测超过三个维度的特征</li>
</ul>
<ul>
<li><p>重构有效的低维特征向量，为数据拓展提供可能</p>
<p>​</p>
</li>
</ul>
<h4 id="2-2-2-1-主成分分析-Principle-Component-Analysis"><a href="#2-2-2-1-主成分分析-Principle-Component-Analysis" class="headerlink" title="2.2.2.1 主成分分析(Principle Component Analysis)"></a>2.2.2.1 主成分分析(Principle Component Analysis)</h4><p>模型介绍</p>
<ul>
<li><strong>最经典使用的特征降维技术；辅助图像识别</strong></li>
<li>举例：若我们有一组2*2的数据[(1,2),(2,4)]，假设这两个数据都反映到一个类别或类簇；若我们的学习模型是线性模型，则这两个模型只能帮助权重参数更新1次，因为他们线性相关，所有特征值只是扩张了相同背书；若使用PCA分析，则此矩阵的“秩”=1，即在多样性程度上，此矩阵只有1个自由度。</li>
<li>可把PCA当做特征选择，<strong>这种特征选择是先把原来的特征空间作了映射，使得新的映射后特征空间数据彼此正交；则我们通过主成分分析就尽可能保留下具备区分性的低维数据特征</strong>。</li>
</ul>
<blockquote>
<p>矩阵的秩：一个矩阵A的列秩是A的<strong>线性独立的纵列的极大数目</strong>，通常表示为r(<em>A</em>)或rank <em>A</em>。</p>
<p>自由度：统计学上，指当以样本的统计量来估计总体的参数时，<strong>样本中独立或能自由变化的数据的个数</strong>；数学上，<strong>自由度是一个随机向量的维度数，即一个向量能被完整描述所需的最少单位向量数</strong>。如从电脑屏幕到厨房的位移能够用三维向量$\widehat{ai}+\widehat{bj}+\widehat{ck}$来描述，因此这个位移向量的自由度是3。自由度也通常与这些向量的座标平方和，以及卡方分布中的参数有所关联。</p>
</blockquote>
<p><strong>求线性相关矩阵的秩</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line">test = np.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">2</span>,<span class="number">4</span>]])</span><br><span class="line"><span class="keyword">print</span> np.linalg.matrix_rank(test,tol=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1</span><br></pre></td></tr></table></figure>
<p>数据描述</p>
<ul>
<li>数据集：digits</li>
<li>展示经PCA处理后，这些数字图像映射在二维空间的分布情况；结果会发现把64维度的图像压缩到2维空间后，依然可发现绝大多数数字之间的区分性</li>
</ul>
<p><strong>1.显示手写体数字图片经PCA压缩后的二维空间分布</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line">digits_train=pd.read_csv(<span class="string">'/Users/scarlett/repository/projects/digits/optdigits.tra'</span>,header=<span class="keyword">None</span>)</span><br><span class="line">digits_test=pd.read_csv(<span class="string">'/Users/scarlett/repository/projects/digits/optdigits.tes'</span>,header=<span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分割训练数据的特征向量和标记，前64维是feature vector，第65维是标记</span></span><br><span class="line">X_digits=digits_train[np.arange(<span class="number">64</span>)]</span><br><span class="line">y_digits=digits_train[<span class="number">64</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入PCA</span></span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="comment"># 初始化一个可将高维向量压缩到二维的PCA</span></span><br><span class="line">estimator=PCA(n_components=<span class="number">2</span>)</span><br><span class="line">X_pca=estimator.fit_transform(X_digits)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示10类图像经PCA压缩后的二维空间分布</span></span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_pca_scatter</span><span class="params">()</span>:</span></span><br><span class="line">    </span><br><span class="line">    colors=[<span class="string">'black'</span>,<span class="string">'blue'</span>,<span class="string">'purple'</span>,<span class="string">'yellow'</span>,<span class="string">'white'</span>,<span class="string">'red'</span>,<span class="string">'lime'</span>,<span class="string">'cyan'</span>,<span class="string">'orange'</span>,<span class="string">'gray'</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> xrange(len(colors)):</span><br><span class="line">        px=X_pca[:,<span class="number">0</span>][y_digits.as_matrix()==i]</span><br><span class="line">        py=X_pca[:,<span class="number">1</span>][y_digits.as_matrix()==i]</span><br><span class="line">        plt.scatter(px,py,c=colors[i])</span><br><span class="line">    </span><br><span class="line">    plt.legend(np.arange(<span class="number">0</span>,<span class="number">10</span>).astype(str))</span><br><span class="line">    plt.xlabel(<span class="string">'First Principle Component'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'Second Principle Component'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    plt_pca_scatter()</span><br></pre></td></tr></table></figure>
<p><strong>2.使用原始像素特征和经PCA压缩重建的低维特征，在相同配置的SVM上分别进行图像识别</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">X_train=digits_train[np.arange(<span class="number">64</span>)]</span><br><span class="line">y_train=digits_train[<span class="number">64</span>]</span><br><span class="line">X_test=digits_test[np.arange(<span class="number">64</span>)]</span><br><span class="line">y_test=digits_test[<span class="number">64</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入基于线性核的SVM分类器,建模，预测</span></span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> LinearSVC</span><br><span class="line">svc=LinearSVC()</span><br><span class="line">svc.fit(X_train,y_train)</span><br><span class="line">y_predict=svc.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 特征压缩到20维,并转化原训练特征</span></span><br><span class="line">estimator=PCA(n_components=<span class="number">20</span>)</span><br><span class="line">pca_X_train=estimator.fit_transform(X_train)</span><br><span class="line">pca_X_test=estimator.transform(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对压缩后的20维特征的训练数据进行建模，并对测试集预测</span></span><br><span class="line">pca_svc=LinearSVC()</span><br><span class="line">pca_svc.fit(pca_X_train,y_train)</span><br><span class="line">pca_y_predict=pca_svc.predict(pca_X_test)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> pca_y_predict,y_predict</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[0 1 1 ... 8 9 8] [0 1 2 ... 8 9 8]</span><br></pre></td></tr></table></figure>
<p><strong>3.性能评估</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> svc.score(X_test,y_test)</span><br><span class="line"><span class="keyword">print</span> classification_report(y_test,y_predict,target_names=np.arange(<span class="number">10</span>).astype(str))</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> pca_svc.score(pca_X_test,y_test)</span><br><span class="line"><span class="keyword">print</span> classification_report(y_test,pca_y_predict,target_names=np.arange(<span class="number">10</span>).astype(str))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">0.9309961046188091</span><br><span class="line">             precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">          0       0.98      0.98      0.98       178</span><br><span class="line">          1       0.85      0.95      0.90       182</span><br><span class="line">          2       0.99      0.97      0.98       177</span><br><span class="line">          3       0.92      0.95      0.93       183</span><br><span class="line">          4       0.95      0.97      0.96       181</span><br><span class="line">          5       0.90      0.96      0.93       182</span><br><span class="line">          6       0.99      0.98      0.99       181</span><br><span class="line">          7       0.98      0.91      0.94       179</span><br><span class="line">          8       0.96      0.74      0.83       174</span><br><span class="line">          9       0.82      0.91      0.86       180</span><br><span class="line"></span><br><span class="line">avg / total       0.93      0.93      0.93      1797</span><br><span class="line"></span><br><span class="line">0.9081803005008348</span><br><span class="line">             precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">          0       0.97      0.96      0.96       178</span><br><span class="line">          1       0.80      0.91      0.85       182</span><br><span class="line">          2       0.96      0.94      0.95       177</span><br><span class="line">          3       0.96      0.91      0.94       183</span><br><span class="line">          4       0.94      0.96      0.95       181</span><br><span class="line">          5       0.86      0.97      0.91       182</span><br><span class="line">          6       0.98      0.96      0.97       181</span><br><span class="line">          7       0.96      0.88      0.92       179</span><br><span class="line">          8       0.82      0.83      0.83       174</span><br><span class="line">          9       0.87      0.75      0.80       180</span><br><span class="line"></span><br><span class="line">avg / total       0.91      0.91      0.91      1797</span><br></pre></td></tr></table></figure>
<p>由上发现，经过PCA处理后会损失2%左右的预测准确性，但相比原始数据64维度的特征，使用PCA可降低68.75%的维度、</p>
<p>特点分析：</p>
<ul>
<li><strong>降维/压缩是选取数据具有代表性的特征，在保持数据多样性(Variance)的基础上，规避掉大量的特征冗余和噪声；并可节省模型训练时间，提高综合效率</strong></li>
<li>但<strong>容易损失一些有用的模式信息</strong></li>
</ul>
<h1 id="3-进阶篇"><a href="#3-进阶篇" class="headerlink" title="3.进阶篇"></a>3.进阶篇</h1><p>前一节使用的数据集都是经过规范化处理的的规整数据集，使用的模型也都是默认配置，但现实生活中我们得到的数据集不会如此规整，默认配置也不一定最佳。</p>
<p>本章目的：掌握如何通过<strong>抽取或筛选数据特征、优化模型配置</strong>，以进一步提升经典模型的性能表现。</p>
<h2 id="3-1-模型实用技巧"><a href="#3-1-模型实用技巧" class="headerlink" title="3.1 模型实用技巧"></a>3.1 模型实用技巧</h2><p>依靠默认配置学习到模型所需的参数，不能保证：</p>
<ul>
<li>所有用于训练的数据特征都是最好的</li>
<li>学习到的参数一定是最优的</li>
<li>默认配置下的模型总是最佳的</li>
</ul>
<p>本节技巧：<strong>预处理数据，控制参数训练、优化模型配置</strong>,etc</p>
<h3 id="3-1-1-特征提升"><a href="#3-1-1-特征提升" class="headerlink" title="3.1.1 特征提升"></a>3.1.1 特征提升</h3><p><strong>特征抽取</strong>：<strong>逐条将原始数据转化为特征向量</strong>的形式，这个过程同时涉及到<strong>对数据特征的量化表示</strong>；</p>
<p><strong>特征筛选</strong>：(更进一步)<strong>在高维度、已量化的特征向量中选择对指定任务更有效的特征组合</strong>，进一步提升模型性能</p>
<h4 id="3-1-1-1-特征抽取"><a href="#3-1-1-1-特征抽取" class="headerlink" title="3.1.1.1 特征抽取"></a>3.1.1.1 特征抽取</h4><p>原始数据的种类有很多：数字化的信号数据(声纹、图像)，符号化的文本；而我们无法直接将符号化的文本用于计算，而需要通过某些处理手段预先将文本良华为特征向量。</p>
<p><strong>1.DictVectorizer对使用字典存储的数据进行特征抽取和向量化</strong></p>
<blockquote>
<p>有些符号化的数据特征已相对结构化，并以字典这种数据结构进行存储，故可使用DictVectorizer对特征进行抽取和向量化。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">M=[&#123;<span class="string">'city'</span>:<span class="string">'Dubai'</span>,<span class="string">'temperature'</span>:<span class="number">33.</span>&#125;,&#123;<span class="string">'city'</span>:<span class="string">'London'</span>,<span class="string">'temperature'</span>:<span class="number">12.</span>&#125;,&#123;<span class="string">'city'</span>:<span class="string">'Beijing'</span>,<span class="string">'temperature'</span>:<span class="number">40.</span>&#125;]</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> DictVectorizer</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化特征抽取器</span></span><br><span class="line">vec=DictVectorizer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出转化后的特征矩阵</span></span><br><span class="line"><span class="keyword">print</span> vec.fit_transform(M).toarray()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出各维度特征的含义</span></span><br><span class="line"><span class="keyword">print</span> vec.get_feature_names()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[[ 0.  1.  0. 33.]</span><br><span class="line"> [ 0.  0.  1. 12.]</span><br><span class="line"> [ 1.  0.  0. 40.]]</span><br><span class="line">[&apos;city=Beijing&apos;, &apos;city=Dubai&apos;, &apos;city=London&apos;, &apos;temperature&apos;]</span><br></pre></td></tr></table></figure>
<p>由输出可知，特征向量化过程中。DictVectorizer对类别型和数值型特征的处理方式不同。</p>
<ul>
<li>类别型(categorical)特征：借助原特征名称组合产生新特征，并用0/1二值方式进行量化</li>
<li>数值型(numerical)：维持原始特征值</li>
</ul>
<p><strong>2.使用CountVectorizer且在不去掉停用词的条件下，对文本特征进行量化的朴素贝叶斯分类性能测试</strong></p>
<p>处理文本数据的方法：<strong>词袋法(Bag of Words)</strong></p>
<ul>
<li><strong>词袋法</strong>：不考虑词语出现的顺序，只将训练文本中的每个出现过的词汇单独视作一列特征；词表：不重复的词汇的集合；每条训练文本都可在高维度词表上映射出一个特征向量；</li>
<li>特征数值的常见计算方式：CountVectorizer &amp; TfidfVectorizer</li>
<li><strong>CountVectorizer</strong>：只考虑每种词汇(Term)在该条训练文本中出现的频率(Term Frequency)</li>
<li><strong>TfidfVectorizer</strong>：既考量某一次会在当前文本中出现的频率，又考虑包含这个词汇的文本条数的倒数(Inverse Document Frequency),即训练的条目越多，TfidfVectorizer的特征量化就越有优势；可剔除在每条文本中都出现的常用词汇，以减少它们对模型分类决策的影响</li>
<li><strong>停用词</strong>(Stop Words)：在每条文本中都出现的常用词汇，如the,a；停用词常在特征抽取中以黑名单的方式过滤掉，以提高模型的性能表现</li>
<li>区别：<strong>CountVectorizer只统计词频，而TfidfVectorizer还过滤掉了停用词</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_20newsgroups</span><br><span class="line">news=fetch_20newsgroups(subset=<span class="string">'all'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train,y_train,X_test,y_test=train_test_split(news.data,news.target,test_size=<span class="number">0.25</span>,random_state=<span class="number">33</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line">count_vec=CountVectorizer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 只使用词频统计将原始训练和测试文本转化为特征向量</span></span><br><span class="line">X_count_train=count_vec.fit_transform(X_train)</span><br><span class="line">X_count_test=count_vec.transform(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入naive bayes,默认配置初始化，使用CountVectorizer(未剔除停用词的)后的训练样本进行学习</span></span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</span><br><span class="line">mnb_count=MultinomialNB()</span><br><span class="line">mnb_count.fit(X_count_train,y_train)</span><br><span class="line">y_count_predict=mnb_count.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出性能评估结果</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"><span class="keyword">print</span> <span class="string">'Accuracy:'</span>,mnb_count.score(X_count_train,y_train)</span><br><span class="line"><span class="keyword">print</span> classification_report(y_test,y_count_predict,target_names=news.target_names)</span><br></pre></td></tr></table></figure>
<p>output：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">The Accuracy of NBC is: 0.8397707979626485</span><br><span class="line">                          precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">             alt.atheism       0.86      0.86      0.86       201</span><br><span class="line">           comp.graphics       0.59      0.86      0.70       250</span><br><span class="line"> comp.os.ms-windows.misc       0.89      0.10      0.17       248</span><br><span class="line">comp.sys.ibm.pc.hardware       0.60      0.88      0.72       240</span><br><span class="line">   comp.sys.mac.hardware       0.93      0.78      0.85       242</span><br><span class="line">          comp.windows.x       0.82      0.84      0.83       263</span><br><span class="line">            misc.forsale       0.91      0.70      0.79       257</span><br><span class="line">               rec.autos       0.89      0.89      0.89       238</span><br><span class="line">         rec.motorcycles       0.98      0.92      0.95       276</span><br><span class="line">      rec.sport.baseball       0.98      0.91      0.95       251</span><br><span class="line">        rec.sport.hockey       0.93      0.99      0.96       233</span><br><span class="line">               sci.crypt       0.86      0.98      0.91       238</span><br><span class="line">         sci.electronics       0.85      0.88      0.86       249</span><br><span class="line">                 sci.med       0.92      0.94      0.93       245</span><br><span class="line">               sci.space       0.89      0.96      0.92       221</span><br><span class="line">  soc.religion.christian       0.78      0.96      0.86       232</span><br><span class="line">      talk.politics.guns       0.88      0.96      0.92       251</span><br><span class="line">   talk.politics.mideast       0.90      0.98      0.94       231</span><br><span class="line">      talk.politics.misc       0.79      0.89      0.84       188</span><br><span class="line">      talk.religion.misc       0.93      0.44      0.60       158</span><br><span class="line"></span><br><span class="line">             avg / total       0.86      0.84      0.82      4712</span><br></pre></td></tr></table></figure>
<p><strong>3.使用TfidfVectorizer且在不去掉停用词的条件下，对文本特征进行量化的朴素贝叶斯分类性能测试</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_20newsgroups</span><br><span class="line"></span><br><span class="line"><span class="comment"># 即时从网上下载数据</span></span><br><span class="line">news=fetch_20newsgroups(subset=<span class="string">'all'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> len(news.data)</span><br><span class="line"><span class="keyword">print</span> news.data[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train,X_test,y_train,y_test=train_test_split(news.data,news.target,test_size=<span class="number">0.25</span>,random_state=<span class="number">33</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line">tfidf_vec=TfidfVectorizer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转化为特征向量</span></span><br><span class="line">X_tfidf_train=tfidf_vec.fit_transform(X_train)</span><br><span class="line">X_tfidf_test=tfidf_vec.transform(X_test)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</span><br><span class="line">mnb_tfidf=MultinomialNB()</span><br><span class="line">mnb_tfidf.fit(X_tfidf_train,y_train)</span><br><span class="line">y_tfidf_predict=mnb_tfidf.predict(X_tfidf_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 性能评估</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'Accuracy:'</span>,mnb_tfidf.score(X_tfidf_test,y_test)</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"><span class="keyword">print</span> classification_report(y_test,y_tfidf_predict,target_names=news.target_names)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">Accuracy: 0.8463497453310697</span><br><span class="line">                          precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">             alt.atheism       0.84      0.67      0.75       201</span><br><span class="line">           comp.graphics       0.85      0.74      0.79       250</span><br><span class="line"> comp.os.ms-windows.misc       0.82      0.85      0.83       248</span><br><span class="line">comp.sys.ibm.pc.hardware       0.76      0.88      0.82       240</span><br><span class="line">   comp.sys.mac.hardware       0.94      0.84      0.89       242</span><br><span class="line">          comp.windows.x       0.96      0.84      0.89       263</span><br><span class="line">            misc.forsale       0.93      0.69      0.79       257</span><br><span class="line">               rec.autos       0.84      0.92      0.88       238</span><br><span class="line">         rec.motorcycles       0.98      0.92      0.95       276</span><br><span class="line">      rec.sport.baseball       0.96      0.91      0.94       251</span><br><span class="line">        rec.sport.hockey       0.88      0.99      0.93       233</span><br><span class="line">               sci.crypt       0.73      0.98      0.83       238</span><br><span class="line">         sci.electronics       0.91      0.83      0.87       249</span><br><span class="line">                 sci.med       0.97      0.92      0.95       245</span><br><span class="line">               sci.space       0.89      0.96      0.93       221</span><br><span class="line">  soc.religion.christian       0.51      0.97      0.67       232</span><br><span class="line">      talk.politics.guns       0.83      0.96      0.89       251</span><br><span class="line">   talk.politics.mideast       0.92      0.97      0.95       231</span><br><span class="line">      talk.politics.misc       0.98      0.62      0.76       188</span><br><span class="line">      talk.religion.misc       0.93      0.16      0.28       158</span><br><span class="line"></span><br><span class="line">             avg / total       0.87      0.85      0.84      4712</span><br></pre></td></tr></table></figure>
<p>由输出可知，在使用TfidfVectorizer而不去掉停用词的条件下，对训练和测试文本进行特征量化，并利用默认配置的naive bayes，在测试文本上可得到比CountVectorizer更高的预测准确性。证明：<strong>在训练文本量较多时，使用TfidfVectorizer压制常用词汇对分类决策的干扰，可提升模型性能</strong>。</p>
<p><strong>4.分别使用CountVectorizer和TfidfVectorizer，并在去掉停用词的条件下，对文本特征进行量化的Naive Bayes分类性能测试</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 分别使用停用词过滤器配置初始化CountVectorizer和TfidfVectorizer</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line">count_filter_vec,tfidf_filter_vec=CountVectorizer(analyzer=<span class="string">'word'</span>,stop_words=<span class="string">'english'</span>),TfidfVectorizer(analyzer=<span class="string">'word'</span>,stop_words=<span class="string">'english'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用带停用词过滤的CountVectorizer对训练和测试文本进行量化处理</span></span><br><span class="line">X_count_filter_train=count_filter_vec.fit_transform(X_train)</span><br><span class="line">X_count_filter_test=count_filter_vec.transform(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用带停用词过滤的TfidfVectorizer对训练和测试文本进行量化处理</span></span><br><span class="line">X_tfidf_filter_train=tfidf_filter_vec.fit_transform(X_train)</span><br><span class="line">X_tfidf_filter_test=tfidf_filter_vec.transform(X_test)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化默认配置的朴素贝叶斯，并对CountVectorizer后的数据进行预测和性能评估</span></span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</span><br><span class="line">mnb_count_filter=MultinomialNB()</span><br><span class="line">mnb_count_filter.fit(X_count_filter_train,y_train)</span><br><span class="line">y_count_predict=mnb_count_filter.predict(X_count_filter_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化另一个默认配置的朴素贝叶斯，并对TfidfVectorizer后的数据进行预测和性能评估</span></span><br><span class="line">mnb_tfidf_filter=MultinomialNB()</span><br><span class="line">mnb_tfidf_filter.fit(X_tfidf_filter_train,y_train)</span><br><span class="line">y_tfidf_predict=mnb_tfidf_filter.predict(X_tfidf_filter_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># CountVectorizer性能评估</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"><span class="keyword">print</span> <span class="string">'Count_Accuracy'</span>,mnb_count_filter.score(X_count_filter_train,y_train)</span><br><span class="line"><span class="keyword">print</span> classification_report(y_test,y_count_predict,target_names=news.target_names)</span><br><span class="line"></span><br><span class="line"><span class="comment"># TfidfVectorizer性能评估</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'Tfidf_Accuracy'</span>,mnb_tfidf_filter.score(X_tfidf_filter_train,y_train)</span><br><span class="line"><span class="keyword">print</span> classification_report(y_test,y_tfidf_predict,target_names=news.target_names)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">Count_Accuracy 0.9439649073156926</span><br><span class="line">                          precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">             alt.atheism       0.85      0.89      0.87       201</span><br><span class="line">           comp.graphics       0.62      0.88      0.73       250</span><br><span class="line"> comp.os.ms-windows.misc       0.93      0.22      0.36       248</span><br><span class="line">comp.sys.ibm.pc.hardware       0.62      0.88      0.73       240</span><br><span class="line">   comp.sys.mac.hardware       0.93      0.85      0.89       242</span><br><span class="line">          comp.windows.x       0.82      0.85      0.84       263</span><br><span class="line">            misc.forsale       0.90      0.79      0.84       257</span><br><span class="line">               rec.autos       0.91      0.91      0.91       238</span><br><span class="line">         rec.motorcycles       0.98      0.94      0.96       276</span><br><span class="line">      rec.sport.baseball       0.98      0.92      0.95       251</span><br><span class="line">        rec.sport.hockey       0.92      0.99      0.95       233</span><br><span class="line">               sci.crypt       0.91      0.97      0.93       238</span><br><span class="line">         sci.electronics       0.87      0.89      0.88       249</span><br><span class="line">                 sci.med       0.94      0.95      0.95       245</span><br><span class="line">               sci.space       0.91      0.96      0.93       221</span><br><span class="line">  soc.religion.christian       0.87      0.94      0.90       232</span><br><span class="line">      talk.politics.guns       0.89      0.96      0.93       251</span><br><span class="line">   talk.politics.mideast       0.95      0.98      0.97       231</span><br><span class="line">      talk.politics.misc       0.84      0.90      0.87       188</span><br><span class="line">      talk.religion.misc       0.91      0.53      0.67       158</span><br><span class="line"></span><br><span class="line">             avg / total       0.88      0.86      0.85      4712</span><br><span class="line"></span><br><span class="line">Tfidf_Accuracy 0.9479977359558511</span><br><span class="line">                          precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">             alt.atheism       0.86      0.81      0.83       201</span><br><span class="line">           comp.graphics       0.85      0.81      0.83       250</span><br><span class="line"> comp.os.ms-windows.misc       0.84      0.87      0.86       248</span><br><span class="line">comp.sys.ibm.pc.hardware       0.78      0.88      0.83       240</span><br><span class="line">   comp.sys.mac.hardware       0.92      0.90      0.91       242</span><br><span class="line">          comp.windows.x       0.95      0.88      0.91       263</span><br><span class="line">            misc.forsale       0.90      0.80      0.85       257</span><br><span class="line">               rec.autos       0.89      0.92      0.90       238</span><br><span class="line">         rec.motorcycles       0.98      0.94      0.96       276</span><br><span class="line">      rec.sport.baseball       0.97      0.93      0.95       251</span><br><span class="line">        rec.sport.hockey       0.88      0.99      0.93       233</span><br><span class="line">               sci.crypt       0.85      0.98      0.91       238</span><br><span class="line">         sci.electronics       0.93      0.86      0.89       249</span><br><span class="line">                 sci.med       0.96      0.93      0.95       245</span><br><span class="line">               sci.space       0.90      0.97      0.93       221</span><br><span class="line">  soc.religion.christian       0.70      0.96      0.81       232</span><br><span class="line">      talk.politics.guns       0.84      0.98      0.90       251</span><br><span class="line">   talk.politics.mideast       0.92      0.99      0.95       231</span><br><span class="line">      talk.politics.misc       0.97      0.74      0.84       188</span><br><span class="line">      talk.religion.misc       0.96      0.29      0.45       158</span><br><span class="line"></span><br><span class="line">             avg / total       0.89      0.88      0.88      4712</span><br></pre></td></tr></table></figure>
<p>由输出可知，<strong>TfidfVectorizer的特征抽取和量化方法更具备优势，对停用词进行过滤后的模型性能比未过滤高3%—4%</strong>。</p>
<h4 id="3-1-1-2-特征筛选"><a href="#3-1-1-2-特征筛选" class="headerlink" title="3.1.1.2 特征筛选"></a>3.1.1.2 特征筛选</h4><p>良好的数据特征组合可提高模型性能，冗余特征会浪费CPU计算资源，不良特征会降低模型精度。</p>
<p>主成分分析(PCA)：用于去除线性相关的特征组合</p>
<p>特征筛选：不是修改特征值，而是寻找对模型性能提升大的少量特征</p>
<p><strong>使用Titanic数据集，通过特征筛选法一步步提升决策树的预测性能</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入数据</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">titanic=pd.read_csv(<span class="string">'/Users/scarlett/repository/projects/titanic/titanic.csv'</span>)</span><br><span class="line"><span class="keyword">print</span> titanic.shape</span><br><span class="line"><span class="keyword">print</span> titanic.info()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分离数据特征与预测目标</span></span><br><span class="line">y=titanic[<span class="string">'survived'</span>]</span><br><span class="line">X=titanic.drop([<span class="string">'row.names'</span>,<span class="string">'name'</span>,<span class="string">'survived'</span>],axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 填充缺失数据</span></span><br><span class="line">X[<span class="string">'age'</span>].fillna(X[<span class="string">'age'</span>].mean(),inplace=<span class="keyword">True</span>)</span><br><span class="line">X.fillna(<span class="string">'UNKNOWN'</span>,inplace=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分割数据</span></span><br><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train,y_train,X_test,y_test=train_test_split(X,y,test_size=<span class="number">0.25</span>,random_state=<span class="number">33</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">(1313, 11)</span><br><span class="line">&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;</span><br><span class="line">RangeIndex: 1313 entries, 0 to 1312</span><br><span class="line">Data columns (total 11 columns):</span><br><span class="line">row.names    1313 non-null int64</span><br><span class="line">pclass       1313 non-null object</span><br><span class="line">survived     1313 non-null int64</span><br><span class="line">name         1313 non-null object</span><br><span class="line">age          633 non-null float64</span><br><span class="line">embarked     821 non-null object</span><br><span class="line">home.dest    754 non-null object</span><br><span class="line">room         77 non-null object</span><br><span class="line">ticket       69 non-null object</span><br><span class="line">boat         347 non-null object</span><br><span class="line">sex          1313 non-null object</span><br><span class="line">dtypes: float64(1), int64(2), object(8)</span><br><span class="line">memory usage: 112.9+ KB</span><br><span class="line">None</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 类别型特征向量化</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> DictVectorizer</span><br><span class="line">vec=DictVectorizer()</span><br><span class="line">X_train=vec.fit_transform(X_train.to_dict(orient=<span class="string">'record'</span>))</span><br><span class="line">X_test=vec.transform(X_test.to_dict(orient=<span class="string">'record'</span>))</span><br><span class="line"><span class="keyword">print</span> len(vec.feature_names_)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line">dt=DecisionTreeClassifier()</span><br><span class="line">dt.fit(X_train,y_train)</span><br><span class="line">dt.score(X_test,y_test)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入特征筛选器</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> feature_selection</span><br><span class="line">fs=feature_selection.SelectPercentile(feature_selection.chi2,percentile=<span class="number">20</span>)</span><br><span class="line">X_train_fs=fs.fit_transform(X_train,y_train)</span><br><span class="line">dt.fit(X_train_fs,y_train)</span><br><span class="line">X_test_fs=fs.transform(X_test)</span><br><span class="line">dt.score(X_test_fs,y_test)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过交叉验证法，按照固定间隔的百分比筛选特征，并作图展示性能岁特征筛选比例的变化</span></span><br><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">percentile=range(<span class="number">1</span>,<span class="number">100</span>,<span class="number">2</span>)</span><br><span class="line">results=[]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> percentile:</span><br><span class="line">    fs=feature_selection.SelectPercentile(feature_selection.chi2,percentile=i)</span><br><span class="line">    X_train_fs=fs.fit_transform(X_train,y_train)</span><br><span class="line">    scores=cross_val_score(dt,X_train_fs,y_train,cv=<span class="number">5</span>)</span><br><span class="line">    results=np.append(results,scores.mean())</span><br><span class="line">    <span class="keyword">print</span> results</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 找到提现最佳性能的特征筛选的百分比</span></span><br><span class="line">opt=np.where(results==results.max())[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">print</span> <span class="string">'Optimal number of features %d'</span>%percentiles[opt]</span><br></pre></td></tr></table></figure>
<h3 id="3-1-2-模型正则化"><a href="#3-1-2-模型正则化" class="headerlink" title="3.1.2 模型正则化"></a>3.1.2 模型正则化</h3><p>任何机器学习模型在训练集上的性能表现都不能作为其对未知测试数据预测能力的评估。</p>
<p>本节：<strong>模型泛化力</strong>(Generalization)，和如何保证模型泛化力</p>
<h4 id="3-1-2-1-欠拟合和过拟合-Underfitting-amp-Overfitting"><a href="#3-1-2-1-欠拟合和过拟合-Underfitting-amp-Overfitting" class="headerlink" title="3.1.2.1 欠拟合和过拟合(Underfitting &amp; Overfitting)"></a>3.1.2.1 欠拟合和过拟合(Underfitting &amp; Overfitting)</h4><p><strong>拟合</strong>：机器学习模型在训练过程中，<strong>通过更新参数，使模型不断契合可观测数据(训练集)的过程</strong>。</p>
<p>阐述：模型复杂度与泛化力的关系</p>
<p>数据描述</p>
<ul>
<li>披萨饼价格预测</li>
<li>每种直径(Diameter)对应一个报价</li>
<li>需要设计一个学习模型，可根据披萨的直径特征来预测售价</li>
</ul>
<p><img src="https://ws3.sinaimg.cn/large/006tKfTcgy1fo4s6o0j24j30ic0aygnn.jpg" alt="biao"></p>
<p>由上表：5组训练数据，4组测试数据且报价未知；只考虑直径与售价的关系，则适合用线性回归模型。</p>
<p><strong>1.使用线性回归模型在披萨训练样本上进行拟合</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">X_train=[[<span class="number">6</span>],[<span class="number">8</span>],[<span class="number">10</span>],[<span class="number">14</span>],[<span class="number">18</span>]]</span><br><span class="line">y_train=[[<span class="number">7</span>],[<span class="number">9</span>],[<span class="number">13</span>],[<span class="number">17.5</span>],[<span class="number">18</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入线性回归模型</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">regressor=LinearRegression()</span><br><span class="line">regressor.fit(X_train,y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入numpy</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 在x轴上从0-25均匀采样100个数据点,并以100个数据点为基准，预测回归直线</span></span><br><span class="line">xx=np.linspace(<span class="number">0</span>,<span class="number">26</span>,<span class="number">100</span>)</span><br><span class="line">xx=xx.reshape(xx.shape[<span class="number">0</span>],<span class="number">1</span>)</span><br><span class="line">yy=regressor.predict(xx)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对预测到的直线作图,</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.scatter(X_train,y_train)</span><br><span class="line"><span class="comment"># 使用plt.plot()画(x,y)曲线,degree=1表示特征是一维的，做个标记</span></span><br><span class="line">plt1,=plt.plot(xx,yy,label=<span class="string">"Degree=1"</span>)</span><br><span class="line">plt.axis([<span class="number">0</span>,<span class="number">25</span>,<span class="number">0</span>,<span class="number">25</span>]) <span class="comment"># axis表示坐标的极值范围</span></span><br><span class="line">plt.xlabel(<span class="string">'Diameter of pizza'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Price'</span>) </span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出模型在训练样本上的R-squared值</span></span><br><span class="line"><span class="keyword">print</span> regressor.score(X_train,y_train)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.9100015964240102</span><br></pre></td></tr></table></figure>
<p>接下来我们尝试将原特征提高一个维度，用2次多项式回归来拟合训练样本</p>
<p><strong>2.使用2次多项式回归模型在训练样本上进行拟合</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line"><span class="comment"># 使用PolynomialFeatures(degree=2)映射出2次多项式特征</span></span><br><span class="line">poly2=PolynomialFeatures(degree=<span class="number">2</span>)</span><br><span class="line">X_train_poly2=poly2.fit_transform(X_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以线性回归模型为基础，初始化模型（特征维度提升，但模型仍是线性回归模型）</span></span><br><span class="line">regressor_poly2=LinearRegression()</span><br><span class="line">regressor_poly2.fit(X_train_poly2,y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从新映射绘图用x轴采样数据</span></span><br><span class="line">xx_poly2=poly2.transform(xx)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">yy_poly2=regressor_poly2.predict(xx_poly2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 作图</span></span><br><span class="line">plt.scatter(X_train,y_train)</span><br><span class="line">plt1,=plt.plot(xx,yy,label=<span class="string">'degree=1'</span>)</span><br><span class="line">plt2,=plt.plot(xx,yy_poly2,label=<span class="string">'degree=2'</span>)</span><br><span class="line">plt.axis([<span class="number">0</span>,<span class="number">25</span>,<span class="number">0</span>,<span class="number">25</span>])</span><br><span class="line">plt.xlabel(<span class="string">'diameter'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'price'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> regressor_poly2.score(X_train_poly2,y_train)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.9816421639597428</span><br></pre></td></tr></table></figure>
<p>果然在升高特征维度后，模型性能更高，对训练数据的拟合程度更好。接下来我们进一步提高特征维度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line"><span class="comment"># 使用PolynomialFeatures(degree=4)映射出2次多项式特征</span></span><br><span class="line">poly4=PolynomialFeatures(degree=<span class="number">4</span>)</span><br><span class="line">X_train_poly4=poly4.fit_transform(X_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以线性回归模型为基础，初始化模型（特征维度提升，但模型仍是线性回归模型）</span></span><br><span class="line">regressor_poly4=LinearRegression()</span><br><span class="line">regressor_poly4.fit(X_train_poly4,y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从新映射绘图用x轴采样数据</span></span><br><span class="line">xx_poly4=poly4.transform(xx)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">yy_poly4=regressor_poly4.predict(xx_poly4)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 作图</span></span><br><span class="line">plt.scatter(X_train,y_train)</span><br><span class="line">plt1,=plt.plot(xx,yy,label=<span class="string">'degree=1'</span>)</span><br><span class="line">plt2,=plt.plot(xx,yy_poly2,label=<span class="string">'degree=2'</span>)</span><br><span class="line">plt3,=plt.plot(xx,yy_poly4,label=<span class="string">'degree=4'</span>)</span><br><span class="line">plt.axis([<span class="number">0</span>,<span class="number">25</span>,<span class="number">0</span>,<span class="number">25</span>])</span><br><span class="line">plt.xlabel(<span class="string">'diameter'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'price'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> regressor_poly4.score(X_train_poly4,y_train)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.0</span><br></pre></td></tr></table></figure>
<p>由图和R平方指标可见，4次多项式曲线几乎完全拟合了所有训练样本点。接下来我们看着三种特征维度下的模型分别在测试集上的性能表现。</p>
<p><strong>3.评估3种回归模型在测试集上的性能表现</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_test=[[<span class="number">6</span>],[<span class="number">8</span>],[<span class="number">11</span>],[<span class="number">16</span>]]</span><br><span class="line">y_test=[[<span class="number">8</span>],[<span class="number">12</span>],[<span class="number">15</span>],[<span class="number">18</span>]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># degree=1</span></span><br><span class="line"><span class="keyword">print</span> regressor.score(X_test,y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># degree=2</span></span><br><span class="line">X_test_poly2=poly2.transform(X_test)</span><br><span class="line"><span class="keyword">print</span> regressor_poly2.score(X_test_poly2,y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># degree=4</span></span><br><span class="line">X_test_poly4=poly4.transform(X_test)</span><br><span class="line"><span class="keyword">print</span> regressor_poly4.score(X_test_poly4,y_test)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">0.809726797707665</span><br><span class="line">0.8675443656345108</span><br><span class="line">0.8095880795788558</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th>特征多项式次数</th>
<th>训练集R-squared值</th>
<th>测试集R-squared值</th>
</tr>
</thead>
<tbody>
<tr>
<td>degree=1</td>
<td>0.9100</td>
<td>0.8097</td>
</tr>
<tr>
<td>degree=2</td>
<td>0.9816</td>
<td>0.8675</td>
</tr>
<tr>
<td>degree=4</td>
<td>1.0000</td>
<td>0.8096</td>
</tr>
</tbody>
</table>
</div>
<p>由输出可见</p>
<ul>
<li><strong>欠拟合</strong>：当模型复杂度很低时(degree=1)，模型既在训练集上拟合不好，又在测试集上表现一般</li>
<li><strong>过拟合</strong>：一味追求高模型复杂度(degree=4)，尽管模型完美拟合了几乎所有训练数据，但模型会变得非常波动，几乎<strong>丧失了对未知数据的预测能力</strong></li>
</ul>
<p>这两种都是<strong>模型缺乏泛化力</strong>的表现。</p>
<p>要求我们<strong>在增加模型复杂度、提高在可观测数据上的性能表现的同时，需要兼顾模型的泛化力，防止发生过拟合</strong>。为了平衡这两种选择，我们通常<strong>采用2种模型正则化方法：L1范数正则化 &amp; L2范数正则化</strong></p>
<h4 id="3-1-2-2-L1范数正则化"><a href="#3-1-2-2-L1范数正则化" class="headerlink" title="3.1.2.2 L1范数正则化"></a>3.1.2.2 L1范数正则化</h4><p><strong>正则化(Regularization)</strong></p>
<ul>
<li>目的：<strong>提高模型在位置测试数据上的泛化力，避免过拟合</strong></li>
<li>常见方法：在原模型优化目标的基础上，<strong>增加对参数的惩罚项(Penalty)</strong></li>
</ul>
<p>以最小二乘优化目标为例：</p>
<p>最小二乘优化目标: $argminL(w,b)=argmin\sum_{m}^{k=1}\qquad(f(w,x,b)-y^k)^2$ </p>
<p>若加入对模型的<em>L1</em>范数正则化，则新的线性回归目标为：</p>
<script type="math/tex; mode=display">
argminL(w,b)=argmin\sum_{m}^{k=1}\qquad(f(w,x,b)-y^k)^2 + \lambda \Arrowvert{w}\Arrowvert_{1}</script><p>即<strong>在原优化目标的基础上，增加了参数向量的L1范数，则在新目标优化过程中需要考虑L1惩罚项的影响</strong>。</p>
<p>为使目标最小化，这种正则化方法的结果是让参数向量中的许多元素趋向于0，使大部分特征失去对优化目标的贡献。而这种让有效特征变得稀疏的<em>L1</em>正则化模型，称为<em>Lasso</em>。</p>
<p><strong>Lasso模型在4次多项式特征上的拟合表现</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso</span><br><span class="line">lasso_poly4=Lasso()</span><br><span class="line">lasso_poly4.fit(X_train_poly4,y_train)</span><br><span class="line"><span class="keyword">print</span> lasso_poly4.score(X_test_poly4,y_test)</span><br><span class="line"><span class="comment"># 输出lasso模型的参数列表</span></span><br><span class="line"><span class="keyword">print</span> lasso_poly4.coef_</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">0.8388926873604382</span><br><span class="line">[ 0.00000000e+00  0.00000000e+00  1.17900534e-01  5.42646770e-05</span><br><span class="line"> -2.23027128e-04]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 回顾普通4次多项式回归模型拟合后的性能和参数列表</span></span><br><span class="line"><span class="keyword">print</span> regressor_poly4.score(X_test_poly4,y_test)</span><br><span class="line"><span class="keyword">print</span> regressor_poly4.coef_</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">0.8095880795788558</span><br><span class="line">[[ 0.00000000e+00 -2.51739583e+01  3.68906250e+00 -2.12760417e-01</span><br><span class="line">   4.29687500e-03]]</span><br></pre></td></tr></table></figure>
<p>由上可见，默认配置的lasso模型性能提高了约3%。lasso模型拟合后的参数列表中，4次与3次特征的参数均为0.0，使得特征更加稀疏。</p>
<h4 id="3-1-2-3-L2范数正则化"><a href="#3-1-2-3-L2范数正则化" class="headerlink" title="3.1.2.3 L2范数正则化"></a>3.1.2.3 L2范数正则化</h4><p>与L1范数正则化略有不同，<strong>L2范数正则化在原优化目标上增加了参数向量的L2范数的惩罚项</strong>，公式如下：</p>
<script type="math/tex; mode=display">
argminL(w,b)=argmin\sum_{m}^{k=1}\qquad(f(w,x,b)-y^k)^2 + \lambda \Arrowvert{w}\Arrowvert_{2}</script><p>为使新优化目标最小化，这种正则化方法的结果会让参数向量中的大部分元素都变得很小，压制了参数之间的差异性，这种<strong>压制参数间差异性</strong>的L2正则化模型被称为<em>Ridge</em>。</p>
<p><strong>Ridge模型在4次多项式特征上的拟合表现</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输出普通4次多项式回归模型的参数列表</span></span><br><span class="line"><span class="keyword">print</span> regressor_poly4.coef_</span><br><span class="line"><span class="comment"># 输出上述参数的平方和，验证参数间的巨大差异</span></span><br><span class="line"><span class="keyword">print</span> np.sum(regressor_poly4.coef_ **<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[ 0.00000000e+00 -2.51739583e+01  3.68906250e+00 -2.12760417e-01</span><br><span class="line">   4.29687500e-03]]</span><br><span class="line">647.3826457370965</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge</span><br><span class="line">ridge_poly4=Ridge()</span><br><span class="line">ridge_poly4.fit(X_train_poly4,y_train)</span><br><span class="line"><span class="keyword">print</span> ridge_poly4.score(X_test_poly4,y_test)</span><br><span class="line"><span class="keyword">print</span> ridge_poly4.coef_</span><br><span class="line"><span class="keyword">print</span> np.sum(ridge_poly4.coef_ **<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">0.8374201759366331</span><br><span class="line">[[ 0.         -0.00492536  0.12439632 -0.00046471 -0.00021205]]</span><br><span class="line">0.01549896520353474</span><br></pre></td></tr></table></figure>
<p>由输出可见，相比普通4次多项式回归模型，默认配置下的Ridge模型性能提高了约3%，且<strong>模型拟合后的参数间差异非常小</strong>。</p>
<p>λ是调节因子。</p>
<h3 id="3-1-3-模型检验"><a href="#3-1-3-模型检验" class="headerlink" title="3.1.3 模型检验"></a>3.1.3 模型检验</h3><p>错误的做法：拿测试集的正确结果反复调优模型与特征。</p>
<p>正确的做法：</p>
<ul>
<li>充分使用现有数据，对现有数据进行采样分割，一部分用于模型参数训练（训练集），另一部分用于调优模型配置和特征选择，并对未知测试性能作出估计（开发集Development Set/验证集Validation Set）</li>
</ul>
<p><strong>根据验证流程复杂度的不同</strong>，模型验证方式分为：留一验证 &amp; 交叉验证</p>
<h4 id="3-1-3-1-留一验证-Leave-one-out-cross-validation"><a href="#3-1-3-1-留一验证-Leave-one-out-cross-validation" class="headerlink" title="3.1.3.1 留一验证(Leave-one-out cross validation)"></a>3.1.3.1 留一验证(Leave-one-out cross validation)</h4><p>留一验证</p>
<ul>
<li>最简单，即从任务提供的数据中，随机采样一定比例作为训练集，剩下的“留作”验证</li>
<li>通常比例为：7：3</li>
<li>缺点：模型性能不稳定（由于对验证集合随机采样的不确定性）</li>
<li>适用：计算能力较弱、而相对数据规模较大的机器学习发展早期（现在应该很少用了）</li>
</ul>
<h4 id="3-1-3-2-交叉验证-K-fold-cross-validation"><a href="#3-1-3-2-交叉验证-K-fold-cross-validation" class="headerlink" title="3.1.3.2 交叉验证(K-fold cross-validation)"></a>3.1.3.2 交叉验证(K-fold cross-validation)</h4><p>交叉验证</p>
<ul>
<li><p><strong>执行了多次留一验证的过程</strong></p>
</li>
<li><p>每次检验所使用的<strong>验证集之间互斥</strong>，且需保证<strong>每一条可用数据都被模型验证过</strong></p>
</li>
<li><p>优点：<strong>保证所有数据都有被训练和验证的机会</strong>，尽最大可能让优化的模型性能表现<strong>可信</strong></p>
<p>​</p>
</li>
</ul>
<p>以5折(five-fold)交叉验证为例：</p>
<ul>
<li><p>全部可用数据被随机分割为平均数量的5组，每次迭代都选取其中的1组数据作为验证集，其余4组作为训练集</p>
<p>​</p>
</li>
</ul>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1fo5rqikbsoj30kv08adi8.jpg" alt="5-fold"></p>
<h3 id="3-1-4-超参数搜索-hyperparameter"><a href="#3-1-4-超参数搜索-hyperparameter" class="headerlink" title="3.1.4 超参数搜索(hyperparameter)"></a>3.1.4 超参数搜索(hyperparameter)</h3><p><strong>超参数</strong></p>
<ul>
<li>指<strong>开始学习之前设置值的参数（模型配置）</strong>，而非通过训练得到的参数</li>
<li>如K近邻算法中的K值，SVM中不同的核函数(Kernal)</li>
<li>多数情况下，<strong>超参数的选择是无限的</strong>；故在有限时间内，除了可验证人工预设的几种超参数组合外，还可<strong>通过启发式的搜索方法对超参数组合进行调优</strong>，这种启发式的超参数搜索法称为<strong>网络搜索</strong></li>
<li>超参数的<strong>验证过程之间彼此独立</strong>，故适合<strong>并行计算</strong></li>
</ul>
<h4 id="3-1-4-1-网格搜索-GridSearch"><a href="#3-1-4-1-网格搜索-GridSearch" class="headerlink" title="3.1.4.1 网格搜索(GridSearch)"></a>3.1.4.1 网格搜索(GridSearch)</h4><p>由于超参数的空间无尽，故<strong>超参数组合配置只能是更优解，没有最优解</strong>。</p>
<p>网格搜索</p>
<ul>
<li><strong>依赖网格搜索对多种超参数组合的空间进行暴力搜索</strong>，<strong>每一套超参数组合被代入到学习函数中作为新的模型</strong>，且为比较新模型间的性能，<strong>每个模型都会采用交叉验证法在多组相同的训练和开发集下进行评估</strong></li>
</ul>
<p><strong>使用单线程对文本分类的Naive Bayes模型的超参数组合执行网格搜索</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_20newsgroups</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">news=fetch_20newsgroups(subset=<span class="string">'all'</span>)</span><br><span class="line"><span class="keyword">print</span> len(news.data)</span><br><span class="line"><span class="keyword">print</span> news.data[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">18846</span><br><span class="line">From: Mamatha Devineni Ratnam &lt;mr47+@andrew.cmu.edu&gt;</span><br><span class="line">Subject: Pens fans reactions</span><br><span class="line">Organization: Post Office, Carnegie Mellon, Pittsburgh, PA</span><br><span class="line">Lines: 12</span><br><span class="line">NNTP-Posting-Host: po4.andrew.cmu.edu</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">I am sure some bashers of Pens fans are pretty confused about the lack</span><br><span class="line">of any kind of posts about the recent Pens massacre of the Devils. Actually,</span><br><span class="line">I am  bit puzzled too and a bit relieved. However, I am going to put an end</span><br><span class="line">to non-PIttsburghers&apos; relief with a bit of praise for the Pens. Man, they</span><br><span class="line">are killing those Devils worse than I thought. Jagr just showed you why</span><br><span class="line">he is much better than his regular season stats. He is also a lot</span><br><span class="line">fo fun to watch in the playoffs. Bowman should let JAgr have a lot of</span><br><span class="line">fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final</span><br><span class="line">regular season game.          PENS RULE!!!</span><br></pre></td></tr></table></figure>
<p>​    </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="comment"># 选取前3000条新闻文本进行分割</span></span><br><span class="line">X_train,X_test,y_train,y_test=train_test_split(news.data[:<span class="number">3000</span>],news.target[:<span class="number">3000</span>],test_size=<span class="number">0.25</span>,random_state=<span class="number">33</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入pipeline，使用pipeline简化系统搭建流程（简化代码），将文本抽取与分类器模型串联起来</span></span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line">clf=Pipeline([(<span class="string">'vect'</span>,TfidfVectorizer(stop_words=<span class="string">'english'</span>,analyzer=<span class="string">'word'</span>)),(<span class="string">'svc'</span>,SVC())])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里需要试验的2个超参数的个数分别是4、3，svc_gamma的参数共有10^-2,10^-1...，则共有12种超参数组合，12种不同参数下的模型</span></span><br><span class="line">parameters=&#123;<span class="string">'svc_gamma'</span>:np.logspace(<span class="number">-2</span>,<span class="number">1</span>,<span class="number">4</span>),<span class="string">'svc_C'</span>:np.logspace(<span class="number">-1</span>,<span class="number">1</span>,<span class="number">3</span>)&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入网络搜索模块GridSearchCV</span></span><br><span class="line"><span class="keyword">from</span> sklearn.grid_search <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="comment"># 将12组参数组合、初始化的Pipeline和3折交叉验证的要求全部告诉GridSearchCV，注意refit=True的设定</span></span><br><span class="line">gs=GridSearchCV(clf,parameters,verbose=<span class="number">2</span>,refit=<span class="keyword">True</span>,cv=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行单线程网络搜索</span></span><br><span class="line">%time_=gs.fit(X_train,y_train)</span><br><span class="line">gs.best_params_,gs.best_score_</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> gs.score(X_test,y_test)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">UsageError: Line magic function `%time_` not found.</span><br></pre></td></tr></table></figure>
<p>注：</p>
<ul>
<li><strong>np.logspace(a,b,c):创建等比数列</strong>，a为起始点，b为终点，c为元素个数;<strong>np.linspace(a,b,c):创建等差数列</strong></li>
<li><strong>refit=True</strong>: 使程序以交叉验证训练集得到的最佳超参数重新对所有可用的训练集合开发集进行，作为最终用于性能评估的最佳模型参数</li>
</ul>
<p>结果分析：</p>
<ul>
<li>使用单线程网格搜索技术对朴素贝叶斯模型在文本分类任务中的超参数组合进行调优，共有12组超参数X3折交叉验证=36项独立运行的计算任务。寻找到的最佳超参数组合在测试集上所能达成的最高分类准确性为82.27%。</li>
<li><strong>缺点：耗时</strong></li>
<li>优点：一旦获取到好超参数组合，则可以保持一段时间使用，是<strong>一劳永逸提高模型性能</strong>的方法</li>
</ul>
<h4 id="3-1-4-2-并行搜索-Parallel-Grid-Search"><a href="#3-1-4-2-并行搜索-Parallel-Grid-Search" class="headerlink" title="3.1.4.2 并行搜索(Parallel Grid Search)"></a>3.1.4.2 并行搜索(Parallel Grid Search)</h4><p>由于<strong>各新模型在执行交叉验证过程中相互独立</strong>，故我们可<strong>充分利用多核处理器甚至是分布式计算资源来从事并行搜索</strong>，则可成倍节省运算时间。</p>
<p><strong>使用多线程对文本分类的Naive Bayes模型的超参数组合执行并行化的网络搜索</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_20newsgroups</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">news=fetch_20newsgroups(subset=<span class="string">'all'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="comment"># 选取前3000条新闻文本进行分割</span></span><br><span class="line">X_train,X_test,y_train,y_test=train_test_split(news.data[:<span class="number">3000</span>],news.target[:<span class="number">3000</span>],test_size=<span class="number">0.25</span>,random_state=<span class="number">33</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入pipeline，使用pipeline简化系统搭建流程（简化代码），将文本抽取与分类器模型串联起来</span></span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line">clf=Pipeline([(<span class="string">'vect'</span>,TfidfVectorizer(stop_words=<span class="string">'english'</span>,analyzer=<span class="string">'word'</span>)),(<span class="string">'svc'</span>,SVC())])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里需要试验的2个超参数的个数分别是4、3，svc_gamma的参数共有10^-2,10^-1...，则共有12种超参数组合，12种不同参数下的模型</span></span><br><span class="line">parameters=&#123;<span class="string">'svc_gamma'</span>:np.logspace(<span class="number">-2</span>,<span class="number">1</span>,<span class="number">4</span>),<span class="string">'svc_C'</span>:np.logspace(<span class="number">-1</span>,<span class="number">1</span>,<span class="number">3</span>)&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入网络搜索模块GridSearchCV</span></span><br><span class="line"><span class="keyword">from</span> sklearn.grid_search <span class="keyword">import</span> GridSearchCV</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将12组参数组合、初始化的Pipeline和3折交叉验证的要求全部告诉GridSearchCV，注意refit=True的设定</span></span><br><span class="line"><span class="comment"># 初始化配置并行网络搜索，n_jobs=-1代表使用该计算机的全部CPU</span></span><br><span class="line">gs=GridSearchCV(clf,parameters,verbose=<span class="number">2</span>,refit=<span class="keyword">True</span>,cv=<span class="number">3</span>,n_jobs=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行多线程并行网络搜索</span></span><br><span class="line">%time_=gs.fit(X_train,y_train)</span><br><span class="line">gs.best_params_,gs.best_score_</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出最佳模型在测试集上的准确性</span></span><br><span class="line"><span class="keyword">print</span> gs.score(X_test,y_test)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">UsageError: Line magic function `%time_` not found.</span><br></pre></td></tr></table></figure>
<p>结果分析：</p>
<ul>
<li>相较于单线程，多线程的计算时间仅为51.8秒，且准确性仍为82.27%</li>
<li><strong>并行搜索：有效利用多核处理器的计算资源，几乎成倍提升运算速度，节省最佳超参数组合的搜索时间</strong></li>
</ul>
<h2 id="3-2-流行库-模型实践"><a href="#3-2-流行库-模型实践" class="headerlink" title="3.2 流行库/模型实践"></a>3.2 流行库/模型实践</h2><h3 id="3-2-1-NLTK自然语言处理包"><a href="#3-2-1-NLTK自然语言处理包" class="headerlink" title="3.2.1 NLTK自然语言处理包"></a>3.2.1 NLTK自然语言处理包</h3><p>计算语言学</p>
<ul>
<li>借助计算机强大的运算能力和海量的互联网文本，来提高自然语言处理能力</li>
<li>如何让计算机处理、生成、理解人类的自然语言</li>
</ul>
<p>NLTK(Natural Language Toolkit)</p>
<ul>
<li>高效的语言学家，快速完成对自然语言文本的深层处理和分析</li>
</ul>
<p><strong>1.使用词袋法(bag of words)对文本进行特征向量化</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">sent1=<span class="string">'The dog is walking on the street.'</span></span><br><span class="line">sent2=<span class="string">'A cat was running across the room.'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line">cvr=CountVectorizer()</span><br><span class="line">sent=[sent1,sent2]</span><br><span class="line"><span class="keyword">print</span> cvr.fit_transform(sent).toarray()</span><br><span class="line"><span class="keyword">print</span> cvr.get_feature_names()</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line">tfidf=TfidfVectorizer()</span><br><span class="line"><span class="keyword">print</span> tfidf.fit_transform(sent).toarray()</span><br><span class="line"><span class="keyword">print</span> tfidf.get_feature_names()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[[0 0 1 1 1 0 0 1 2 1 0]</span><br><span class="line"> [1 1 0 0 0 1 1 0 1 0 1]]</span><br><span class="line">[u&apos;across&apos;, u&apos;cat&apos;, u&apos;dog&apos;, u&apos;is&apos;, u&apos;on&apos;, u&apos;room&apos;, u&apos;running&apos;, u&apos;street&apos;, u&apos;the&apos;, u&apos;walking&apos;, u&apos;was&apos;]</span><br><span class="line">[[0.         0.         0.37729199 0.37729199 0.37729199 0.</span><br><span class="line">  0.         0.37729199 0.53689271 0.37729199 0.        ]</span><br><span class="line"> [0.4261596  0.4261596  0.         0.         0.         0.4261596</span><br><span class="line">  0.4261596  0.         0.30321606 0.         0.4261596 ]]</span><br><span class="line">[u&apos;across&apos;, u&apos;cat&apos;, u&apos;dog&apos;, u&apos;is&apos;, u&apos;on&apos;, u&apos;room&apos;, u&apos;running&apos;, u&apos;street&apos;, u&apos;the&apos;, u&apos;walking&apos;, u&apos;was&apos;]</span><br></pre></td></tr></table></figure>
<p><strong>2.使用NLTK对文本进行语言学分析</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对句子进行分词和正规化</span></span><br><span class="line">tokens_1=nltk.word_tokenize(sent1)</span><br><span class="line"><span class="keyword">print</span> tokens_1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化词性标注器，对每个词汇进行标注</span></span><br><span class="line">pos_tag_1=nltk.tag.pos_tag(tokens_1)</span><br><span class="line"><span class="keyword">print</span> pos_tag_1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化stemmer，寻找各个词汇最原始的词根</span></span><br><span class="line">stemmer=nltk.stem.PorterStemmer()</span><br><span class="line">stem_1=[stemmer.stem(t) <span class="keyword">for</span> t <span class="keyword">in</span> tokens_1]</span><br><span class="line"><span class="keyword">print</span> stem_1</span><br></pre></td></tr></table></figure>
<h3 id="3-2-2-词向量技术-Word2Vec"><a href="#3-2-2-词向量技术-Word2Vec" class="headerlink" title="3.2.2 词向量技术(Word2Vec)"></a>3.2.2 词向量技术(Word2Vec)</h3><p><strong>词袋法可对文本向量化，但无法计算两段文本间的相似性</strong>。如sent1与sent2在词袋法看来唯一相同的词汇是the，找不到任何语义层面的联系。</p>
<p><strong>Word2Vec模型</strong>：</p>
<ul>
<li>用来<strong>产生词向量</strong>的相关模型，为<strong>浅层神经网络，是监督学习系统</strong></li>
<li>网络以词表现，并需猜测相邻位置的输入词；训练完成后，Word2vec模型可用来映射每个词到一个向量，表示词对词之间的关系（该向量为神经网络之隐藏层）</li>
<li><strong>词汇间的联系通过上下文(context)建立</strong></li>
<li>如”A cat was running across the room.”,若需要上下文数量为4的连续词汇片段，则有A cat was running、cat was running across、was running across the、running across the room. 每个连续词汇片段的最后一个单词有可能是什么都是受到前面3个词汇的制约，故形成由前3个词汇预测最后一个单词的监督学习系统</li>
<li>当上下文数量为n时，提供给网络的输入(Input)都是前n-1个连续词汇$w<em>{t-m+1},…,w</em>{t-1}$，指向的目标输出(Output)就是最后一个单词$w<em>{t}$；而在网络中用于计算的都是这些词汇的向量表示，如$C(w</em>{t-1})$，每一个实心红色圆都代表词向量中的元素；每个词汇红色实心圆的个数代表词向量的维度(dimension),且所有词向量的维度一致。神经网络的训练也是一个通过不断迭代、更新参数循环往复的过程，<strong>从网络中最终获得的即每个词汇独特的向量表示</strong>。</li>
</ul>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1fo5wh0szkej30sa0nfq9l.jpg" alt="word2vec"></p>
<p><strong>实践：用20类新闻文本进行词向量训练</strong></p>
<h3 id="3-2-3-XGBoost模型-extreme-gradient-boosting"><a href="#3-2-3-XGBoost模型-extreme-gradient-boosting" class="headerlink" title="3.2.3 XGBoost模型(extreme gradient boosting)"></a>3.2.3 XGBoost模型(extreme gradient boosting)</h3><p>XGBoost模型</p>
<ul>
<li>提升(Boosting)分类器隶属于集成学习模型，基本思想：把成千上万个分类准确率较低的树模型组合起来，成为一个准确率很高的模型</li>
<li>特点：不断迭代，每次迭代都生成一棵新的树;能自动利用CPU的多线程进行并行计算</li>
<li>如何生成合理的树？如梯度提升树</li>
</ul>
<p><strong>实践：对比随机决策森林和XGBoost模型对titanic上的乘客是否生还的预测能力</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line">titanic=pd.read_csv(<span class="string">'/Users/scarlett/repository/projects/titanic/titanic.csv'</span>)</span><br><span class="line"><span class="keyword">print</span> (titanic.info())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选取训练特征</span></span><br><span class="line">X=titanic[[<span class="string">'pclass'</span>,<span class="string">'age'</span>,<span class="string">'sex'</span>]]</span><br><span class="line">y=titanic[<span class="string">'survived'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 补全缺失值</span></span><br><span class="line">X[<span class="string">'age'</span>].fillna(X[<span class="string">'age'</span>].mean(),inplace=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据分割</span></span><br><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=<span class="number">0.25</span>,random_state=<span class="number">33</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入文本特征向量化的模块DictVectorizer</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> DictVectorizer</span><br><span class="line">vec=DictVectorizer(sparse=<span class="keyword">False</span>)</span><br><span class="line"><span class="comment"># 对原数据进行特征向量化处理</span></span><br><span class="line">X_train=vec.fit_transform(X_train.to_dict(orient=<span class="string">'record'</span>))</span><br><span class="line">X_test=vec.transform(X_test.to_dict(orient=<span class="string">'record'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 采用默认配置下的随机森林分类器对测试集进行预测</span></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line">rfc=RandomForestClassifier()</span><br><span class="line">rfc.fit(X_train,y_train)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'rfc accuracy:'</span>,rfc.score(X_test,y_test))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 采用默认配置的xgboost模型对相同测试集进行预测</span></span><br><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBClassifier</span><br><span class="line">xgbc=XGBClassifier()</span><br><span class="line">xgbc.fit(X_train,y_train)</span><br><span class="line"><span class="keyword">print</span> <span class="string">'xgbc accuracy:'</span>,xgbc.score(X_test,y_test)</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rfc accuracy:0.775075987842</span><br><span class="line">xgbc accuracy:0.78234042553</span><br></pre></td></tr></table></figure>
<p>由输出可知：XGBoost分类模型的确可发挥出更好的预测能力.</p>
<h3 id="3-2-4-TensorFlow框架"><a href="#3-2-4-TensorFlow框架" class="headerlink" title="3.2.4 TensorFlow框架"></a>3.2.4 TensorFlow框架</h3><p>TensorFlow</p>
<ul>
<li>一个完整的编码框架</li>
<li>使用图(Graph)来表示计算任务，使用会话(Session)来执行图</li>
</ul>
<p><strong>1.使用TensorFlow输出一句话</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用TensorFlow输出一句话</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化一个TensorFlow常量，使greeting作为一个计算模块</span></span><br><span class="line">greeting=tf.constant(<span class="string">"Hello Google!"</span>)</span><br><span class="line"><span class="comment"># 启动一个会话</span></span><br><span class="line">sess=tf.Session()</span><br><span class="line"><span class="comment"># 使用会话执行greeting计算模块</span></span><br><span class="line">result=sess.run(greeting)</span><br><span class="line"><span class="comment"># 输出会话执行结果</span></span><br><span class="line"><span class="keyword">print</span> result</span><br><span class="line"><span class="comment"># 关闭会话</span></span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hello Google!</span><br></pre></td></tr></table></figure>
<p><strong>2.使用TensorFlow完成一次线性函数的运算</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用TensorFlow完成一次线性函数的运算</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 声明matrix1为一个1*2的行向量，matrix2为一个2*1的列向量</span></span><br><span class="line">matrix1=tf.constant([[<span class="number">3.</span>,<span class="number">3.</span>]])</span><br><span class="line">matrix2=tf.constant([[<span class="number">2.</span>],[<span class="number">2.</span>]])</span><br><span class="line"><span class="comment"># Product将上面两个算子相乘，作为新算例</span></span><br><span class="line">product=tf.matmul(matrix1, matrix2)</span><br><span class="line"><span class="comment"># 继续讲Product与一个标量2.0求和拼接，作为最终linear算例</span></span><br><span class="line">linear=tf.add(product, tf.constant(<span class="number">2.0</span>))</span><br><span class="line"><span class="comment"># 直接在会话中执行linear算例，相当于将上面所有单独的算例拼接成流程图来执行</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">	result=sess.run(linear)</span><br><span class="line">	<span class="keyword">print</span> result</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[14.]]</span><br></pre></td></tr></table></figure>
<p>Skflow</p>
<ul>
<li>sklearn X tensorflow</li>
</ul>
<p>人工神经网络(Artificial Neural Network)</p>
<ul>
<li>神经信息传递的大致工作原理：神经元的树突(Dendrite)接收其他神经元传递过来的信息，然后神经细胞体对信息进行加工后，会通过轴突(Axon)把加工后的信息传递到轴突终端(Axon Terminal)然后再传递给其他神经元的树突。如此，则大量神经元就连接成了一个结构复杂的神经网络。</li>
</ul>
<p>感知机</p>
<ul>
<li>模拟神经元</li>
<li>n维输入信号(Input) $x=<x_1,x_2,..,x_n>$，对应的参数向量$w=<w_1,w_2,..,w_n>$，和截距b，输出信号y(Output)等</w_1,w_2,..,w_n></x_1,x_2,..,x_n></li>
<li>感知机在具体运算上采用线性加权求和的方式处理输入信号，即</li>
</ul>
<script type="math/tex; mode=display">
y=sign(w^Tx+b)</script><p>为模拟神经元行为，定义如下激活(符号)函数，由此可知感知机最终会产生两种离散数值的输出(output)信号:</p>
<script type="math/tex; mode=display">
\begin{eqnarray}sign(z)=
\begin{cases}
+1, &z\ge0\cr  \cr -1, &z<0\end{cases}
\end{eqnarray}</script><ul>
<li>感知机模型的最大贡献在于：设计了一套算法使模型可<strong>通过不断根据训练数据更新参数，达到具备线性二分类模型的学习能力</strong></li>
</ul>
<p>多层感知机(Multi-layer Perceptrons人工神经网络)</p>
<p><strong>实践：使用skflow内置的LinearRegressor、DNN和Sciki-learn中的集成回归模型对“美国波士顿房价”数据进行回归预测</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets,metrics,preprocessing,cross_validation</span><br><span class="line">boston=datasets.load_boston()</span><br><span class="line">X,y=boston.data,boston.target</span><br><span class="line"></span><br><span class="line">X_train,X_test,y_train,y_test=cross_validation.train_test_split(X,y,test_size=<span class="number">0.25</span>,random_state=<span class="number">33</span>)</span><br><span class="line">scaler=preprocessing.StandardScaler()</span><br><span class="line">X_train=scaler.fit_transform(X_train)</span><br><span class="line">X_test=scaler.transform(X_test)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> skflow</span><br><span class="line">tf_lr=skflow.TensorFlowLinearRegressor(steps=<span class="number">10000</span>,learning_rate=<span class="number">0.01</span>,batch_size=<span class="number">50</span>)</span><br><span class="line">tf_lr.fit(X_train, y_train)</span><br><span class="line">tf_lr_y_predict=tf_lr.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出skflow中linearregressor模型的回归性能</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'MAE:'</span>,metrics.mean_absolute_error(tf_lr_y_predict, y_test)</span><br><span class="line"><span class="keyword">print</span> <span class="string">'MSE:'</span>,metrics.mean_squared_error(tf_lr_y_predict, y_test)</span><br><span class="line"><span class="keyword">print</span> <span class="string">'R-squared:'</span>,metrics.r2_score(tf_lr_y_predict, y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用skflow的DNNRegressor，并注意其每个隐层特征数量的配置</span></span><br><span class="line">tf_dnn_regressor=skflow.TensorFlowDNNRegressor(hidden_units=[<span class="number">100</span>,<span class="number">40</span>],</span><br><span class="line">	steps=<span class="number">10000</span>,learning_rate=<span class="number">0.01</span>,batch_size=<span class="number">50</span>)</span><br><span class="line">tf_dnn_regressor.fit(X_train,y_train)</span><br><span class="line">tf_dnn_regressor_y_predict=tf_dnn_regressor.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出skflow中DNNRegressor模型的回归性能</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'MSE dnn:'</span>,metrics.mean_squared_error(tf_dnn_regressor_y_predict, y_test)</span><br><span class="line"><span class="keyword">print</span> <span class="string">'MAE dnn:'</span>,metrics.mean_absolute_error(tf_dnn_regressor_y_predict, y_test)</span><br><span class="line"><span class="keyword">print</span> <span class="string">'R-squared dnn:'</span>,metrics.r2_score(tf_dnn_regressor_y_predict, y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用sklearn中的RandomForestRegressor</span></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</span><br><span class="line">rfr=RandomForestRegressor()</span><br><span class="line">rfr.fit(X_train,y_train)</span><br><span class="line">rfr_y_predict=rfr.predict(X_test)</span><br><span class="line"><span class="keyword">print</span> <span class="string">'MSE rfr:'</span>,metrics.mean_squared_error(rfr_y_predict, y_test)</span><br><span class="line"><span class="keyword">print</span> <span class="string">'MAE rfr:'</span>,metrics.mean_absolute_error(rfr_y_predict, y_test)</span><br><span class="line"><span class="keyword">print</span> <span class="string">'R-squared rfr:'</span>,metrics.r2_score(rfr_y_predict, y_test)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">MAE:3.5107</span><br><span class="line">MSE:25.1175</span><br><span class="line">R_squared:0.62000</span><br><span class="line"></span><br><span class="line">MSE dnn:14.2555</span><br><span class="line">MAE dnn:2.5278</span><br><span class="line">R_squared dnn:0.8035</span><br><span class="line"></span><br><span class="line">MSE rfr:2.5077</span><br><span class="line">MAE rfr:14.3742</span><br><span class="line">R_squared rfr:0.79669</span><br></pre></td></tr></table></figure>
<p>由输出可知：深度神经网络性能更佳。但需要注意：</p>
<ul>
<li>越是具备描述复杂数据的强力模型，越容易在训练时陷入过拟合，这一点需要在配置DNN的层数和每层特征元的数量时特别注意。</li>
</ul>
<h1 id="4-实战篇"><a href="#4-实战篇" class="headerlink" title="4.实战篇"></a>4.实战篇</h1><p>本章目的：如何使用学习过的模型和编程技巧挑战业界IT公司、科研院所在Kaggle上发布的机器学习任务。</p>
<h2 id="4-1-Kaggle"><a href="#4-1-Kaggle" class="headerlink" title="4.1 Kaggle"></a>4.1 Kaggle</h2><p>Kaggle是目前世界上最流行的，采用众包策略(Crowdsouring)，为科技公司、研究院所乃至高校课程提供数据分析和预测模型的竞赛平台。</p>
<p>平台宗旨: 汇聚全世界从事数据分析和预测的专家和兴趣爱好者的集体智慧，利用公开数据竞赛的方式，为科技公司、研究院所和高校课程中的研发课题，提供有效的解决方案。使问题提出者与解决者获得双赢。</p>
<ul>
<li>问题提出者：支付少量奖金即可获得全世界聪明人的解决方案</li>
<li>解决者：获得大量可供分析的真实业务数据</li>
</ul>
<p>参与流程</p>
<ul>
<li>下载数据(download)</li>
<li>搭建模型(build)</li>
<li>提交结果(submit)</li>
</ul>
<h2 id="4-2-Titanic遇难乘客预测"><a href="#4-2-Titanic遇难乘客预测" class="headerlink" title="4.2 Titanic遇难乘客预测"></a>4.2 Titanic遇难乘客预测</h2><p><strong>Description</strong></p>
<ul>
<li>In this challenge, we ask you to complete the analysis of <strong>what sorts of people were likely to survive</strong>.</li>
<li>In particular, we ask you to apply the tools of machine learning to <strong>predict which passengers survived the tragedy</strong>.</li>
</ul>
<p><strong>Practice Skills</strong></p>
<ul>
<li><strong>Binary classification</strong></li>
<li>Python and R basics</li>
</ul>
<p><strong>Goal</strong></p>
<p>It is your job to predict if a passenger survived the sinking of the Titanic or not.<br><strong>For each PassengerId in the test set, you must predict a 0 or 1 value for the Survived variable</strong>.</p>
<p><strong>Metric</strong></p>
<p>Accuracy：the percentage of passengers you correctly predict</p>
<p><strong>Submission File Format</strong></p>
<p>You should submit <strong>a csv file with exactly 418 entries plus a header row</strong>. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.</p>
<p>The file should have exactly 2 columns:</p>
<ul>
<li>PassengerId (sorted in any order)</li>
<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"></span><br><span class="line">train=pd.read_csv(<span class="string">'train.csv'</span>)</span><br><span class="line">test=pd.read_csv(<span class="string">'test.csv'</span>)</span><br><span class="line"><span class="keyword">print</span> train.shape</span><br><span class="line"><span class="keyword">print</span> train.info()</span><br><span class="line"><span class="keyword">print</span> test.info()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 人工选取用于预测的有效特征</span></span><br><span class="line">selected_features=[<span class="string">'Pclass'</span>,<span class="string">'Sex'</span>,<span class="string">'Age'</span>,<span class="string">'Embarked'</span>,<span class="string">'SibSp'</span>,<span class="string">'Parch'</span>,<span class="string">'Fare'</span>]</span><br><span class="line">X_train=train[selected_features]</span><br><span class="line">X_test=test[selected_features]</span><br><span class="line">y_train=train[<span class="string">'Survived'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 需要补全Embarked特征的缺失值</span></span><br><span class="line"><span class="keyword">print</span> X_train[<span class="string">'Embarked'</span>].value_counts()</span><br><span class="line"><span class="keyword">print</span> X_test[<span class="string">'Embarked'</span>].value_counts()</span><br><span class="line"><span class="comment"># 对于embarked这种类别型特征，可使用出现频率最高的特征值来补充，以减少误差</span></span><br><span class="line">X_test[<span class="string">'Embarked'</span>].fillna(<span class="string">'S'</span>,inplace=<span class="keyword">True</span>)</span><br><span class="line">X_train[<span class="string">'Embarked'</span>].fillna(<span class="string">'S'</span>,inplace=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># 对于Age这种数值型特征，可使用平均值或中位数来填充缺失值</span></span><br><span class="line">X_train[<span class="string">'Age'</span>].fillna(X_train[<span class="string">'Age'</span>].mean(),inplace=<span class="keyword">True</span>)</span><br><span class="line">X_test[<span class="string">'Age'</span>].fillna(X_test[<span class="string">'Age'</span>].mean(),inplace=<span class="keyword">True</span>)</span><br><span class="line">X_test[<span class="string">'Fare'</span>].fillna(X_test[<span class="string">'Fare'</span>].mean(),inplace=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># 检查是否已补全</span></span><br><span class="line"><span class="keyword">print</span> X_train.info()</span><br><span class="line"><span class="keyword">print</span> X_test.info()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 采用DictVectorizer进行特征向量化</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> DictVectorizer</span><br><span class="line">dict_vec=DictVectorizer(sparse=<span class="keyword">False</span>)</span><br><span class="line">X_train=dict_vec.fit_transform(X_train.to_dict(orient=<span class="string">'record'</span>))</span><br><span class="line">dict_vec.feature_names_</span><br><span class="line">X_test=dict_vec.transform(X_test.to_dict(orient=<span class="string">'record'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line">rfc=RandomForestClassifier()</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBClassifier</span><br><span class="line">xgbc=XGBClassifier()</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="comment"># 使用5折交叉验证法在训练集上分别对默认配置的RandomForestClassifier和XGBClassifier进行性能评估，并获得平均分类准确性的得分</span></span><br><span class="line">cross_val_score(rfc, X_train,y_train,cv=<span class="number">5</span>).mean()</span><br><span class="line">cross_val_score(xgbc,X_train,y_train,cv=<span class="number">5</span>).mean()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用模型进行预测</span></span><br><span class="line">rfc.fit(X_train,y_train)</span><br><span class="line">rfc_y_predict=rfc.predict(X_test)</span><br><span class="line">rfc_submission=pd.DataFrame(&#123;<span class="string">'PassengerId'</span>: test[<span class="string">'PassengerId'</span>],<span class="string">'Survived'</span>: rfc_y_predict&#125;)</span><br><span class="line">rfc_submission.to_csv(<span class="string">'/Users/scarlett/repository/projects/kaggle-titanic/rfc_submission.csv'</span>,index=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">xgbc.fit(X_train, y_train)</span><br><span class="line">xgbc_y_predict=xgbc.predict(X_test)</span><br><span class="line">xgbc_submission=pd.DataFrame(&#123;<span class="string">'PassengerId'</span>: test[<span class="string">'PassengerId'</span>],<span class="string">'Survived'</span>: rfc_y_predict&#125;)</span><br><span class="line">xgbc_submission.to_csv(<span class="string">'/Users/scarlett/repository/projects/kaggle-titanic/xgbc_submission.csv'</span>,index=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用并行网络搜索的方式寻找更好的参数组合，以进一步提高XGBC的预测性能</span></span><br><span class="line"><span class="keyword">from</span> sklearn.grid_search <span class="keyword">import</span> GridSearchCV</span><br><span class="line">params=&#123;<span class="string">'max_depth'</span>:range(<span class="number">2</span>,<span class="number">7</span>),<span class="string">'n_estimators'</span>:range(<span class="number">100</span>,<span class="number">1100</span>,<span class="number">200</span>),<span class="string">'learning_rate'</span>:[<span class="number">0.05</span>,<span class="number">0.1</span>,<span class="number">0.25</span>,<span class="number">0.5</span>,<span class="number">1.0</span>]&#125;</span><br><span class="line">xgbc_best=XGBClassifier()</span><br><span class="line">gs=GridSearchCV(xgbc_best, params,n_jobs=<span class="number">-1</span>,cv=<span class="number">5</span>,verbose=<span class="number">1</span>)</span><br><span class="line">gs.fit(X_train,y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查验优化之后的XGBC的超参数配置和交叉验证的准确性</span></span><br><span class="line"><span class="keyword">print</span> gs.best_score_</span><br><span class="line"><span class="keyword">print</span> gs.best_params_</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用经优化超参数配置的XGBC对测试数据的预测结果存储在文件中</span></span><br><span class="line">xgbc_best_y_predict=gs.predict(X_test)</span><br><span class="line">xgbc_best_submission=pd.DataFrame(&#123;<span class="string">'PassengerId'</span>: test[<span class="string">'PassengerId'</span>],<span class="string">'Survived'</span>:xgbc_best_y_predict&#125;)</span><br><span class="line">xgbc_best_submission.to_csv(<span class="string">'/Users/scarlett/repository/projects/kaggle-titanic/xgbc_best_submission.csv'</span>,index=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line">(891, 12)</span><br><span class="line">&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;</span><br><span class="line">RangeIndex: 891 entries, 0 to 890</span><br><span class="line">Data columns (total 12 columns):</span><br><span class="line">PassengerId    891 non-null int64</span><br><span class="line">Survived       891 non-null int64</span><br><span class="line">Pclass         891 non-null int64</span><br><span class="line">Name           891 non-null object</span><br><span class="line">Sex            891 non-null object</span><br><span class="line">Age            714 non-null float64</span><br><span class="line">SibSp          891 non-null int64</span><br><span class="line">Parch          891 non-null int64</span><br><span class="line">Ticket         891 non-null object</span><br><span class="line">Fare           891 non-null float64</span><br><span class="line">Cabin          204 non-null object</span><br><span class="line">Embarked       889 non-null object</span><br><span class="line">dtypes: float64(2), int64(5), object(5)</span><br><span class="line">memory usage: 83.6+ KB</span><br><span class="line">None</span><br><span class="line">&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;</span><br><span class="line">RangeIndex: 418 entries, 0 to 417</span><br><span class="line">Data columns (total 11 columns):</span><br><span class="line">PassengerId    418 non-null int64</span><br><span class="line">Pclass         418 non-null int64</span><br><span class="line">Name           418 non-null object</span><br><span class="line">Sex            418 non-null object</span><br><span class="line">Age            332 non-null float64</span><br><span class="line">SibSp          418 non-null int64</span><br><span class="line">Parch          418 non-null int64</span><br><span class="line">Ticket         418 non-null object</span><br><span class="line">Fare           417 non-null float64</span><br><span class="line">Cabin          91 non-null object</span><br><span class="line">Embarked       418 non-null object</span><br><span class="line">dtypes: float64(2), int64(4), object(5)</span><br><span class="line">memory usage: 36.0+ KB</span><br><span class="line">None</span><br><span class="line">S    644</span><br><span class="line">C    168</span><br><span class="line">Q     77</span><br><span class="line">Name: Embarked, dtype: int64</span><br><span class="line">S    270</span><br><span class="line">C    102</span><br><span class="line">Q     46</span><br><span class="line">Name: Embarked, dtype: int64</span><br><span class="line">/Users/scarlett/anaconda3/envs/python27/lib/python2.7/site-packages/pandas/core/generic.py:4355: SettingWithCopyWarning: </span><br><span class="line">A value is trying to be set on a copy of a slice from a DataFrame</span><br><span class="line"></span><br><span class="line">See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy</span><br><span class="line">  self._update_inplace(new_data)</span><br><span class="line">&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;</span><br><span class="line">RangeIndex: 891 entries, 0 to 890</span><br><span class="line">Data columns (total 7 columns):</span><br><span class="line">Pclass      891 non-null int64</span><br><span class="line">Sex         891 non-null object</span><br><span class="line">Age         891 non-null float64</span><br><span class="line">Embarked    891 non-null object</span><br><span class="line">SibSp       891 non-null int64</span><br><span class="line">Parch       891 non-null int64</span><br><span class="line">Fare        891 non-null float64</span><br><span class="line">dtypes: float64(2), int64(3), object(2)</span><br><span class="line">memory usage: 48.8+ KB</span><br><span class="line">None</span><br><span class="line">&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;</span><br><span class="line">RangeIndex: 418 entries, 0 to 417</span><br><span class="line">Data columns (total 7 columns):</span><br><span class="line">Pclass      418 non-null int64</span><br><span class="line">Sex         418 non-null object</span><br><span class="line">Age         418 non-null float64</span><br><span class="line">Embarked    418 non-null object</span><br><span class="line">SibSp       418 non-null int64</span><br><span class="line">Parch       418 non-null int64</span><br><span class="line">Fare        418 non-null float64</span><br><span class="line">dtypes: float64(2), int64(3), object(2)</span><br><span class="line">memory usage: 22.9+ KB</span><br><span class="line">None</span><br></pre></td></tr></table></figure>
<p>kaggle scores：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">xgbc: 0.75598</span><br><span class="line">rfc：0.77990</span><br></pre></td></tr></table></figure>
<h2 id="4-3-IMDB影评得分估计"><a href="#4-3-IMDB影评得分估计" class="headerlink" title="4.3 IMDB影评得分估计"></a>4.3 IMDB影评得分估计</h2><p>要求分析IMDB网站上的留言，判断每条留言的情感倾向。</p>
<p>数据描述</p>
<ul>
<li>labeledTrainData.tsv 已标注情感倾向的训练文件</li>
<li>testData.tsv 待测试文件</li>
<li>unlabeledTrainData 无标注但数据量更大的影评文件</li>
<li>sampleSubmission.csv 样例文件(提交结果的格式)</li>
<li>Address:<a href="https://www.kaggle.com/c/word2vec-nlp-tutorial/data" target="_blank" rel="noopener">Bag of Words Meets Bags of Popcorn</a></li>
</ul>
<p>模型描述</p>
<ul>
<li>采用sklearn的朴素贝叶斯和隶属于集成模型的梯度提升树分类模型，对电影评论文本进行情感分析</li>
<li>先利用无标注影评文件训练词向量，然后将每条电影评论中所有词汇的平均向量作为特征训练梯度提升树分类模型</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line">train=pd.read_csv(<span class="string">'/Users/scarlett/repository/projects/kaggle-IMDB/labeledTrainData.tsv'</span>,delimiter=<span class="string">'\t'</span>)</span><br><span class="line">test=pd.read_csv(<span class="string">'/Users/scarlett/repository/projects/kaggle-IMDB/testData.tsv'</span>,delimiter=<span class="string">'\t'</span>)</span><br><span class="line"><span class="comment"># print train.head()</span></span><br><span class="line"><span class="comment"># print test.head()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用bs4来清洗原始文本</span></span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="comment"># 导入正则表达式工具包</span></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="comment"># 导入停用词列表</span></span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> stopwords</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义review_to_text函数，完成对原始评论的三项数据预处理任务</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">review_to_text</span><span class="params">(review,remove_stopwords)</span>:</span></span><br><span class="line">	<span class="comment"># 1.去掉HTML标记</span></span><br><span class="line">	raw_text=BeautifulSoup(review,<span class="string">"html.parser"</span>).get_text()</span><br><span class="line">	<span class="comment"># 2.去掉非字母字符</span></span><br><span class="line">	letters=re.sub(<span class="string">'[^a-zA-Z]'</span>, <span class="string">' '</span>, raw_text)</span><br><span class="line">	words=letters.lower().split()</span><br><span class="line">	<span class="comment"># 3.若remove_stopwords被激活，则进一步去掉评论中的停用词</span></span><br><span class="line">	<span class="keyword">if</span> remove_stopwords:</span><br><span class="line">		stop_words=set(stopwords.words(<span class="string">'english'</span>))</span><br><span class="line">		words=[w <span class="keyword">for</span> w <span class="keyword">in</span> words <span class="keyword">if</span> w <span class="keyword">not</span> <span class="keyword">in</span> stop_words]</span><br><span class="line">	<span class="keyword">return</span> words</span><br><span class="line"><span class="comment"># 返回每条评论经此三项预处理的词汇列表</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 分别对原始训练和测试数据集进行上述三项预处理</span></span><br><span class="line">X_train=[]</span><br><span class="line"><span class="keyword">for</span> review <span class="keyword">in</span> train[<span class="string">'review'</span>]:</span><br><span class="line">	X_train.append(<span class="string">' '</span>.join(review_to_text(review, <span class="keyword">True</span>)))</span><br><span class="line">X_test=[]</span><br><span class="line"><span class="keyword">for</span> review <span class="keyword">in</span> test[<span class="string">'review'</span>]:</span><br><span class="line">	X_test.append(<span class="string">' '</span>.join(review_to_text(review, <span class="keyword">True</span>)))</span><br><span class="line"></span><br><span class="line">y_train=train[<span class="string">'sentiment'</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入文本特征抽取器</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer,TfidfVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</span><br><span class="line"><span class="comment"># 导入pipeline方便搭建系统流程,导入GridSearchCV用于超参数组合的网格搜索</span></span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.grid_search <span class="keyword">import</span> GridSearchCV</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用pipeline搭建两组使用朴素贝叶斯分类器，区别在于分别使用2个不同特征抽取器进行文本特征抽取</span></span><br><span class="line">pip_count=Pipeline([(<span class="string">'count_vec'</span>,CountVectorizer(analyzer=<span class="string">'word'</span>)),(<span class="string">'mnb'</span>,MultinomialNB())])</span><br><span class="line">pip_tfidf=Pipeline([(<span class="string">'tfidf_vec'</span>,TfidfVectorizer(analyzer=<span class="string">'word'</span>)),(<span class="string">'mnb'</span>,TfidfVectorizer())])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分别配置用于模型超参数搜索的组合</span></span><br><span class="line">params_count=&#123;<span class="string">'count_vec_binary'</span>:[<span class="keyword">True</span>,<span class="keyword">False</span>],<span class="string">'count_vec_ngram_range'</span>:[(<span class="number">1</span>,<span class="number">1</span>),(<span class="number">1</span>,<span class="number">2</span>)],<span class="string">'mnb_alpha'</span>:[<span class="number">0.1</span>,<span class="number">1.0</span>,<span class="number">10.0</span>]&#125;</span><br><span class="line">params_tfidf=&#123;<span class="string">'tfidf_vec_binary'</span>:[<span class="keyword">True</span>,<span class="keyword">False</span>],<span class="string">'tfidf_vec_ngram_range'</span>:[(<span class="number">1</span>,<span class="number">1</span>),(<span class="number">1</span>,<span class="number">2</span>)],<span class="string">'mnb_alpha'</span>:[<span class="number">0.1</span>,<span class="number">1.0</span>,<span class="number">10.0</span>]&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用4折交叉验证法分别对使用CountVectorizer和TfidfVectorizer的朴素贝叶斯模进行并行化超参数搜索，并分别输出交叉验证中最佳的准确性得分和超参数组合</span></span><br><span class="line">gs_count=GridSearchCV(pip_count, params_count,cv=<span class="number">4</span>,verbose=<span class="number">1</span>)</span><br><span class="line">gs_count.fit(X_train,y_train)</span><br><span class="line"><span class="keyword">print</span> (gs_count.best_score_)</span><br><span class="line"><span class="keyword">print</span> (gs_count.best_params_)</span><br><span class="line"></span><br><span class="line">gs_tfidf=GridSearchCV(pip_count, params_tfidf,cv=<span class="number">4</span>,verbose=<span class="number">1</span>)</span><br><span class="line">gs_tfidf.fit(X_train,y_train)</span><br><span class="line"><span class="keyword">print</span> (gs_tfidf.best_score_)</span><br><span class="line"><span class="keyword">print</span> (gs_tfidf.best_params_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以最佳超参数组合配置模型并对测试数据进行预测</span></span><br><span class="line">tfidf_y_predict=gs_tfidf.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用pandas对需要提交的数据进行格式化</span></span><br><span class="line">submission_count=pd.DataFrame(&#123;<span class="string">'id'</span>: test[<span class="string">'id'</span>],<span class="string">'sentiment'</span>: coount_y_predict&#125;)</span><br><span class="line">submission_tfidf=pd.DataFrame(&#123;<span class="string">'id'</span>: test[<span class="string">'id'</span>],<span class="string">'sentiment'</span>: tfidf_y_predict&#125;)</span><br><span class="line"><span class="comment"># 结果输出到硬盘</span></span><br><span class="line">submission_count.to_csv(<span class="string">'/Users/scarlett/repository/projects/kaggle-IMDB/submission_count.csv'</span>,index=<span class="keyword">False</span>)</span><br><span class="line">submission_tfidf.to_csv(<span class="string">'/Users/scarlett/repository/projects/kaggle-IMDB/submission_tfidf.csv'</span>,index=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从本地读取未标记数据</span></span><br><span class="line">unlabeled_train=pd.read_csv(<span class="string">'/Users/scarlett/repository/projects/kaggle-IMDB/unlabeledTrainData.tsv'</span>,delimiter=<span class="string">'\t'</span>,quoting=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> nktk.data </span><br><span class="line"><span class="comment"># 准备使用nltk的tokenizer对影评中的英文句子进行分割</span></span><br><span class="line">tokenizer=nltk.data.load(<span class="string">'tokenizers/punkt/english.pickel'</span>)</span><br><span class="line"><span class="comment"># 定义函数review_to_sentences逐条对影评进行分句</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">review_to_sentences</span><span class="params">(review,tokenizer)</span>:</span></span><br><span class="line">	raw_sentences=tokenizer.tokenize(review.strip())</span><br><span class="line">	sentences=[]</span><br><span class="line">	<span class="keyword">for</span> raw_sentences <span class="keyword">in</span> raw_sentences:</span><br><span class="line">		<span class="keyword">if</span> len(raw_sentences) &gt; <span class="number">0</span>:</span><br><span class="line">			sentences.append(review_to_text(raw_sentences, <span class="keyword">False</span>))</span><br><span class="line">		<span class="keyword">return</span> sentences</span><br><span class="line"></span><br><span class="line">corpora=[]</span><br><span class="line"><span class="comment"># 准备用于训练词向量的数据</span></span><br><span class="line"><span class="keyword">for</span> review <span class="keyword">in</span> unlabeled_train[<span class="string">'review'</span>]:</span><br><span class="line">	corpora+=review_to_sentences(review.decode(<span class="string">'utf8'</span>), tokenizer)</span><br><span class="line"><span class="comment"># 配置训练词向量模型的超参数</span></span><br><span class="line">num_features=<span class="number">300</span></span><br><span class="line">min_word_count=<span class="number">20</span></span><br><span class="line">context=<span class="number">10</span></span><br><span class="line">downsampling=le<span class="number">-3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入word2vec</span></span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> word2vec</span><br><span class="line"><span class="comment"># 开始词向量模型训练</span></span><br><span class="line">model=word2vec.Word2Vec(corpora,workers=num_workers,\</span><br><span class="line">	size=num_features,min_count=min_word_count,\</span><br><span class="line">	window=context,sample=downsampling)</span><br><span class="line">model.init_sims(replace=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">model_names=<span class="string">"/Users/scarlett/repository/projects/kaggle-IMDB/300features_20minwords_10context"</span></span><br><span class="line"><span class="comment"># 保存词向量模型的训练结果</span></span><br><span class="line">model.save(model_names)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 直接读入已训练好的词向量模型</span></span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> Word2Vec</span><br><span class="line">model=Word2Vec.load(<span class="string">"/Users/scarlett/repository/projects/kaggle-IMDB/300features_20minwords_10context"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 探查下该词向量模型的训练结果</span></span><br><span class="line">model.most_similar(<span class="string">"man"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="comment"># 定义一个函数使用词向量产生文本特征向量</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">makeFeatureVec</span><span class="params">(words,model,num_features)</span>:</span></span><br><span class="line">	featureVec=np.zeros((num_features,), dtype=<span class="string">"float32"</span>)</span><br><span class="line">	nwords=<span class="number">0.</span></span><br><span class="line">	index2word_set=set(model.index2word)</span><br><span class="line">	<span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">		<span class="keyword">if</span> word <span class="keyword">in</span> index2word_set:</span><br><span class="line">			nwords=nwords+<span class="number">1</span></span><br><span class="line">			featureVec=np.add(featureVec,model[word])</span><br><span class="line">	featureVec=np.divide(featureVec, nwords)</span><br><span class="line">	<span class="keyword">return</span> featureVec</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义每一条影评转化为基于词向量的特征向量(平均词向量)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getAvgFeatureVecs</span><span class="params">(reviews,model,num_features)</span>:</span></span><br><span class="line">	counts=<span class="number">0</span></span><br><span class="line">	reviewFeatureVecs=np.zeros(len(reviews),num_features,dtype=<span class="string">"float32"</span>)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span> review <span class="keyword">in</span> reviews:</span><br><span class="line">		reviewFeatureVecs[counter]=makeFeatureVec(review, model, num_features)</span><br><span class="line">		counter+=<span class="number">1</span></span><br><span class="line">	<span class="keyword">return</span> reviewFeatureVecs</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备新的基于词向量表示的训练和测试特征向量</span></span><br><span class="line">clean_train_reviews=[]</span><br><span class="line"><span class="keyword">for</span> review <span class="keyword">in</span> train[<span class="string">"review"</span>]:</span><br><span class="line">	clean_train_reviews.append(review_to_text(review,remove_stopwords=<span class="keyword">True</span>))</span><br><span class="line"></span><br><span class="line">trainDataVecs=getAvgFeatureVecs(clean_train_reviews, model, num_features)</span><br><span class="line"></span><br><span class="line">clean_test_reviews=[]</span><br><span class="line"><span class="keyword">for</span> review <span class="keyword">in</span> test[<span class="string">"review"</span>]:</span><br><span class="line">	clean_test_reviews.append(review_to_text(review, remove_stopwords=<span class="keyword">True</span>))</span><br><span class="line"></span><br><span class="line">testDataVecs=getAvgFeatureVecs(clean_test_reviews, model, num_features)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入GBC模型进行情感分析</span></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.grid_search <span class="keyword">import</span> GridSearchCV</span><br><span class="line">gbc=GradientBoostingClassifier()</span><br><span class="line">params_gbc=&#123;<span class="string">'n_estimators'</span>:[<span class="number">10</span>,<span class="number">100</span>,<span class="number">500</span>],<span class="string">'learning_rate'</span>:[<span class="number">0.01</span>,<span class="number">0.1</span>,<span class="number">1.0</span>],<span class="string">'max_depth'</span>:[<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]&#125;</span><br><span class="line">gs=GridSearchCV(gbc, params_gbc,cv=<span class="number">4</span>,n_jobs=<span class="number">-1</span>,verbose=<span class="number">1</span>)</span><br><span class="line">gs.fit(trainDataVecs,y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出网格搜索得到的最佳性能和最优超参数组合</span></span><br><span class="line"><span class="keyword">print</span> (gs.best_score_)</span><br><span class="line"><span class="keyword">print</span> (gs.best_params_)</span><br><span class="line"></span><br><span class="line">result=gs.predict(testDataVecs)</span><br><span class="line">output=pd.DataFrame(data=&#123;<span class="string">"id"</span>:test[<span class="string">"id"</span>],<span class="string">"sentiment"</span>:result&#125;)</span><br><span class="line">output.to_csv(<span class="string">"/Users/scarlett/repository/projects/kaggle-IMDB/submission_w2v.csv"</span>,index=<span class="keyword">False</span>,quoting=<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<h2 id="4-4-MNIST手写体数字图片识别"><a href="#4-4-MNIST手写体数字图片识别" class="headerlink" title="4.4 MNIST手写体数字图片识别"></a>4.4 MNIST手写体数字图片识别</h2><p>项目介绍</p>
<ul>
<li>CV选手的入门级课题</li>
<li>关注对数字图片的识别</li>
<li>Address: <a href="https://www.kaggle.com/c/digit-recognizer" target="_blank" rel="noopener">Digit Recognizer</a></li>
</ul>
<p>数据描述</p>
<ul>
<li>train.csv + test.csv</li>
<li>每个手写体数字图像都被首尾拼接为一个<strong>28X28=784维的像素向量</strong>，且每个像素都<strong>使用[0,1]之间的灰度值来显示手写笔画的明暗程度</strong></li>
</ul>
<p>MNIST手写体图片像素表示矩阵:</p>
<p><img src="http://img.blog.csdn.net/20160403174832218?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="mnist手写体图片像素表示矩阵"></p>
<p>搭建模型</p>
<ul>
<li>采用多种基于skflow的模型完成大规模手写体数字图片识别任务</li>
<li>模型：线性回归器，全连接并包含3个隐藏层的深度神经网络(DNN)，1个较为复杂但性能强大的卷积神经网络(CNN)</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line">train=pd.read_csv(<span class="string">'train.csv'</span>)</span><br><span class="line"><span class="keyword">print</span> train.shape</span><br><span class="line"><span class="comment"># print train.head()</span></span><br><span class="line">test=pd.read_csv(<span class="string">'test.csv'</span>)</span><br><span class="line"><span class="keyword">print</span> test.shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分离训练集中的数据特征和标记</span></span><br><span class="line">y_train=train[<span class="string">'label'</span>]</span><br><span class="line">X_train=train.drop(<span class="string">'label'</span>,<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 准备测试特征</span></span><br><span class="line">X_test=test</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入TensorFlow和skflow，使用skflow中已封装好的基于tf搭建的线性分类器TensorFlowLinearClassifier进行学习预测</span></span><br><span class="line"><span class="keyword">import</span> skflow</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf </span><br><span class="line">classifier=skflow.TensorFlowLinearClassifier(n_classes=<span class="number">10</span>,batch_size=<span class="number">100</span>,steps=<span class="number">1000</span>,learning_rate=<span class="number">0.01</span>)</span><br><span class="line">classifier.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">linear_y_predict=classifier.predict(X_test)</span><br><span class="line">linear_submission=pd.DataFrame(&#123;<span class="string">'ImageId'</span>:range(<span class="number">1</span>,<span class="number">28001</span>),<span class="string">'label'</span>:linear_y_predict&#125;)</span><br><span class="line">linear_submission.to_csv(<span class="string">'/Users/scarlett/repository/projects/kaggle-mnist/linear_submission.csv'</span>,index=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用tf里已封装好的TensorFlowDNNClassifier进行学习预测</span></span><br><span class="line">classifier=skflow.TensorFlowDNNClassifier(hidden_units=[<span class="number">200</span>,<span class="number">50</span>,<span class="number">10</span>], n_classes=<span class="number">10</span>,steps=<span class="number">5000</span>,learning_rate=<span class="number">0.01</span>,batch_size=<span class="number">50</span>)</span><br><span class="line">classifier.fit(X_train, y_train)</span><br><span class="line">dnn_y_predict=classifier.predict(X_test)</span><br><span class="line">dnn_submission=pd.DataFrame(&#123;<span class="string">'ImageId'</span>:range(<span class="number">1</span>,<span class="number">28001</span>),<span class="string">'label'</span>:dnn_y_predict&#125;)</span><br><span class="line">dnn_submission.to_csv(<span class="string">'/Users/scarlett/repository/projects/kaggle-mnist/dnn_submission.csv'</span>,index=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用tf中的算子自行搭建更为复杂的卷积神经网络，并使用skflow的程序接口从事NINST数据的学习与预测</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(tensor_in)</span>:</span></span><br><span class="line">	<span class="keyword">return</span> tf.nn.max_pool(tensor_in,ksize=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],strides=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_model</span><span class="params">(X,y)</span>:</span></span><br><span class="line">	X=tf.reshape(X, [<span class="number">-1</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>])</span><br><span class="line">	<span class="keyword">with</span> tf.variable_scope(<span class="string">'conv_layer1'</span>):</span><br><span class="line">		h_conv1=skflow.ops.conv2d(X, n_filters=<span class="number">32</span>, filter_shape=[<span class="number">5</span>,<span class="number">5</span>],bias=<span class="keyword">True</span>,activation=tf.nn.relu)</span><br><span class="line">		h_pool11=max_pool_2x2(h_conv1)</span><br><span class="line">	<span class="keyword">with</span> tf.variable_scope(<span class="string">'conv_layer2'</span>):</span><br><span class="line">		h_conv2=skflow.ops.conv2d(h_pool11, n_filters=<span class="number">64</span>, filter_shape=[<span class="number">5</span>,<span class="number">5</span>],bias=<span class="keyword">True</span>,activation=tf.nn.relu)</span><br><span class="line">		h_pool2=max_pool_2x2(h_conv2)</span><br><span class="line">		h_pool2_flat=tf.reshape(h_pool2, [<span class="number">-1</span>,<span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>])</span><br><span class="line">	h_fc1=skflow.ops.dnn(h_pool2_flat, [<span class="number">1024</span>],activation=tf.dnn.relu,keep_prob=<span class="number">0.5</span>)</span><br><span class="line">	<span class="keyword">return</span> skflow.models.logistic_regression(h_fc1, y)</span><br><span class="line"></span><br><span class="line">classifier=skflow.TensorFlowEstimator(model_fn=conv_model, n_classes=<span class="number">10</span>,batch_size=<span class="number">100</span>,steps=<span class="number">20000</span>,learning_rate=<span class="number">0.001</span>)</span><br><span class="line">classifier.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">conv_y_predict=[]</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> np.arange(<span class="number">100</span>,<span class="number">28001</span>,<span class="number">100</span>):</span><br><span class="line">	conv_y_predict=np.append(conv_y_predict,classifier.predict(X_test[i<span class="number">-100</span>:i]))</span><br><span class="line">conv_submission=pd.DataFrame(&#123;<span class="string">'ImageId'</span>:range(<span class="number">1</span>,<span class="number">28001</span>),<span class="string">'label'</span>:np.int32(conv_y_predict)&#125;)</span><br><span class="line">conv_submission.to_csv(<span class="string">'/Users/scarlett/repository/projects/kaggle-mnist/conv_submission'</span>,index=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<p>Outcome: CNN取得最佳性能表现。</p>

      
    </div>
    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>Thanks!</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>Donate</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechat.png" alt="Scarlett Huang WeChat Pay"/>
        <p>WeChat Pay</p>
      </div>
    

    

    

  </div>
</div>

      </div>
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Artificial-Intelligence/" rel="tag"><i class="fa fa-tag"></i> Artificial Intelligence</a>
          
            <a href="/tags/Machine-Learning/" rel="tag"><i class="fa fa-tag"></i> Machine Learning</a>
          
            <a href="/tags/Reading-Notes/" rel="tag"><i class="fa fa-tag"></i> Reading Notes</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/03/05/统计学-电视收视率是如何获得的？/" rel="next" title="统计学| 电视收视率是如何获得的？">
                <i class="fa fa-chevron-left"></i> 统计学| 电视收视率是如何获得的？
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/03/12/总结-产品设计的四大原则/" rel="prev" title="总结|产品设计的四大原则">
                总结|产品设计的四大原则 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    
  </div>


        </div>
        
          
  


  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.png"
               alt="Scarlett Huang" />
          <p class="site-author-name" itemprop="name">Scarlett Huang</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
           
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">192</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">35</span>
                <span class="site-state-item-name">categories</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">62</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://www.scarletthuang.cn" target="_blank" title="Biography">
                  
                    <i class="fa fa-fw fa-user"></i>
                  
                    
                      Biography
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://github.com/ScarlettYellow" target="_blank" title="Github">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                    
                      Github
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://www.woshipm.com/u/192348" target="_blank" title="WoShiPM">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                    
                      WoShiPM
                    
                </a>
              </span>
            
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-inline">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              Friend Links
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="https://unique-ailab.github.io/" title="Unique-AILab" target="_blank">Unique-AILab</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="hubertwang.me/" title="MR WHY (ML Dev. & AI PM)" target="_blank">MR WHY (ML Dev. & AI PM)</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="https://zekangli.com/" title="Zekang Li (NLP Researcher)" target="_blank">Zekang Li (NLP Researcher)</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://wondervictor.github.io/" title="Vic Chan (CV Dev.)" target="_blank">Vic Chan (CV Dev.)</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://blog.qzwlecr.com/" title="qzwlecr (Alg. Dev.)" target="_blank">qzwlecr (Alg. Dev.)</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="https://alisahhh.github.io/" title="Alisa (Alg. Dev.)" target="_blank">Alisa (Alg. Dev.)</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="https://llag9810.github.io/" title="yifan (Android Dev.)" target="_blank">yifan (Android Dev.)</a>
                </li>
              
            </ul>
          </div>
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1-简介篇"><span class="nav-number">1.</span> <span class="nav-text">1.简介篇</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1-机器学习"><span class="nav-number">1.1.</span> <span class="nav-text">1.1 机器学习</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-python编程库"><span class="nav-number">1.2.</span> <span class="nav-text">1.2 python编程库</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-3-python基础"><span class="nav-number">1.3.</span> <span class="nav-text">1.3 python基础</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-基础篇"><span class="nav-number">2.</span> <span class="nav-text">2.基础篇</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-监督学习经典模型"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 监督学习经典模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-1-监督学习任务的基本流程"><span class="nav-number">2.1.1.</span> <span class="nav-text">2.1.1 监督学习任务的基本流程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-2-分类学习"><span class="nav-number">2.1.2.</span> <span class="nav-text">2.1.2 分类学习</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-2-1-线性分类器（linear-classifiers）"><span class="nav-number">2.1.2.1.</span> <span class="nav-text">2.1.2.1 线性分类器（linear classifiers）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-2-2-支持向量机（Suport-Vector-Classifier）（分类）"><span class="nav-number">2.1.2.2.</span> <span class="nav-text">2.1.2.2 支持向量机（Suport Vector Classifier）（分类）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-2-3-朴素贝叶斯（Naive-Bayes）（分类）"><span class="nav-number">2.1.2.3.</span> <span class="nav-text">2.1.2.3 朴素贝叶斯（Naive Bayes）（分类）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-2-4-K近邻-k-Nearest-Neighbor，KNN-（分类）"><span class="nav-number">2.1.2.4.</span> <span class="nav-text">2.1.2.4 K近邻(k-Nearest Neighbor，KNN)（分类）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-2-5-决策树（Decision-Tree）（分类）"><span class="nav-number">2.1.2.5.</span> <span class="nav-text">2.1.2.5 决策树（Decision Tree）（分类）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-2-6-集成模型（Ensemble）（分类）"><span class="nav-number">2.1.2.6.</span> <span class="nav-text">2.1.2.6 集成模型（Ensemble）（分类）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-3-回归预测"><span class="nav-number">2.1.3.</span> <span class="nav-text">2.1.3 回归预测</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-3-1-线性回归器"><span class="nav-number">2.1.3.1.</span> <span class="nav-text">2.1.3.1 线性回归器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-3-2-支持向量机-回归"><span class="nav-number">2.1.3.2.</span> <span class="nav-text">2.1.3.2 支持向量机(回归)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-2-3-K近邻-回归"><span class="nav-number">2.1.3.3.</span> <span class="nav-text">2.1.2.3 K近邻(回归)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-3-4-回归树"><span class="nav-number">2.1.3.4.</span> <span class="nav-text">2.1.3.4 回归树</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-3-5-集成模型（回归）"><span class="nav-number">2.1.3.5.</span> <span class="nav-text">2.1.3.5 集成模型（回归）</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-无监督学习经典模型"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 无监督学习经典模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-1-数据聚类"><span class="nav-number">2.2.1.</span> <span class="nav-text">2.2.1 数据聚类</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-1-1-K-means算法"><span class="nav-number">2.2.1.1.</span> <span class="nav-text">2.2.1.1 K-means算法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-2-特征降维"><span class="nav-number">2.2.2.</span> <span class="nav-text">2.2.2 特征降维</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-2-1-主成分分析-Principle-Component-Analysis"><span class="nav-number">2.2.2.1.</span> <span class="nav-text">2.2.2.1 主成分分析(Principle Component Analysis)</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-进阶篇"><span class="nav-number">3.</span> <span class="nav-text">3.进阶篇</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-模型实用技巧"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 模型实用技巧</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-1-特征提升"><span class="nav-number">3.1.1.</span> <span class="nav-text">3.1.1 特征提升</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-1-1-特征抽取"><span class="nav-number">3.1.1.1.</span> <span class="nav-text">3.1.1.1 特征抽取</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-1-2-特征筛选"><span class="nav-number">3.1.1.2.</span> <span class="nav-text">3.1.1.2 特征筛选</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-2-模型正则化"><span class="nav-number">3.1.2.</span> <span class="nav-text">3.1.2 模型正则化</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-2-1-欠拟合和过拟合-Underfitting-amp-Overfitting"><span class="nav-number">3.1.2.1.</span> <span class="nav-text">3.1.2.1 欠拟合和过拟合(Underfitting &amp; Overfitting)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-2-2-L1范数正则化"><span class="nav-number">3.1.2.2.</span> <span class="nav-text">3.1.2.2 L1范数正则化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-2-3-L2范数正则化"><span class="nav-number">3.1.2.3.</span> <span class="nav-text">3.1.2.3 L2范数正则化</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-3-模型检验"><span class="nav-number">3.1.3.</span> <span class="nav-text">3.1.3 模型检验</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-3-1-留一验证-Leave-one-out-cross-validation"><span class="nav-number">3.1.3.1.</span> <span class="nav-text">3.1.3.1 留一验证(Leave-one-out cross validation)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-3-2-交叉验证-K-fold-cross-validation"><span class="nav-number">3.1.3.2.</span> <span class="nav-text">3.1.3.2 交叉验证(K-fold cross-validation)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-4-超参数搜索-hyperparameter"><span class="nav-number">3.1.4.</span> <span class="nav-text">3.1.4 超参数搜索(hyperparameter)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-4-1-网格搜索-GridSearch"><span class="nav-number">3.1.4.1.</span> <span class="nav-text">3.1.4.1 网格搜索(GridSearch)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-4-2-并行搜索-Parallel-Grid-Search"><span class="nav-number">3.1.4.2.</span> <span class="nav-text">3.1.4.2 并行搜索(Parallel Grid Search)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-流行库-模型实践"><span class="nav-number">3.2.</span> <span class="nav-text">3.2 流行库/模型实践</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-1-NLTK自然语言处理包"><span class="nav-number">3.2.1.</span> <span class="nav-text">3.2.1 NLTK自然语言处理包</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-2-词向量技术-Word2Vec"><span class="nav-number">3.2.2.</span> <span class="nav-text">3.2.2 词向量技术(Word2Vec)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-3-XGBoost模型-extreme-gradient-boosting"><span class="nav-number">3.2.3.</span> <span class="nav-text">3.2.3 XGBoost模型(extreme gradient boosting)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-4-TensorFlow框架"><span class="nav-number">3.2.4.</span> <span class="nav-text">3.2.4 TensorFlow框架</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-实战篇"><span class="nav-number">4.</span> <span class="nav-text">4.实战篇</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#4-1-Kaggle"><span class="nav-number">4.1.</span> <span class="nav-text">4.1 Kaggle</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-2-Titanic遇难乘客预测"><span class="nav-number">4.2.</span> <span class="nav-text">4.2 Titanic遇难乘客预测</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-3-IMDB影评得分估计"><span class="nav-number">4.3.</span> <span class="nav-text">4.3 IMDB影评得分估计</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-4-MNIST手写体数字图片识别"><span class="nav-number">4.4.</span> <span class="nav-text">4.4 MNIST手写体数字图片识别</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>

  </aside>




        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>

<%- partial('totop') %>
<script src="<%- config.root %>js/totop.js"></script>

<div class="copyright" >
  
  &copy;  2017 &mdash; 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Scarlett Huang</span>

  
</div>


 <!-- <div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div>

  <span class="post-meta-divider">|</span>

  <div class="theme-info">Theme &mdash; <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.2</div>
-->

<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">共597.8k字</span>
</div>




<span id="busuanzi_container_site_pv">
<div class="powered-by"></div>
      本站总访问量<span id="busuanzi_value_site_pv"></span>次
  </span>
  <span id="busuanzi_container_site_uv">
  <div class="powered-by"></div>
    本站访客数<span id="busuanzi_value_site_uv"></span>人次
  </span>



        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  

    
      <script id="dsq-count-scr" src="https://scarletthuang-blog.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://ScarlettHuang.cn/2018/03/05/阅读笔记：《Python机器学习及实战》/';
          this.page.identifier = '2018/03/05/阅读笔记：《Python机器学习及实战》/';
          this.page.title = '阅读笔记：《Python机器学习及实战》';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://scarletthuang-blog.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	

		<script type="text/javascript">
		_hcwp = window._hcwp || [];

		_hcwp.push({widget:"Bloggerstream", widget_id: 100332, selector:".hc-comment-count", label: "{\%COUNT%\}" });

		
		_hcwp.push({widget:"Stream", widget_id: 100332, xid: "2018/03/05/阅读笔记：《Python机器学习及实战》/"});
		

		(function() {
		if("HC_LOAD_INIT" in window)return;
		HC_LOAD_INIT = true;
		var lang = (navigator.language || navigator.systemLanguage || navigator.userLanguage || "en").substr(0, 2).toLowerCase();
		var hcc = document.createElement("script"); hcc.type = "text/javascript"; hcc.async = true;
		hcc.src = ("https:" == document.location.protocol ? "https" : "http")+"://w.hypercomments.com/widget/hc/100332/"+lang+"/widget.js";
		var s = document.getElementsByTagName("script")[0];
		s.parentNode.insertBefore(hcc, s.nextSibling);
		})();
		</script>

	












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


  

  
  


  

  


</body>
</html>
